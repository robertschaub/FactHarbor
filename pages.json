{"generated": "2026-02-22T17:38:14.753824Z", "generator": "build_ghpages.py", "version": "1.0", "commitHash": "a300ac0", "rootRef": "WebHome", "pageCount": 202, "aliases": {"TestReports": "Product Development.TestReports.WebHome"}, "tree": [{"type": "folder", "name": "FactHarbor", "segments": ["FactHarbor"], "children": [{"type": "folder", "name": "About FactHarbor", "segments": ["About FactHarbor"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "About FactHarbor.WebHome", "segments": ["About FactHarbor", "WebHome"], "relPath": "About FactHarbor/WebHome.xwiki", "parentPath": "About FactHarbor"}]}, {"type": "folder", "name": "Organisation", "segments": ["Organisation"], "children": [{"type": "folder", "name": "Governance", "segments": ["Organisation", "Governance"], "children": [{"type": "folder", "name": "Organisational Model", "segments": ["Organisation", "Governance", "Organisational Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.Organisational Model.WebHome", "segments": ["Organisation", "Governance", "Organisational Model", "WebHome"], "relPath": "Organisation/Governance/Organisational Model/WebHome.xwiki", "parentPath": "Organisation.Governance.Organisational Model"}]}, {"type": "folder", "name": "Roles and Bodies", "segments": ["Organisation", "Governance", "Roles and Bodies"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.Roles and Bodies.WebHome", "segments": ["Organisation", "Governance", "Roles and Bodies", "WebHome"], "relPath": "Organisation/Governance/Roles and Bodies/WebHome.xwiki", "parentPath": "Organisation.Governance.Roles and Bodies"}]}, {"type": "folder", "name": "Decision Processes", "segments": ["Organisation", "Governance", "Decision Processes"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.Decision Processes.WebHome", "segments": ["Organisation", "Governance", "Decision Processes", "WebHome"], "relPath": "Organisation/Governance/Decision Processes/WebHome.xwiki", "parentPath": "Organisation.Governance.Decision Processes"}]}, {"type": "folder", "name": "Transition Model", "segments": ["Organisation", "Governance", "Transition Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.Transition Model.WebHome", "segments": ["Organisation", "Governance", "Transition Model", "WebHome"], "relPath": "Organisation/Governance/Transition Model/WebHome.xwiki", "parentPath": "Organisation.Governance.Transition Model"}]}, {"type": "folder", "name": "Operational Readiness", "segments": ["Organisation", "Governance", "Operational Readiness"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.Operational Readiness.WebHome", "segments": ["Organisation", "Governance", "Operational Readiness", "WebHome"], "relPath": "Organisation/Governance/Operational Readiness/WebHome.xwiki", "parentPath": "Organisation.Governance.Operational Readiness"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Governance.WebHome", "segments": ["Organisation", "Governance", "WebHome"], "relPath": "Organisation/Governance/WebHome.xwiki", "parentPath": "Organisation.Governance"}]}, {"type": "folder", "name": "How-We-Work-Together", "segments": ["Organisation", "How-We-Work-Together"], "children": [{"type": "folder", "name": "GlobalRules", "segments": ["Organisation", "How-We-Work-Together", "GlobalRules"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.How-We-Work-Together.GlobalRules.WebHome", "segments": ["Organisation", "How-We-Work-Together", "GlobalRules", "WebHome"], "relPath": "Organisation/How-We-Work-Together/GlobalRules/WebHome.xwiki", "parentPath": "Organisation.How-We-Work-Together.GlobalRules"}]}, {"type": "folder", "name": "Contributor Processes", "segments": ["Organisation", "How-We-Work-Together", "Contributor Processes"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.How-We-Work-Together.Contributor Processes.WebHome", "segments": ["Organisation", "How-We-Work-Together", "Contributor Processes", "WebHome"], "relPath": "Organisation/How-We-Work-Together/Contributor Processes/WebHome.xwiki", "parentPath": "Organisation.How-We-Work-Together.Contributor Processes"}]}, {"type": "file", "name": "Workplace-Culture.xwiki", "baseName": "Workplace-Culture", "ref": "Organisation.How-We-Work-Together.Workplace-Culture", "segments": ["Organisation", "How-We-Work-Together", "Workplace-Culture"], "relPath": "Organisation/How-We-Work-Together/Workplace-Culture.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Continuous-Improvement.xwiki", "baseName": "Continuous-Improvement", "ref": "Organisation.How-We-Work-Together.Continuous-Improvement", "segments": ["Organisation", "How-We-Work-Together", "Continuous-Improvement"], "relPath": "Organisation/How-We-Work-Together/Continuous-Improvement.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Consent-Based-Decision-Making.xwiki", "baseName": "Consent-Based-Decision-Making", "ref": "Organisation.How-We-Work-Together.Consent-Based-Decision-Making", "segments": ["Organisation", "How-We-Work-Together", "Consent-Based-Decision-Making"], "relPath": "Organisation/How-We-Work-Together/Consent-Based-Decision-Making.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Terms-of-Service.xwiki", "baseName": "Terms-of-Service", "ref": "Organisation.How-We-Work-Together.Terms-of-Service", "segments": ["Organisation", "How-We-Work-Together", "Terms-of-Service"], "relPath": "Organisation/How-We-Work-Together/Terms-of-Service.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Privacy-Policy.xwiki", "baseName": "Privacy-Policy", "ref": "Organisation.How-We-Work-Together.Privacy-Policy", "segments": ["Organisation", "How-We-Work-Together", "Privacy-Policy"], "relPath": "Organisation/How-We-Work-Together/Privacy-Policy.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Transparency-Policy.xwiki", "baseName": "Transparency-Policy", "ref": "Organisation.How-We-Work-Together.Transparency-Policy", "segments": ["Organisation", "How-We-Work-Together", "Transparency-Policy"], "relPath": "Organisation/How-We-Work-Together/Transparency-Policy.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "Security-Policy.xwiki", "baseName": "Security-Policy", "ref": "Organisation.How-We-Work-Together.Security-Policy", "segments": ["Organisation", "How-We-Work-Together", "Security-Policy"], "relPath": "Organisation/How-We-Work-Together/Security-Policy.xwiki", "parentPath": "Organisation.How-We-Work-Together"}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.How-We-Work-Together.WebHome", "segments": ["Organisation", "How-We-Work-Together", "WebHome"], "relPath": "Organisation/How-We-Work-Together/WebHome.xwiki", "parentPath": "Organisation.How-We-Work-Together"}]}, {"type": "folder", "name": "Legal and Compliance", "segments": ["Organisation", "Legal and Compliance"], "children": [{"type": "folder", "name": "Legal Framework", "segments": ["Organisation", "Legal and Compliance", "Legal Framework"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.Legal Framework.WebHome", "segments": ["Organisation", "Legal and Compliance", "Legal Framework", "WebHome"], "relPath": "Organisation/Legal and Compliance/Legal Framework/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.Legal Framework"}]}, {"type": "folder", "name": "Open Source Model and Licensing", "segments": ["Organisation", "Legal and Compliance", "Open Source Model and Licensing"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome", "segments": ["Organisation", "Legal and Compliance", "Open Source Model and Licensing", "WebHome"], "relPath": "Organisation/Legal and Compliance/Open Source Model and Licensing/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.Open Source Model and Licensing"}]}, {"type": "folder", "name": "CLA", "segments": ["Organisation", "Legal and Compliance", "CLA"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.CLA.WebHome", "segments": ["Organisation", "Legal and Compliance", "CLA", "WebHome"], "relPath": "Organisation/Legal and Compliance/CLA/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.CLA"}]}, {"type": "folder", "name": "Finance and Compliance", "segments": ["Organisation", "Legal and Compliance", "Finance and Compliance"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.Finance and Compliance.WebHome", "segments": ["Organisation", "Legal and Compliance", "Finance and Compliance", "WebHome"], "relPath": "Organisation/Legal and Compliance/Finance and Compliance/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.Finance and Compliance"}]}, {"type": "folder", "name": "Large Donations Policy", "segments": ["Organisation", "Legal and Compliance", "Large Donations Policy"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.Large Donations Policy.WebHome", "segments": ["Organisation", "Legal and Compliance", "Large Donations Policy", "WebHome"], "relPath": "Organisation/Legal and Compliance/Large Donations Policy/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.Large Donations Policy"}]}, {"type": "folder", "name": "Reimbursable Expenses", "segments": ["Organisation", "Legal and Compliance", "Reimbursable Expenses"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.Reimbursable Expenses.WebHome", "segments": ["Organisation", "Legal and Compliance", "Reimbursable Expenses", "WebHome"], "relPath": "Organisation/Legal and Compliance/Reimbursable Expenses/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance.Reimbursable Expenses"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Legal and Compliance.WebHome", "segments": ["Organisation", "Legal and Compliance", "WebHome"], "relPath": "Organisation/Legal and Compliance/WebHome.xwiki", "parentPath": "Organisation.Legal and Compliance"}]}, {"type": "folder", "name": "Strategy", "segments": ["Organisation", "Strategy"], "children": [{"type": "folder", "name": "Core Problems FactHarbor Solves", "segments": ["Organisation", "Strategy", "Core Problems FactHarbor Solves"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.Core Problems FactHarbor Solves.WebHome", "segments": ["Organisation", "Strategy", "Core Problems FactHarbor Solves", "WebHome"], "relPath": "Organisation/Strategy/Core Problems FactHarbor Solves/WebHome.xwiki", "parentPath": "Organisation.Strategy.Core Problems FactHarbor Solves"}]}, {"type": "folder", "name": "Automation Philosophy", "segments": ["Organisation", "Strategy", "Automation Philosophy"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.Automation Philosophy.WebHome", "segments": ["Organisation", "Strategy", "Automation Philosophy", "WebHome"], "relPath": "Organisation/Strategy/Automation Philosophy/WebHome.xwiki", "parentPath": "Organisation.Strategy.Automation Philosophy"}]}, {"type": "folder", "name": "Competitive Analysis", "segments": ["Organisation", "Strategy", "Competitive Analysis"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.Competitive Analysis.WebHome", "segments": ["Organisation", "Strategy", "Competitive Analysis", "WebHome"], "relPath": "Organisation/Strategy/Competitive Analysis/WebHome.xwiki", "parentPath": "Organisation.Strategy.Competitive Analysis"}]}, {"type": "folder", "name": "Ideal Customer Profile", "segments": ["Organisation", "Strategy", "Ideal Customer Profile"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.Ideal Customer Profile.WebHome", "segments": ["Organisation", "Strategy", "Ideal Customer Profile", "WebHome"], "relPath": "Organisation/Strategy/Ideal Customer Profile/WebHome.xwiki", "parentPath": "Organisation.Strategy.Ideal Customer Profile"}]}, {"type": "folder", "name": "Cooperation Opportunities", "segments": ["Organisation", "Strategy", "Cooperation Opportunities"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.Cooperation Opportunities.WebHome", "segments": ["Organisation", "Strategy", "Cooperation Opportunities", "WebHome"], "relPath": "Organisation/Strategy/Cooperation Opportunities/WebHome.xwiki", "parentPath": "Organisation.Strategy.Cooperation Opportunities"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Strategy.WebHome", "segments": ["Organisation", "Strategy", "WebHome"], "relPath": "Organisation/Strategy/WebHome.xwiki", "parentPath": "Organisation.Strategy"}]}, {"type": "folder", "name": "Diagrams", "segments": ["Organisation", "Diagrams"], "children": [{"type": "folder", "name": "Domain Interaction Map", "segments": ["Organisation", "Diagrams", "Domain Interaction Map"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Diagrams.Domain Interaction Map.WebHome", "segments": ["Organisation", "Diagrams", "Domain Interaction Map", "WebHome"], "relPath": "Organisation/Diagrams/Domain Interaction Map/WebHome.xwiki", "parentPath": "Organisation.Diagrams.Domain Interaction Map"}]}, {"type": "folder", "name": "Governance Structure", "segments": ["Organisation", "Diagrams", "Governance Structure"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Diagrams.Governance Structure.WebHome", "segments": ["Organisation", "Diagrams", "Governance Structure", "WebHome"], "relPath": "Organisation/Diagrams/Governance Structure/WebHome.xwiki", "parentPath": "Organisation.Diagrams.Governance Structure"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.Diagrams.WebHome", "segments": ["Organisation", "Diagrams", "WebHome"], "relPath": "Organisation/Diagrams/WebHome.xwiki", "parentPath": "Organisation.Diagrams"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Organisation.WebHome", "segments": ["Organisation", "WebHome"], "relPath": "Organisation/WebHome.xwiki", "parentPath": "Organisation"}]}, {"type": "folder", "name": "Product Development", "segments": ["Product Development"], "children": [{"type": "folder", "name": "Requirements", "segments": ["Product Development", "Requirements"], "children": [{"type": "folder", "name": "Roles", "segments": ["Product Development", "Requirements", "Roles"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Requirements.Roles.WebHome", "segments": ["Product Development", "Requirements", "Roles", "WebHome"], "relPath": "Product Development/Requirements/Roles/WebHome.xwiki", "parentPath": "Product Development.Requirements.Roles"}]}, {"type": "folder", "name": "User Needs", "segments": ["Product Development", "Requirements", "User Needs"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Requirements.User Needs.WebHome", "segments": ["Product Development", "Requirements", "User Needs", "WebHome"], "relPath": "Product Development/Requirements/User Needs/WebHome.xwiki", "parentPath": "Product Development.Requirements.User Needs"}]}, {"type": "file", "name": "GapAnalysis.xwiki", "baseName": "GapAnalysis", "ref": "Product Development.Requirements.GapAnalysis", "segments": ["Product Development", "Requirements", "GapAnalysis"], "relPath": "Product Development/Requirements/GapAnalysis.xwiki", "parentPath": "Product Development.Requirements"}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Requirements.WebHome", "segments": ["Product Development", "Requirements", "WebHome"], "relPath": "Product Development/Requirements/WebHome.xwiki", "parentPath": "Product Development.Requirements"}]}, {"type": "folder", "name": "Specification", "segments": ["Product Development", "Specification"], "children": [{"type": "folder", "name": "Data Model", "segments": ["Product Development", "Specification", "Data Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Data Model.WebHome", "segments": ["Product Development", "Specification", "Data Model", "WebHome"], "relPath": "Product Development/Specification/Data Model/WebHome.xwiki", "parentPath": "Product Development.Specification.Data Model"}]}, {"type": "folder", "name": "Workflows", "segments": ["Product Development", "Specification", "Workflows"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Workflows.WebHome", "segments": ["Product Development", "Specification", "Workflows", "WebHome"], "relPath": "Product Development/Specification/Workflows/WebHome.xwiki", "parentPath": "Product Development.Specification.Workflows"}]}, {"type": "folder", "name": "Automation", "segments": ["Product Development", "Specification", "Automation"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Automation.WebHome", "segments": ["Product Development", "Specification", "Automation", "WebHome"], "relPath": "Product Development/Specification/Automation/WebHome.xwiki", "parentPath": "Product Development.Specification.Automation"}]}, {"type": "folder", "name": "Architecture", "segments": ["Product Development", "Specification", "Architecture"], "children": [{"type": "folder", "name": "AKEL Pipeline", "segments": ["Product Development", "Specification", "Architecture", "AKEL Pipeline"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.AKEL Pipeline.WebHome", "segments": ["Product Development", "Specification", "Architecture", "AKEL Pipeline", "WebHome"], "relPath": "Product Development/Specification/Architecture/AKEL Pipeline/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.AKEL Pipeline"}]}, {"type": "folder", "name": "Data Model", "segments": ["Product Development", "Specification", "Architecture", "Data Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Data Model.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Data Model", "WebHome"], "relPath": "Product Development/Specification/Architecture/Data Model/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Data Model"}]}, {"type": "folder", "name": "Deep Dive", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive"], "children": [{"type": "folder", "name": "Calculations and Verdicts", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Calculations and Verdicts"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Calculations and Verdicts", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Calculations and Verdicts/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts"}]}, {"type": "folder", "name": "Confidence Calibration", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Confidence Calibration"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Confidence Calibration", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Confidence Calibration/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Confidence Calibration"}]}, {"type": "folder", "name": "Context Detection", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Context Detection"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Context Detection", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Context Detection/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Context Detection"}]}, {"type": "folder", "name": "Direction Semantics", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Direction Semantics"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Direction Semantics.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Direction Semantics", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Direction Semantics/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Direction Semantics"}]}, {"type": "folder", "name": "Evidence Quality Filtering", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Evidence Quality Filtering"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Evidence Quality Filtering", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Evidence Quality Filtering/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering"}]}, {"type": "folder", "name": "KeyFactors Design", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "KeyFactors Design"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "KeyFactors Design", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/KeyFactors Design/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.KeyFactors Design"}]}, {"type": "folder", "name": "Pipeline Variants", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Pipeline Variants"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Pipeline Variants", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Pipeline Variants/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Pipeline Variants"}]}, {"type": "folder", "name": "Prompt Architecture", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Prompt Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Prompt Architecture", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Prompt Architecture/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Prompt Architecture"}]}, {"type": "folder", "name": "Quality Gates", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Quality Gates"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Quality Gates", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Quality Gates/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Quality Gates"}]}, {"type": "folder", "name": "Schema Migration", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Schema Migration"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Schema Migration.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Schema Migration", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Schema Migration/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Schema Migration"}]}, {"type": "folder", "name": "Source Reliability", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability"], "children": [{"type": "folder", "name": "Admin and Implementation", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Admin and Implementation"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Admin and Implementation", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/Admin and Implementation/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation"}]}, {"type": "folder", "name": "Architecture and Verdicts", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Architecture and Verdicts"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Architecture and Verdicts", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/Architecture and Verdicts/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts"}]}, {"type": "folder", "name": "Configuration and Scoring", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Configuration and Scoring"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Configuration and Scoring", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/Configuration and Scoring/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring"}]}, {"type": "folder", "name": "Overview and Quick Start", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Overview and Quick Start"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Overview and Quick Start", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/Overview and Quick Start/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start"}]}, {"type": "folder", "name": "Refinement and Multi-Language", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Refinement and Multi-Language"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "Refinement and Multi-Language", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/Refinement and Multi-Language/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Source Reliability", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Source Reliability/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Source Reliability"}]}, {"type": "folder", "name": "Storage Technology", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Storage Technology"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.Storage Technology.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "Storage Technology", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/Storage Technology/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive.Storage Technology"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Deep Dive.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Deep Dive", "WebHome"], "relPath": "Product Development/Specification/Architecture/Deep Dive/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Deep Dive"}]}, {"type": "folder", "name": "External Dependencies", "segments": ["Product Development", "Specification", "Architecture", "External Dependencies"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.External Dependencies.WebHome", "segments": ["Product Development", "Specification", "Architecture", "External Dependencies", "WebHome"], "relPath": "Product Development/Specification/Architecture/External Dependencies/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.External Dependencies"}]}, {"type": "folder", "name": "Future", "segments": ["Product Development", "Specification", "Architecture", "Future"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Future.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Future", "WebHome"], "relPath": "Product Development/Specification/Architecture/Future/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Future"}]}, {"type": "folder", "name": "Quality and Trust", "segments": ["Product Development", "Specification", "Architecture", "Quality and Trust"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Quality and Trust.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Quality and Trust", "WebHome"], "relPath": "Product Development/Specification/Architecture/Quality and Trust/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Quality and Trust"}]}, {"type": "folder", "name": "Security and Operations", "segments": ["Product Development", "Specification", "Architecture", "Security and Operations"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Security and Operations.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Security and Operations", "WebHome"], "relPath": "Product Development/Specification/Architecture/Security and Operations/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Security and Operations"}]}, {"type": "folder", "name": "Storage and Configuration", "segments": ["Product Development", "Specification", "Architecture", "Storage and Configuration"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.Storage and Configuration.WebHome", "segments": ["Product Development", "Specification", "Architecture", "Storage and Configuration", "WebHome"], "relPath": "Product Development/Specification/Architecture/Storage and Configuration/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.Storage and Configuration"}]}, {"type": "folder", "name": "System Design", "segments": ["Product Development", "Specification", "Architecture", "System Design"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.System Design.WebHome", "segments": ["Product Development", "Specification", "Architecture", "System Design", "WebHome"], "relPath": "Product Development/Specification/Architecture/System Design/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture.System Design"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Architecture.WebHome", "segments": ["Product Development", "Specification", "Architecture", "WebHome"], "relPath": "Product Development/Specification/Architecture/WebHome.xwiki", "parentPath": "Product Development.Specification.Architecture"}]}, {"type": "folder", "name": "Implementation", "segments": ["Product Development", "Specification", "Implementation"], "children": [{"type": "folder", "name": "Implementation Status and Quality", "segments": ["Product Development", "Specification", "Implementation", "Implementation Status and Quality"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Implementation.Implementation Status and Quality.WebHome", "segments": ["Product Development", "Specification", "Implementation", "Implementation Status and Quality", "WebHome"], "relPath": "Product Development/Specification/Implementation/Implementation Status and Quality/WebHome.xwiki", "parentPath": "Product Development.Specification.Implementation.Implementation Status and Quality"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Implementation.WebHome", "segments": ["Product Development", "Specification", "Implementation", "WebHome"], "relPath": "Product Development/Specification/Implementation/WebHome.xwiki", "parentPath": "Product Development.Specification.Implementation"}]}, {"type": "folder", "name": "Reference", "segments": ["Product Development", "Specification", "Reference"], "children": [{"type": "folder", "name": "Terminology", "segments": ["Product Development", "Specification", "Reference", "Terminology"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Terminology.WebHome", "segments": ["Product Development", "Specification", "Reference", "Terminology", "WebHome"], "relPath": "Product Development/Specification/Reference/Terminology/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Terminology"}]}, {"type": "folder", "name": "Data Models and Schemas", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas"], "children": [{"type": "folder", "name": "LLM Schema Mapping", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas", "LLM Schema Mapping"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Data Models and Schemas.LLM Schema Mapping.WebHome", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas", "LLM Schema Mapping", "WebHome"], "relPath": "Product Development/Specification/Reference/Data Models and Schemas/LLM Schema Mapping/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Data Models and Schemas.LLM Schema Mapping"}]}, {"type": "folder", "name": "Metrics Schema", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas", "Metrics Schema"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema.WebHome", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas", "Metrics Schema", "WebHome"], "relPath": "Product Development/Specification/Reference/Data Models and Schemas/Metrics Schema/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Data Models and Schemas.WebHome", "segments": ["Product Development", "Specification", "Reference", "Data Models and Schemas", "WebHome"], "relPath": "Product Development/Specification/Reference/Data Models and Schemas/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Data Models and Schemas"}]}, {"type": "folder", "name": "Prompt Engineering", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering"], "children": [{"type": "folder", "name": "Prompt Guidelines", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering", "Prompt Guidelines"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Prompt Engineering.Prompt Guidelines.WebHome", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering", "Prompt Guidelines", "WebHome"], "relPath": "Product Development/Specification/Reference/Prompt Engineering/Prompt Guidelines/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Prompt Engineering.Prompt Guidelines"}]}, {"type": "folder", "name": "Provider-Specific Formatting", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering", "Provider-Specific Formatting"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting.WebHome", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering", "Provider-Specific Formatting", "WebHome"], "relPath": "Product Development/Specification/Reference/Prompt Engineering/Provider-Specific Formatting/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.Prompt Engineering.WebHome", "segments": ["Product Development", "Specification", "Reference", "Prompt Engineering", "WebHome"], "relPath": "Product Development/Specification/Reference/Prompt Engineering/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference.Prompt Engineering"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Reference.WebHome", "segments": ["Product Development", "Specification", "Reference", "WebHome"], "relPath": "Product Development/Specification/Reference/WebHome.xwiki", "parentPath": "Product Development.Specification.Reference"}]}, {"type": "folder", "name": "FAQ", "segments": ["Product Development", "Specification", "FAQ"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FAQ.WebHome", "segments": ["Product Development", "Specification", "FAQ", "WebHome"], "relPath": "Product Development/Specification/FAQ/WebHome.xwiki", "parentPath": "Product Development.Specification.FAQ"}]}, {"type": "folder", "name": "Examples", "segments": ["Product Development", "Specification", "Examples"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Examples.WebHome", "segments": ["Product Development", "Specification", "Examples", "WebHome"], "relPath": "Product Development/Specification/Examples/WebHome.xwiki", "parentPath": "Product Development.Specification.Examples"}]}, {"type": "folder", "name": "Review & Data Use", "segments": ["Product Development", "Specification", "Review & Data Use"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Review & Data Use.WebHome", "segments": ["Product Development", "Specification", "Review & Data Use", "WebHome"], "relPath": "Product Development/Specification/Review & Data Use/WebHome.xwiki", "parentPath": "Product Development.Specification.Review & Data Use"}]}, {"type": "folder", "name": "Federation & Decentralization", "segments": ["Product Development", "Specification", "Federation & Decentralization"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.Federation & Decentralization.WebHome", "segments": ["Product Development", "Specification", "Federation & Decentralization", "WebHome"], "relPath": "Product Development/Specification/Federation & Decentralization/WebHome.xwiki", "parentPath": "Product Development.Specification.Federation & Decentralization"}]}, {"type": "folder", "name": "FH Analysis Reports", "segments": ["Product Development", "Specification", "FH Analysis Reports"], "children": [{"type": "folder", "name": "FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude"}]}, {"type": "folder", "name": "FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude"}]}, {"type": "folder", "name": "FactHarbor_Analysis_Bolsonaro_Trial_Fairness", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analysis_Bolsonaro_Trial_Fairness"], "children": [{"type": "folder", "name": "Analysis Summary", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analysis_Bolsonaro_Trial_Fairness", "Analysis Summary"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.Analysis Summary.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analysis_Bolsonaro_Trial_Fairness", "Analysis Summary", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FactHarbor_Analysis_Bolsonaro_Trial_Fairness/Analysis Summary/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.Analysis Summary"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FactHarbor_Analysis_Bolsonaro_Trial_Fairness", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FactHarbor_Analysis_Bolsonaro_Trial_Fairness/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness"}]}, {"type": "folder", "name": "FHA - AHA Alcohol CVD Statement", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement"], "children": [{"type": "folder", "name": "Analysis Summary", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement", "Analysis Summary"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Analysis Summary.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement", "Analysis Summary", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA - AHA Alcohol CVD Statement/Analysis Summary/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Analysis Summary"}]}, {"type": "folder", "name": "Claim Summary", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement", "Claim Summary"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Claim Summary.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement", "Claim Summary", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA - AHA Alcohol CVD Statement/Claim Summary/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Claim Summary"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - AHA Alcohol CVD Statement", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA - AHA Alcohol CVD Statement/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement"}]}, {"type": "folder", "name": "FHA - CitizenGO UN Doha Petition", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - CitizenGO UN Doha Petition"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA - CitizenGO UN Doha Petition.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - CitizenGO UN Doha Petition", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA - CitizenGO UN Doha Petition/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA - CitizenGO UN Doha Petition"}]}, {"type": "folder", "name": "FHA - F-35 Remote Control and 'Kill Switch' Claims", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - F-35 Remote Control and 'Kill Switch' Claims"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA - F-35 Remote Control and 'Kill Switch' Claims.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA - F-35 Remote Control and 'Kill Switch' Claims", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA - F-35 Remote Control and 'Kill Switch' Claims/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA - F-35 Remote Control and 'Kill Switch' Claims"}]}, {"type": "folder", "name": "FHA Claude - Hagebuttenpulver Arthrose DE", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA Claude - Hagebuttenpulver Arthrose DE"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.FHA Claude - Hagebuttenpulver Arthrose DE.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "FHA Claude - Hagebuttenpulver Arthrose DE", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/FHA Claude - Hagebuttenpulver Arthrose DE/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports.FHA Claude - Hagebuttenpulver Arthrose DE"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.FH Analysis Reports.WebHome", "segments": ["Product Development", "Specification", "FH Analysis Reports", "WebHome"], "relPath": "Product Development/Specification/FH Analysis Reports/WebHome.xwiki", "parentPath": "Product Development.Specification.FH Analysis Reports"}]}, {"type": "folder", "name": "POC", "segments": ["Product Development", "Specification", "POC"], "children": [{"type": "folder", "name": "API-and-Schemas", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas"], "children": [{"type": "folder", "name": "Codegen Contract", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Codegen Contract"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.Codegen Contract.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Codegen Contract", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/Codegen Contract/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas.Codegen Contract"}]}, {"type": "folder", "name": "Data Schemas and Cache", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Data Schemas and Cache"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Data Schemas and Cache", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/Data Schemas and Cache/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache"}]}, {"type": "folder", "name": "LLM Abstraction Layer", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "LLM Abstraction Layer"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "LLM Abstraction Layer", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/LLM Abstraction Layer/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer"}]}, {"type": "folder", "name": "Pipeline Architecture", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Pipeline Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "Pipeline Architecture", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/Pipeline Architecture/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture"}]}, {"type": "folder", "name": "REST API Contract", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "REST API Contract"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "REST API Contract", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/REST API Contract/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas.REST API Contract"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.API-and-Schemas.WebHome", "segments": ["Product Development", "Specification", "POC", "API-and-Schemas", "WebHome"], "relPath": "Product Development/Specification/POC/API-and-Schemas/WebHome.xwiki", "parentPath": "Product Development.Specification.POC.API-and-Schemas"}]}, {"type": "file", "name": "Article-Verdict-Problem.xwiki", "baseName": "Article-Verdict-Problem", "ref": "Product Development.Specification.POC.Article-Verdict-Problem", "segments": ["Product Development", "Specification", "POC", "Article-Verdict-Problem"], "relPath": "Product Development/Specification/POC/Article-Verdict-Problem.xwiki", "parentPath": "Product Development.Specification.POC"}, {"type": "file", "name": "Requirements.xwiki", "baseName": "Requirements", "ref": "Product Development.Specification.POC.Requirements", "segments": ["Product Development", "Specification", "POC", "Requirements"], "relPath": "Product Development/Specification/POC/Requirements.xwiki", "parentPath": "Product Development.Specification.POC"}, {"type": "file", "name": "Specification.xwiki", "baseName": "Specification", "ref": "Product Development.Specification.POC.Specification", "segments": ["Product Development", "Specification", "POC", "Specification"], "relPath": "Product Development/Specification/POC/Specification.xwiki", "parentPath": "Product Development.Specification.POC"}, {"type": "file", "name": "Summary.xwiki", "baseName": "Summary", "ref": "Product Development.Specification.POC.Summary", "segments": ["Product Development", "Specification", "POC", "Summary"], "relPath": "Product Development/Specification/POC/Summary.xwiki", "parentPath": "Product Development.Specification.POC"}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.POC.WebHome", "segments": ["Product Development", "Specification", "POC", "WebHome"], "relPath": "Product Development/Specification/POC/WebHome.xwiki", "parentPath": "Product Development.Specification.POC"}]}, {"type": "file", "name": "Design-Decisions.xwiki", "baseName": "Design-Decisions", "ref": "Product Development.Specification.Design-Decisions", "segments": ["Product Development", "Specification", "Design-Decisions"], "relPath": "Product Development/Specification/Design-Decisions.xwiki", "parentPath": "Product Development.Specification"}, {"type": "file", "name": "System-Performance-Metrics.xwiki", "baseName": "System-Performance-Metrics", "ref": "Product Development.Specification.System-Performance-Metrics", "segments": ["Product Development", "Specification", "System-Performance-Metrics"], "relPath": "Product Development/Specification/System-Performance-Metrics.xwiki", "parentPath": "Product Development.Specification"}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Specification.WebHome", "segments": ["Product Development", "Specification", "WebHome"], "relPath": "Product Development/Specification/WebHome.xwiki", "parentPath": "Product Development.Specification"}]}, {"type": "folder", "name": "Diagrams", "segments": ["Product Development", "Diagrams"], "children": [{"type": "folder", "name": "System Architecture", "segments": ["Product Development", "Diagrams", "System Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.System Architecture.WebHome", "segments": ["Product Development", "Diagrams", "System Architecture", "WebHome"], "relPath": "Product Development/Diagrams/System Architecture/WebHome.xwiki", "parentPath": "Product Development.Diagrams.System Architecture"}]}, {"type": "folder", "name": "System Context", "segments": ["Product Development", "Diagrams", "System Context"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.System Context.WebHome", "segments": ["Product Development", "Diagrams", "System Context", "WebHome"], "relPath": "Product Development/Diagrams/System Context/WebHome.xwiki", "parentPath": "Product Development.Diagrams.System Context"}]}, {"type": "folder", "name": "Request Lifecycle", "segments": ["Product Development", "Diagrams", "Request Lifecycle"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Request Lifecycle.WebHome", "segments": ["Product Development", "Diagrams", "Request Lifecycle", "WebHome"], "relPath": "Product Development/Diagrams/Request Lifecycle/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Request Lifecycle"}]}, {"type": "folder", "name": "Technology Stack", "segments": ["Product Development", "Diagrams", "Technology Stack"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Technology Stack.WebHome", "segments": ["Product Development", "Diagrams", "Technology Stack", "WebHome"], "relPath": "Product Development/Diagrams/Technology Stack/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Technology Stack"}]}, {"type": "folder", "name": "External Dependencies Map", "segments": ["Product Development", "Diagrams", "External Dependencies Map"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.External Dependencies Map.WebHome", "segments": ["Product Development", "Diagrams", "External Dependencies Map", "WebHome"], "relPath": "Product Development/Diagrams/External Dependencies Map/WebHome.xwiki", "parentPath": "Product Development.Diagrams.External Dependencies Map"}]}, {"type": "folder", "name": "LLM Model Tiering", "segments": ["Product Development", "Diagrams", "LLM Model Tiering"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.LLM Model Tiering.WebHome", "segments": ["Product Development", "Diagrams", "LLM Model Tiering", "WebHome"], "relPath": "Product Development/Diagrams/LLM Model Tiering/WebHome.xwiki", "parentPath": "Product Development.Diagrams.LLM Model Tiering"}]}, {"type": "folder", "name": "LLM Abstraction Architecture", "segments": ["Product Development", "Diagrams", "LLM Abstraction Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.LLM Abstraction Architecture.WebHome", "segments": ["Product Development", "Diagrams", "LLM Abstraction Architecture", "WebHome"], "relPath": "Product Development/Diagrams/LLM Abstraction Architecture/WebHome.xwiki", "parentPath": "Product Development.Diagrams.LLM Abstraction Architecture"}]}, {"type": "folder", "name": "Storage Architecture", "segments": ["Product Development", "Diagrams", "Storage Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Storage Architecture.WebHome", "segments": ["Product Development", "Diagrams", "Storage Architecture", "WebHome"], "relPath": "Product Development/Diagrams/Storage Architecture/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Storage Architecture"}]}, {"type": "folder", "name": "Storage Roadmap", "segments": ["Product Development", "Diagrams", "Storage Roadmap"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Storage Roadmap.WebHome", "segments": ["Product Development", "Diagrams", "Storage Roadmap", "WebHome"], "relPath": "Product Development/Diagrams/Storage Roadmap/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Storage Roadmap"}]}, {"type": "folder", "name": "Deployment Topology", "segments": ["Product Development", "Diagrams", "Deployment Topology"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Deployment Topology.WebHome", "segments": ["Product Development", "Diagrams", "Deployment Topology", "WebHome"], "relPath": "Product Development/Diagrams/Deployment Topology/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Deployment Topology"}]}, {"type": "folder", "name": "Security Model", "segments": ["Product Development", "Diagrams", "Security Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Security Model.WebHome", "segments": ["Product Development", "Diagrams", "Security Model", "WebHome"], "relPath": "Product Development/Diagrams/Security Model/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Security Model"}]}, {"type": "folder", "name": "Circuit Breaker States", "segments": ["Product Development", "Diagrams", "Circuit Breaker States"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Circuit Breaker States.WebHome", "segments": ["Product Development", "Diagrams", "Circuit Breaker States", "WebHome"], "relPath": "Product Development/Diagrams/Circuit Breaker States/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Circuit Breaker States"}]}, {"type": "folder", "name": "Federation Architecture", "segments": ["Product Development", "Diagrams", "Federation Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Federation Architecture.WebHome", "segments": ["Product Development", "Diagrams", "Federation Architecture", "WebHome"], "relPath": "Product Development/Diagrams/Federation Architecture/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Federation Architecture"}]}, {"type": "folder", "name": "AKEL Engine Overview", "segments": ["Product Development", "Diagrams", "AKEL Engine Overview"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.AKEL Engine Overview.WebHome", "segments": ["Product Development", "Diagrams", "AKEL Engine Overview", "WebHome"], "relPath": "Product Development/Diagrams/AKEL Engine Overview/WebHome.xwiki", "parentPath": "Product Development.Diagrams.AKEL Engine Overview"}]}, {"type": "folder", "name": "AKEL Analysis Pipeline", "segments": ["Product Development", "Diagrams", "AKEL Analysis Pipeline"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.AKEL Analysis Pipeline.WebHome", "segments": ["Product Development", "Diagrams", "AKEL Analysis Pipeline", "WebHome"], "relPath": "Product Development/Diagrams/AKEL Analysis Pipeline/WebHome.xwiki", "parentPath": "Product Development.Diagrams.AKEL Analysis Pipeline"}]}, {"type": "folder", "name": "AKEL Pipeline Detail", "segments": ["Product Development", "Diagrams", "AKEL Pipeline Detail"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.AKEL Pipeline Detail.WebHome", "segments": ["Product Development", "Diagrams", "AKEL Pipeline Detail", "WebHome"], "relPath": "Product Development/Diagrams/AKEL Pipeline Detail/WebHome.xwiki", "parentPath": "Product Development.Diagrams.AKEL Pipeline Detail"}]}, {"type": "folder", "name": "AKEL Shared Modules", "segments": ["Product Development", "Diagrams", "AKEL Shared Modules"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.AKEL Shared Modules.WebHome", "segments": ["Product Development", "Diagrams", "AKEL Shared Modules", "WebHome"], "relPath": "Product Development/Diagrams/AKEL Shared Modules/WebHome.xwiki", "parentPath": "Product Development.Diagrams.AKEL Shared Modules"}]}, {"type": "folder", "name": "AKEL Quality Assurance", "segments": ["Product Development", "Diagrams", "AKEL Quality Assurance"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.AKEL Quality Assurance.WebHome", "segments": ["Product Development", "Diagrams", "AKEL Quality Assurance", "WebHome"], "relPath": "Product Development/Diagrams/AKEL Quality Assurance/WebHome.xwiki", "parentPath": "Product Development.Diagrams.AKEL Quality Assurance"}]}, {"type": "folder", "name": "Analysis Entity Model ERD", "segments": ["Product Development", "Diagrams", "Analysis Entity Model ERD"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Analysis Entity Model ERD.WebHome", "segments": ["Product Development", "Diagrams", "Analysis Entity Model ERD", "WebHome"], "relPath": "Product Development/Diagrams/Analysis Entity Model ERD/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Analysis Entity Model ERD"}]}, {"type": "folder", "name": "Core Data Model ERD", "segments": ["Product Development", "Diagrams", "Core Data Model ERD"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Core Data Model ERD.WebHome", "segments": ["Product Development", "Diagrams", "Core Data Model ERD", "WebHome"], "relPath": "Product Development/Diagrams/Core Data Model ERD/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Core Data Model ERD"}]}, {"type": "folder", "name": "Entity Views", "segments": ["Product Development", "Diagrams", "Entity Views"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Entity Views.WebHome", "segments": ["Product Development", "Diagrams", "Entity Views", "WebHome"], "relPath": "Product Development/Diagrams/Entity Views/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Entity Views"}]}, {"type": "folder", "name": "Job Lifecycle ERD", "segments": ["Product Development", "Diagrams", "Job Lifecycle ERD"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Job Lifecycle ERD.WebHome", "segments": ["Product Development", "Diagrams", "Job Lifecycle ERD", "WebHome"], "relPath": "Product Development/Diagrams/Job Lifecycle ERD/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Job Lifecycle ERD"}]}, {"type": "folder", "name": "KeyFactor Entity Model", "segments": ["Product Development", "Diagrams", "KeyFactor Entity Model"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.KeyFactor Entity Model.WebHome", "segments": ["Product Development", "Diagrams", "KeyFactor Entity Model", "WebHome"], "relPath": "Product Development/Diagrams/KeyFactor Entity Model/WebHome.xwiki", "parentPath": "Product Development.Diagrams.KeyFactor Entity Model"}]}, {"type": "folder", "name": "KeyFactor Data Flow", "segments": ["Product Development", "Diagrams", "KeyFactor Data Flow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.KeyFactor Data Flow.WebHome", "segments": ["Product Development", "Diagrams", "KeyFactor Data Flow", "WebHome"], "relPath": "Product Development/Diagrams/KeyFactor Data Flow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.KeyFactor Data Flow"}]}, {"type": "folder", "name": "KeyFactor Claim Mapping", "segments": ["Product Development", "Diagrams", "KeyFactor Claim Mapping"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.KeyFactor Claim Mapping.WebHome", "segments": ["Product Development", "Diagrams", "KeyFactor Claim Mapping", "WebHome"], "relPath": "Product Development/Diagrams/KeyFactor Claim Mapping/WebHome.xwiki", "parentPath": "Product Development.Diagrams.KeyFactor Claim Mapping"}]}, {"type": "folder", "name": "KeyFactor Hierarchy", "segments": ["Product Development", "Diagrams", "KeyFactor Hierarchy"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.KeyFactor Hierarchy.WebHome", "segments": ["Product Development", "Diagrams", "KeyFactor Hierarchy", "WebHome"], "relPath": "Product Development/Diagrams/KeyFactor Hierarchy/WebHome.xwiki", "parentPath": "Product Development.Diagrams.KeyFactor Hierarchy"}]}, {"type": "folder", "name": "Verdict Scale", "segments": ["Product Development", "Diagrams", "Verdict Scale"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Verdict Scale.WebHome", "segments": ["Product Development", "Diagrams", "Verdict Scale", "WebHome"], "relPath": "Product Development/Diagrams/Verdict Scale/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Verdict Scale"}]}, {"type": "folder", "name": "Audit Trail ERD", "segments": ["Product Development", "Diagrams", "Audit Trail ERD"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Audit Trail ERD.WebHome", "segments": ["Product Development", "Diagrams", "Audit Trail ERD", "WebHome"], "relPath": "Product Development/Diagrams/Audit Trail ERD/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Audit Trail ERD"}]}, {"type": "folder", "name": "UCM Config Precedence", "segments": ["Product Development", "Diagrams", "UCM Config Precedence"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.UCM Config Precedence.WebHome", "segments": ["Product Development", "Diagrams", "UCM Config Precedence", "WebHome"], "relPath": "Product Development/Diagrams/UCM Config Precedence/WebHome.xwiki", "parentPath": "Product Development.Diagrams.UCM Config Precedence"}]}, {"type": "folder", "name": "UCM Config Architecture", "segments": ["Product Development", "Diagrams", "UCM Config Architecture"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.UCM Config Architecture.WebHome", "segments": ["Product Development", "Diagrams", "UCM Config Architecture", "WebHome"], "relPath": "Product Development/Diagrams/UCM Config Architecture/WebHome.xwiki", "parentPath": "Product Development.Diagrams.UCM Config Architecture"}]}, {"type": "folder", "name": "Monolithic Dynamic Pipeline Internal", "segments": ["Product Development", "Diagrams", "Monolithic Dynamic Pipeline Internal"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Monolithic Dynamic Pipeline Internal.WebHome", "segments": ["Product Development", "Diagrams", "Monolithic Dynamic Pipeline Internal", "WebHome"], "relPath": "Product Development/Diagrams/Monolithic Dynamic Pipeline Internal/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Monolithic Dynamic Pipeline Internal"}]}, {"type": "folder", "name": "Pipeline Shared Primitives", "segments": ["Product Development", "Diagrams", "Pipeline Shared Primitives"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Pipeline Shared Primitives.WebHome", "segments": ["Product Development", "Diagrams", "Pipeline Shared Primitives", "WebHome"], "relPath": "Product Development/Diagrams/Pipeline Shared Primitives/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Pipeline Shared Primitives"}]}, {"type": "folder", "name": "Pipeline Variant Dispatch", "segments": ["Product Development", "Diagrams", "Pipeline Variant Dispatch"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Pipeline Variant Dispatch.WebHome", "segments": ["Product Development", "Diagrams", "Pipeline Variant Dispatch", "WebHome"], "relPath": "Product Development/Diagrams/Pipeline Variant Dispatch/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Pipeline Variant Dispatch"}]}, {"type": "folder", "name": "Claim and Scenario Lifecycle (Overview)", "segments": ["Product Development", "Diagrams", "Claim and Scenario Lifecycle (Overview)"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Claim and Scenario Lifecycle (Overview).WebHome", "segments": ["Product Development", "Diagrams", "Claim and Scenario Lifecycle (Overview)", "WebHome"], "relPath": "Product Development/Diagrams/Claim and Scenario Lifecycle (Overview)/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Claim and Scenario Lifecycle (Overview)"}]}, {"type": "folder", "name": "Claim and Scenario Workflow", "segments": ["Product Development", "Diagrams", "Claim and Scenario Workflow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Claim and Scenario Workflow.WebHome", "segments": ["Product Development", "Diagrams", "Claim and Scenario Workflow", "WebHome"], "relPath": "Product Development/Diagrams/Claim and Scenario Workflow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Claim and Scenario Workflow"}]}, {"type": "folder", "name": "Evidence and Verdict Workflow", "segments": ["Product Development", "Diagrams", "Evidence and Verdict Workflow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Evidence and Verdict Workflow.WebHome", "segments": ["Product Development", "Diagrams", "Evidence and Verdict Workflow", "WebHome"], "relPath": "Product Development/Diagrams/Evidence and Verdict Workflow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Evidence and Verdict Workflow"}]}, {"type": "folder", "name": "Quality and Audit Workflow", "segments": ["Product Development", "Diagrams", "Quality and Audit Workflow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Quality and Audit Workflow.WebHome", "segments": ["Product Development", "Diagrams", "Quality and Audit Workflow", "WebHome"], "relPath": "Product Development/Diagrams/Quality and Audit Workflow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Quality and Audit Workflow"}]}, {"type": "folder", "name": "Quality Gates Flow", "segments": ["Product Development", "Diagrams", "Quality Gates Flow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Quality Gates Flow.WebHome", "segments": ["Product Development", "Diagrams", "Quality Gates Flow", "WebHome"], "relPath": "Product Development/Diagrams/Quality Gates Flow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Quality Gates Flow"}]}, {"type": "folder", "name": "Quality Gates Integration", "segments": ["Product Development", "Diagrams", "Quality Gates Integration"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Quality Gates Integration.WebHome", "segments": ["Product Development", "Diagrams", "Quality Gates Integration", "WebHome"], "relPath": "Product Development/Diagrams/Quality Gates Integration/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Quality Gates Integration"}]}, {"type": "folder", "name": "Evidence Defence in Depth", "segments": ["Product Development", "Diagrams", "Evidence Defence in Depth"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Evidence Defence in Depth.WebHome", "segments": ["Product Development", "Diagrams", "Evidence Defence in Depth", "WebHome"], "relPath": "Product Development/Diagrams/Evidence Defence in Depth/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Evidence Defence in Depth"}]}, {"type": "folder", "name": "Evidence Quality Filtering Pipeline", "segments": ["Product Development", "Diagrams", "Evidence Quality Filtering Pipeline"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Evidence Quality Filtering Pipeline.WebHome", "segments": ["Product Development", "Diagrams", "Evidence Quality Filtering Pipeline", "WebHome"], "relPath": "Product Development/Diagrams/Evidence Quality Filtering Pipeline/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Evidence Quality Filtering Pipeline"}]}, {"type": "folder", "name": "Context Detection Phases", "segments": ["Product Development", "Diagrams", "Context Detection Phases"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Context Detection Phases.WebHome", "segments": ["Product Development", "Diagrams", "Context Detection Phases", "WebHome"], "relPath": "Product Development/Diagrams/Context Detection Phases/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Context Detection Phases"}]}, {"type": "folder", "name": "Context Detection Decision Tree", "segments": ["Product Development", "Diagrams", "Context Detection Decision Tree"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Context Detection Decision Tree.WebHome", "segments": ["Product Development", "Diagrams", "Context Detection Decision Tree", "WebHome"], "relPath": "Product Development/Diagrams/Context Detection Decision Tree/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Context Detection Decision Tree"}]}, {"type": "folder", "name": "Doubted vs Contested Flow", "segments": ["Product Development", "Diagrams", "Doubted vs Contested Flow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Doubted vs Contested Flow.WebHome", "segments": ["Product Development", "Diagrams", "Doubted vs Contested Flow", "WebHome"], "relPath": "Product Development/Diagrams/Doubted vs Contested Flow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Doubted vs Contested Flow"}]}, {"type": "folder", "name": "Source Reliability Overview", "segments": ["Product Development", "Diagrams", "Source Reliability Overview"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Source Reliability Overview.WebHome", "segments": ["Product Development", "Diagrams", "Source Reliability Overview", "WebHome"], "relPath": "Product Development/Diagrams/Source Reliability Overview/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Source Reliability Overview"}]}, {"type": "folder", "name": "Source Reliability Flow", "segments": ["Product Development", "Diagrams", "Source Reliability Flow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Source Reliability Flow.WebHome", "segments": ["Product Development", "Diagrams", "Source Reliability Flow", "WebHome"], "relPath": "Product Development/Diagrams/Source Reliability Flow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Source Reliability Flow"}]}, {"type": "folder", "name": "Source Reliability Prefetch Flow", "segments": ["Product Development", "Diagrams", "Source Reliability Prefetch Flow"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Source Reliability Prefetch Flow.WebHome", "segments": ["Product Development", "Diagrams", "Source Reliability Prefetch Flow", "WebHome"], "relPath": "Product Development/Diagrams/Source Reliability Prefetch Flow/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Source Reliability Prefetch Flow"}]}, {"type": "folder", "name": "Human User Roles", "segments": ["Product Development", "Diagrams", "Human User Roles"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Human User Roles.WebHome", "segments": ["Product Development", "Diagrams", "Human User Roles", "WebHome"], "relPath": "Product Development/Diagrams/Human User Roles/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Human User Roles"}]}, {"type": "folder", "name": "Technical and System Users", "segments": ["Product Development", "Diagrams", "Technical and System Users"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Technical and System Users.WebHome", "segments": ["Product Development", "Diagrams", "Technical and System Users", "WebHome"], "relPath": "Product Development/Diagrams/Technical and System Users/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Technical and System Users"}]}, {"type": "folder", "name": "User Class Diagram", "segments": ["Product Development", "Diagrams", "User Class Diagram"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.User Class Diagram.WebHome", "segments": ["Product Development", "Diagrams", "User Class Diagram", "WebHome"], "relPath": "Product Development/Diagrams/User Class Diagram/WebHome.xwiki", "parentPath": "Product Development.Diagrams.User Class Diagram"}]}, {"type": "folder", "name": "Role-Based Access Control", "segments": ["Product Development", "Diagrams", "Role-Based Access Control"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Role-Based Access Control.WebHome", "segments": ["Product Development", "Diagrams", "Role-Based Access Control", "WebHome"], "relPath": "Product Development/Diagrams/Role-Based Access Control/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Role-Based Access Control"}]}, {"type": "folder", "name": "Development Roadmap", "segments": ["Product Development", "Diagrams", "Development Roadmap"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Development Roadmap.WebHome", "segments": ["Product Development", "Diagrams", "Development Roadmap", "WebHome"], "relPath": "Product Development/Diagrams/Development Roadmap/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Development Roadmap"}]}, {"type": "folder", "name": "Architecture Roadmap", "segments": ["Product Development", "Diagrams", "Architecture Roadmap"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Architecture Roadmap.WebHome", "segments": ["Product Development", "Diagrams", "Architecture Roadmap", "WebHome"], "relPath": "Product Development/Diagrams/Architecture Roadmap/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Architecture Roadmap"}]}, {"type": "folder", "name": "Promptfoo Test Coverage", "segments": ["Product Development", "Diagrams", "Promptfoo Test Coverage"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Promptfoo Test Coverage.WebHome", "segments": ["Product Development", "Diagrams", "Promptfoo Test Coverage", "WebHome"], "relPath": "Product Development/Diagrams/Promptfoo Test Coverage/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Promptfoo Test Coverage"}]}, {"type": "folder", "name": "Automation Level", "segments": ["Product Development", "Diagrams", "Automation Level"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Automation Level.WebHome", "segments": ["Product Development", "Diagrams", "Automation Level", "WebHome"], "relPath": "Product Development/Diagrams/Automation Level/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Automation Level"}]}, {"type": "folder", "name": "Automation Roadmap", "segments": ["Product Development", "Diagrams", "Automation Roadmap"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Automation Roadmap.WebHome", "segments": ["Product Development", "Diagrams", "Automation Roadmap", "WebHome"], "relPath": "Product Development/Diagrams/Automation Roadmap/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Automation Roadmap"}]}, {"type": "folder", "name": "Manual vs Automated matrix", "segments": ["Product Development", "Diagrams", "Manual vs Automated matrix"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Manual vs Automated matrix.WebHome", "segments": ["Product Development", "Diagrams", "Manual vs Automated matrix", "WebHome"], "relPath": "Product Development/Diagrams/Manual vs Automated matrix/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Manual vs Automated matrix"}]}, {"type": "folder", "name": "Outdated", "segments": ["Product Development", "Diagrams", "Outdated"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.Outdated.WebHome", "segments": ["Product Development", "Diagrams", "Outdated", "WebHome"], "relPath": "Product Development/Diagrams/Outdated/WebHome.xwiki", "parentPath": "Product Development.Diagrams.Outdated"}]}, {"type": "folder", "name": "ClaimAssessmentBoundary Pipeline Detail", "segments": ["Product Development", "Diagrams", "ClaimAssessmentBoundary Pipeline Detail"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail.WebHome", "segments": ["Product Development", "Diagrams", "ClaimAssessmentBoundary Pipeline Detail", "WebHome"], "relPath": "Product Development/Diagrams/ClaimAssessmentBoundary Pipeline Detail/WebHome.xwiki", "parentPath": "Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Diagrams.WebHome", "segments": ["Product Development", "Diagrams", "WebHome"], "relPath": "Product Development/Diagrams/WebHome.xwiki", "parentPath": "Product Development.Diagrams"}]}, {"type": "folder", "name": "Planning", "segments": ["Product Development", "Planning"], "children": [{"type": "folder", "name": "Project Status", "segments": ["Product Development", "Planning", "Project Status"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.Project Status.WebHome", "segments": ["Product Development", "Planning", "Project Status", "WebHome"], "relPath": "Product Development/Planning/Project Status/WebHome.xwiki", "parentPath": "Product Development.Planning.Project Status"}]}, {"type": "folder", "name": "POC1", "segments": ["Product Development", "Planning", "POC1"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.POC1.WebHome", "segments": ["Product Development", "Planning", "POC1", "WebHome"], "relPath": "Product Development/Planning/POC1/WebHome.xwiki", "parentPath": "Product Development.Planning.POC1"}]}, {"type": "folder", "name": "POC2", "segments": ["Product Development", "Planning", "POC2"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.POC2.WebHome", "segments": ["Product Development", "Planning", "POC2", "WebHome"], "relPath": "Product Development/Planning/POC2/WebHome.xwiki", "parentPath": "Product Development.Planning.POC2"}]}, {"type": "folder", "name": "POC to Alpha Transition", "segments": ["Product Development", "Planning", "POC to Alpha Transition"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.POC to Alpha Transition.WebHome", "segments": ["Product Development", "Planning", "POC to Alpha Transition", "WebHome"], "relPath": "Product Development/Planning/POC to Alpha Transition/WebHome.xwiki", "parentPath": "Product Development.Planning.POC to Alpha Transition"}]}, {"type": "folder", "name": "Beta0", "segments": ["Product Development", "Planning", "Beta0"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.Beta0.WebHome", "segments": ["Product Development", "Planning", "Beta0", "WebHome"], "relPath": "Product Development/Planning/Beta0/WebHome.xwiki", "parentPath": "Product Development.Planning.Beta0"}]}, {"type": "folder", "name": "V10", "segments": ["Product Development", "Planning", "V10"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.V10.WebHome", "segments": ["Product Development", "Planning", "V10", "WebHome"], "relPath": "Product Development/Planning/V10/WebHome.xwiki", "parentPath": "Product Development.Planning.V10"}]}, {"type": "folder", "name": "Requirements-Roadmap-Matrix", "segments": ["Product Development", "Planning", "Requirements-Roadmap-Matrix"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.Requirements-Roadmap-Matrix.WebHome", "segments": ["Product Development", "Planning", "Requirements-Roadmap-Matrix", "WebHome"], "relPath": "Product Development/Planning/Requirements-Roadmap-Matrix/WebHome.xwiki", "parentPath": "Product Development.Planning.Requirements-Roadmap-Matrix"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Planning.WebHome", "segments": ["Product Development", "Planning", "WebHome"], "relPath": "Product Development/Planning/WebHome.xwiki", "parentPath": "Product Development.Planning"}]}, {"type": "folder", "name": "DevOps", "segments": ["Product Development", "DevOps"], "children": [{"type": "folder", "name": "Guidelines", "segments": ["Product Development", "DevOps", "Guidelines"], "children": [{"type": "folder", "name": "Getting Started", "segments": ["Product Development", "DevOps", "Guidelines", "Getting Started"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.Getting Started.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "Getting Started", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/Getting Started/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines.Getting Started"}]}, {"type": "folder", "name": "Coding Guidelines", "segments": ["Product Development", "DevOps", "Guidelines", "Coding Guidelines"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.Coding Guidelines.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "Coding Guidelines", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/Coding Guidelines/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines.Coding Guidelines"}]}, {"type": "folder", "name": "Scope Definition Guidelines", "segments": ["Product Development", "DevOps", "Guidelines", "Scope Definition Guidelines"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.Scope Definition Guidelines.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "Scope Definition Guidelines", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/Scope Definition Guidelines/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines.Scope Definition Guidelines"}]}, {"type": "folder", "name": "Testing Strategy", "segments": ["Product Development", "DevOps", "Guidelines", "Testing Strategy"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.Testing Strategy.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "Testing Strategy", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/Testing Strategy/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines.Testing Strategy"}]}, {"type": "folder", "name": "When to Add Complexity", "segments": ["Product Development", "DevOps", "Guidelines", "When to Add Complexity"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.When to Add Complexity.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "When to Add Complexity", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/When to Add Complexity/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines.When to Add Complexity"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Guidelines.WebHome", "segments": ["Product Development", "DevOps", "Guidelines", "WebHome"], "relPath": "Product Development/DevOps/Guidelines/WebHome.xwiki", "parentPath": "Product Development.DevOps.Guidelines"}]}, {"type": "folder", "name": "Tooling", "segments": ["Product Development", "DevOps", "Tooling"], "children": [{"type": "folder", "name": "1st Run Checklist", "segments": ["Product Development", "DevOps", "Tooling", "1st Run Checklist"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.1st Run Checklist.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "1st Run Checklist", "WebHome"], "relPath": "Product Development/DevOps/Tooling/1st Run Checklist/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.1st Run Checklist"}]}, {"type": "folder", "name": "Installation Checklist", "segments": ["Product Development", "DevOps", "Tooling", "Installation Checklist"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.Installation Checklist.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "Installation Checklist", "WebHome"], "relPath": "Product Development/DevOps/Tooling/Installation Checklist/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.Installation Checklist"}]}, {"type": "folder", "name": "Tools Decisions", "segments": ["Product Development", "DevOps", "Tooling", "Tools Decisions"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.Tools Decisions.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "Tools Decisions", "WebHome"], "relPath": "Product Development/DevOps/Tooling/Tools Decisions/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.Tools Decisions"}]}, {"type": "folder", "name": "Documentation Viewer", "segments": ["Product Development", "DevOps", "Tooling", "Documentation Viewer"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.Documentation Viewer.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "Documentation Viewer", "WebHome"], "relPath": "Product Development/DevOps/Tooling/Documentation Viewer/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.Documentation Viewer"}]}, {"type": "folder", "name": "Promptfoo Testing", "segments": ["Product Development", "DevOps", "Tooling", "Promptfoo Testing"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.Promptfoo Testing.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "Promptfoo Testing", "WebHome"], "relPath": "Product Development/DevOps/Tooling/Promptfoo Testing/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.Promptfoo Testing"}]}, {"type": "folder", "name": "Source Reliability Bundle", "segments": ["Product Development", "DevOps", "Tooling", "Source Reliability Bundle"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.Source Reliability Bundle.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "Source Reliability Bundle", "WebHome"], "relPath": "Product Development/DevOps/Tooling/Source Reliability Bundle/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling.Source Reliability Bundle"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Tooling.WebHome", "segments": ["Product Development", "DevOps", "Tooling", "WebHome"], "relPath": "Product Development/DevOps/Tooling/WebHome.xwiki", "parentPath": "Product Development.DevOps.Tooling"}]}, {"type": "folder", "name": "Deployment", "segments": ["Product Development", "DevOps", "Deployment"], "children": [{"type": "folder", "name": "Zero-Cost Hosting Implementation Guide", "segments": ["Product Development", "DevOps", "Deployment", "Zero-Cost Hosting Implementation Guide"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide.WebHome", "segments": ["Product Development", "DevOps", "Deployment", "Zero-Cost Hosting Implementation Guide", "WebHome"], "relPath": "Product Development/DevOps/Deployment/Zero-Cost Hosting Implementation Guide/WebHome.xwiki", "parentPath": "Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Deployment.WebHome", "segments": ["Product Development", "DevOps", "Deployment", "WebHome"], "relPath": "Product Development/DevOps/Deployment/WebHome.xwiki", "parentPath": "Product Development.DevOps.Deployment"}]}, {"type": "folder", "name": "Subsystems and Components", "segments": ["Product Development", "DevOps", "Subsystems and Components"], "children": [{"type": "folder", "name": "Admin Interface", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Admin Interface"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Admin Interface", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/Admin Interface/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components.Admin Interface"}]}, {"type": "folder", "name": "Unified Config Management", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Unified Config Management"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Unified Config Management", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/Unified Config Management/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components.Unified Config Management"}]}, {"type": "folder", "name": "LLM Configuration", "segments": ["Product Development", "DevOps", "Subsystems and Components", "LLM Configuration"], "children": [{"type": "folder", "name": "Debate Role Configuration", "segments": ["Product Development", "DevOps", "Subsystems and Components", "LLM Configuration", "Debate Role Configuration"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.LLM Configuration.Debate Role Configuration.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "LLM Configuration", "Debate Role Configuration", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/LLM Configuration/Debate Role Configuration/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components.LLM Configuration.Debate Role Configuration"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "LLM Configuration", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/LLM Configuration/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components.LLM Configuration"}]}, {"type": "folder", "name": "Source Reliability Export", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Source Reliability Export"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.Source Reliability Export.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "Source Reliability Export", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/Source Reliability Export/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components.Source Reliability Export"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.Subsystems and Components.WebHome", "segments": ["Product Development", "DevOps", "Subsystems and Components", "WebHome"], "relPath": "Product Development/DevOps/Subsystems and Components/WebHome.xwiki", "parentPath": "Product Development.DevOps.Subsystems and Components"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.DevOps.WebHome", "segments": ["Product Development", "DevOps", "WebHome"], "relPath": "Product Development/DevOps/WebHome.xwiki", "parentPath": "Product Development.DevOps"}]}, {"type": "folder", "name": "Media Source Database", "segments": ["Product Development", "Media Source Database"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.Media Source Database.WebHome", "segments": ["Product Development", "Media Source Database", "WebHome"], "relPath": "Product Development/Media Source Database/WebHome.xwiki", "parentPath": "Product Development.Media Source Database"}]}, {"type": "folder", "name": "TestReports", "segments": ["Product Development", "TestReports"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.TestReports.WebHome", "segments": ["Product Development", "TestReports", "WebHome"], "relPath": "Product Development/TestReports/WebHome.xwiki", "parentPath": "Product Development.TestReports"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "Product Development.WebHome", "segments": ["Product Development", "WebHome"], "relPath": "Product Development/WebHome.xwiki", "parentPath": "Product Development"}]}, {"type": "folder", "name": "License and Disclaimer", "segments": ["License and Disclaimer"], "children": [{"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "License and Disclaimer.WebHome", "segments": ["License and Disclaimer", "WebHome"], "relPath": "License and Disclaimer/WebHome.xwiki", "parentPath": "License and Disclaimer"}]}, {"type": "file", "name": "WebHome.xwiki", "baseName": "WebHome", "ref": "WebHome", "segments": ["WebHome"], "relPath": "WebHome.xwiki", "parentPath": ""}]}], "pages": {"About FactHarbor.WebHome": "= About FactHarbor =\n\nWe live in a time of overwhelming information.\n\nSome people believe bold claims with little evidence.\nOthers step back entirely: \"I don't read the news anymore. I can't trust anything.\"\n\nBoth reactions are understandable.\n\nWhen reasoning is hidden and sources lack transparency, trust erodes. The problem is not that people don't care about truth  it's that the structure behind information is often invisible.\n\nFactHarbor was created to change that.\n\nWe organize public debates into transparent evidence models, connecting claims with assumptions, supporting and opposing evidence, and the reliability of their sources. Instead of asking people to simply believe or reject, we make the reasoning visible.\n\nNot to win arguments.\nBut to strengthen trust through clarity.\n\nBecause when reasoning is transparent, judgment becomes possible again.\n\n== Founder ==\n\n(% class=\"box infomessage\" %)\n(((\n**Robert Schaub**\n\nFounder of FactHarbor.\n\n* [[LinkedIn>>https://www.linkedin.com/in/robertschaub/]]\n* [[info@factharbor.ch>>mailto:info@factharbor.ch]]\n)))\n", "License and Disclaimer.WebHome": "= License and Disclaimer =\n\n== Overview ==\n\nFactHarbor is an **open-source nonprofit project** committed to transparency and accessibility. All resources are licensed under multiple open licenses to maximize reuse while protecting the project's transparency mission.\n\n**FactHarbor operates under a multi-license model:**\n\n* Documentation and specifications: CC BY-SA 4.0\n* Code: MIT (default) or AGPL-3.0 (core components)\n* Structured data: Open Database License (ODbL)\n\n== Licensing ==\n \n=== Documentation ===\n\nAll general **documentation** (organizational and technical) is licensed under:\n\n* **[[Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)>>http://creativecommons.org/licenses/by-sa/4.0/]]**\n\n**You may:**\n* Use, share, and adapt the documentation (including commercially)\n\n**You must:**\n* Provide attribution to FactHarbor\n* Share derivative works under CC BY-SA 4.0\n\n**Exception:** Security-sensitive documentation may be published partially or kept internal.\n\n=== Core Protocol & Data Model ===\n\nThe **core protocol** and **data model** are licensed under:\n\n* **[[Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)>>http://creativecommons.org/licenses/by-sa/4.0/]]**\n\n**You may:**\n* Use, implement, and modify the protocol/data model\n* Publish derivative specifications under CC BY-SA 4.0\n\n**You must:**\n* Attribute to FactHarbor\n* Use different branding (the \"FactHarbor\" name is trademark-protected)\n* State derivation from FactHarbor protocol\n* Share derivatives under CC BY-SA 4.0\n\n=== Code ===\n\n**Default License:** **[[MIT License>>https://opensource.org/licenses/MIT]]**\n\n**Core Components License:** **[[AGPL-3.0>>https://www.gnu.org/licenses/agpl-3.0.en.html]]** for:\n* Core verdict engine\n* AKEL reasoning logic\n* Scenario evaluation engine\n\n**Rationale:** AGPL-3.0 prevents black-box network services that contradict our transparency mission.\n\n=== Structured Data ===\n\nCurated knowledge and datasets are licensed under:\n\n* **[[Open Database License (ODbL)>>https://opendatacommons.org/licenses/odbl/]]**\n\nThis ensures derivative databases remain open and prevents proprietary capture of community data.\n\n== Attribution ==\n\nWhen creating derivative works, attribute as follows:\n\n**Documentation/Specifications:**\n{{code}}\nThis work, '[Your Work Name]', is a derivative of 'FactHarbor' by Robert Schaub \nand the FactHarbor community. Licensed under CC BY-SA 4.0 by [Your Name].\n{{/code}}\n\n**Code (MIT):**\n{{code}}\nBased on FactHarbor by Robert Schaub and contributors\nLicensed under MIT License\n{{/code}}\n\n**Code (AGPL):**\n{{code}}\nBased on FactHarbor Core Engine by Robert Schaub and contributors\nLicensed under GNU AGPL v3.0\nSource code available at: [repository URL]\n{{/code}}\n\n**Databases (ODbL):**\n{{code}}\nDerived from FactHarbor data by Robert Schaub and the FactHarbor community.\nLicensed under ODbL by [Your Name].\n{{/code}}\n\n== AI Models & Third-Party Components ==\n\n**AKEL (AI Knowledge Extraction Layer):**\n* Open-source models preferred\n* Proprietary models allowed but must be disclosed to users\n* AI-generated content marked with **AuthorType = AI**\n* Integration code remains open (MIT/AGPL)\n\n**Third-Party Libraries:**\n* Must be license-compatible with MIT/AGPL\n* Documented in LICENSE/NOTICE files\n\n== Organizational Transparency ==\n\nFactHarbor commits to publishing:\n\n**Annually:**\n* Financial statements\n* Income sources and expenses\n* Major funding relationships\n\n**Twice Yearly:**\n* Transparency reports\n* AKEL performance metrics\n* Content moderation statistics\n\n**Ongoing:**\n* Governance documents\n* Policy changes\n* Incident reports\n\nSee [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]] for details.\n\n== Disclaimer ==\n\n=== No Warranty ===\n\nFactHarbor is provided **\"AS IS\"** without warranty of any kind, including:\n* Merchantability\n* Fitness for purpose\n* Accuracy or completeness\n* Uninterrupted service\n\n=== No Liability ===\n\nFactHarbor creators, contributors, and operators are **not liable** for:\n* Decisions made based on FactHarbor analyses\n* Errors, inaccuracies, or incompleteness\n* Any direct, indirect, incidental, or consequential damages\n\n=== Educational Purpose Only ===\n\nFactHarbor analyses are for **educational and informational purposes**. They are not:\n* Professional advice (legal, medical, financial)\n* Definitive truth or final judgments\n* Replacements for independent verification\n\n**Users should:**\n* Verify information independently\n* Consult qualified professionals when appropriate\n* Apply critical thinking and judgment\n\n=== Privacy ===\n\nUser privacy is protected per applicable laws (Swiss FADP, EU GDPR).\n\nSee [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]] for details.\n\n== Governing Law ==\n\nThese terms are governed by **Swiss law** under Swiss jurisdiction.\n\n== Changes to Terms ==\n\nLicense terms may be updated with:\n* Clear versioning and documentation\n* Community announcement\n* Prospective application (existing content remains under original terms)\n\n== License Summary ==\n\n| **Content Type** | **License** | **Key Principle** |\n|---|---|---|\n| Documentation | CC BY-SA 4.0 | Share openly, attribute, share-alike |\n| Protocol & Data Model | CC BY-SA 4.0 | Open specs, trademark-protected brand |\n| Code (Default) | MIT | Maximum reuse |\n| Code (Core/AKEL) | AGPL-3.0 | Network transparency |\n| Structured Data | ODbL | Open data, share-alike |\n\n== Contact ==\n\n* **Project Website**: [[https://schaubgroup.ch/bin/view/FactHarbor/]]\n* **Licensing Questions**: [To be established]\n\n== Related Pages ==\n\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Terms of Service>>FactHarbor.Organisation.How-We-Work-Together.Terms-of-Service]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n* [[Contributor License Agreement>>FactHarbor.Organisation.Legal and Compliance.CLA.WebHome]]\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]\n\n** 2024-2025 by Robert Schaub and the FactHarbor Community**\n\n**FactHarbor** is a trademark. Use of the name and logo requires permission.\n\n**Core Values:** Non-profit | Open Source | Transparent | Accessible to All", "Organisation.Diagrams.Domain Interaction Map.WebHome": "= Domain Interaction Map =\n\n{{mermaid}}\nflowchart LR\n classDef tech fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;\n classDef org fill:#fce4ec,stroke:#c2185b,stroke-width:2px;\n classDef ops fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;\n classDef pr fill:#fff3e0,stroke:#ef6c00,stroke-width:2px;\n RD[R&D Domain]:::tech\n Org[Organisation Domain]:::org\n PR[PR, Care & Marketing]:::pr\n Ops[Operations Domain]:::ops\n RD <-->|Specs & Quality| Org\n Org <-->|Governance & Rules| PR\n Org <-->|Policies & Access| Ops\n PR <-->|Tools & Channels| Ops\n RD -.->|Tech Requirements| Ops\n{{/mermaid}}\n", "Organisation.Diagrams.Governance Structure.WebHome": "= Governance Structure =\n\n{{mermaid}}\ngraph TD\n A[General Assembly] -->|Elects| B[Governing Team 3-7]\n B -->|Oversees| C[Team Members full-time]\n C -->|Technical Coordinator| D[AKEL & Infrastructure]\n C -->|Community Coordinator| E[Moderators part-time]\n E -->|Moderate| F[Content]\n G[Readers - Guests] -->|Browse/View| F\n G2[Users - Registered] -->|Submit/Flag| F\n H[UCM Administrators] -->|Configure| I[UCM System Config]\n style A fill:#e1f5ff\n style B fill:#ffe1f5\n style D fill:#fff5e1\n{{/mermaid}}\n\nSimplified structure: General Assembly  Governing Team  Team Members. Readers (guests) browse and view. Registered users submit content (rate-limited). UCM Administrators manage system configuration.\n", "Organisation.Diagrams.WebHome": "= Diagrams =\n\nVisual diagrams for the Organisation domain, rendered using Mermaid.\n\n== Available Diagrams ==\n\n* [[Governance Structure>>FactHarbor.Organisation.Diagrams.Governance Structure.WebHome]]\n* [[Domain Interaction Map>>FactHarbor.Organisation.Diagrams.Domain Interaction Map.WebHome]]\n\n== Diagram Pattern ==\n\nEach diagram is a single page (e.g. ##Governance Structure/WebHome.xwiki##) containing a ##= Title =## heading and a ##{{mermaid}}## block. Other pages can ##{{include}}## the diagram page to embed it inline.\n\nNew diagrams should follow this pattern. Always use Mermaid syntax (not images) so diagrams remain editable and version-controlled. See DOC-R-028 for Mermaid syntax rules (empty lines required before/after the mermaid block).", "Organisation.Governance.Decision Processes.WebHome": "= Decision Processes =\n== 1. Overview ==\nThis page describes the main types of decisions in FactHarbor and how they are escalated and documented.\n== 2. Types of Decisions ==\n* **Operational decisions**  day-to-day actions within an agreed budget or scope.\n* **Strategic decisions**  long-term direction, major priorities, and partnerships.\n* **Governance decisions**  changes to rules, roles, and organisational structures.\n* **Exceptional decisions**  sensitive topics, serious conflicts, or rule violations.\n== 2.5 Automation Boundary ==\n**Critical distinction**: What AKEL decides vs. what humans decide.\n=== 2.5.1 Automated Decisions (AKEL) ===\n**All content decisions are automated**:\n*  Claim verdicts and confidence scores\n*  Evidence assessment and relevance scoring\n*  Source track record scoring\n*  Risk tier classification\n*  Scenario extraction\n*  Publication decisions\n*  Contradiction detection\n**Why automated?**\n* Scale: Cannot manually process millions of claims\n* Consistency: Algorithms apply rules uniformly\n* Transparency: Code can be audited\n* No bias: No human subjective judgment\n* 24/7: Always available\n**Human role**: Monitor aggregate metrics, identify systematic issues, improve algorithms.\n=== 2.5.2 Human Decisions ===\n**System decisions (not content)**:\n**Strategic** (General Assembly, 2/3 majority):\n* Mission and values changes\n* Risk tier policy definitions\n* Major architectural changes\n* Budget allocation >CHF 50,000\n* Organizational structure\n* Dissolution\n**Tactical** (Governing Team, consent-based):\n* Algorithm parameter adjustments (within policy)\n* Policy updates and clarifications\n* Infrastructure investments\n* Hiring and role assignments\n* Partnership agreements\n* Community guidelines\n**Operational** (Domain owners, autonomous):\n* Technical Coordinator: AKEL optimizations, infrastructure changes\n* Community Coordinator: Process improvements, documentation\n* Moderators: Handling AKEL-flagged items\n**Emergency** (Any team member, ratified later):\n* Critical security vulnerabilities\n* Legal compliance requirements\n* Immediate safety threats\n=== 2.5.3 Principle: Fix the System, Not the Data ===\n**When AKEL makes what appears to be a wrong decision**:\n **Don't do this**:\n* Manually override that specific verdict\n* Adjust that source's score manually\n* Create special case for that claim\n **Do this instead**:\n1. Investigate: Is this a systematic issue?\n2. Analyze: What pattern caused this?\n3. Improve: Change algorithm/policy systematically\n4. Test: Validate on historical data\n5. Deploy: Roll out improved system\n6. Monitor: Check if metrics improve\n**Example**:\n*  Bad: \"AKEL rated this credible source too low. I'll manually boost it from 0.6 to 0.8.\"\n*  Good: \"AKEL consistently under-rates peer-reviewed medical journals. Let's adjust the scoring algorithm to weight peer-review certification +15% for medical sources. Test on 1000 historical claims first.\"\n== 3. Principles ==\n* Decide at the lowest reasonable level.\n* Involve the people who are affected.\n* Record context, reasoning, and outcome for significant decisions.\n* Keep escalation paths clear and predictable.\n=== 3.5 Decision Methods ===\nDifferent types of decisions use different methods:\n**Consent-Based Decision Making** (from [[Sociocracy 3.0>>https://sociocracy30.org/]]):\n* **Use for**: Algorithm changes, policy updates, process changes, role assignments\n* **Process**: Proposal  Questions  Reactions  Amend  Consent round\n* **Criteria**: No principled objections (not consensus)\n* **See**: [[Consent-Based Decision Making>>FactHarbor.Organisation.How-We-Work-Together.Consent-Based-Decision-Making]]\n**Voting**:\n* **Use for**: Strategic decisions, when consent fails\n* **General Assembly**: 2/3 majority for major decisions\n* **Governing Team**: Simple majority for tactical decisions\n* **Emergency**: Simple majority, ratified later\n**Autonomous**:\n* **Use for**: Decisions within defined domain boundaries\n* **Technical Coordinator**: Technical changes within domain\n* **Community Coordinator**: Community process changes\n* **Moderators**: Handling flagged items\n* **Accountability**: Document decision, report to Governing Team quarterly\n**RFC Process** (for technical/policy proposals):\n* **Use for**: Significant system changes\n* **Process**: \n 1. Create RFC (Request for Comments)\n 2. Community discussion (7-appropriate time period)\n 3. Revise based on feedback\n 4. Decision by appropriate authority (consent or voting)\n 5. Document and implement\n== 4. Escalation Paths ==\nExamples (to be adapted as the organisation grows):\n* Content and modelling issues  Contributor  Contributor  Organisation Lead  Executive Lead  Governing Team.\n* Behaviour and moderation issues  Moderator  Governance Steward  Executive Lead  Governing Team.\n* Finance and compliance issues  Finance & Compliance Lead  Governance Steward  Governing Team.\n== 5. Documentation ==\nFor significant decisions, at minimum record:\n* date and time\n* people involved and responsible role/body\n* options considered\n* reasoning and trade-offs\n* final decision and expected impact\n* follow-up actions and deadlines.\n=== 5.1 Decision Record Template ===\nFor formal decisions, use the following template:\n**Decision Record: [Title/Topic]**\n* **Date:** YYYY-MM-DD\n* **Decider(s):** [Names/Roles]\n* **Context:** [What is the issue? What constraints exist?]\n* **Options Considered:**\n * Option A: ...\n * Option B: ...\n* **Decision:** [The chosen path]\n* **Reasoning:** [Why this option? Trade-offs accepted?]\n* **Consequences:** [Expected impact, risks, follow-up tasks]\n== 6. Integration with Tools ==\nOver time, decision logs may be maintained in dedicated tools (e.g. XWiki pages, issue trackers, or lightweight registers.\nWhatever the tool, Governance must be able to reconstruct how important decisions were made.", "Organisation.Governance.Operational Readiness.WebHome": "= Operational Readiness Checklist =\n== 1. Purpose and Scope ==\nThis checklist documents prerequisite tasks that must be completed before FactHarbor can launch to the public.\n**Organization Reality:** Starting as a solo project with team growth expected within the first year. \n**Status as of:** December 17, 2025 \n**Target Launch Date:** [To be determined]\n**Important:** Initially, one person handles multiple functions. This is normal and legal. As the team grows, responsibilities can be distributed.\n== 2. Critical Tasks (MUST Complete Before Launch) ==\nThese tasks are mandatory for legal compliance and core functionality.\n=== 2.1 Legal & Compliance ===\n| Task | Status | Notes |\n|------|--------|-------|\n| **Engage Swiss legal advisor for policy review** |  Not Started | Review all policies, bylaws |\n| **Draft and adopt Verein bylaws (statutes)** |  Not Started | Required for legal existence |\n| **Appoint founding board (minimum two members)** |  Not Started | Can include yourself |\n| **Apply for Swiss tax-exempt status** |  Not Started | Cantonal tax authority |\n| **Designate Swiss representative** |  Not Started | Can be yourself with Swiss address |\n| **Create processing activities register** |  Not Started | Internal document |\n| **Conduct initial DPIA for AKEL system** |  Not Started | Can use templates |\n| **Set effective dates for policies** |  Not Started | Privacy & Transparency |\n=== 2.2 Technical Implementation ===\n| Task | Status | Notes |\n|------|--------|-------|\n| **Implement opt-in cookie consent banner** |  Not Started | Open source libraries available |\n| **Build user data export functionality** |  Not Started | JSON/CSV export |\n| **Build account deletion functionality** |  Not Started | With grace period |\n| **Implement data retention automation** |  Not Started | Automated cleanup |\n| **Set up breach notification procedures** |  Not Started | Document + FDPIC contact |\n| **Implement TLS/HTTPS encryption** |  Not Started | Let's Encrypt or similar |\n| **Set up security logging** |  Not Started | One year retention |\n=== 2.3 Organizational Infrastructure ===\n| Task | Status | Notes |\n|------|--------|-------|\n| **Set up contact infrastructure** |  Not Started | See Section 5 |\n| **Establish document storage** |  Not Started | Secure storage for bylaws, minutes |\n| **Create incident response plan** |  Not Started | Brief document |\n| **Set up basic accounting** |  Not Started | Spreadsheet initially acceptable |\n| **Establish board meeting schedule** |  Not Started | Quarterly minimum |\n== 3. Important Tasks (SHOULD Complete Before Launch) ==\nThese tasks are strongly recommended before launch.\n=== 3.1 Governance & Policy ===\n| Task | Status | Priority |\n|------|--------|----------|\n| **Appoint DPO (if serving EU users from day 1)** |  Not Started | HIGH - Can be yourself |\n| **Create Terms of Service** |  Not Started | HIGH - Adapt templates |\n| **Create basic Security Policy** |  Not Started | MEDIUM |\n| **Create simple CLA** |  Not Started | HIGH - Adapt existing |\n| **Document internal escalation** |  Not Started | LOW |\n=== 3.2 Technical & Operational ===\n| Task | Status | Priority |\n|------|--------|----------|\n| **Set up vulnerability disclosure** |  Not Started | HIGH |\n| **Implement 2FA** |  Not Started | MEDIUM |\n| **Create user documentation** |  Not Started | HIGH |\n| **Set up monitoring** |  Not Started | HIGH |\n| **Set up backup systems** |  Not Started | HIGH |\n=== 3.3 Licensing & Open Source ===\n| Task | Status | Priority |\n|------|--------|----------|\n| **Decide: Code licensing model** |  Not Started | HIGH - MIT vs MIT+AGPL |\n| **Create LICENSE files** |  Not Started | HIGH |\n| **Set up code repository** |  Not Started | HIGH |\n| **Create CONTRIBUTING.md** |  Not Started | MEDIUM |\n== 4. Recommended Tasks (Can Be Post-Launch) ==\nThese can wait until after launch or until team grows.\n| Task | Priority | Notes |\n|------|----------|-------|\n| **Trademark registration** | MEDIUM | When budget allows |\n| **Penetration testing** | MEDIUM | When feasible |\n| **Transparency Committee** | LOW | When team grows |\n| **Independent audit** | LOW | When required by revenue threshold |\n== 5. Required Infrastructure ==\n=== 5.1 Contact Infrastructure ===\n**Minimum Required:**\nAt minimum, you need contact methods for:\n* General inquiries\n* Privacy/data requests (FADP/GDPR requirement)\n* Security/abuse reports\n* Governing Team/governance\n**Options:**\n**Option A: Single Contact Point**\n* One email or contact form\n* Routes internally as needed\n* State response times clearly\n**Option B: Functional Separation**\n* Few key addresses for different purposes\n* Still manageable by one person\n**Recommendation:** Wait to set up infrastructure until you have domain and email hosting.\n=== 5.2 Documentation to Prepare ===\n**Must Exist Before Launch:**\n* Processing activities register (internal)\n* Initial DPIA for AKEL (internal)\n* Breach response procedure\n* Privacy Policy (done, set effective date)\n* Transparency Policy (done, set effective date)\n**Should Exist:**\n* Terms of Service\n* Simple security policy\n* CLA\n**Can Wait:**\n* Detailed security documentation\n* Complex governance processes\n=== 5.3 Tools and Services ===\n**Hosting:**\n* Swiss providers (Hetzner, Infomaniak) or other reliable hosting\n* Start small, scale up\n**Email/Contact:**\n* Swiss privacy-focused providers (ProtonMail, Tutanota)\n* Free tiers available initially\n**Development:**\n* GitHub or GitLab (free for public repos)\n**Monitoring:**\n* Free tier services available (UptimeRobot, etc.)\n**Documentation:**\n* GitHub Wiki, GitBook, or XWiki\n== 6. Decision Points ==\nStrategic decisions needed before implementation:\n=== 6.1 Critical Decisions ===\n| Decision | Options | Consideration |\n|----------|---------|---------------|\n| **Serve EU users day 1?** | Yes/No/Later | Affects DPO requirement |\n| **Code licensing** | MIT / MIT+AGPL | Simpler vs. stronger copyleft |\n| **Hosting location** | CH/EU/US | Swiss aligns with mission |\n| **AI model** | Open/API | Infrastructure vs. simplicity |\n=== 6.2 Organizational Decisions ===\n| Decision | Options |\n|----------|---------|\n| **Governing Team size** | Two minimum, can expand later |\n| **Governing Team meetings** | Quarterly minimum |\n| **DPO** | Only if/when needed |\n| **Commercial Register** | Optional for non-profit |\n== 7. Launch Blockers - Go/No-Go Checklist ==\n**Cannot launch until ALL are complete:**\n**Legal:**\n- [ ] Verein bylaws adopted\n- [ ] Governing Team appointed (two members minimum)\n- [ ] Swiss representative designated\n- [ ] Privacy Policy effective date set\n- [ ] Processing activities register created\n- [ ] Initial DPIA completed\n**Technical:**\n- [ ] HTTPS encryption implemented\n- [ ] Cookie consent (opt-in) working\n- [ ] Data export functionality working\n- [ ] Account deletion working\n- [ ] Breach notification procedure documented\n**Operational:**\n- [ ] Contact infrastructure established\n- [ ] Security incident procedure documented\n- [ ] Data retention automation configured\n- [ ] Terms of Service created\n== 8. Post-Launch Compliance ==\n**Immediate Response Required:**\n* Data subject requests (within required timeframe)\n* Security breaches (immediate FDPIC notification if high risk)\n* Abuse reports (timely)\n**Recurring:**\n* Governing Team meeting\n* Review data retention\n* Security check\n* Publish transparency report\n* Review policies\n**Periodic:**\n* Publish financial statements\n* Policy review\n* Privacy audit\n* External audit (if above revenue threshold)\n== 9. As Team Grows ==\n**Initial (Solo):**\n* One person handles all functions\n* Document everything\n* Use templates and tools\n**Early Growth (First Helpers):**\n* Distribute technical vs. governance tasks\n* Cross-training important\n* Keep communication clear\n**Established Team:**\n* Specialized roles emerge naturally\n* Formal responsibility assignments\n* More sophisticated processes\n**Key:** Start simple, scale processes as team and complexity grow.\n== 10. Budget Considerations ==\n**Pre-Launch:**\n* Legal advisor (essential)\n* Minimal infrastructure\n* Free tools where possible\n**Ongoing:**\n* Hosting (start small)\n* Email/contact infrastructure\n* Legal support as needed\n* Scale as revenue permits\n**Later:**\n* Security assessments\n* Trademark registration\n* Professional audits\n* Better tooling\n**Philosophy:** Start lean, invest as you validate product-market fit.\n== 11. Risk Management ==\n**Key Risks:**\n* Legal delays\n* Technical complexity\n* Time management (solo)\n* Volunteer coordination\n* Burnout\n**Mitigation:**\n* Start legal work early\n* Build MVP, iterate\n* Realistic scope\n* Good documentation\n* Don't overcommit\n== 12. Success Criteria ==\n**Ready to launch when:**\n* All launch blockers complete\n* Legal advisor approves policies\n* Governing Team formally approves launch\n* Contact infrastructure works\n* Core functions operational\n* Capacity to handle support exists\n**Remember:** Launch with working MVP, not perfect system.\n== 13. Timeline Considerations ==\n**Factors:**\n* Legal processes take time\n* Technical implementation scope\n* Part-time vs. full-time work\n* Availability of help\n* Budget constraints\n**Approach:**\n* Start critical path items early\n* Build in buffer time\n* Be realistic about capacity\n* Iterate after launch\n== 14. Final Notes ==\n**Don't Let Perfect Be the Enemy of Good:**\nYou don't need:\n* Complex infrastructure\n* Large team\n* Expensive tools\nYou do need:\n* Legal compliance\n* Working functionality\n* Clear communication\n**You can launch with:**\n* Yourself initially\n* Basic infrastructure\n* MVP implementation\n* Free/low-cost tools\n* Volunteers for help\n**Focus on:**\n* Legal requirements (non-negotiable)\n* Core functionality (working > perfect)\n* Good documentation (for future team)\n* Clear communication (honest about solo start)\n**Scale when:**\n* You have users\n* You have validation\n* Team grows naturally\n* Revenue supports it\n== 15. Version History ==\n* **V0.9.30** (2025-12-17): Adapted for small organization reality\n== 16. Related Documents ==\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n* [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n* [[Finance & Compliance>>FactHarbor.Organisation.Legal and Compliance.Finance and Compliance.WebHome]]\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]\n**Last Updated:** December 17, 2025 \n**Status:** Adapted for solo start with team growth expected", "Organisation.Governance.Organisational Model.WebHome": "= Organisational Model =\nFactHarbor operates as a **Swiss Verein** (non-profit association) with a **simple, flat structure** focused on automation over bureaucracy.\n== 1. Legal Structure ==\n**Entity Type**: Swiss Verein (Association) \n**Jurisdiction**: Switzerland \n**Governed By**: Swiss Civil Code (Art. 60-79)\n**Key Characteristics**:\n* Non-profit purpose\n* Member-based governance\n* Democratic decision-making\n* Limited liability\n* Tax-exempt status (if public benefit)\n== 2. Governance Structure ==\n=== 2.1 General Assembly ===\n**Composition**: All Verein members\n**Powers**:\n* Elect Governing Team\n* Approve annual budget\n* Amend statutes\n* Decide on major strategic changes\n* Dissolve organization\n**Meetings**: Annually (+ extraordinary as needed)\n=== 2.2 Governing Team ===\n**Size**: small group\n**Composition**:\n* Facilitator (chair)\n* Coordinator\n* Treasurer\n* 0-4 additional team members\n**Term**: 2 years (renewable)\n**Powers**:\n* Strategic direction\n* Budget approval\n* Policy decisions\n* Hiring/firing key staff\n* Represent organization externally\n**Meetings**: Quarterly (minimum)\n=== 2.3 Team Member Roles ===\n**Philosophy**: Minimal team, automation-first. Team members improve the SYSTEM, not the DATA.\n**Core Principle**: AKEL makes content decisions. Humans monitor system performance and improve algorithms.\n**Essential Roles**:\n**Technical Coordinator** (1 FTE):\n* **Domain**: AKEL performance and infrastructure\n* **Monitors**: Processing speed, error rates, resource utilization, algorithm performance metrics\n* **Improves**: Algorithms, infrastructure optimization, processing efficiency\n* **Does NOT**: Review individual claim verdicts, override AKEL decisions\n* **Authority**: Approve technical changes within domain boundaries\n* **Decision Method**: Autonomous within domain, consent for cross-domain changes\n**Community Coordinator** (1 FTE):\n* **Domain**: Community health and feedback loop optimization\n* **Monitors**: User feedback patterns, contribution quality trends, community engagement\n* **Improves**: Onboarding processes, documentation, feedback mechanisms, community guidelines\n* **Does NOT**: Moderate individual disputes (AKEL handles), make content decisions\n* **Authority**: Approve community process changes\n* **Decision Method**: Autonomous within domain\n**Moderators** (part-time):\n* **Domain**: Handle AKEL-flagged escalations and system gaming\n* **Monitors**: Items AKEL marks for human review (abuse, manipulation attempts)\n* **Improves**: Detection algorithms based on observed patterns\n* **Does NOT**: Routine content review, override AKEL verdicts\n* **Authority**: Handle flagged items, propose detection improvements\n* **Decision Method**: Autonomous for flagged items, propose improvements to Technical Coordinator\n**Optional Contractors** (as needed):\n* Developers: Specific feature development\n* Data Scientists: Algorithm research and optimization\n* Legal Counsel: Compliance and legal review\n* Accountant: Annual audits\n**Total Core Staff**: full-timeE + part-time moderators\n**Key Distinction**:\n*  Team members monitor AGGREGATE metrics and improve SYSTEMS\n*  Team members do NOT review individual claims or override AKEL\n*  When metrics show problems, fix the ALGORITHM\n*  Do NOT manually correct individual outputs\n=== 2.3.5 Team Roles Diagram ===\n{{include reference=\"FactHarbor.Product Development.Diagrams.Human User Roles.WebHome\"/}}\n== 3. Decision-Making ==\n=== 3.1 Routine Operations ===\n**Handled by**: Technical Coordinator + Community Coordinator\n**Scope**:\n* Day-to-day operations\n* Content moderation (via moderators)\n* Technical maintenance\n* Minor policy clarifications\n**Transparency**: All decisions documented\n=== 3.2 Strategic Decisions ===\n**Handled by**: Governing Team\n**Scope**:\n* Budget allocation\n* Major feature additions\n* Policy changes\n* Partnerships\n* Hiring\n**Process**: Majority vote (quorum: 50%+1)\n=== 3.3 Major Changes ===\n**Handled by**: General Assembly\n**Scope**:\n* Statute amendments\n* Dissolution\n* Merger/acquisition\n* Major strategic pivots\n**Process**: 2/3 majority vote\n== 4. User Community Roles ==\n{{include reference=\"FactHarbor.Organisation.Diagrams.Governance Structure.WebHome\"/}}\n=== 4.1 Readers ===\n**Rights**:\n* Browse content\n* Flag issues\n* Submit claims\n**Responsibilities**:\n* Respect community standards\n* Provide constructive feedback\n=== 4.2 UCM Administrators ===\n**Appointed by Governing Team** to manage system configuration.\n**Requirements**:\n* Appointed by Governing Team\n* Technical understanding of UCM configuration\n**Rights**:\n* Manage UCM configuration (prompt templates, quality thresholds, model selection)\n* View config audit trail and system metrics\n* Trigger re-analysis with updated config\n**Responsibilities**:\n* Maintain system quality through configuration improvements\n* Document config change rationale\n* Monitor quality metrics after config changes\n=== 4.3 Moderators ===\n**Selection**:\n* Appointed by Governing Team\n* Must have high reputation\n* Must have clean track record\n**Rights**:\n* Review flagged content\n* Hide harmful content\n* Issue warnings/bans\n* Resolve disputes\n**Responsibilities**:\n* Impartiality\n* Transparency\n* Documented decisions\n* Timely response\n**Oversight**:\n* Governing Team reviews moderator actions\n* Can overturn decisions\n* Annual performance review\n== 5. Membership ==\n=== 5.1 Verein Membership ===\n**Who can join**:\n* Anyone supporting FactHarbor's mission\n* Age 18+\n* Accept statutes\n**Membership fee**: Symbolic (e.g., CHF 20/year)\n**Rights**:\n* Vote in General Assembly\n* Stand for Governing Team election\n* Access to member-only information\n* Participate in strategic discussions\n**Termination**:\n* Voluntary resignation\n* Non-payment of dues (after warning)\n* Serious violation of principles\n* Decision by General Assembly\n=== 5.2 Contributor Status ===\n**Separate from membership**: Can be contributor without being member\n**Based on**: Activity and reputation\n**No fees**: Open to all\n== 6. Funding Model ==\n=== 6.1 Revenue Sources ===\n**Primary**:\n* Grants from foundations\n* Donations from individuals\n* Institutional partnerships\n**Secondary**:\n* API access fees (commercial users)\n* Data licensing (with open data commitment)\n* Training/consulting services\n**Prohibited**:\n* Advertising\n* Selling user data\n* Pay-for-rating\n* Conflicts of interest\n=== 6.2 Budget Transparency ===\n**Publicly disclosed**:\n* Annual financial statements\n* Major funding sources (>10% of budget)\n* Team Members compensation ranges\n* Major expenses\n**Published**: On website, annually\n== 7. Conflict of Interest Policy ==\n=== 7.1 Governing Team Members & Team Members ===\n**Must disclose**:\n* Financial interests\n* Employment relationships\n* Family connections\n* Other potential conflicts\n**Must recuse** from decisions where conflicted\n=== 7.2 Contributors ===\n**Prohibited**:\n* Direct financial stake in claim outcomes\n* Paid to promote/attack specific claims\n**Required**:\n* Disclosure of other potential conflicts\n* In evaluation history, not in account\n== 8. Accountability Mechanisms ==\n=== 8.1 Internal ===\n* Governing Team oversight of staff\n* Regular audits (financial, security)\n* Performance metrics published\n* Community feedback channels\n* Moderator appeal process\n=== 8.2 External ===\n* Annual report to members\n* Public financial statements\n* Independent audits\n* Media scrutiny\n* Academic research welcome\n=== 8.3 Transparency Commitments ===\n* Open source code\n* Public data exports\n* Documented decisions\n* Algorithm transparency\n* Quality metrics dashboard\n== 9. Dispute Resolution ==\n=== 9.1 User Disputes ===\n**Level 1**: Moderator decision \n**Level 2**: Appeal to different moderator \n**Level 3**: Governing Team review (final)\n**Timeline**: Reasonable timeframe\n=== 9.2 Team Members/Governing Team Disputes ===\n**Internal**: Direct discussion \n**Mediation**: If needed, external mediator \n**Arbitration**: If unresolved, Swiss arbitration\n=== 9.3 Legal Disputes ===\n**Jurisdiction**: Swiss courts \n**Applicable law**: Swiss law \n**Venue**: Zrich\n== 10. Succession Planning ==\n=== 10.1 Governing Team Succession ===\n* Staggered terms (not all expire simultaneously)\n* Nominating committee identifies candidates\n* General Assembly elects\n* Transitional handover period\n=== 10.2 Team Members Succession ===\n* Key staff document their processes\n* Cross-training where possible\n* External recruitment if needed\n* Contractor support during transitions\n=== 10.3 Organizational Continuity ===\n* Documented procedures\n* Automated systems reduce key person risk\n* Regular backups and documentation\n* Emergency response plan\n== 11. Evolution & Adaptation ==\n=== 11.1 Regular Reviews ===\n**Periodic reviews**:\n* Structure assessment\n* Process improvements\n* Policy updates\n* Strategic planning\n**Major reviews**:\n* Comprehensive review\n* Major reforms if needed\n* Statutes amendments\n=== 11.2 Community Input ===\n* Open RFC (Request for Comments) process\n* Public discussion forums\n* Surveys and feedback\n* Trial periods for major changes\n=== 11.3 Emergency Changes ===\n**Governing Team can act quickly** for:\n* Security issues\n* Legal compliance\n* Critical bugs\n* Abuse crises\n**Must be ratified** by General Assembly at next meeting\n== 12. Related Pages ==\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]\n* [[Legal Framework>>FactHarbor.Organisation.Legal and Compliance.Legal Framework.WebHome]]\n* [[Open Source Model>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]", "Organisation.Governance.Roles and Bodies.WebHome": "= Roles & Bodies =\n== 1. Overview ==\nThis page summarises the main roles and bodies that appear in the Organisation domain.\nIn a small organisation, one person may hold several of these roles at the same time.\n== 2. Executive Roles (Leads) ==\n* **Executive Lead**  Ensures coherence across all domains, oversees implementation of Governance decisions, and represents the organisation externally where appropriate.\n* **Research & Development Lead**  Owns the technical architecture, data model, and quality of reasoning ALGORITHMS and models (the systems that evaluate claims, not the evaluation outputs).\n* **Organisation Lead**  Maintains organisational documentation, governance rules, contributor processes, and the XWiki structure.\n* **PR & Care & Marketing Lead**  Oversees communication, user support, campaigns, and community-facing material while respecting neutrality rules.\n* **Finance & Compliance Lead**  Maintains the financial ledger, budgeting, reporting, and the conflict-of-interest register.\n== 3. Governance Bodies ==\n* **Governing Team**  Strategic oversight, compliance monitoring, and resolution of escalated issues.\n* **Governance Steward**  Safeguards neutrality, transparency, and fairness of procedures.\n== 4. Support Roles ==\nExamples of support roles that may be added as the project grows:\n* **Infrastructure Manager**  Coordinates hosting, deployment, monitoring, and backups.\n* **Repository Steward**  Keeps repositories structured, tagged, and consistent.\n* **Process Support Specialist**  Helps contributors follow defined workflows and improves processes.\n* **Onboarding Coordinator**  Supports new contributors with documentation and introductory sessions.\n== 5. Advisory Roles ==\n* **Legal Advisor**\n* **Ethics Advisor**\n* **Scientific / Domain Advisors**\nThese advisory roles provide input and critique but do not replace the formal responsibilities of the Governing Team or Executives.\n== 5.5 Domain Concept (from [[Sociocracy 3.0>>https://sociocracy30.org/]]) ==\nEach role has a **domain** - a clearly defined area of authority and responsibility.\n**Domain definition includes**:\n* **Purpose**: Why this domain exists\n* **Responsibilities**: What this role is accountable for\n* **Dependencies**: What this role needs from others\n* **Authority**: What decisions this role can make autonomously\n* **Constraints**: What boundaries limit this authority\n**Examples**:\n**Technical Coordinator Domain**:\n* Purpose: Ensure AKEL performs optimally\n* Responsibilities: System performance, infrastructure, algorithm improvements\n* Dependencies: Performance metrics from monitoring, improvement suggestions from community\n* Authority: Approve technical changes that don't affect policy\n* Constraints: Must use consent-based decision for policy-affecting changes\n**Community Coordinator Domain**:\n* Purpose: Enable effective community contribution\n* Responsibilities: Documentation, onboarding, feedback loops, community health\n* Dependencies: User feedback, contribution patterns\n* Authority: Approve community process changes\n* Constraints: Cannot change technical systems or policies\n**Key principle**: Clear domains prevent overlaps, enable autonomous decisions, and clarify escalation paths.\n== 6. Principles for Role Design ==\n* Responsibilities must be written down and accessible.\n* The same person may hold several roles, but conflicts of interest must be declared.\n* For sensitive areas (finance, governance, moderation) there should always be at least one other person able to review or approve critical actions.", "Organisation.Governance.Transition Model.WebHome": "= Transition Model =\n(% class=\"box warningmessage\" %)\n(((\n**Current Status: Phase 0 (Pre-Alpha / Founder Mode)**\nFactHarbor is currently in an early setup phase. Processes described here are being established.\n)))\n== 1. Purpose ==\nThe Transition Model explains how FactHarbor evolves from an early experimental phase to a more stable organisation and, eventually, to a federated ecosystem.\n== 2. Phases (Illustrative) ==\n* **Prototype / Proof of Concept**  Very small core team; focus on validating the basic approach.\n* **Beta**  More users and contributors; governance and processes become clearer and more formal.\n* **Release 1.x**  Stable operation, documented contributor processes, and clearer separation of roles.\n* **Federated Phase**  Multiple instances or partners adopt the model; shared governance principles across a network.\nThe exact naming and number of phases can be adapted later, but the idea of deliberate transitions should remain.\n== 3. Transition Principles ==\n* Preserve mission and neutrality during growth.\n* Document which responsibilities move from individuals to shared bodies.\n* Make role changes and new structures transparent to contributors and users.\n* Keep early experimental material accessible as historical context.\n== 4. Practical Use ==\nThis page is intentionally lightweight. More detailed transition plans and roadmaps will be held in project management tools or dedicated XWiki pages when needed.", "Organisation.Governance.WebHome": "= Governance =\nFactHarbor governance is **simple, transparent, flat, and focused on enabling automation**.\n{{include reference=\"FactHarbor.Organisation.Diagrams.Governance Structure.WebHome\"/}}\n== 1. Governance Philosophy ==\n**Core Principles**:\n* **Automation over bureaucracy**: Minimize manual processes\n* **Transparency by default**: Open decision-making\n* **Community input**: Listen but decide decisively\n* **Measured outcomes**: Data drives decisions\n* **Adaptive structure**: Evolve as needed\n== 2. Organizational Structure ==\n**Flat Cooperative Model**:\n* Small organization with collaborative teamwork\n* No hierarchical management layers\n* Decisions by consensus when possible, voting when needed\n* Everyone contributes across multiple areas\n**General Assembly**  **Governing Team**  **Team Members** \n=== 2.1 General Assembly (All Members) ===\nDecides: Governing Team election, statutes, major strategic changes, budget\nMeets: Annually\n=== 2.2 Governing Team ===\nDecides: Strategy, policy, budget allocation, hiring\nMeets: Quarterly\nSize: small group (Facilitator, Coordinator, Treasurer + others)\n=== 2.3 Team Members ===\n* **Technical Coordinator**: AKEL & infrastructure\n* **Community Coordinator**: Moderators & contributors\n* **Moderators** (part-time): Handle abuse/disputes\n== 3. Decision Authority ==\n**Day-to-day**: Technical Coordinator + Community Coordinator\n**Tactical**: Governing Team (simple majority)\n**Strategic**: General Assembly (2/3 majority)\n== 4. Policy Development ==\n**RFC Process** (Request for Comments):\n1. Anyone drafts proposal\n2. Community discussion \n3. Governing Team reviews and votes\n4. Decision published\n**Emergency changes**: Technical Coordinator can act immediately, Governing Team ratifies later\n== 5. Financial Governance ==\n* Annual budget approved by General Assembly\n* Two-signature requirement for >CHF 5,000\n* Governing Team approval for >CHF 20,000\n* Quarterly financial reports\n* Annual independent audit\n== 6. Automation Governance ==\n**Core Principle**: AKEL makes content decisions. Humans make system decisions.\n=== 6.1 Decision Boundary ===\n**What AKEL Decides (Automated)**:\n* All claim verdicts and confidence scores\n* All evidence assessments and relevance scores\n* All source track record scores\n* All risk tier classifications\n* All publication decisions\n* All scenario extractions\n**Rationale**: These decisions must be automated for scale, consistency, transparency, and to avoid human bias. Humans cannot process millions of claims reliably.\n**Human Role**: Monitor aggregate performance metrics, identify systematic issues, improve algorithms.\n**What Humans Decide**:\n**Strategic Decisions** (General Assembly, 2/3 majority):\n* Mission and values\n* Risk tier policy definitions\n* Major architectural changes\n* Budget allocation\n* Dissolution\n**Tactical Decisions** (Governing Team, simple majority):\n* Algorithm parameter ranges (within policy)\n* Infrastructure investments\n* Hiring and role assignments\n* Community policies\n* Partnership agreements\n**Operational Decisions** (Domain Owners, autonomous):\n* Technical Coordinator: AKEL performance optimizations, infrastructure changes\n* Community Coordinator: Community process improvements, documentation\n* Moderators: Handling AKEL-flagged items, detection improvement proposals\n**Emergency Decisions** (any team member, ratified by Governing Team):\n* Critical security issues\n* Legal compliance requirements\n* Immediate safety concerns\n=== 6.2 Principle: Fix the System, Not the Data ===\n**When AKEL makes a \"wrong\" decision**:\n*  Do NOT manually override that specific verdict\n*  DO investigate: Is this a systematic issue?\n*  DO improve: Change algorithm/policy to handle such cases better\n*  DO test: Validate improvement on historical data\n*  DO deploy: Roll out improved system\n*  DO monitor: Check if metrics improve\n**Example**:\n* Bad: \"AKEL rated this source too low, I'll manually boost it\"\n* Good: \"AKEL consistently under-rates peer-reviewed sources. Let's adjust the scoring algorithm to weight peer-review more heavily.\"\n=== 6.3 Governance of AKEL ===\n**Periodic Performance Review**:\n* Who: Governing Team + Technical Coordinator\n* What: Review AKEL performance metrics, bias audits, user feedback patterns\n* Output: Performance report, improvement priorities, policy updates if needed\n**Performance Metrics Monitored**:\n* Processing speed (P50, P95, P99)\n* Success rate and error rate\n* Evidence completeness\n* Confidence score distribution\n* User feedback (helpful/unhelpful ratio)\n* Bias indicators (by domain, source type, etc.)\n**Triggers for Policy Review**:\n* Metrics outside acceptable ranges\n* Systematic bias detected\n* Major user complaints about fairness\n* Legal/compliance concerns\n* New domains requiring special handling\n**Algorithm Change Process**:\n1. Identify issue from metrics\n2. Propose solution (RFC - Request for Comments)\n3. Test in staging environment\n4. Measure impact on historical data\n5. Technical Coordinator approves (or escalates to Governing Team for policy changes)\n6. Deploy with monitoring\n7. Evaluate results\n=== 6.4 Human Intervention Criteria ===\n**Legitimate reasons to intervene**:\n*  AKEL explicitly flags item for human review\n*  System metrics show performance degradation\n*  Legal/safety issue requires immediate action\n*  User reports reveal systematic bias pattern\n**Illegitimate reasons** (system improvement needed instead):\n*  \"I disagree with this verdict\"  Improve algorithm\n*  \"This source should rank higher\"  Improve scoring rules\n*  \"Manual quality gate before publication\"  Defeats purpose of automation\n*  \"I know better than the algorithm\"  Then improve the algorithm\n=== 6.5 Consent-Based Decision Making ===\n**For system changes**, use consent not consensus (from [[Sociocracy 3.0>>https://sociocracy30.org/]]):\n**Consent** = No principled objections\n* Faster than consensus\n* Respects concerns without requiring full agreement\n* \"I can live with this and support it\"\n**Process**:\n1. Proposal presented (RFC)\n2. Clarifying questions\n3. Reactions and concerns\n4. Proposer integrates feedback\n5. Consent round: Any principled objections?\n6. If no objections  Decision made\n7. If objections  Integrate and repeat\n**Use for**:\n* Algorithm changes\n* Policy updates\n* Infrastructure investments\n* Process changes\n**Not for**:\n* Strategic decisions (use voting)\n* Emergency decisions (use autonomous authority)\n== 7. Transparency & Accountability ==\n**Always public**: Policies, structure, board membership, financials, quality metrics, major decisions\n**Published periodically**: Activity reports, metrics, moderation stats\n**Internal documentation**: All meetings, decisions, actions retained indefinitely\n== 7. Conflict Resolution ==\n**User disputes**: Moderator  Appeal to different moderator  Governing Team (final)\n**Timeline**: Reasonable timeframe\n== 8. Moderation Oversight ==\n**Moderator requirements**: high reputation, 6+ months active, clean record\n**Review**: Periodically by Community Coordinator and Governing Team\n**Appeal**: Any user can appeal promptly\n== 9. Code of Conduct ==\n**Governing Team**: Act in org interest, disclose conflicts, maintain confidentiality\n**Team Members**: Follow procedures, professional conduct, document decisions\n**Moderators**: Impartial decisions, respect privacy, respond timely\n== 10. Amendment Process ==\n**Minor changes**: Governing Team decision\n**Major changes**: General Assembly (2/3 vote)\n**Emergency**: Governing Team can act, must ratify at next Assembly\n== 11. Related Pages ==\n* [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]]\n* [[Legal Framework>>FactHarbor.Organisation.Legal and Compliance.Legal Framework.WebHome]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]", "Organisation.How-We-Work-Together.Consent-Based-Decision-Making": "= Consent-Based Decision Making =\n**From [[Sociocracy 3.0>>https://sociocracy30.org/]]**: A decision-making method for system changes and policies.\n== 1. What is Consent? ==\n**Consent** = No principled objections to a proposal.\n**This is NOT**:\n*  Consensus (everyone must agree)\n*  Majority voting (51% wins)\n*  Unanimity (everyone must enthusiastically support)\n**This IS**:\n*  \"I can live with this and support it\"\n*  \"Good enough for now, safe enough to try\"\n*  Respects concerns without requiring full agreement\n== 2. Why Consent over Consensus? ==\n**Consensus problems**:\n* Takes too long\n* One person can block everything\n* Encourages compromise that satisfies no one\n* Discourages bold proposals\n**Consent advantages**:\n*  Faster decisions\n*  Respects principled objections\n*  Encourages experimentation\n*  \"Good enough for now\" mindset\n*  Can evolve decisions over time\n**Example**:\n* Consensus: \"Everyone must love this algorithm change\"  Takes weeks, diluted solution\n* Consent: \"Can everyone support trying this?\"  Decision in days, learn from results\n== 3. What is a Principled Objection? ==\n**Principled objection** = Shows that proposal would:\n* Harm the organization\n* Violate core principles\n* Create unacceptable risk\n* Block achieving objectives\n**Valid objections**:\n*  \"This would violate our transparency principle\"\n*  \"This creates security vulnerability\"\n*  \"Our metrics show this will worsen performance\"\n*  \"This conflicts with existing policy X\"\n**Invalid objections** (preferences, not principles):\n*  \"I don't like this approach\"\n*  \"I would do it differently\"\n*  \"This is not perfect\"\n*  \"I'm uncomfortable with change\"\n**Key question**: \"Does this make things worse, or just not as good as your preferred solution?\"\n== 4. Consent Decision-Making Process ==\n=== Step 1: Proposal Presentation ===\n**Proposer presents**:\n* What problem are we solving?\n* What is the proposal?\n* Why this approach?\n* What are the trade-offs?\n**Time**: 5-10 minutes\n=== Step 2: Clarifying Questions ===\n**Purpose**: Understand the proposal, not evaluate it yet\n**Questions like**:\n* \"How does this affect X?\"\n* \"What happens if Y?\"\n* \"Can you explain Z?\"\n**NOT yet**:\n* Concerns\n* Suggestions\n* Opinions\n**Time**: 5-15 minutes\n=== Step 3: Reactions & Brief Discussion ===\n**Each person shares**:\n* Initial thoughts\n* Potential concerns\n* Suggested improvements\n**Proposer listens**, doesn't defend yet.\n**Time**: 10-20 minutes\n=== Step 4: Amend & Clarify ===\n**Proposer**:\n* Integrates feedback\n* Clarifies misunderstandings\n* May amend proposal\n* Or explains why not\n**Time**: 5-10 minutes\n=== Step 5: Consent Round ===\n**Each person states**:\n* \"I consent\" (no principled objections)\n* \"I have an objection: [specific concern]\"\n**Go around circle systematically**.\n**Time**: 5 minutes\n=== Step 6: Integrate Objections ===\n**If objections raised**:\n* Discuss each objection\n* Is it principled? (facilitator decides if unclear)\n* How can we integrate it?\n* Amend proposal\n**Then repeat consent round**.\n**Time**: Variable (10-30 minutes per objection)\n=== Step 7: Celebrate & Document ===\n**When consent achieved**:\n*  Decision documented\n*  Next steps assigned\n*  Timeline set\n*  Success metrics defined\n*  Review date scheduled\n**Decision record created** (see [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]]).\n== 5. When to Use Consent ==\n**Use consent for**:\n*  Algorithm changes\n*  Policy updates\n*  Infrastructure investments\n*  Process changes\n*  Role assignments\n*  Community guidelines\n**Don't use consent for**:\n*  Strategic decisions  Use voting (Governing Team or General Assembly)\n*  Emergency decisions  Use autonomous authority\n*  Routine operations  Use autonomous authority within domain\n*  Content decisions  AKEL decides (not humans at all)\n== 6. Roles in Consent Process ==\n=== Proposer ===\n* Presents proposal\n* Answers questions\n* Integrates feedback\n* Amends as needed\n=== Facilitator ===\n* Guides process\n* Keeps time\n* Determines if objections are principled\n* Ensures everyone heard\n* Remains neutral\n=== Participants ===\n* Listen actively\n* Ask clarifying questions\n* Share concerns honestly\n* Support decision once made\n== 7. Tips for Good Proposals ==\n**Make proposals**:\n*  Specific and actionable\n*  Time-bound (try for 3 months, then review)\n*  Measurable (how do we know if it works?)\n*  Reversible if possible\n*  Good enough for now, safe enough to try\n**Avoid**:\n*  Vague aspirations\n*  Permanent, unchangeable decisions\n*  Trying to be perfect\n*  Solving every possible edge case\n**Example of good proposal**:\n\"Let's adjust the source scoring algorithm to weight peer-review 20% higher for 3 months and monitor the quality metrics. If evidence completeness doesn't improve by >10%, we'll revert.\"\n== 8. Tips for Good Objections ==\n**Raise objections that**:\n*  Are specific: \"This will cause X problem\"\n*  Are principled: \"This violates Y principle\"\n*  Suggest alternatives: \"What if we instead...\"\n*  Are about harm, not perfection\n**Avoid objections that are**:\n*  Personal preferences\n*  Fear of change\n*  Desire for perfect solution\n*  \"Not invented here\" syndrome\n**Ask yourself**: \"Will this harm the organization, or just not be my preferred approach?\"\n== 9. After the Decision ==\n**Everyone who participated must**:\n*  Support the decision publicly\n*  Give it a genuine try\n*  Provide constructive feedback\n*  Not undermine the decision\n**Can you still disagree?**: Yes, internally. But externally, support it.\n**Can you revisit?**: Yes, at the scheduled review date, or if metrics show problems.\n== 10. Examples ==\n=== Example 1: Algorithm Change ===\n**Proposal**: \"Increase evidence extraction timeout from 5s to 10s to capture more evidence from slow sites.\"\n**Process**:\n1. Presentation: Explained problem (missing evidence), solution, trade-off (slower processing)\n2. Questions: \"How many claims affected?\" \"What's CPU impact?\"\n3. Reactions: Concern about processing time, suggestion to A/B test first\n4. Amended: Added A/B test requirement, success metric\n5. Consent round: All consent\n6. Result: Approved with A/B test requirement\n=== Example 2: Policy Change ===\n**Proposal**: \"Add new risk tier A+ for medical life-or-death claims requiring expert review.\"\n**Process**:\n1. Presentation: Explained need, proposed criteria\n2. Questions: \"Who are experts?\" \"How many claims affected?\"\n3. Reactions: Objection - \"This creates human approval gate, violates automation principle\"\n4. Discussion: Valid principled objection\n5. Amended: \"A+ tier requires AKEL to be more conservative (higher evidence bar), not human approval\"\n6. Consent round: All consent\n7. Result: Approved with automation preserved\n=== Example 3: Failed Consensus, Successful Consent ===\n**Scenario**: Choosing between two database optimization approaches\n**Consensus attempt**:\n* Team split 50/50\n* Argued for weeks\n* No decision\n**Consent approach**:\n* Proposer: \"Let's try approach A for 2 months, monitor query times. If not 20% faster, we try B.\"\n* Consent round: Team B supporters say \"I prefer B, but can support trying A first with clear metrics.\"\n* Result: Decision in one meeting\n== 11. Common Pitfalls ==\n**Pitfall 1**: \"Silent consent\"\n* Problem: People consent but don't actually support\n* Solution: Facilitator explicitly asks each person\n**Pitfall 2**: \"Too perfect\"\n* Problem: Trying to address every edge case before deciding\n* Solution: \"Good enough for now, safe enough to try\"\n**Pitfall 3**: \"Preference as objection\"\n* Problem: Personal preferences disguised as principled objections\n* Solution: Facilitator asks \"How does this harm the organization?\"\n**Pitfall 4**: \"Never revisiting\"\n* Problem: Treat consent decisions as permanent\n* Solution: Always include review date in decision\n== 12. Integration with FactHarbor ==\n**Applied to**:\n* Technical Coordinator decisions (within domain)  Autonomous\n* Cross-domain technical decisions  Consent with affected coordinators\n* Policy changes  Consent with Governing Team\n* Major strategic changes  Voting (General Assembly)\n**See also**:\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - Overall governance structure\n* [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]] - Types of decisions\n* [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] - How to propose changes", "Organisation.How-We-Work-Together.Continuous-Improvement": "= Continuous Improvement =\n**From [[Sociocracy 3.0>>https://sociocracy30.org/]]**: Empirical approach to improving FactHarbor systems.\n== 1. Philosophy ==\n**Continuous improvement** means:\n* We're never \"done\" - systems always improve\n* Learn from data, not opinions\n* Small experiments, frequent iteration\n* Measure everything\n* Build, measure, learn, repeat\n**Inspired by**:\n* [[Sociocracy 3.0>>https://sociocracy30.org/]] empiricism principle\n* Agile/lean methodologies\n* Scientific method\n* DevOps continuous deployment\n== 2. What We Improve ==\n=== 2.1 AKEL Performance ===\n**Processing speed**:\n* Faster claim parsing\n* Optimized evidence extraction\n* Efficient source lookups\n* Reduced latency\n**Quality**:\n* Better evidence detection\n* More accurate verdicts\n* Improved source scoring\n* Enhanced contradiction detection\n**Reliability**:\n* Fewer errors\n* Better error handling\n* Graceful degradation\n* Faster recovery\n=== 2.2 Policies ===\n**Risk tier definitions**:\n* Clearer criteria\n* Better domain coverage\n* Edge case handling\n**Evidence weighting**:\n* More appropriate weights by domain\n* Better peer-review recognition\n* Improved recency handling\n**Source scoring**:\n* More nuanced credibility assessment\n* Better handling of new sources\n* Domain-specific adjustments\n=== 2.3 Infrastructure ===\n**Performance**:\n* Database optimization\n* Caching strategies\n* Network efficiency\n* Resource utilization\n**Scalability**:\n* Handle more load\n* Geographic distribution\n* Cost efficiency\n**Monitoring**:\n* Better dashboards\n* Faster alerts\n* More actionable metrics\n=== 2.4 Processes ===\n**Contributor workflows**:\n* Easier onboarding\n* Clearer documentation\n* Better tools\n**Decision-making**:\n* Faster decisions\n* Better documentation\n* Clearer escalation\n== 3. Improvement Cycle ==\n=== 3.1 Observe ===\n**Continuously monitor**:\n* Performance metrics dashboards\n* User feedback patterns\n* AKEL processing logs\n* Error reports\n* Community discussions\n**Look for**:\n* Metrics outside acceptable ranges\n* Systematic patterns in errors\n* User pain points\n* Opportunities for optimization\n=== 3.2 Analyze ===\n**Dig deeper**:\n* Why is this metric problematic?\n* Is this a systematic issue or one-off?\n* What's the root cause?\n* What patterns exist?\n* How widespread is this?\n**Tools**:\n* Data analysis (SQL queries, dashboards)\n* Code profiling\n* A/B test results\n* User interviews\n* Historical comparison\n=== 3.3 Hypothesize ===\n**Propose explanation**:\n* \"We believe X is happening because Y\"\n* \"If we change Z, we expect W to improve\"\n* \"The root cause is likely A, not B\"\n**Make testable**:\n* What would prove this hypothesis?\n* What would disprove it?\n* What metrics would change?\n=== 3.4 Design Solution ===\n**Propose specific change**:\n* Algorithm adjustment\n* Policy clarification\n* Infrastructure upgrade\n* Process refinement\n**Consider**:\n* Trade-offs\n* Risks\n* Rollback plan\n* Success metrics\n=== 3.5 Test ===\n**Before full deployment**:\n* Test environment deployment\n* Historical data validation\n* A/B testing if feasible\n* Load testing if infrastructure\n**Measure**:\n* Did metrics improve as expected?\n* Any unexpected side effects?\n* Is the improvement statistically significant?\n=== 3.6 Deploy ===\n**Gradual rollout**:\n* Deploy to small % of traffic first\n* Monitor closely\n* Increase gradually if successful\n* Rollback if problems\n**Deployment strategies**:\n* Canary (1%  5%  25%  100%)\n* Blue-green (instant swap with rollback ready)\n* Feature flags (enable for specific users first)\n=== 3.7 Evaluate ===\n**After deployment**:\n* Review metrics - did they improve?\n* User feedback - positive or negative?\n* Unexpected effects - any surprises?\n* Lessons learned - what would we do differently?\n=== 3.8 Iterate ===\n**Based on results**:\n* If successful: Document, celebrate, move to next improvement\n* If partially successful: Refine and iterate\n* If unsuccessful: Rollback, analyze why, try different approach\n**Document learnings**: Update RFC with actual outcomes.\n== 4. Improvement Cadence ==\n=== 4.1 Continuous (Ongoing) ===\n**Continuous**:\n* Monitor dashboards\n* Review user feedback\n* Identify emerging issues\n* Quick fixes and patches\n**Who**: Technical Coordinator, Community Coordinator\n=== 4.2 Sprint Cycles ===\n**Each sprint**:\n* Sprint planning: Select improvements to tackle\n* Implementation: Build and test\n* Sprint review: Demo what was built\n* Retrospective: How can we improve the improvement process?\n**Who**: Core team + regular contributors\n=== 4.3 Periodic Reviews ===\n**Periodically**:\n* Comprehensive performance review\n* Policy effectiveness assessment\n* Strategic improvement priorities\n* Architectural decisions\n**Who**: Governing Team + Technical Coordinator\n**Output**: Review report, next improvement priorities\n=== 4.4 Strategic Planning ===\n**At strategic intervals**:\n* Major strategic direction\n* Significant architectural changes\n* Multi-quarter initiatives\n* Budget allocation\n**Who**: General Assembly\n== 5. Metrics-Driven Improvement ==\n=== 5.1 Key Performance Indicators (KPIs) ===\n**AKEL Performance**:\n* Processing time (P50, P95, P99)\n* Success rate\n* Evidence completeness\n* Confidence distribution\n**Content Quality**:\n* User feedback (helpful/unhelpful ratio)\n* Contradiction rate\n* Source diversity\n* Scenario coverage\n**System Health**:\n* Uptime\n* Error rate\n* Response time\n* Resource utilization\n**See**: [[System Performance Metrics>>FactHarbor.Product Development.Specification.System-Performance-Metrics]]\n=== 5.2 Targets and Thresholds ===\n**For each metric**:\n* Target: Where we want to be\n* Acceptable range: What's OK\n* Alert threshold: When to intervene\n* Critical threshold: Emergency\n**Example**:\n* Processing time P95\n * Target: 15 seconds\n * Acceptable: 10-18 seconds\n * Alert: >20 seconds\n * Critical: >30 seconds\n=== 5.3 Metric-Driven Decisions ===\n**Improvements prioritized by**:\n* Impact on metrics\n* Effort required\n* Risk level\n* Strategic importance\n**Not by**:\n* Personal preferences\n* Loudest voice\n* Political pressure\n* Gut feeling\n== 6. Experimentation ==\n=== 6.1 A/B Testing ===\n**When feasible**:\n* Run two versions simultaneously\n* Randomly assign users/claims\n* Measure comparative performance\n* Choose winner based on data\n**Good for**:\n* Algorithm parameter tuning\n* UI/UX changes\n* Policy variations\n=== 6.2 Canary Deployments ===\n**Small-scale first**:\n* Deploy to 1% of traffic\n* Monitor closely for issues\n* Gradually increase if successful\n* Full rollback if problems\n**Benefits**:\n* Limits blast radius of failures\n* Real-world validation\n* Quick feedback loop\n=== 6.3 Feature Flags ===\n**Controlled rollout**:\n* Deploy code but disable by default\n* Enable for specific users/scenarios\n* Gather feedback before full release\n* Easy enable/disable without redeployment\n== 7. Retrospectives ==\n=== 7.1 Sprint Retrospectives ===\n**Questions**:\n* What went well?\n* What could be improved?\n* What will we commit to improving?\n**Format** (30 minutes):\n* Gather data: Everyone writes thoughts (5 min)\n* Generate insights: Discuss patterns (15 min)\n* Decide actions: Pick 1-3 improvements (10 min)\n**Output**: 1-3 concrete actions for next sprint\n=== 7.2 Project Retrospectives (After major changes) ===\n**After significant changes**:\n* What was the goal?\n* What actually happened?\n* What went well?\n* What went poorly?\n* What did we learn?\n* What would we do differently?\n**Document**: Update project documentation with learnings\n=== 7.3 Incident Retrospectives (After failures) ===\n**After incidents/failures**:\n* Timeline: What happened when?\n* Root cause: Why did it happen?\n* Impact: What was affected?\n* Response: How did we handle it?\n* Prevention: How do we prevent this?\n**Blameless**: Focus on systems, not individuals.\n**Output**: Action items to prevent recurrence\n== 8. Knowledge Management ==\n=== 8.1 Documentation ===\n**Keep updated**:\n* Architecture docs\n* API documentation\n* Operational runbooks\n* Decision records\n* Retrospective notes\n**Principle**: Future you/others need to understand why decisions were made.\n=== 8.2 Decision Records ===\n**For significant decisions, document**:\n* What was decided?\n* What problem does this solve?\n* What alternatives were considered?\n* What are the trade-offs?\n* What are the success metrics?\n* Review date?\n**See**: [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]]\n=== 8.3 Learning Library ===\n**Collect**:\n* Failed experiments (what didn't work)\n* Successful patterns (what worked well)\n* External research relevant to FactHarbor\n* Best practices from similar systems\n**Share**: Make accessible to all contributors\n== 9. Continuous Improvement of Improvement ==\n**Meta-improvement**: Improve how we improve.\n**Questions to ask**:\n* Is our improvement cycle effective?\n* Are we measuring the right things?\n* Are decisions actually data-driven?\n* Is knowledge being captured?\n* Are retrospectives actionable?\n* Are improvements sustained?\n**Periodically review**: How can our improvement process itself improve?\n== 10. Cultural Practices ==\n=== 10.1 Safe to Fail ===\n**Encourage experimentation**:\n*  Try new approaches\n*  Test hypotheses\n*  Learn from failures\n*  Share what didn't work\n**Not blame**:\n*  \"Who broke it?\"\n*  \"Why didn't you know?\"\n*  \"This was a stupid idea\"\n**Instead**:\n*  \"What did we learn?\"\n*  \"How can we prevent this?\"\n*  \"What will we try next?\"\n=== 10.2 Data Over Opinions ===\n**Settle debates with**:\n*  Metrics and measurements\n*  A/B test results\n*  User feedback data\n*  Performance benchmarks\n**Not with**:\n*  \"I think...\"\n*  \"In my experience...\"\n*  \"I've seen this before...\"\n*  \"Trust me...\"\n=== 10.3 Bias Toward Action ===\n**Good enough for now, safe enough to try**:\n* Don't wait for perfect solution\n* Test and learn\n* Iterate quickly\n* Prefer reversible decisions\n**But not reckless**:\n* Do test before deploying\n* Do monitor after deploying\n* Do have rollback plan\n* Do document decisions\n== 11. Tools and Infrastructure ==\n**Support continuous improvement with**:\n**Monitoring**:\n* Real-time dashboards\n* Alerting systems\n* Log aggregation\n* Performance profiling\n**Testing**:\n* Automated testing (unit, integration, regression)\n* Test environments\n* A/B testing framework\n* Load testing tools\n**Deployment**:\n* CI/CD pipelines\n* Canary deployment support\n* Feature flag system\n* Quick rollback capability\n**Collaboration**:\n* RFC repository\n* Decision log\n* Knowledge base\n* Retrospective notes\n---\n**Remember**: Continuous improvement means we're always learning, always testing, always getting better.\n== 12. Related Pages ==\n* [[Automation Philosophy>>FactHarbor.Organisation.Strategy.Automation Philosophy.WebHome]] - Why we automate\n* [[System Performance Metrics>>FactHarbor.Product Development.Specification.System-Performance-Metrics]] - What we measure\n* [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] - How to propose improvements\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - How improvements are approved", "Organisation.How-We-Work-Together.Contributor Processes.WebHome": "= Contributor Processes =\n== 1. Purpose ==\nThis page explains how contributors improve **the system that evaluates claims**, not the claims themselves.\n**Key Principle**: AKEL makes content decisions. Contributors improve the algorithms, policies, and infrastructure that enable AKEL to make better decisions.\n== 2. What Contributors Do ==\nContributors work on **system improvements**, not content review:\n **Algorithm improvements**: Better evidence detection, improved source scoring, enhanced contradiction detection\n **Policy proposals**: Risk tier definitions, domain-specific rules, moderation criteria\n **Infrastructure**: Performance optimization, scaling improvements, monitoring tools\n **Documentation**: User guides, API docs, architecture documentation\n **Testing**: A/B tests, regression tests, performance benchmarks\n== 3. What Contributors Do NOT Do ==\n **Review individual claims for correctness** - That's AKEL's job\n **Override AKEL verdicts** - Fix the algorithm, not the output\n **Manually adjust source scores** - Improve scoring rules systematically\n **Act as approval gates** - Defeats purpose of automation\n **Make ad-hoc content decisions** - All content decisions must be algorithmic\n**If you think AKEL made a mistake**: Don't fix that one case. Fix the algorithm so it handles all similar cases correctly.\n== 4. Contributor Journey ==\n=== 4.1 Visitor ===\n* Reads documentation\n* Explores repositories\n* May open issues reporting bugs or suggesting improvements\n=== 4.2 New Contributor ===\n* First contributions: Documentation fixes, clarifications, minor improvements\n* Learns: System architecture, RFC process, testing procedures\n* Builds: Understanding of FactHarbor principles\n=== 4.3 Regular Contributor ===\n* Contributes regularly to system improvements\n* Follows project rules and RFC process\n* Track record of quality contributions\n=== 4.4 Trusted Contributor ===\n* Extensive track record of high-quality work\n* Deep understanding of system architecture\n* Can review others' contributions\n* Participates in technical decisions\n=== 4.5 Maintainer ===\n* Approves system changes within domain\n* Technical Coordinator or designated by them\n* Authority over specific system components\n* Accountable for system performance in domain\n=== 4.6 Moderator (Separate Track) ===\n* Handles AKEL-flagged escalations\n* Focuses on abuse, manipulation, system gaming\n* Proposes detection improvements\n* Does NOT review content for correctness\n== 4.7 Contributor Roles and Trust Levels ==\nThe following describes the different levels of contributors and their permissions:\n== 1. Purpose ==\nThis page describes how people can participate in FactHarbor and how responsibilities grow with trust and experience.\n== 2. Contributor Journey ==\n1. **Visitor**  explores the platform, reads documentation, may raise questions.\n2. **New Contributor**  submits first improvements (typo fixes, small clarifications, new issues).\n3. **Contributor**  contributes regularly and follows project conventions.\n4. **Trusted Contributor**  has a track record of high-quality work and reliable judgement.\n5. **Contributor**  reviews changes for correctness, neutrality, and process compliance.\n6. **Moderator**  focuses on behaviour, tone, and conflict moderation.\n7. **Trusted Contributor (optional)**  offers domain expertise without changing governance authority.\n== 3. Principles ==\n* Low barrier to entry for new contributors.\n* Transparent criteria for gaining and losing responsibilities.\n* Clear separation between content quality review and behavioural moderation.\n* Documented processes for escalation and appeal.\n== 4. Processes ==\nTypical contributor processes include:\n* proposal and review of documentation or code changes\n* reporting and triaging issues or suspected errors\n* moderation of discussions and conflict resolution\n* onboarding support for new contributors.\nDetails of the process steps are aligned with the [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]] and [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]] pages.\n== 5. System Improvement Workflow ==\n=== 5.1 Identify Issue ===\n**Sources**:\n* Performance metrics dashboard shows anomaly\n* User feedback reveals pattern\n* AKEL processing logs show systematic error\n* Code review identifies technical debt\n**Key**: Focus on PATTERNS, not individual cases.\n=== 5.2 Diagnose Root Cause ===\n**Analysis methods**:\n* Run experiments in test environment\n* Analyze AKEL decision patterns\n* Review algorithm parameters\n* Check training data quality\n* Profile performance bottlenecks\n**Output**: Clear understanding of systematic issue.\n=== 5.3 Propose Solution (RFC) ===\n**Create Request for Comments (RFC)**:\n**RFC Template**:\n{{{\n## Problem Statement\nWhat systematic issue exists? What metrics show it?\n## Proposed Solution\nWhat specific changes to algorithm/policy/infrastructure?\n## Alternatives Considered\nWhat other approaches were evaluated? Why not chosen?\n## Trade-offs\nWhat are downsides? What metrics might worsen?\n## Success Metrics\nHow will we know this works? What metrics will improve?\n## Testing Plan\nHow will this be validated before full deployment?\n## Rollback Plan\nIf this doesn't work, how do we revert?\n}}}\n=== 5.4 Community Discussion ===\n**RFC review period**: 7-appropriate time period (based on impact)\n**Participants**:\n* Other contributors comment\n* Maintainers review for feasibility\n* Technical Coordinator for architectural impact\n* Governing Team for policy implications\n**Goal**: Surface concerns, improve proposal, build consensus\n=== 5.5 Test & Validate ===\n**Required before approval**:\n*  Deploy to test environment\n*  Run on historical data (regression test)\n*  Measure impact on key metrics\n*  A/B testing if feasible\n*  Document results\n**Pass criteria**:\n* Solves stated problem\n* Doesn't break existing functionality\n* Metrics improve or remain stable\n* No unacceptable trade-offs\n=== 5.6 Review & Approval ===\n**Review by**:\n* **Technical changes**: Technical Coordinator (or designated Maintainer)\n* **Policy changes**: Governing Team (consent-based decision)\n* **Infrastructure**: Technical Coordinator\n* **Documentation**: Community Coordinator\n**Approval criteria**:\n* Solves problem effectively\n* Test results positive\n* No principled objections (for consent-based decisions)\n* Aligns with FactHarbor principles\n=== 5.7 Deploy & Monitor ===\n**Deployment strategy**:\n* Gradual rollout (canary deployment)\n* Monitor key metrics closely\n* Ready to rollback if problems\n* Document deployment\n**Monitoring period**: intensive, then ongoing\n**Success indicators**:\n* Target metrics improve\n* No unexpected side effects\n* User feedback positive\n* System stability maintained\n=== 5.8 Evaluate & Iterate ===\n**Post-deployment review**:\n* Did metrics improve as expected?\n* Any unexpected effects?\n* What did we learn?\n* What should we do differently next time?\n**Document learnings**: Update RFC with actual outcomes.\n== 6. Contribution Types in Detail ==\n=== 6.1 Algorithm Improvements ===\n**Examples**:\n* Better evidence extraction from web pages\n* Improved source reliability scoring\n* Enhanced contradiction detection\n* Faster claim parsing\n* More accurate risk classification\n**Process**: RFC  Test  Review  Deploy  Monitor\n**Skills needed**: Python, ML/AI, data analysis, testing\n=== 6.2 Policy Proposals ===\n**Examples**:\n* Risk tier definition refinements\n* New domain-specific guidelines\n* Moderation criteria updates\n* Community behavior standards\n**Process**: RFC  Community discussion  Governing Team consent  Deploy  Monitor\n**Skills needed**: Domain knowledge, policy writing, ethics\n=== 6.3 Infrastructure Improvements ===\n**Examples**:\n* Database query optimization\n* Caching strategy improvements\n* Monitoring tool enhancements\n* Deployment automation\n* Scaling improvements\n**Process**: RFC  Test  Technical Coordinator review  Deploy  Monitor\n**Skills needed**: DevOps, databases, system architecture, performance tuning\n=== 6.4 Documentation ===\n**Examples**:\n* User guides\n* API documentation\n* Architecture documentation\n* Onboarding materials\n* Tutorial videos\n**Process**: Draft  Community feedback  Community Coordinator review  Publish\n**Skills needed**: Technical writing, understanding of FactHarbor\n== 7. Quality Standards ==\n=== 7.1 Code Quality ===\n**Required**:\n*  Follows project coding standards\n*  Includes tests\n*  Documented (code comments + docs update)\n*  Passes CI/CD checks\n*  Reviewed by maintainer\n=== 7.2 Testing Requirements ===\n**Algorithm changes**:\n* Unit tests\n* Integration tests\n* Regression tests on historical data\n* Performance benchmarks\n**Policy changes**:\n* Validation on test cases\n* Impact analysis on existing claims\n* Edge case coverage\n=== 7.3 Documentation Requirements ===\n**All changes must include**:\n* Updated architecture docs (if applicable)\n* Updated API docs (if applicable)\n* Migration guide (if breaking change)\n* Changelog entry\n== 8. Handling Disagreements ==\n=== 8.1 Technical Disagreements ===\n**Process**:\n1. Discuss in RFC comments\n2. Present data/evidence\n3. Consider trade-offs openly\n4. Technical Coordinator makes final decision (or escalates)\n5. Document reasoning\n**Principle**: Data and principles over opinions\n=== 8.2 Policy Disagreements ===\n**Process**:\n1. Discuss in RFC\n2. Clarify principles at stake\n3. Consider stakeholder impact\n4. Governing Team uses consent-based decision\n5. Document reasoning\n**Principle**: Consent-based (not consensus) - can you support this even if not perfect?\n=== 8.3 Escalation Path ===\n**For unresolved issues**:\n* Technical  Technical Coordinator  Governing Team\n* Policy  Governing Team  General Assembly (if fundamental)\n* Behavior  Moderator  Governance Steward  Governing Team\n== 9. Behavior Standards ==\n=== 9.1 Expected Behavior ===\n**Contributors are expected to**:\n*  Assume good faith\n*  Focus on system improvements, not personal opinions\n*  Support decisions once made (even if you disagreed)\n*  Be constructive in criticism\n*  Document your reasoning\n*  Test thoroughly before proposing\n*  Learn from mistakes\n=== 9.2 Unacceptable Behavior ===\n**Will not be tolerated**:\n*  Personal attacks\n*  Harassment or discrimination\n*  Attempting to game the system\n*  Circumventing the RFC process for significant changes\n*  Deploying untested changes to production\n*  Ignoring feedback without explanation\n=== 9.3 Enforcement ===\n**Process**:\n* First offense: Warning + coaching\n* Second offense: Temporary suspension (duration based on severity)\n* Third offense: Permanent ban\n**Severe violations** (harassment, malicious code): Immediate ban\n**Appeal**: To Governance Steward, then Governing Team\n== 10. Recognition ==\n**Contributors are recognized through**:\n* Public acknowledgment in release notes\n* Contribution statistics on profile\n* Special badges for significant contributions\n* Invitation to contributor events\n* Potential hiring opportunities\n**Not recognized through**:\n* Payment (unless contracted separately)\n* Automatic role promotions\n* Special privileges in content decisions (there are none)\n== 11. Getting Started ==\n**New contributors should**:\n1. Read this page + [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]]\n2. Join community forum\n3. Review open issues labeled \"good first issue\"\n4. Start with documentation improvements\n5. Learn the RFC process by observing\n6. Make first contribution\n7. Participate in discussions\n8. Build track record\n**Resources**:\n* Developer guide: [Coming soon]\n* RFC template: [In repository]\n* Community forum: [Link]\n* Slack/Discord: [Link]\n---\n**Remember**: You improve the SYSTEM. AKEL improves the CONTENT.\n== 12. Related Pages ==\n* [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] - Roles and trust levels\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - Decision-making structure\n* [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]] - Team structure\n* [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]] - How decisions are made", "Organisation.How-We-Work-Together.GlobalRules.WebHome": "= Global Rules =\n**Last Updated:** December 17, 2025 \nThese rules apply to all FactHarbor users, content, and processes.\n**FactHarbor's Role:** Collect, aggregate, and summarize information that already exists. FactHarbor does not create original information or conduct investigations.\n== 1. Core Principles ==\n=== 1.1 Seek Truth ===\n* Truth is the primary goal\n* Evaluate existing information fairly\n* Weight evidence by quality, not preference\n* Update understanding as evidence changes\n* Acknowledge uncertainty clearly\n=== 1.2 Act Independently ===\n* Free from obligation to any interest except truth\n* Commercial interests in claim outcomes are disqualifying\n* Disclose conflicts that might bias evaluation\n* Independence from funders and sponsors\n=== 1.3 Be Accountable and Transparent ===\n**Strive to:**\n* Make reasoning traceable where feasible\n* Cite sources whenever possible\n* Disclose conflicts of interest\n* Mark AI involvement\n* Correct mistakes promptly when discovered\n* Explain decisions when challenged\n**Goal:** Maximum transparency within practical constraints. Perfect traceability may not always be achievable, but the commitment to transparency remains.\n=== 1.4 Intellectual Humility ===\n* Acknowledge uncertainty\n* Correct mistakes promptly\n* Respect expertise but verify - anyone/any source could be wrong, biased, or dishonest\n* Welcome disagreement\n* Seek consensus, don't force it\n== 2. Source Evaluation ==\n=== 2.1 Source Evaluation Criteria ===\n**Sources are evaluated by track record, not credentials.**\n**Evaluation Factors:**\n* **Accuracy history:** Has the source been reliable in the past?\n* **Correction policy:** Do they promptly correct errors?\n* **Methodology transparency:** Do they show their work?\n* **Independence:** Are they free from controlling interests?\n* **Trusted Contributorise:** Do they have relevant knowledge?\n* **Consistency:** Do multiple independent sources agree?\n**Track Record Over Title:**\n Government statistics from agencies with documented accuracy (Swiss Federal Statistical Office, etc.)\n Peer-reviewed research from journals with strong retraction/correction practices\n Journalism from outlets with fact-checking and correction policies\n**Red Flags:**\n* Consistent refusal to correct proven errors\n* Conflicts of interest not disclosed\n* Methodology hidden or vague\n* Single-source claims without verification\n* History of misinformation\n* Authoritarian control (government sources)\n**Principle:** Evaluate each source's track record individually. No type of source is automatically trustworthy.\n=== 2.2 Multiple Sources ===\n* Use multiple independent sources when possible\n* Cross-reference claims\n* Note when only single source available\n* Prefer primary sources over secondary\n=== 2.3 Source Attribution ===\n* Always cite sources clearly\n* Link to original when online\n* Include author, publication, date\n* Archive links for unstable URLs\n== 3. Neutrality in Summarization ==\n=== 3.1 Neutral Language ===\n**Descriptive, not prescriptive:**\n*  \"Policy reduced emissions by 15%\"\n**Balanced presentation:**\n* Show competing views proportional to evidence\n* Don't hide inconvenient evidence\n* Weight by quality, not just quantity\n**Precise, not loaded (in FactHarbor summaries):**\n*  \"Protesters\" \n**Note:** Source materials may contain loaded language. Quote exactly when citing sources, but use neutral language in FactHarbor's own summaries and analysis.\n=== 3.2 No Cherry-Picking ===\n* Present full picture\n* Include contrary evidence\n* Show trends, not just selected points\n* Acknowledge limitations\n=== 3.3 Context Matters ===\n* Provide historical context\n* Show comparisons\n* Explain significance\n* Present expert perspective\n=== 3.4 Statistical Literacy ===\n* Distinguish absolute vs percentage\n* Show baselines and comparisons\n* Note confidence intervals\n* Distinguish correlation from causation\n== 4. Accuracy and Corrections ==\n=== 4.1 Accuracy Standards ===\n* Verify facts before using\n* Check spelling, dates, numbers\n* Confirm quotes and attribution\n* Cross-check details\n=== 4.2 Corrections ===\nCorrect errors promptly and prominently:\n{{{\nCORRECTION: [Date] - [What was wrong and correction]\n}}}\n=== 4.3 Updates ===\nWhen new information emerges:\n{{{\nUPDATE: [Date] - [New information]\n}}}\n== 5. Conflicts of Interest ==\n=== 5.1 Commercial Interests Prohibited ===\nContributors with direct commercial interests in claim outcomes cannot contribute to those claims.\n**Disqualifying:**\n* Paid to promote/attack claim\n* Stock/investment affected by verdict\n* Employment contingent on outcome\n* Consulting fees tied to position\n=== 5.2 Other Conflicts - Disclosure Required ===\nDisclose:\n* Personal relationships to subjects\n* Professional stake in conclusion\n* Prior public positions on exact claim\n* Ideological commitments affecting judgment\nFormat:\n{{{\nCOI: [Clear description]\n}}}\n=== 5.3 When to Recuse ===\n* Direct commercial interest (always)\n* Cannot evaluate objectively\n* Appearance of bias would harm credibility\n== 6. Topic Scope ==\n=== 6.1 What's In Scope ===\n**All factual claims where information exists.**\nFactHarbor collects, aggregates, and summarizes available information. FactHarbor does not create information.\n**Examples:**\n* Historical events\n* Scientific findings\n* Policy outcomes\n* Economic data\n* Medical information (factual)\n* Political statements and outcomes\n=== 6.2 What's Out of Scope ===\n* Pure value judgments without factual basis\n* Unfalsifiable metaphysical claims\n* Pure aesthetic judgments\n* Personal preferences\n* Opinions presented as opinions\n== 7. Community Standards ==\n=== 7.1 Respectful Discourse ===\n* Assume good faith\n* Focus on substance\n* Professional tone\n* Patient with disagreement\n=== 7.2 Prohibited ===\n* Personal attacks\n* Insults or harassment\n* Bad faith arguments\n* Gaming the system\n=== 7.3 Constructive Criticism ===\n**When giving feedback:**\n* Be specific\n* Suggest improvements\n* Focus on content, not person\n**When receiving:**\n* Consider merits\n* Update if convinced\n* Respond to substance\n== 8. System Integrity ==\n=== 8.1 No Manipulation ===\nProhibited:\n* Multiple accounts\n* Coordinated campaigns\n* Source fabrication\n* Evidence hiding\n=== 8.2 Automation Disclosure ===\nMark all automated contributions:\n* Human-written\n* AI-assisted\n* AI-generated\n* Fully automated (AKEL only)\n== 9. Enforcement ==\n**Scope:** This section applies only to **FactHarbor contributors and users**, not to external entities being evaluated.\nFactHarbor cannot enforce rules against:\n* Governments\n* Corporations\n* Media outlets\n* External sources\nFor external entities, FactHarbor can only:\n* Evaluate their track record\n* Document their accuracy/inaccuracy\n* Flag unreliability\n* Warn users\n=== 9.1 Progressive Discipline (FactHarbor Participants) ===\n* Warning\n* Temporary suspension\n* Longer suspension\n* Permanent ban\nException: Severe violations may result in immediate ban.\n=== 9.2 Appeals ===\nAppeal with clear grounds and evidence to moderators.\n== 10. Updates ==\nRules evolve with platform. Major changes require notice and community input.\n== 11. Related Documents ==\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n* [[Terms of Service>>FactHarbor.Organisation.How-We-Work-Together.Terms-of-Service]]\n**Version:** 0.9.33 \n**Foundation:** Truth, Transparency, Independence", "Organisation.How-We-Work-Together.Privacy-Policy": "= Privacy Policy =\n**Effective Date:** [To be determined before production launch] \n**Last Updated:** December 17, 2025 (V0.9.29 - Legal Compliance Update)\n== 1. Introduction ==\nFactHarbor is committed to protecting your privacy while maintaining the transparency necessary for our mission of supporting well-grounded, manipulation-resistant judgments.\nThis Privacy Policy explains:\n* What information we collect and why\n* How we use and protect that information\n* Your rights and choices\n* How we balance privacy with transparency\n**Important:** By using FactHarbor services, you agree to this Privacy Policy.\n== 2. Who We Are ==\nFactHarbor is a Swiss non-profit association (Verein) under Swiss law, pursuing tax-exempt status. Our mission is to create a transparent, community-driven platform for evaluating factual claims.\n**Initial Phase:** FactHarbor is a small organization starting with one person, with team growth expected. Contact methods will be established before launch.\n**Contact:**\n* General inquiries: [To be established]\n* Privacy and data requests: [To be established]\n* Data Protection Officer: [To be designated if serving EU users]\n* Swiss Representative: [To be designated before launch]\n== 3. What Information We Collect ==\n=== 3.1 Information You Provide ===\n**Account Information** (if you register):\n* Username (required)\n* Email address (required)\n* Optional profile information you choose to add\n**Contributions** (if you contribute):\n* Content you create (claims, scenarios, verdicts, reviews)\n* Edits and modifications\n* Comments and discussions\n* Flags and quality reports\n**Communications**:\n* Messages you send to us\n* Survey responses\n* Feedback submissions\n=== 3.2 Information We Collect Automatically ===\n**Technical Data**:\n* IP address\n* Browser type and version\n* Operating system\n* Device information\n* Referrer URL\n* Pages viewed and time spent\n**Usage Data**:\n* Features you use\n* Actions you take\n* Search queries\n* Interaction patterns\n**Cookies and Similar Technologies**:\n* Session cookies (essential for functionality)\n* Preference cookies (remember your settings)\n* Analytics cookies (understand usage patterns)\nSee Section 8 for cookie management.\n=== 3.3 Information We DO NOT Collect ===\nWe do not collect:\n* Financial information (no payment processing currently)\n* Biometric data\n* Precise geolocation (only general location from IP)\n* Social security numbers or government IDs\n* Unnecessary personal information\n== 4. How We Use Your Information ==\nWe use collected information only for these purposes:\n=== 4.1 Provide Services ===\n* Create and maintain your account\n* Display your public contributions\n* Enable community features\n* Personalise your experience\n=== 4.2 Maintain Quality and Safety ===\n* Detect and prevent abuse\n* Enforce our Terms of Service\n* Identify and address quality issues\n* Prevent spam and manipulation\n=== 4.3 Improve Services ===\n* Understand how FactHarbor is used\n* Identify bugs and issues\n* Test new features\n* Improve algorithms and quality gates\n=== 4.4 Communicate ===\n* Send important service updates\n* Respond to your requests\n* Notify you of policy changes\n* Send opt-in newsletters (if you subscribe)\n=== 4.5 Comply with Law ===\n* Respond to valid legal requests\n* Enforce legal rights\n* Prevent fraud or illegal activity\n== 5. Public Information ==\n**Important:** Much of your activity on FactHarbor is public by design. This transparency is essential to our mission.\n=== 5.1 Always Public ===\n* **Contributions**: All content you create is permanently public\n* **Submission history**: All submissions are tracked and visible\n* **Username**: Your username is visible on your contributions\n* **Contribution metadata**: Timestamps, edit summaries\n=== 5.2 Public if You Choose ===\n* Profile information you add\n* Real name (if you provide it)\n* Social media links\n* Biography\n=== 5.3 Private (Not Public) ===\n* Email address\n* IP address (if you're logged in)\n* Private messages (if feature exists)\n* Account settings and preferences\n**Key Principle:** Transparency of submissions builds trust. Your submissions are attributed to your username, and submission history ensures accountability.\n== 6. How We Share Information ==\n=== 6.1 We Never ===\n* **Sell** your information\n* **Rent** your information\n* Share your information for **marketing** purposes\n* Share with **data brokers**\n=== 6.2 We May Share With ===\n**Service Providers**:\n* Hosting services (server infrastructure)\n* Email services (for communications)\n* Analytics providers (aggregated data only)\n* Security services (DDoS protection, etc.)\nAll service providers are bound by contract to protect your data.\n**Legal Requirements**:\n* Valid subpoenas or court orders\n* Government requests (where legally required)\n* Emergency situations (to prevent harm)\nSee Section 12 for transparency about government requests.\n**Public Data Releases**:\n* Anonymized, aggregated statistics\n* Research datasets (with privacy protections)\n* Full public contribution history (attributions maintained)\n=== 6.3 We Do Not Share ===\n* Your email address (except as required by law)\n* Your IP address (except as required by law)\n* Your private messages\n* Your account settings\n== 7. How Long We Keep Information ==\nWe follow **data minimization** principles - keeping data only as long as necessary.\n=== 7.1 Detailed Retention Periods ===\n| Data Type | Retention Period | Rationale |\n|-----------|------------------|-----------|\n| **Account Data** | Active + 90 days after deletion | User may wish to restore account |\n| **Email Addresses** | Active + 90 days after deletion | Required for communication during active period |\n| **IP Addresses (logged in)** | 90 days | Fraud detection, abuse prevention |\n| **IP Addresses (logged out)** | 30 days | Basic security, rate limiting |\n| **Web Server Logs** | 30 days | Technical troubleshooting |\n| **Error Logs** | 90 days | Bug investigation and fixing |\n| **Security Logs** | 1 year | Security incident investigation, required for compliance |\n| **Support Emails** | 2 years | Service improvement, warranty claims |\n| **Public Contributions** | **Permanent** | Transparency requirement, attribution |\n| **Contribution Metadata** | **Permanent** | Audit trail, quality assurance |\n| **AKEL Evaluation Logs** | 5 years | Algorithmic accountability, appeals |\n| **Financial Records** | 10 years | Swiss legal requirement (OR Art. 958f) |\n| **Tax Documents** | 10 years | Swiss legal requirement |\n=== 7.2 Retention Justification ===\nEach retention period is based on:\n* **Legal requirements** (financial records, security logs)\n* **Operational necessity** (abuse prevention, appeals)\n* **Data minimization** (shortest possible while meeting needs)\n* **Transparency mission** (public contributions permanent)\n=== 7.3 Longer Retention ===\nWe may retain data longer if:\n* Required by law\n* Necessary for ongoing investigation\n* Needed to enforce Terms of Service\n* You explicitly consent\n=== 7.4 What Happens When You Delete Your Account ===\nWhen you delete your account:\n**Immediately**:\n* Account deactivated\n* Email address deleted\n* Profile information removed\n* You cannot log in\n**Within 90 days**:\n* All personal data deleted or anonymized\n* Username may remain on contributions (for attribution)\n* Contributions remain public (transparency requirement)\n**Permanent**:\n* Your public contributions remain (anonymized to deleted user if requested)\n* Submission history preserved (essential for trust)\n== 8. Cookies and Tracking ==\n=== 8.1 Types of Cookies We Use ===\n**Essential Cookies** (cannot be disabled):\n* Session management (keep you logged in)\n* Security features (CSRF protection)\n* Load balancing\n**Functional Cookies** (can be disabled):\n* Language preferences\n* Display settings\n* User interface choices\n**Analytics Cookies** (can be disabled):\n* Page views and usage patterns\n* Feature effectiveness\n* Performance monitoring\n**We Do NOT Use**:\n* Advertising cookies\n* Third-party tracking cookies\n* Cross-site tracking\n=== 8.2 Managing Cookies ===\n**Cookie Consent Banner:**\nOn your first visit, we display a cookie consent banner allowing you to:\n* Accept all cookies\n* Accept only essential cookies\n* Customize preferences (analytics, functional)\n**Consent Requirements:**\n* **Essential cookies**: No consent required (necessary for functionality)\n* **Functional & Analytics cookies**: **Opt-in consent required** (not pre-checked)\n* **Withdrawal**: As easy as giving consent (click banner icon anytime)\n**Your Choices:**\n* Accept all non-essential cookies\n* Reject all non-essential cookies \n* Customize by category\n* Change preferences anytime via cookie settings\n**Browser Controls:**\nYou can also block cookies via browser settings, but this may affect functionality.\n**No Consent = No Non-Essential Cookies:**\nIf you reject non-essential cookies, we only use cookies necessary for the service to function.\n**Implementation Note:** We use opt-in (not pre-checked boxes) for all non-essential cookies, in compliance with Swiss and EU law.\n== 9. Your Rights and Choices ==\nYou have these rights regarding your personal data:\n=== 9.1 Access ===\n* Request a copy of your personal data\n* Review what we have about you\n* Export your data in machine-readable format\n=== 9.2 Correction ===\n* Update your account information\n* Correct inaccurate data\n* Complete incomplete data\n=== 9.3 Deletion ===\n* Delete your account\n* Remove specific personal information\n* Request anonymization of contributions\n=== 9.4 Data Portability ===\nYou have the right to receive your personal data in a structured, commonly used, and machine-readable format.\n**What You Can Export:**\n* Account information (JSON, CSV)\n* Your contributions (JSON, XML, Markdown)\n* Contribution history (CSV)\n* Profile settings (JSON)\n* Communication preferences (JSON)\n**Formats Available:**\n* **JSON** - Structured, machine-readable, most complete\n* **CSV** - Spreadsheet-compatible, for tabular data\n* **XML** - Alternative structured format\n* **Markdown** - Human-readable for text content\n**Export Process:**\n1. Log in to your account\n2. Go to Settings > Data Export\n3. Select data types and format\n4. Receive download link via email (within 48 hours)\n5. Download expires after 7 days\n**What's NOT Included:**\n* Other users' data (privacy protection)\n* Internal security logs (security reasons)\n* Algorithmic scores (proprietary, but results are included)\n**Transfer to Other Services:**\nWhile we provide machine-readable formats, each service has different import capabilities. We cannot guarantee compatibility with specific third-party services.\n**API Access (Future):**\nWe plan to offer API access for automated data exports for users who need regular portability.\n=== 9.5 Object ===\n* Object to certain processing\n* Opt out of analytics cookies\n* Unsubscribe from emails\n=== 9.6 Lodge Complaint ===\n* File complaint with us\n* Contact Swiss FDPIC (www.edoeb.admin.ch)\n* EU residents: contact local data protection authority\n* Seek legal remedies\n=== 9.7 How to Exercise Your Rights ===\nContact: [Method to be established before launch]\nInclude:\n* Your username\n* Specific request\n* validation information\nWe respond promptly.\n== 10. Data Security and Compliance ==\nWe protect your information with industry-standard security measures:\n=== 10.1 Technical Measures ===\n* **Encryption in transit**: TLS/HTTPS for all connections\n* **Encryption at rest**: Sensitive data encrypted in databases\n* **Access controls**: Role-based access to systems\n* **Authentication**: Strong password requirements, optional 2FA\n* **Secure development**: Security reviews, code audits\n* **Penetration testing**: Regular security assessments\n=== 10.2 Organisational Measures ===\n* **Team Members training**: Security awareness programs\n* **Access logging**: All admin actions logged\n* **Incident response**: Documented procedures\n* **Vendor assessment**: Security review of third parties\n* **Data minimization**: Collect only what's needed\n=== 10.3 Data Protection Impact Assessment (DPIA) ===\nFor high-risk processing activities, we conduct Data Protection Impact Assessments (DPIA) as required by Swiss FADP Article 22, including:\n* Description of processing operations\n* Assessment of necessity and proportionality\n* Evaluation of risks to user rights\n* Mitigation measures\n* Documentation and regular review\n**High-risk activities include:**\n* AI-powered automated decision systems (AKEL)\n* Large-scale content moderation\n* Processing of sensitive personal data (political opinions, health information)\n* Systematic monitoring of user behavior\n=== 10.4 Processing Activities Register ===\nWe maintain a comprehensive register of all processing activities as required by Swiss FADP Article 12, including:\n* Controller identification and contact details\n* Purposes of processing\n* Categories of data subjects and personal data\n* Categories of recipients\n* Retention periods\n* Description of security measures\n* Details of international data transfers\nThis register is available for inspection by the Swiss Federal Data Protection and Information Commissioner (FDPIC) upon request.\n=== 10.5 Data Protection Officer (DPO) ===\n**If we serve users in the European Union**, we will appoint a Data Protection Officer (DPO) as required by EU GDPR Article 37.\nThe DPO will:\n* Advise on data protection compliance\n* Monitor FADP and GDPR compliance\n* Serve as contact point for FDPIC and EU authorities\n* Conduct privacy audits\n* Handle data subject requests\nContact: [To be established if appointed]\n**Note:** Swiss law does not require a DPO for organizations of our size, but we commit to appointing one if we process data of EU residents to ensure full GDPR compliance.\n=== 10.6 Limitations ===\nNo system is 100% secure. While we implement strong protections:\n* We cannot guarantee absolute security\n* You are responsible for your password security\n* Public contributions are permanently public\n== 11. Data Breaches ==\nIf we experience a data breach:\n=== 11.1 Our Response ===\n**Immediately (without undue delay):**\n* Contain the breach\n* Assess scope and impact\n* **Notify Swiss FDPIC immediately** if breach likely results in high risk to data subjects (as required by FADP Article 24)\n* Begin investigation\n**Within 72 hours:**\n* Complete detailed assessment\n* Notify affected users if high risk confirmed\n* Provide details on what was compromised\n* Explain steps we're taking\n* Advise on protective actions\n=== 11.2 Transparency ===\n* Public incident report published (after resolution)\n* Root cause analysis shared\n* Improvements implemented\n* Follow-up report after resolution\n== 12. Government Requests and Transparency ==\n=== 12.1 Our Principles ===\n* We require valid legal process\n* We notify users unless prohibited by law\n* We challenge overly broad requests\n* We publish transparency reports\n=== 12.2 What We Require ===\n* **User data requests**: Court order or warrant\n* **Content removal**: Valid legal basis, not just request\n* **Emergency disclosure**: Credible threat to life/safety\n=== 12.3 User Notification ===\nWe notify affected users unless:\n* Legally prohibited (gag order)\n* Would interfere with investigation\n* User is the subject of investigation\nWe challenge gag orders exceeding 1 year.\n=== 12.4 Transparency Reports ===\nPublished twice yearly:\n* Number of requests by type\n* Compliance rate\n* Users affected\n* Challenges filed\n== 13. International Data Transfers ==\nFactHarbor may transfer personal data internationally for the following purposes:\n* Cloud hosting services (servers may be in EU, Switzerland, US)\n* AI model providers (if using hosted models)\n* Content delivery networks\n* Email and communication services\n=== 13.1 Legal Basis for Transfers ===\n**European Economic Area (EEA):**\nSwitzerland has an EU adequacy decision (confirmed January 15, 2024), allowing free data flow between Switzerland and EEA countries without additional safeguards.\n**United States:**\nWe transfer data only to companies certified under the Swiss-US Data Privacy Framework (effective September 15, 2024) or use Standard Contractual Clauses (SCCs) approved by the Swiss Federal Council.\n**Other Countries:**\nFor countries without adequacy decision, we use:\n* Swiss/EU Standard Contractual Clauses (SCCs), or\n* Binding Corporate Rules, or\n* Explicit user consent for specific transfers\n=== 13.2 Safeguards ===\nAll international transfers include:\n* Contractual data protection obligations\n* Technical encryption measures (TLS/HTTPS)\n* Access controls and logging\n* Regular compliance audits\n* validation of recipient's data protection capabilities\n=== 13.3 Disclosure to Users ===\nWe will inform you before your data is transferred to:\n* Countries without adequacy decision from Switzerland or EU\n* Processors outside Switzerland/EEA\n* Government authorities in foreign jurisdictions (if legally compelled)\n=== 13.4 Your Rights ===\nYou may:\n* Object to specific international transfers\n* Request information about transfer safeguards\n* Lodge complaints with Swiss FDPIC or your local data protection authority\nContact: [Data requests contact to be established] with concerns about international transfers.\n== 14. Children's Privacy ==\nFactHarbor is not intended for children and we take children's privacy very seriously.\n=== 14.1 Age Requirements ===\nFactHarbor is not intended for children under:\n* **13 years old** (US COPPA)\n* **16 years old** (EU GDPR, or lower age set by EU member state)\n* **13 years old** (Swiss FADP - default age of consent for most processing)\n=== 14.2 No Knowing Collection ===\nWe do not knowingly collect personal data from children below these ages. If you believe a child has provided us data, contact [Privacy contact to be established] immediately.\n=== 14.3 Discovery and Deletion ===\nIf we learn a user is below the age requirement:\n1. We immediately suspend the account\n2. We delete all personal data promptly\n3. We notify the account holder (if email provided)\n4. We document the deletion for compliance\n=== 14.4 Parental Rights ===\nParents or guardians may:\n* Request information about data collected from their child\n* Request immediate deletion of that data\n* Prohibit further collection from their child\nContact: [Privacy contact to be established] with subject \"Child Data Request\"\n=== 14.5 validation ===\nWe may request verification of parental/guardian status before processing requests.\n=== 14.6 Public Contributions ===\nContent contributed by underage users (before age verification) will be:\n* Attributed to \"Deleted User [ID]\"\n* Content remains for transparency (anonymized)\n* No personal data retained\n== 15. Changes to This Policy ==\nWe may update this Privacy Policy:\n* Material changes require 30-day notice\n* Notice via email or prominent site banner\n* Continued use after notice = acceptance\n* Previous versions archived and accessible\n== 16. Contact Us ==\n**Before Launch:**\nContact infrastructure will be established before any user data collection begins.\n**After Launch, contact points will include:**\n* General privacy questions\n* Data subject access requests (FADP/GDPR)\n* Data Protection Officer (if serving EU users)\n* Swiss Representative (required for FADP)\n* Security incident reporting\n**Mailing Address**: [To be determined based on Verein registration]\n**Note:** As a small organization, contact functions may be handled by the same individual initially, but legal requirements for response times and procedures will be met.\n== 17. Governing Law and Jurisdiction ==\n=== 17.1 Applicable Law ===\nThis Privacy Policy is governed by:\n* **Swiss Federal Act on Data Protection (FADP)** - Primary data protection law\n* **Swiss Civil Code (ZGB)** - For Verein organizational matters\n* **EU General Data Protection Regulation (GDPR)** - When processing data of EU/EEA residents\n* **Swiss Telecommunications Act** - For electronic communications\n=== 17.2 Jurisdiction ===\nFor disputes arising from this policy:\n**Primary Jurisdiction:** Swiss courts (canton to be determined based on Verein location)\n**Data Protection Disputes:**\n* First, contact [DPO contact to be established if needed] or [Privacy contact to be established]\n* File complaint with Swiss FDPIC (www.edoeb.admin.ch)\n* EU residents may file with local data protection authority\n* Legal action available in Swiss courts or (for EU residents) in EU member state courts\n**Alternative Dispute Resolution:**\nWe are committed to resolving disputes amicably through:\n* Internal escalation process\n* Mediation before litigation\n* Transparent decision rationale\n=== 17.3 International Users ===\n* **EU/EEA users**: May enforce GDPR rights in EU courts\n* **US users**: Subject to Swiss law, may invoke Swiss-US Data Privacy Framework\n* **Other jurisdictions**: Swiss law applies, local rights respected where possible\n=== 17.4 Severability ===\nIf any provision of this Privacy Policy is found invalid or unenforceable, the remaining provisions continue in full force.\n== 18. Effective Date and Version ==\n**Version**: 0.9.29 (Legal Compliance Update) \n**Effective Date**: [To be determined before launch] \n**Last Updated**: December 17, 2025\nThis is a draft policy. Final version will be published before any user data collection begins.\n== 19. Related Policies ==\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n* [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n* [[Operational Readiness Checklist>>FactHarbor.Organisation.Governance.Operational Readiness.WebHome]]\n* [[Terms of Service>>FactHarbor.Organisation.How-We-Work-Together.Terms-of-Service]] (to be created)\n* [[Security Policy>>FactHarbor.Organisation.How-We-Work-Together.Security-Policy]] (to be created)", "Organisation.How-We-Work-Together.Security-Policy": "= Security Policy =\n**Last Updated:** December 17, 2025\n== 1. Purpose ==\nThis Security Policy outlines how FactHarbor protects user data and maintains platform security.\n== 2. Security Principles ==\n* **Defense in depth**: Multiple layers of security controls\n* **Least privilege**: Minimal access rights by default\n* **Transparency**: Open about security practices\n* **Continuous improvement**: Regular security assessments\n== 3. Data Protection ==\n=== 3.1 Encryption ===\n* All connections use TLS/HTTPS\n* Sensitive data encrypted at rest\n* Secure key management practices\n=== 3.2 Access Controls ===\n* Role-based access control (RBAC)\n* Strong authentication requirements\n* Two-factor authentication available\n* Regular access reviews\n=== 3.3 Data Minimization ===\n* Collect only necessary data\n* Automatic deletion per retention schedule\n* Regular data cleanup processes\n== 4. Infrastructure Security ==\n=== 4.1 Hosting ===\n* Secure hosting environment\n* Regular security updates\n* Network security controls\n* DDoS protection\n=== 4.2 Monitoring ===\n* Security event logging\n* Anomaly detection\n* Regular log reviews\n* Incident alerting\n=== 4.3 Backups ===\n* Regular automated backups\n* Encrypted backup storage\n* Tested restoration procedures\n== 5. Application Security ==\n=== 5.1 Secure Development ===\n* Security code reviews\n* Dependency vulnerability scanning\n* Secure coding practices\n* Regular security testing\n=== 5.2 Authentication ===\n* Strong password requirements\n* Password hashing (bcrypt or better)\n* Account lockout protection\n* Session management\n=== 5.3 Input Validation ===\n* All user input validated\n* Protection against injection attacks\n* Content security policies\n* XSS prevention\n== 6. Vulnerability Management ==\n=== 6.1 Vulnerability Disclosure ===\n**How to Report:**\n* Email: [Security contact to be established]\n* Provide detailed description\n* Include reproduction steps if possible\n* We respond within reasonable timeframe\n**What to Expect:**\n* Acknowledgment of report\n* Investigation and validation\n* Fix development and deployment\n* Public disclosure after fix (coordinated)\n=== 6.2 Bug Bounty ===\n* May be established in the future\n* Details to be announced\n=== 6.3 Responsible Disclosure ===\nWe request:\n* Reasonable time to fix vulnerabilities\n* No public disclosure before fix\n* No exploitation of vulnerabilities\n* Good faith security research\n== 7. Incident Response ==\n=== 7.1 Incident Handling ===\n* Immediate containment\n* Investigation and assessment\n* User notification (if required)\n* Regulator notification (if required)\n* Public transparency report\n=== 7.2 Data Breaches ===\nPer [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]:\n* Immediate FDPIC notification (if high risk)\n* User notification (if required)\n* Public incident report\n* Root cause analysis\n== 8. Compliance ==\n=== 8.1 Standards ===\n* Swiss FADP compliance\n* EU GDPR compliance (if serving EU users)\n* Industry best practices\n=== 8.2 Audits ===\n* Regular internal security reviews\n* External audits when feasible\n* Penetration testing\n* Compliance assessments\n== 9. User Responsibilities ==\n=== 9.1 Account Security ===\n* Use strong, unique passwords\n* Enable two-factor authentication\n* Keep credentials confidential\n* Report suspicious activity\n=== 9.2 Security Awareness ===\n* Verify official communications\n* Be cautious of phishing\n* Report security concerns\n* Follow security guidelines\n== 10. Third-Party Services ==\n* Vet security practices of partners\n* Contractual security requirements\n* Regular vendor assessments\n* Data protection agreements\n== 11. Updates to This Policy ==\nThis Security Policy may be updated as security practices evolve. Major changes will be announced.\n== 12. Contact ==\n**Security Issues:** [Contact to be established]\n**Related Policies:**\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n**Version:** Draft \n**Status:** To be finalized before launch", "Organisation.How-We-Work-Together.Terms-of-Service": "= Terms of Service =\n**Effective Date:** [To be determined before production launch] \n**Last Updated:** December 17, 2025\n== 1. Acceptance of Terms ==\nBy accessing or using FactHarbor, you agree to be bound by these Terms of Service and our [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]].\n== 2. Description of Service ==\nFactHarbor is a platform for collaborative evaluation of factual claims using transparent, community-driven processes.\n== 3. User Accounts ==\n=== 3.1 Registration ===\n* You must provide accurate information when registering\n* You are responsible for maintaining the security of your account\n* You must be at least the minimum age required by applicable law\n=== 3.2 Account Responsibilities ===\n* You are responsible for all activity under your account\n* Notify us immediately of any unauthorized use\n* Do not share your account credentials\n== 4. User Conduct ==\n=== 4.1 Acceptable Use ===\nYou agree to:\n* Contribute in good faith\n* Respect other users\n* Follow community guidelines\n* Respect intellectual property rights\n=== 4.2 Prohibited Activities ===\nYou may not:\n* Submit false or misleading information\n* Harass or abuse other users\n* Attempt to manipulate the platform\n* Violate any applicable laws\n* Circumvent security measures\n* Use automated tools to abuse the system\n== 5. Content and Intellectual Property ==\n=== 5.1 Your Content ===\n* You retain ownership of content you submit\n* You grant FactHarbor a license to use, display, and distribute your contributions\n* You represent that you have the right to submit your content\n* Your contributions are licensed under [[our licensing terms>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n=== 5.2 Public Nature of Contributions ===\n* Contributions are public and permanent\n* Submission history is maintained for transparency\n* Deletion requests may result in anonymization rather than removal\n=== 5.3 Platform Content ===\n* Platform code and documentation are open source under our specified licenses\n* FactHarbor trademarks remain our property\n== 6. Privacy ==\nYour privacy is important. Please review our [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]] to understand how we collect, use, and protect your data.\n== 7. Termination ==\n=== 7.1 By You ===\nYou may delete your account at any time through account settings.\n=== 7.2 By Us ===\nWe may suspend or terminate accounts that:\n* Violate these Terms\n* Engage in abusive behavior\n* Pose security risks\n* Are required to be terminated by law\n== 8. Disclaimers ==\n=== 8.1 \"As Is\" Service ===\nFactHarbor is provided \"as is\" without warranties of any kind, express or implied.\n=== 8.2 Content Accuracy ===\n* We do not guarantee the accuracy of user-submitted content\n* Evaluations represent community assessments, not absolute truth\n* Use your own judgment when relying on information\n=== 8.3 Availability ===\nWe do not guarantee uninterrupted or error-free service.\n== 9. Limitation of Liability ==\nTo the maximum extent permitted by law, FactHarbor and its operators shall not be liable for any indirect, incidental, special, consequential, or punitive damages.\n== 10. Governing Law ==\nThese Terms are governed by Swiss law. Disputes will be resolved under Swiss jurisdiction.\n== 11. Changes to Terms ==\nWe may update these Terms from time to time. Material changes will be announced with reasonable notice.\n== 12. Contact ==\nFor questions about these Terms: [Contact information to be established]\n== 13. Related Policies ==\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]\n**Version:** Draft \n**Status:** To be finalized before launch", "Organisation.How-We-Work-Together.Transparency-Policy": "= Transparency Policy =\n== 1. Purpose and Scope ==\nThis Transparency Policy defines FactHarbor's commitment to openness in all aspects of operations, governance, and finances. It establishes what information is public by default, what may be kept private, and the processes for requesting information.\n**This policy applies to:**\n* FactHarbor Organisation (legal entity)\n* All FactHarbor projects and services\n* Governing Team, staff, and contractors\n* All decision-making processes\n== 2. Core Principle: Default to Public ==\n**Default Rule:** All organisational information is public unless it meets a specific exception.\nThis principle reflects FactHarbor's mission: a project claiming to support well-grounded, manipulation-resistant judgments must itself be transparent and accountable.\n== 3. What Must Be Public ==\n=== 3.1 Financial Information ===\nPublished annually (within 6 months of fiscal year end):\n* **Complete financial statements** (audited where possible)\n* **Tax filings** (Swiss tax filings per cantonal requirements)\n* **Income statement** showing:\n * Grants and donations (aggregate)\n * Sponsorships and contracts (aggregate)\n * Other revenue sources\n* **Expense statement** showing:\n * Program expenses by category\n * Administrative costs\n * Fundraising costs\n* **Compensation ranges** by role (not individual salaries)\n* **Major funding relationships** (>$50,000 per year or >10% of budget)\n=== 3.2 Governance Information ===\nPublished continuously (promptly of changes):\n* **Governance documents**:\n * Verein statutes (bylaws)\n * Operating procedures\n * Decision-making authority matrix\n * Conflict of interest policy\n* **Governing Team information**:\n * Current board composition\n * Governing Team member bios and affiliations\n * Meeting schedules\n * Governing Team meeting minutes (with limited exceptions - see section 4)\n * Governing Team decisions and resolutions\n* **Policy changes**:\n * All policy updates with rationale\n * Effective dates\n * Community input periods\n* **Organisational structure**:\n * Reporting relationships\n * Key staff roles (not individual names unless they choose)\n * Advisory bodies and committees\n=== 3.3 Operational Information ===\nPublished regularly:\n* **Transparency Reports** (twice yearly):\n * Government requests for user data\n * Content moderation statistics\n * Takedown requests (DMCA, legal)\n * Policy violation reports\n * Security incident disclosures (after resolution)\n* **Technical Performance**:\n * AKEL performance metrics\n * Quality gate pass rates\n * Risk tier distribution statistics\n * System uptime and availability\n* **Content Statistics**:\n * Number of claims, scenarios, verdicts\n * Publication mode distribution\n * Review and audit rates\n* **Partnership Information**:\n * Major partnerships and collaborations\n * Funding relationships\n * Technical dependencies\n=== 3.4 Source Code and Technical Specifications ===\nPublished continuously:\n* All source code per open source licenses (MIT, AGPL, CC BY-SA)\n* Technical architecture documentation\n* Protocol and data model specifications\n* API documentation\n* Quality gate algorithms and parameters\n* Risk tier assignment criteria\n== 4. What May Be Private ==\nInformation may be withheld ONLY when disclosure would:\n=== 4.1 Individual Privacy (Highest Priority) ===\nPrivate:\n* User personal data (emails, IP addresses, phone numbers)\n* Contributor real names (if pseudonymous)\n* Personnel files and reviews\n* Individual salaries (publish ranges only)\n* Medical or family information\n* Background checks\n=== 4.2 Security ===\nTemporarily private (with time limits):\n* Unpatched security vulnerabilities (public after patch + 30-90 days)\n* Active security incidents (public after resolution)\n* Penetration test results (sanitized version public after fixes)\n* Authentication credentials and API keys\n* Infrastructure-specific security configurations\n=== 4.3 Legal ===\nPrivate while active:\n* Ongoing litigation details (summary public, details after resolution)\n* Attorney-client privileged communications\n* Settlement negotiations\n* Subpoenas with gag orders (challenge orders exceeding 1 year)\n* Whistleblower identity (protected permanently unless they consent)\n=== 4.4 Operational ===\nPrivate with conditions:\n* Donor information (unless donor consents to publication)\n* Abuse investigation details (protect victims)\n* Governing Team discussions on personnel matters (outcomes public)\n* Strategic plans that would create competitive disadvantage (time-limited: public after 12 months or execution)\n== 5. Time Limits on Privacy ==\nAll private information has an expiration date:\n* **Security vulnerabilities**: Public 30-90 days after patch\n* **Security incidents**: Public immediately after resolution (sanitized)\n* **Governing Team personnel discussions**: Outcomes public, process private for 1 year then reviewed\n* **Strategic plans**: Public after execution or 12 months, whichever comes first\n* **Legal matters**: Public after resolution\n* **Donor information**: May be withheld permanently only with donor objection\n**Annual Review:** All information marked \"private\" is reviewed annually. If exception no longer applies, information becomes public.\n== 6. Transparency Reports ==\nPublished **twice yearly** (January and July):\n=== 6.1 Government Requests ===\n* Number of requests for user data (by type)\n* Number of requests complied with\n* Number of requests challenged\n* Number of users affected\n* Types of data requested\n=== 6.2 Content Moderation ===\n* Total moderation actions by category\n* Content state changes (e.g. Published  Hidden)\n* Quality gate failures by gate\n* Community flags and expert reviews\n* Takedown requests and responses\n=== 6.3 Security ===\n* Security incidents (after resolution)\n* Vulnerability reports received\n* Bounties paid\n* Patches deployed\n* Audit findings (sanitized)\n=== 6.4 Performance ===\n* AKEL performance metrics\n* User growth and engagement\n* Content growth\n* Community contributions\n* System availability\n== 7. Information Request Process ==\n=== 7.1 Submitting a Request ===\nAnyone may request organisational information:\n1. **Email**: [Transparency contact to be established]\n2. **Include**:\n * Specific information requested\n * Rationale for request\n * Preferred format (if applicable)\n3. **Expect**: Initial response within 14 business days\n=== 7.2 Request Evaluation ===\nRequests are evaluated against:\n* Is information already public? (link provided)\n* Does exception in Section 4 apply?\n* Can information be disclosed with redactions?\n* Is time limit on privacy expired?\n=== 7.3 Response Types ===\n* **Granted**: Information provided promptly\n* **Partially Granted**: Information with redactions provided, explanation of redactions\n* **Denied**: Written explanation of which exception applies\n* **Deferred**: If time-limited exception, date when information will become public\n== 8. Appeals Process ==\nIf request is denied:\n=== 8.1 First Appeal ===\n1. Submit appeal to **Transparency Committee** (if established) or Governing Team\n2. Include:\n * Original request\n * Denial reason\n * Additional context or rationale\n3. Decision promptly\n=== 8.2 Final Appeal ===\n1. Appeal to **Full Governing Team** of Leads\n2. Governing Team reviews at next scheduled meeting\n3. Governing Team decision is final\n4. Rationale published (unless it would disclose the private information)\n== 9. Community Input ==\n=== 9.1 Policy Changes ===\nBefore making material changes to transparency commitments:\n1. **Proposal published** with rationale\n2. **Public comment period** (minimum 30 days)\n3. **Community input** considered\n4. **Decision rationale** published with final policy\n=== 9.2 Ongoing Input ===\nCommunity may:\n* Request additional transparency commitments\n* Suggest improvements to reporting\n* Identify information that should be public\n* Challenge exceptions\nSubmit suggestions to: [Transparency contact to be established]\n== 10. Compliance and Oversight ==\n=== 10.1 Internal Oversight ===\n* **Transparency Officer** (staff or board designee):\n * Reviews all privacy classifications\n * Manages information requests\n * Prepares transparency reports\n* **Annual Transparency Audit**:\n * Reviews all \"private\" classifications\n * Checks compliance with publication schedules\n * Assesses process effectiveness\n=== 10.2 Public Reporting ===\nAnnual transparency compliance report includes:\n* Number of information requests received\n* Request grant/deny statistics\n* Exception usage (how often each applied)\n* Privacy expiration reviews\n* Improvements made to process\n=== 10.3 Independent Audit ===\nWhen feasible (budget permitting):\n* Independent third-party transparency audit\n* Results published\n* Recommendations implemented or explanations provided\n== 11. Enforcement ==\n=== 11.1 Violations ===\nViolation of this policy includes:\n* Withholding information that should be public\n* Failing to publish required reports on schedule\n* Misclassifying public information as private\n* Extending privacy beyond time limits without review\n=== 11.2 Consequences ===\n* Internal violations: Performance review, retraining, or disciplinary action\n* Governing Team violations: Governing Team review, potential removal\n* Persistent violations: Independent investigation\n=== 11.3 Whistleblower Protection ===\nAnyone may report transparency violations to:\n* [Transparency contact to be established]\n* Any board member directly\n* External parties (regulators, media)\nWhistleblowers are protected from retaliation. Reports may be anonymous.\n== 12. Updates to This Policy ==\nChanges to this Transparency Policy:\n* Require Governing Team approval\n* Must include 30-day public comment period\n* Are published with rationale\n* Take effect 30 days after final publication\n**Version History:**\n* 0.9.28 (2025-12-17): Initial policy based on best practices from Wikimedia Foundation and Mozilla Foundation\n== 13. Contact ==\n**Transparency Requests**: [Transparency contact to be established]\n**Appeals**: [Governing Team contact to be established]\n**Whistleblower Reports**: [To be established - secure channel]\n== 14. Related Policies ==\n* [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]\n* [[Finance & Compliance>>FactHarbor.Organisation.Legal and Compliance.Finance and Compliance.WebHome]]", "Organisation.How-We-Work-Together.WebHome": "= How We Work Together =\n\nThis section describes how FactHarbor operates as a collaborative organization, including our culture, decision-making processes, and policies.\n\n== Overview ==\n\nFactHarbor combines culture, governance, and policies:\n\n* **Culture & Values**: How we work together (Workplace Culture, Continuous Improvement, Consent-Based Decision Making)\n* **Guidelines**: Rules and standards for evaluation (Global Rules)\n* **Policies**: Legal and operational commitments (Terms of Service, Privacy Policy, Transparency Policy, Security Policy)\n\n== Culture & Values ==\n\n**[[Workplace Culture>>FactHarbor.Organisation.How-We-Work-Together.Workplace-Culture]]**\n* [[Best Workplace Blueprint>>https://robertschaub.github.io/BestWorkplace/#The%20Best%20Workplace.WebHome]] principles\n* [[Sociocracy 3.0>>https://sociocracy30.org/]] integration\n* AI-first culture\n* How we collaborate\n\n**[[Continuous Improvement>>FactHarbor.Organisation.How-We-Work-Together.Continuous-Improvement]]**\n* Improvement cycle\n* Improvement cadence\n* Retrospectives and learning\n\n**[[Consent-Based Decision Making>>FactHarbor.Organisation.How-We-Work-Together.Consent-Based-Decision-Making]]**\n* [[Sociocracy 3.0>>https://sociocracy30.org/]] decision pattern\n* Consent vs consensus\n* Process steps and examples\n\n== Guidelines ==\n\n**[[Global Rules>>FactHarbor.Organisation.How-We-Work-Together.GlobalRules.WebHome]]**\n* Evidence evaluation rules\n* Risk tier definitions\n* Source scoring criteria\n\n== Policies ==\n\n**[[Terms of Service>>FactHarbor.Organisation.How-We-Work-Together.Terms-of-Service]]**\n* Rules and agreements for using FactHarbor\n\n**[[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]**\n* How we handle and protect your data\n\n**[[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]**\n* Our commitment to openness and accountability\n\n**[[Security Policy>>FactHarbor.Organisation.How-We-Work-Together.Security-Policy]]**\n* Security principles and practices\n* Incident response\n* Vulnerability reporting\n\n== Contributors ==\n\n**[[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]]**\n* How to improve the system (not review content)\n* RFC workflow and contributor roles\n* Trust levels and quality standards\n\n== Contact ==\n\nQuestions about these policies? Contact the governance team or open an issue on our repository.\n", "Organisation.How-We-Work-Together.Workplace-Culture": "= Workplace Culture =\n**How we work together to build FactHarbor.**\n== 1. Our Inspiration ==\nFactHarbor's workplace culture draws from three complementary sources:\n**[[Best Workplace Blueprint>>https://robertschaub.github.io/BestWorkplace/#The%20Best%20Workplace.WebHome]]** (Schaub Group):\n* User-centered collaboration\n* Empowered, self-organizing teams\n* Iterative improvement\n* Enabling leadership\n* Experimentation and learning\n* Transparency and trust\n**[[Sociocracy 3.0>>https://sociocracy30.org/]]**:\n* Consent-based decisions\n* Clear domains\n* Empirical approach\n* Continuous improvement\n* Equivalence\n* Accountability\n**AI-First Philosophy**:\n* Automation over bureaucracy\n* Systems thinking\n* Metrics-driven improvement\n* Scale through code\n**Together**: A collaborative, transparent, learning culture that enables both humans and AI to thrive.\n== 2. We Engage with Our Users ==\n**From BWB**: \"We connect with users and empathize with them, listen to their needs, and learn from their insights.\"\n=== 2.1 FactHarbor Application ===\n**Our users**:\n* Internal: Contributors, moderators, team members\n* External: Claim submitters, information seekers, researchers, journalists\n**How we engage**:\n*  User feedback continuously monitored (helpful/unhelpful ratings)\n*  Community forums for discussion\n*  Regular user surveys\n*  Feedback loops inform system improvements\n*  Transparent communication about changes\n*  Public roadmap\n**User journey consideration**:\n* Claim submission  AKEL processing  Verdict  Evidence exploration\n* Every step designed for clarity and trust\n* User privacy prioritized\n* Accessibility important\n**User-driven improvement**:\n* User feedback patterns  System improvements (not ad-hoc fixes)\n* Example: \"Users find evidence unclear\"  Improve evidence presentation algorithm\n* Example: \"Users confused by confidence scores\"  Better explanation system\n=== 2.2 Internal Users (Contributors) ===\n**We support contributors**:\n* Clear onboarding documentation\n* Welcoming community\n* Responsive to questions\n* Recognize contributions\n* Safe to experiment and learn\n**We learn from contributors**:\n* RFC feedback shapes decisions\n* Community insights improve processes\n* Diverse perspectives strengthen the system\n== 3. We Build Networks of Empowered Teams ==\n**From BWB**: \"Teams self-organize and wield decision-making power... Networks of teams align around shared strategic goals with agreed priorities.\"\n=== 3.1 FactHarbor Application ===\n**Self-organizing within domains**:\n* Technical Coordinator: Autonomous over AKEL performance\n* Community Coordinator: Autonomous over community processes\n* Moderators: Autonomous in handling flagged items\n**Decision-making power** (see [[Consent-Based Decision Making>>FactHarbor.Organisation.How-We-Work-Together.Consent-Based-Decision-Making]]):\n* Consent-based decisions for system changes\n* No hierarchy - decisions by domain expertise\n* Clear escalation for cross-domain issues\n**Empowerment through**:\n* Clear domain boundaries (S3 pattern)\n* Authority to make decisions within domain\n* Support and resources to succeed\n* Trust by default\n**Networks alignment**:\n* Shared goal: Make FactHarbor's claim evaluation excellent\n* Regular coordination meetings\n* Cross-domain collaboration on complex issues\n* Transparent decision documentation\n=== 3.2 Ownership and Accountability ===\n**Team members own outcomes**:\n* Technical Coordinator accountable for AKEL performance metrics\n* Community Coordinator accountable for contributor satisfaction\n* Moderators accountable for healthy community\n**Recognition for achievements**:\n* Public recognition of contributions\n* Performance metrics show impact\n* Success celebrated as team achievement\n=== 3.3 Small, Focused Team ===\n**By design**: full-timeE + part-time moderators\n**Why small?**\n* Automation does most work\n* Less coordination overhead\n* Clearer communication\n* Each person's impact visible\n**Scaling through**:\n* Better algorithms (not more people)\n* Improved processes\n* Community contributions\n== 4. We Work in Iterations ==\n**From BWB**: \"We operate in iterative cycles, allowing us to make early adjustments and minimize the risk and impact of errors.\"\n=== 4.1 FactHarbor Application ===\n**Sprint cycles**:\n* Select improvement priorities\n* Implement and test\n* Review and retrospect\n* Adjust and iterate\n**Feedback loops** (see [[Continuous Improvement>>FactHarbor.Organisation.How-We-Work-Together.Continuous-Improvement]]):\n* Monitor metrics (continuous)\n* Review trends (recurring)\n* Comprehensive analysis (periodic)\n* Strategic review (periodic)\n**Early adjustments**:\n* Canary deployments (1%  5%  25%  100%)\n* Quick rollback if issues\n* A/B testing for validation\n* Frequent small changes > big releases\n**Quality embedded in every increment**:\n* Automated testing\n* Code review\n* Performance validation\n* Metrics monitoring\n=== 4.2 Learning from Errors ===\n**Errors are learning opportunities**:\n* Blameless retrospectives\n* Document what went wrong\n* Identify systematic causes\n* Improve to prevent recurrence\n**Safe to fail**:\n* Test environments for experimentation\n* Rollback plans for all changes\n* Low-risk experimentation encouraged\n* Failures documented and shared\n== 5. We Are Visionary Leaders and Enablers ==\n**From BWB**: \"Leaders create an enabling environment, removing impediments and empowering individuals and teams to self-organize... viewing failure and mistakes as opportunities for continuous growth and learning.\"\n=== 5.1 FactHarbor Application ===\n**Enabling environment**:\n* Clear documentation and onboarding\n* Tools and infrastructure provided\n* Support readily available\n* Resources allocated appropriately\n**Removing impediments**:\n* Technical Coordinator: Removes technical blockers\n* Community Coordinator: Removes process blockers\n* Governing Team: Removes strategic/policy blockers\n**Empowerment through clarity**:\n* Clear domains (S3 pattern)\n* Clear decision processes\n* Clear success metrics\n* Clear escalation paths\n**Psychological safety**:\n*  Safe to ask questions\n*  Safe to admit mistakes\n*  Safe to disagree (with data)\n*  Safe to experiment\n*  Safe to fail (and learn)\n**Not safe**:\n*  Harassment or discrimination\n*  Ignoring feedback\n*  Circumventing processes for convenience\n*  Hiding problems\n=== 5.2 Leadership Philosophy ===\n**Leadership is service**:\n* Leaders serve the team\n* Leaders remove obstacles\n* Leaders enable success\n* Leaders develop people\n**Not command-and-control**:\n* No \"my way or highway\"\n* No hoarding information\n* No blame culture\n* No micromanagement\n**\"Go to Gemba\"** (BWB principle):\n* Leaders actively observe work\n* Understand challenges firsthand\n* Learn from doing\n* Coach and support\n=== 5.3 Continuous Growth and Learning ===\n**Individual growth**:\n* Learn new technologies\n* Develop new skills\n* Expand domain knowledge\n* Take on stretch challenges\n**Team learning**:\n* Retrospectives\n* Knowledge sharing\n* Documentation\n* Mentoring\n**Organizational learning**:\n* System improvements documented\n* Failures analyzed\n* Successes replicated\n* Knowledge base maintained\n== 6. We Foster Change ==\n**From BWB**: \"We create environments that encourage experimentation and hypothesis exploration. We embrace changes grounded in evidence and tested hypotheses.\"\n=== 6.1 FactHarbor Application ===\n**Experimentation encouraged**:\n* RFC process enables anyone to propose\n* Test environments available\n* A/B testing supported\n* \"Good enough for now, safe enough to try\"\n**Evidence-based change**:\n* Measure before and after\n* Data drives decisions\n* Hypotheses tested, not assumed\n* Metrics validate success\n**Change process**:\n1. Identify issue (data-driven)\n2. Hypothesize solution\n3. Design experiment\n4. Test thoroughly\n5. Deploy gradually\n6. Measure impact\n7. Learn and iterate\n**Not change for change's sake**:\n* Clear problem being solved\n* Expected improvement defined\n* Success measurable\n* Rollback if unsuccessful\n=== 6.2 Innovation Culture ===\n**Encourage**:\n*  Trying new approaches\n*  Challenging assumptions\n*  Learning from other domains\n*  Cross-pollinating ideas\n**But maintain**:\n*  Testing before deployment\n*  Measuring impact\n*  Documentation\n*  Code quality\n=== 6.3 Adaptation ===\n**System adapts to**:\n* New domains (medical, legal, etc.)\n* New evidence types\n* New attack vectors\n* User needs\n* Scale requirements\n**Culture of adaptation**:\n* Nothing is sacred (except principles)\n* Processes evolve\n* Tools change\n* Structures adapt\n== 7. We Practice Transparency and Build Trust ==\n**From BWB**: \"We actively share information... We proactively communicate about the fulfillment of our promises and commitments... We recognize our limitations and mistakes and persistently strive for improvement.\"\n=== 7.1 FactHarbor Application ===\n**Transparency in decisions**:\n* Decision records documented\n* Rationale explained\n* Trade-offs acknowledged\n* Open to questions\n**Transparency in metrics** (see [[System Performance Metrics>>FactHarbor.Product Development.Specification.System-Performance-Metrics]]):\n* Performance dashboards public\n* Success/failure metrics visible\n* Improvement progress tracked\n* Problems acknowledged\n**Transparency in code**:\n* Open source (where possible)\n* Algorithm parameters documented\n* Changes tracked in git\n* Auditable by anyone\n**Transparency in governance**:\n* Governance structure documented\n* Meeting minutes published\n* Policies openly accessible\n* Decision processes clear\n=== 7.2 Communication ===\n**Proactive communication**:\n* Regular updates on progress\n* Advance notice of changes\n* Explanation of decisions\n* Status of commitments\n**Honest communication**:\n* Acknowledge limitations\n* Admit mistakes openly\n* Share both successes and failures\n* Don't hide problems\n**Responsive communication**:\n* Answer questions promptly\n* Engage with feedback\n* Clarify misunderstandings\n* Address concerns\n=== 7.3 Trust Building ===\n**Trust through**:\n*  Consistent behavior\n*  Keeping commitments\n*  Admitting mistakes\n*  Sharing information\n*  Including people in decisions\n*  Demonstrating competence\n**Trust eroded by**:\n*  Hiding information\n*  Breaking commitments\n*  Blaming others\n*  Inconsistent decisions\n*  Ignoring feedback\n== 8. Integrating All Three Frameworks ==\n=== 8.1 How They Complement ===\n**[[Best Workplace Blueprint>>https://robertschaub.github.io/BestWorkplace/#The%20Best%20Workplace.WebHome]]** provides:\n* Human-centered values\n* Leadership principles\n* Cultural practices\n**[[Sociocracy 3.0>>https://sociocracy30.org/]]** provides:\n* Decision-making patterns\n* Organizational structures\n* Governance frameworks\n**AI-First Philosophy** provides:\n* Automation principles\n* Scalability approach\n* Systems thinking\n**Together**: Culture + Patterns + Technology = Effective Organization\n=== 8.2 Practical Example ===\n**Scenario**: AKEL processing time increasing\n**BWB perspective**:\n* Engage users: Check if users experiencing delays\n* Work iteratively: Quick fixes first, then systematic\n* Transparency: Communicate problem and progress\n* Learning: What can we learn from this?\n**S3 perspective**:\n* Domain clarity: Technical Coordinator owns this\n* Consent decision: Proposed solution needs team consent\n* Empiricism: Test solutions, measure impact\n* Continuous improvement: Part of ongoing cycle\n**AI-First perspective**:\n* Fix the system: Optimize algorithm, not override\n* Metrics-driven: What metrics show the problem?\n* Automation: Solution must scale\n* Monitor: Watch metrics after fix\n**Result**: Comprehensive, principled approach to problem-solving.\n== 9. Working Remotely (Future) ==\n**FactHarbor may operate as remote-first or hybrid organization.**\n**BWB principles applied remotely**:\n* Engagement: Video calls, async communication\n* Empowerment: Trust-based work\n* Iterations: Sprint structure works remotely\n* Transparency: Even more important when distributed\n* Change: Remote tools and processes\n**Best practices** (to be developed as team grows):\n* Documented communication preferred\n* Overlap hours for collaboration\n* Async-first with sync as needed\n* Regular video check-ins\n* Virtual retrospectives\n== 10. Living Document ==\n**This culture page evolves**:\n* Periodic review: Does this reflect reality?\n* Feedback welcome: Suggest improvements\n* Updated as we learn: Culture grows with us\n**Culture is practiced, not proclaimed**:\n* These aren't just words\n* Actions matter more than statements\n* Everyone accountable for culture\n* Leaders exemplify these values\n== 11. For New Team Members ==\n**If you're new to FactHarbor**:\n**Expect**:\n* Transparency in decisions and metrics\n* Autonomy within your domain\n* Support and resources\n* Learning opportunities\n* Safe environment for experimentation\n* Data-driven decisions\n* Iterative improvement\n**Contribute by**:\n* Asking questions\n* Proposing improvements (RFCs)\n* Supporting team decisions\n* Sharing knowledge\n* Taking ownership\n* Learning continuously\n* Upholding these values\n== 12. Related Pages ==\n* [[Automation Philosophy>>FactHarbor.Organisation.Strategy.Automation Philosophy.WebHome]] - Why we automate\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - How we govern\n* [[Continuous Improvement>>FactHarbor.Organisation.How-We-Work-Together.Continuous-Improvement]] - How we improve\n* [[Consent-Based Decision Making>>FactHarbor.Organisation.How-We-Work-Together.Consent-Based-Decision-Making]] - How we decide\n* [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] - How to contribute", "Organisation.Legal and Compliance.CLA.WebHome": "= Contributor License Agreement (CLA) =\n== 1. Purpose ==\nThis agreement ensures that FactHarbor has the legal standing to defend the project, maintain its open nature, and properly attribute contributions while respecting different contributor types.\n== 2. Contributor Types ==\nFactHarbor recognizes two types of contributors with different copyright arrangements:\n=== 2.1 Unpaid Volunteers (Community Contributors) ===\n**Copyright Retention**: You retain full copyright ownership of your contributions.\n**License Grant**: You grant FactHarbor (the Organisation) a perpetual, worldwide, non-exclusive, royalty-free license to:\n* Use, reproduce, modify, and distribute your contributions\n* Sublicense your contributions under the project's open source licenses\n* Enforce intellectual property rights on behalf of the project\n**Attribution**: Your contributions will be attributed to you in project documentation and version control systems.\n=== 2.2 Paid Contributors (Employees, Contractors, Sponsored Work) ===\n**Copyright Assignment**: Copyright ownership transfers to FactHarbor Organisation upon contribution.\n**Rationale**: This ensures clear ownership for commercially sponsored work and simplifies long-term project governance.\n**Compensation**: Paid contributors receive agreed compensation for their work.\n**Attribution**: Contributions are still attributed to the individual contributor in version control and documentation.\n== 3. Common Terms (All Contributors) ==\n=== 3.1 Grant of License ===\nAll contributions are made available under the project's designated open source licenses (see [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]).\n=== 3.2 Right to Enforce ===\nYou grant FactHarbor the specific right to enforce intellectual property rights related to your contributions, including the right to take legal action against license violations.\n=== 3.3 Originality ===\nYou represent that your contribution is your original work, or that you have the legal right to submit it under these terms.\n=== 3.4 Support Well-grounded, Independent Judgments ===\nYour contributions should support the project's goal of providing transparent, evidence-based analysis rather than promoting predetermined conclusions.\n== 4. Determining Contributor Type ==\n**Default**: Contributors are considered unpaid volunteers unless they have a written agreement specifying paid status.\n**Paid Status Indicators**:\n* Employment contract with FactHarbor Organisation or a sponsoring entity\n* Written contracting agreement specifying payment for contributions\n* Grant or sponsorship agreement designating work as paid\n**Transparency**: Contributor type and any material sponsorship relationships should be disclosed in contribution metadata where applicable.\n== 5. Relation to Open Source Licenses ==\nThis CLA works in conjunction with the project's open source licenses:\n* **For Users**: The open source license (e.g., MIT, Apache 2.0) governs how you can use FactHarbor\n* **For Contributors**: This CLA governs the legal relationship between contributors and the FactHarbor Organisation\n* **Compatibility**: The CLA ensures the Organisation can maintain and enforce the open source license\nSee [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]] for details on specific licenses used by different components.\n== 6. Acceptance ==\nBy submitting a contribution to FactHarbor (via pull request, patch, or other mechanism), you acknowledge that you have read, understood, and agree to these terms.\nFor questions about contributor classification or this agreement, contact the FactHarbor Organisation governance team.\n", "Organisation.Legal and Compliance.Finance and Compliance.WebHome": "= Finance & Compliance =\n== 1. Purpose ==\nThe Finance & Compliance framework ensures that FactHarbor is funded in a transparent, neutral, and sustainable way, and that financial decisions can be audited and explained.\n== 2. Funding Principles ==\n* Full transparency about all significant funding sources and major expenses.\n* No political or lobbying money.\n* No funding that is conditional on specific claim outcomes.\n* Preference for funding models that protect independence and neutrality.\n== 3. Allowed Funding ==\nExamples of allowed funding sources:\n* public donations and small recurring contributions\n* neutral grants and research funding with no control over outcomes\n* institutional support where the institution has no editorial or governance influence.\n== 4. Prohibited Funding ==\nExamples of prohibited or highly discouraged funding:\n* money from political parties or lobbying organisations\n* funding that is explicitly tied to desired conclusions or claim outcomes\n* funding that requires hidden influence on content, moderation, or recommendation logic.\n== 5. Budgeting and Documentation ==\n* Budgets are planned per phase and reviewed regularly.\n* Every significant expense must be linked to a budget line and justification.\n* The Finance & Compliance Lead is responsible for keeping the ledger up-to-date.\n== 6. Reimbursable Expense Report ==\nSee [[Reimbursable Expense Report>>FactHarbor.Organisation.Legal and Compliance.Reimbursable Expenses.WebHome]].\nFactHarbor maintains a transparency ledger that records:\n* incoming donations and grants\n* major expenses\n* open obligations and earmarked funds\n* links to public reports where appropriate.\n== 7. Internal Controls ==\n* For sensitive transactions, at least two people must be involved (e.g. Finance & Compliance Lead plus a second reviewer).\n* Access to financial systems and banking credentials is limited and logged.\n* Regular internal reviews check for anomalies or conflicts of interest.\n== 8. Reporting ==\n* Internal: periodic financial updates for the Governing Team.\n* External: at least annual public reports summarising funding sources, major expenses, and key risks.\n== 9. Compliance Dimensions ==\nFinance & Compliance covers:\n* legal compliance (non-profit law, contracts, licences)\n* financial compliance (book-keeping standards, taxation where applicable)\n* data protection (handling of donor and user data)\n* operational compliance (e.g. auditability and traceability of decisions).\n== 10. Funding Model Table ==\n(% style=\"width:100%\" %)\n|=Category|=Revenue model|=Primary compromise / risk|=Suitable phases|\n|Foundation|Donations, grants|Relies on public goodwill; uncertain funding.|Any|\n|Services|Paid support / consulting|Diverts time from core development.|Released products|\n|Knowledge|Training & certification|Creates incentive to gate expertise.|Released products with federation|\n|Ecosystem|Recurring community membership|Requires continuous value delivery.|Any|\n|(% rowspan=\"2\" %)Development|One-time goal campaigns|(% rowspan=\"2\" %)Campaign failure delays progress.|(% rowspan=\"2\" %)Best for early phases|\n|e.g. crowdfunding|\n== 11. Startup Phase & Founder Financing ==\nIn the earliest phase, FactHarbor may operate as a **founder-driven, small organisation** with very limited formal structure.\nThis section collects design principles for that phase, before a formal non-profit entity is created.\n* **Founder self-funding**\n The Founder (or a small initial circle) can cover early expenses  for example hosting, domain names, or basic tooling  without creating an expectation of profit.\n* **No fixed salaries at the very start**\n In the most early phase, work is typically unpaid or compensated informally.\n Salaries for the Founder or other contributors only become appropriate when:\n * sustainable funding is available, and\n * this fits the chosen non-profit legal form and its rules.\n* **Recording early expenses**\n Early out-of-pocket expenses by the Founder can optionally be recorded as:\n * a **non-interest-bearing loan** or\n * a clearly documented \"Founder contribution balance\".\n Any later reimbursement:\n * is limited to actual documented costs,\n * is treated as operational reimbursement, **not** as profit distribution, and\n * must be transparently documented in the financial records.\n* **Transition to a mission-locked non-profit**\n When FactHarbor moves to a formal non-profit entity (e.g. a Swiss Verein or a comparable form), the Finance & Compliance framework should:\n * ensure that the new entity is **mission-locked** (no profit distribution; all income used for the public purpose),\n * define whether and how recorded founder contributions can be reimbursed, and\n * document any **Asset Transfer Protocol** (e.g. domains, repositories, trademarks) in a transparent, auditable way.\nThese points are design guidelines and must be checked against the final legal form and jurisdiction before they are implemented.\nThey complement, but do not replace, the general principles in this page.\n", "Organisation.Legal and Compliance.Large Donations Policy.WebHome": "= Large Donations Policy & Strategy =\n\n**Executive Summary**\n\n{{info}}\n**Core Principle:** No single donor may finance more than **20% of annual budget** (exception: competitive public funding programs like Innosuisse).\n\n**Why:** Editorial independence is the foundation of fact-checking credibility. Large donations create perception of bias, even when none exists.\n\n**IFCN Requirement:** International Fact-Checking Network demands full funding transparency and proven independence.\n{{/info}}\n\n**Quick Decision Guide:**\n\n* **CHF 5K-80K**  Accept (if <20% budget) + Publish donor name\n* **CHF 80K-200K**  Spread over years OR reduce to 20%\n* **CHF 200K-1M**  Propose foundation model\n* **>CHF 1M**  Endowment structure only (capital separate, only returns flow)\n\n**Always Reject:**\n* Political parties\n* Media companies (Facebook, Google News, etc.)\n* Donor demands editorial control\n* Pharma companies (frequent fact-check subject)\n* Donors involved in active fact-checks\n\n----\n\n== 1. Legal Framework (Switzerland) ==\n\n=== Tax Deductibility for Donors ===\n\n**Federal Tax:**\n* Maximum deductible: 20% of net income\n* Example: Income CHF 500,000  max. CHF 100,000 deductible\n\n**Cantonal Taxes:**\n* Most cantons: 10-20%\n* Zurich: 20%\n* Zug: 20%\n* Geneva: 20%\n\n**Example:** Donor with CHF 1M income can deduct max. CHF 200,000 (20%). If they donate CHF 500,000, only CHF 200,000 is tax-deductible.\n\n=== No Legal Limits ===\n\nSwiss law has NO cap on donation amounts. Donors can give CHF 1M, 5M, 10M - but only a portion is tax-deductible.\n\n=== Anti-Money Laundering ===\n\nFor very large donations (CHF 100,000+):\n* Verify donor identity\n* Document source of funds\n* Report suspicious transactions to MROS\n\n----\n\n== 2. IFCN/EFCSN Requirements ==\n\n{{warning}}\n**IFCN Code of Principles requires:**\n\n**Principle 2 - Funding Transparency**\nAll fact-checking organizations must be transparent about funding sources.\n\n**Principle 3 - Methodology Transparency**\nOrganizations must explain their fact-checking methodology.\n\n**Principle 4 - Open & Honest Corrections**\nOrganizations must have transparent corrections policy.\n{{/warning}}\n\n**What IFCN requires:**\n1. Publish all funders above threshold\n2. Prove financial independence\n3. No conflicts of interest\n4. Editorial firewall - no influence on fact-checks\n\n**Typical thresholds among IFCN members:**\n* Publish donors above CHF 1,000-5,000\n* Extra transparency for donors above CHF 10,000-50,000\n* Written editorial firewall agreements\n\n**Examples from IFCN Members:**\n\n**Full Fact (UK)**\n* Publishes all donors over 1,000\n* Policy: \"No funder has any influence over our fact-checking\"\n\n**FactCheck.org (USA)**\n* Publishes all donors over $1,000\n* Maximum: Single donor may NOT finance >10% of budget\n\n**Correctiv (Germany)**\n* Publishes all donors over 5,000\n* Rule: No single source may exceed 20% of budget\n\n----\n\n== 3. Risk Analysis ==\n\n**Credibility Risks by Donation Amount:**\n\n|=Amount|=Risk Level|=Notes\n|CHF 10K-50K|**Low**|No problem with transparency\n|CHF 50K-200K|**Medium**|Needs clear governance rules\n|CHF 200K-1M|**High**|Could be >20% budget  dangerous\n|>CHF 1M|**Very High**|Almost certainly >50%  toxic for credibility\n\n**Specific Risks:**\n\n{{error}}\n**Risk 1: Perceived Dependence**\nEven if untrue, perception damages credibility. Public thinks: \"FactHarbor won't fact-check against main donor.\"\n**Solution:** Transparency + Editorial Firewall\n{{/error}}\n\n{{error}}\n**Risk 2: Actual Influence**\nDonor tries to influence fact-checks. Example: \"I donate CHF 500K, but don't fact-check my company.\"\n**Solution:** Clear contracts, no editorial rights, return donation if violated\n{{/error}}\n\n{{warning}}\n**Risk 3: IFCN/EFCSN Rejection**\nToo much dependence on single donor  IFCN could deny signatory status, EFCSN could reject membership\n{{/warning}}\n\n{{warning}}\n**Risk 4: Partnership Problems**\ndpa, SRF, Full Fact might refuse cooperation if they question independence\n{{/warning}}\n\n----\n\n== 4. Solution: The 20% Rule ==\n\n{{success}}\n**Implement These Rules:**\n\n**1. Publication Threshold**\n* Publish all donors above **CHF 5,000** (name, amount, optional purpose)\n* Publication: On website and in annual report\n\n**2. Maximum Rule (CRITICAL!)**\n* Single donor maximum **20% of annual budget**\n* Example: Budget CHF 500K  max. CHF 100K per donor\n* Exception: Multi-year commitments with exit strategy\n\n**3. Editorial Firewall**\n* Written declaration: Donor has NO say in fact-checks\n* In donation contract: Explicitly stated\n* If violated: Return donation immediately\n\n**4. Diversification Goal**\n* Minimum 5 funding sources\n* No source >30% over 2+ years\n{{/success}}\n\n**Add to Bylaws:**\n\n{{code language=\"text\"}}\n 10 - Financing and Donations\n\n1. The association finances itself through:\n - Grants (Innosuisse, Gebert Rf, etc.)\n - Donations from individuals and organizations\n - Services (within nonprofit regulations)\n\n2. Transparency:\n a) All funders above CHF 5,000 are published annually\n b) Annual report with detailed financial overview\n\n3. Independence:\n a) No single funder may finance more than 20% of annual budget\n b) Donors have NO say in fact-checks whatsoever\n c) Editorial Firewall must be in all agreements\n\n4. Exceptions:\n Competitive public funding (Innosuisse, SNF, etc.) exempt\n from 20% rule when secured through academic partner.\n{{/code}}\n\n----\n\n== 5. Foundation Model (for CHF 1M+ donations) ==\n\nWhen someone wants to donate very large amounts:\n\n**Option 1: Separate Foundation**\n* Donor establishes own foundation with CHF 1M+\n* Foundation funds FactHarbor long-term at <20% annually\n* FactHarbor remains independent\n* Advantage: Foundation can fund other projects too\n\n**Option 2: Endowment Model**\n* CHF 2M+ as capital stock in FactHarbor Foundation\n* Only returns (3-5%) flow to association  CHF 60-100K/year\n* Advantage: Sustainable, never >20% of budget\n\n**Option 3: Matching Fund**\n* Donor matches other donations up to CHF 500K\n* Example: For every CHF 1 donated, major donor gives CHF 1\n* Advantage: Encourages diversification automatically\n\n----\n\n== 6. Governance Safeguards ==\n\n=== Independent Ethics Advisory Board ===\n\n**Establish:**\n* 3-5 independent experts (journalism, academia, ethics)\n* Task: Monitor independence\n* Authority: Can reject donations if conflict of interest\n\n=== Donation Rejection Policy ===\n\n{{error}}\n**Always Reject:**\n* Political party  REJECT\n* Media company  Only with strict firewall (critical!)\n* Government direct grant (except competitive public funding)  Critically review\n* Donor demands editorial control  REJECT\n* Donor involved in active fact-checks  REJECT\n{{/error}}\n\n**Examples:**\n\n Meta/Facebook donates CHF 500K  **REJECT** (fact-check subject!)\n Political party donates CHF 100K  **REJECT**\n Philanthropic foundation donates CHF 100K  **OK** (with transparency)\n Tech company outside media CHF 50K  **OK** (e.g., SAP)\n\n=== Conflict Management Process ===\n\n**When large donation arrives:**\n\n**Step 1: Due Diligence (Board)**\n* Who is the donor?\n* Any conflicts of interest?\n* Is source of funds clear?\n\n**Step 2: Ethics Board Review**\n* Automatic for >CHF 50,000\n* Recommendation: Accept / Reject / Conditions\n\n**Step 3: Transparency Protocol**\n* Document donation\n* Sign firewall agreement\n* Prepare publication\n\n**Step 4: Annual Review**\n* Is diversification adequate?\n* Does donor exceed 20% threshold?\n* Exit strategy needed?\n\n----\n\n== 7. Implementation Steps ==\n\n=== Before First Large Donation ===\n\n**Milestone: Amend Bylaws**\n1. Add  Transparency & Financing\n2. Codify 20% rule\n3. Define Editorial Firewall\n4. Get General Assembly approval\n\n**Milestone: Prepare Documentation**\n1. Donation contract template (with firewall clause)\n2. Donor due diligence form\n3. Rejection letter template\n\n**Month 2: Website**\n1. Create \"Our Funding\" page\n2. List all funders >CHF 5,000\n3. Explain independence principles\n\n=== When Donation Arrives ===\n\n**Step 1: Initial Conversation**\nClarify: Motivation? Expectations? Accept 20% rule? Accept transparency?\n\n**Step 2: Due Diligence**\nCheck: Conflict of interest? Legal source? Exceeds 20%? Political/media connections?\n\n**Step 3: Ethics Board** (for >CHF 50K)\nConsult 2-3 external advisors minimum\n\n**Step 4: Decision**\n\n**Accept** (if all criteria met):\n* Sign contract with firewall\n* Receive donation\n* Publish on website\n\n**Reduce:**\n* Example: Donor wants CHF 200K, budget is CHF 500K\n* Proposal: Accept CHF 100K (=20%) OR spread over 2 years\n\n**Restructure** (Foundation Model):\n* Example: Donor wants CHF 2M\n* Proposal: Separate foundation gives CHF 60K/year\n\n**Reject:**\n* Irresolvable conflict of interest\n* Donor wants editorial control\n* Unclear source\n* Political party/media company\n\n----\n\n== 8. Example Scenarios ==\n\n=== Scenario 1: Tech Billionaire - CHF 500,000 ===\n\n**Context:**\n* FactHarbor budget: CHF 400K\n* Donor: Swiss tech entrepreneur\n* Problem: CHF 500K = 125% of budget  TOO HIGH!\n\n**Solution Options:**\n\n{{success}}\n**Option A: Reduce to CHF 80K (20%)**\n\"Thank you! We can accept maximum CHF 80K to maintain independence. Would you like to spread this over multiple years?\"\n\n**Option B: Foundation Model**\n\"Would you establish a foundation with CHF 500K that funds FactHarbor long-term (CHF 15-25K/year)?\"\n\n**Option C: Matching Fund**\n\"Could you match every donation up to CHF 250K? This helps us build diverse funding.\"\n{{/success}}\n\n{{error}}\n**DO NOT:** Simply accept CHF 500K  Credibility destroyed!\n{{/error}}\n\n=== Scenario 2: Pharma Company - CHF 100,000 ===\n\n**Context:**\n* Donor: Novartis/Roche\n* Problem: Pharma is frequent fact-check subject!\n\n{{error}}\n**REJECT!**\n\nRejection letter:\n\"Thank you for your interest. Unfortunately, we cannot accept as pharmaceutical companies are regularly subject to fact-checks. We must avoid any appearance of conflict of interest (IFCN Code of Principles). Our editorial independence is non-negotiable.\"\n{{/error}}\n\n=== Scenario 3: Anonymous Donor - CHF 50,000 ===\n\n**Context:**\n* Private person wants anonymity\n* Problem: Contradicts IFCN transparency\n\n**Solution:**\n\n{{success}}\n**Option A: Internal ID, Public Anonymous**\n\"We must know your identity internally (due diligence), but can list you publicly as 'Private individual, CHF 50K'.\"\n\n**Option B: Fully Transparent**\n\"For IFCN compliance, we must publish all donors >CHF 5K. Can we list you as '[Name], CHF 50K'?\"\n\n**Option C: Below Threshold**\n\"Would you donate CHF 4,500? Then it's not published.\"\n{{/success}}\n\n{{error}}\n**DO NOT:** Accept CHF 50K anonymously without internal verification  AML risk, IFCN violation\n{{/error}}\n\n=== Scenario 4: Government - CHF 200,000 ===\n\n**Context:**\n* Swiss federal office funding\n* Problem: Government as funder is critical!\n\n**Distinguish:**\n\n{{success}}\n**Public Funding Programs (OK)**\n* Innosuisse:  (competitive, scientifically reviewed)\n* SNSF:  (peer-reviewed)\n* Structured programs with academic partner: \n{{/success}}\n\n{{error}}\n**Direct Government Grants (CRITICAL)**\n* Direct from minister/council:  REJECT\n* \"Government funds fact-checker\"  Toxic!\n{{/error}}\n\n----\n\n== 9. Donation Contract Template ==\n\n{{code language=\"text\"}}\nDONATION AGREEMENT\n\nFactHarbor Association (\"Recipient\")\n[Donor Name] (\"Donor\")\n\n 1 - Donation\nAmount: CHF [AMOUNT]\nPurpose: General activities  / Specific project \n\n 2 - Editorial Firewall (CRITICAL!)\nThe Donor acknowledges:\na) NO say in selection, execution, or evaluation of fact-checks\nb) Recipient may fact-check ALL topics, including Donor\nc) Donor will not influence editorial decisions\nd) Violation  Recipient may return donation\n\n 3 - Transparency\na) Published on website (name/amount) if >CHF 5,000\nb) Mentioned in annual report\n Full name  \"Private individual\" (only if <CHF 50K)\n\n 4 - Tax Receipt\nRecipient issues receipt. Recipient is nonprofit.\n\n 5 - No Quid Pro Quo\nNo expectation of services in return.\n\n 6 - Return Right\nRecipient may return if: conflict discovered, violates 20% rule,\nor other problems.\n\n 7 - Law: Swiss law, jurisdiction [LOCATION]\n\n[DATE]\n_____________ _____________\nDonor President FactHarbor\n{{/code}}\n\n----\n\n== 10. Monitoring & Reporting ==\n\n=== Periodic Review ===\n\n**Periodically:**\n* List all donations >CHF 5K\n* Calculate percentage of each donor\n* Check if anyone >20%\n* Identify risk donors\n* Update website\n\n=== Annual Report ===\n\n{{info}}\n**Publish:**\n\n**1. Financial Overview**\n* Total revenue\n* Breakdown: Grants / Major donations / Small donations / Services\n\n**2. All Funders >CHF 5K**\n* Name, Amount, Percentage of budget\n\n**3. Independence Statement**\n\"FactHarbor is independent. No funder influences our fact-checks. We follow IFCN Code of Principles.\"\n{{/info}}\n\n=== Website: \"Our Funding\" Page ===\n\n**Include:**\n\n**Principles:**\n* 20% Rule\n* Editorial Firewall\n* Transparency (publish >CHF 5K)\n* Diversification (min. 5 sources)\n\n**Current Funders (example):**\n\n**Institutional Funding (65%)**\n* Innosuisse Innovation: CHF 400,000\n* ETH Zurich Partnership: CHF 50,000\n\n**Foundations & Donations (25%)**\n* Foundation A: CHF 80,000\n* Individual B: CHF 50,000\n* Company C: CHF 20,000\n\n**Small Donations (10%)**\n* 234 donors: CHF 65,000\n\n**Total: CHF 665,000**\n\n----\n\n== 11. Long-Term Strategy ==\n\n**Year 1-2: Grants First**\n* 70% Institutional grants\n* 20% Foundations/Medium donations\n* 10% Small donations\n\n**Year 3-5: Diversify**\n* 40% Institutional grants\n* 30% Foundations/Partnerships\n* 20% Earned income (services, licensing)\n* 10% Small donations\n\n**Year 5+: Sustainable**\n* 30% Endowment returns\n* 30% Institutional grants\n* 20% Earned income\n* 20% Many small donations\n\n**Diversification Steps:**\n1. Combine multiple grants (Innosuisse, Gebert Rf, Google, Prototype Fund)\n2. Corporate partnerships (tech companies, NOT media) - each <CHF 50K\n3. Small donor campaigns (1,000  CHF 50 = CHF 50K)\n4. Earned income (workshops, API licensing, consulting)\n\n----\n\n== Summary & Quick Reference ==\n\n===  DOs ===\n\n{{success}}\n1. Enforce 20% rule (exception: competitive public funding)\n2. Transparency: Publish all donors >CHF 5K\n3. Editorial Firewall: Always in writing\n4. Due Diligence: Verify donor identity\n5. Ethics Board: Consult for >CHF 50K\n6. Diversify: Minimum 5 sources\n7. Monitor: Periodic review\n{{/success}}\n\n===  DON'Ts ===\n\n{{error}}\n1. NEVER >20% from one donor\n2. NEVER political parties\n3. NEVER media companies\n4. NEVER accept editorial control\n5. NEVER without written firewall\n6. NEVER without due diligence\n7. NEVER hide donors (>CHF 5K)\n{{/error}}\n\n=== Standard Response to Large Donors ===\n\n{{code language=\"text\"}}\n\"Thank you for your interest in supporting FactHarbor!\n\nFor credibility as an independent fact-checking organization,\nno single funder may finance more than 20% of our budget\n(IFCN Code of Principles).\n\nWith our current budget of CHF [X], we can accept maximum\nCHF [20%] as direct donation.\n\nAlternative models:\n- Distribution over multiple years\n- Establishing supporting foundation\n- Matching fund model\n\nYour support means a great deal. We're confident we can find\na solution that matches your intent while preserving our\nindependence.\"\n{{/code}}\n\n=== Checklist Before Accepting ===\n\n{{code language=\"text\"}}\n Amount <20% of annual budget?\n NOT media company?\n NOT political party?\n NOT involved in active fact-checks?\n Source clear and legal?\n Contract with firewall ready?\n Publication planned?\n Ethics board consulted (>CHF 50K)?\n Board approved?\n Monitoring system ready?\n\nAll checked?  ACCEPT \n{{/code}}\n\n----\n\n**Related Pages:**\n* FactHarbor Bylaws\n* Funding Sources\n* IFCN Signatory Application\n* Governance Structure\n* Transparency Policy\n\n**Document Owner:** Board of Directors | **Review:** Annual\n\n----\n\n**For Questions:**\n* Legal counsel for contracts\n* IFCN for best practices\n* Other fact-checkers for experience\n", "Organisation.Legal and Compliance.Legal Framework.WebHome": "= Legal Framework =\n== 1. Purpose ==\nThe Legal Framework page summarises the main legal aspects that affect FactHarbor as a small organisation and open-source project.\n== 2. Organisational Form ==\nThe precise legal form (e.g. association, foundation, company, or hybrid) will be chosen based on jurisdiction, funding needs, and governance requirements.\nRegardless of the final form, the organisation should support:\n* transparent governance\n* open collaboration and community involvement\n* clear rules for asset ownership (code, data, documentation).\n== 3. Future Non-Profit Entity and Mission-Lock ==\nIn the medium term, FactHarbor is intended to be operated by a **mission-locked non-profit entity**, for example a Swiss Verein or a comparable legal form in another jurisdiction.\nThe key design intentions are:\n* **Mission-locked purpose**\n * The legal entity exists to pursue the FactHarbor mission: helping people understand complex, unclear, and contested information on an evidence-based foundation.\n * All activities and uses of funds must remain compatible with this public-interest purpose.\n* **No profit distribution**\n * Any surplus is reinvested into the mission (infrastructure, development, organisational work, community support).\n * There is no distribution of profits to founders, members, or donors.\n * Fair, market-aligned salaries and reimbursements for work are allowed where they fit the legal form and funding situation.\n* **Independence from funders**\n * Funders (donors, grant-makers, customers) cannot unilaterally override the Governing Team or the mission.\n * Governance rules should prevent capture by a single sponsor or interest group.\n* **Asset and IP stewardship**\n * Critical assets (domains, repositories, trade names, trademarks where applicable) are held by the non-profit entity, not by individuals.\n * Open source and open content licences remain the primary tools for protecting long-term use and auditability.\n * Where early assets were held by the Founder or an initial entity, a transparent **Asset Transfer Protocol** must document how they are transferred to the non-profit.\n* **Dissolution and continuity**\n * If the non-profit is dissolved, remaining assets should be transferred to another mission-compatible, non-profit organisation.\n * The goal is that the knowledge and artefacts created by FactHarbor remain usable and accessible as far as legally and technically possible.\nThese principles are design guidelines.\nThe concrete implementation (legal form, statutes, contracts) must be reviewed and adapted by qualified legal counsel before being used in practice.\n== 4. Intellectual Property and Licences ==\n* Code, documentation, and data are licensed as described on the [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]] page.\n* Contributors retain copyright to their contributions but grant the organisation and the public the rights described in the relevant licences.\n* Contribution processes should include clear licence notices so that contributors know what they agree to.\n== 5. Data Protection ==\nFactHarbor must respect applicable data-protection laws (e.g. GDPR or local equivalents). Key principles:\n* minimisation of personal data stored\n* clear consent and privacy notices where required\n* secure handling of donor and user data\n* the ability to respond to access and deletion requests where applicable.\n== 6. Contracts and External Partners ==\nWhere contracts with funders, hosting providers, or partners are required:\n* they must not compromise FactHarbor's independence and neutrality\n* they must be documented and accessible to the Governing Team\n* conflicts of interest must be disclosed.\n== 7. Future Work ==\nMore detailed legal material (e.g. draft statutes, terms of use, privacy notice) will be developed in collaboration with legal advisors and stored as separate documents or pages.\n", "Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome": "= Open Source Model and Licensing =\n== 1. Purpose and Relation to Other Documents ==\nThis page explains **how FactHarbor is run from a licensing and enforcement perspective**  as an open, trustworthy, non-profit oriented, but professionally maintained project.\nIt covers in particular:\n* the **licensing choices** for code, documentation, data, and core specifications,\n* how contributors grant the project the right to use and enforce those licenses,\n* how AI-related components (such as AKEL) fit into the licensing picture,\n* how licence choices support manipulation-resistance and long-term openness,\n* **organisational transparency commitments**,\n* **privacy and data protection standards**.\nTogether with the other Organisation pages, it defines **how FactHarbor is run**:\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]  who decides what, and under which principles\n* [[Finance & Compliance>>FactHarbor.Organisation.Legal and Compliance.Finance and Compliance.WebHome]]  how funding, transparency, and internal controls work\n* [[Legal Framework>>FactHarbor.Organisation.Legal and Compliance.Legal Framework.WebHome]]  legal forms, contracts, and regulatory aspects\nThe **Specification** (Mission, Requirements, Architecture, Data Model, Workflows, etc.) describes **what FactHarbor does**.\nThis Open Source Model and Licensing page (together with Governance and Finance & Compliance) describes **how FactHarbor is run and protected**.\nFor historical context, earlier drafts used a purely AGPLv3-centric model for the core software.\nThe current licence mix is defined in the sections below and takes precedence over any older drafts.\n== 2. Overview ==\nFactHarbor is, and will remain, an **open source project** that:\n* publishes its work openly whenever legally and ethically possible\n* makes its reasoning and evidence inspectable\n* invites contributions under clear, transparent rules\n* avoids situations where a \"FactHarbor-branded\" system becomes a black box\n* maintains exceptional organisational transparency to build trust\nThis page defines:\n* the **licensing choices** currently used,\n* the **goals and principles** behind these choices,\n* how contributors are governed from a licensing/enforcement perspective,\n* how AI models and third-party components are handled,\n* the standards that repositories must follow,\n* **organisational transparency and privacy commitments**.\nNormative licensing decisions on this page **override** any older variants or drafts.\n== 3. Licensing (Current Decisions) ==\n=== 3.1 Documentation ===\nAll general **documentation** (organisational and technical) is licensed under:\n* **Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)**\nThis allows:\n* reuse, adaptation, and translation of documentation,\n* including commercial reuse,\n* as long as:\n * clear attribution to FactHarbor is preserved, and\n * derivative works are shared under the **same license (CC BY-SA 4.0)**.\nException handling:\n* In rare cases, **security-sensitive or abuse-enabling documentation** may be:\n * published only in partial form, or\n * made available under more restrictive terms, or\n * kept internal.\n* Any such exceptions must be **explicitly documented** where they apply.\n=== 3.2 Core Protocol & Data Model ===\nThe **core protocol**, core **data model** (including key ERDs), and other \"defining specifications\" are licensed under:\n* **Creative Commons Attribution-ShareAlike 4.0 (CC BY-SA 4.0)**\nIntent:\n* enable **collaborative evolution** of the protocol and data model,\n* allow broad reuse, referencing, and implementation,\n* ensure derivative specifications remain open (share-alike requirement),\n* maintain canonical status through **trademark control** rather than license restrictions.\nImplications:\n* You may **use, implement, and modify** the protocol/data model in your own systems.\n* You may **publish derivative or modified specifications** under CC BY-SA 4.0.\n* Derivative specifications must:\n * be clearly attributed to FactHarbor,\n * use different branding/names (trademark protection),\n * state they are \"derived from FactHarbor protocol\",\n * remain under CC BY-SA 4.0 (share-alike).\n* Changes to the **canonical FactHarbor specification** are governed through FactHarbor's internal review and release processes.\n**Trademark Protection:**\nThe \"FactHarbor\" name and associated marks are protected separately from the license. Derivative protocols may not use \"FactHarbor\" branding without explicit permission, ensuring users can distinguish official from derivative implementations.\nThis approach (license for sharing + trademark for brand protection) follows successful models like Mozilla Firefox and the W3C.\n=== 3.3 Code ===\n**Default License:** Unless explicitly stated otherwise, **code** produced under the FactHarbor project is licensed under:\n* **MIT License**\nThis allows:\n* broad reuse, including in commercial software,\n* proprietary integrations and extensions,\n* as long as:\n * the MIT license text is included, and\n * attribution to the FactHarbor project is preserved.\n**Hybrid Licensing for Core Components:**\nFor the **core reasoning engine** and **AKEL components**, we recommend using **AGPL-3.0** to prevent black-box deployments and ensure transparency of modifications.\nThe recommended hybrid approach:\n* **AGPL-3.0** for: Core verdict engine, AKEL reasoning logic, scenario evaluation engine\n* **MIT** for: Integrations, utilities, frontend clients, libraries, tools\nThis hybrid model (similar to Wikimedia's use of AGPL for MediaWiki) balances maximum adoption with protection of the transparency mission.\n**Rationale:**\n* AGPL-3.0 is **network copyleft**  requires source disclosure for network services\n* Prevents \"FactHarbor-as-a-service\" black boxes that contradict transparency mission\n* MIT for peripheral components maximizes ecosystem growth\n* Strong protection of **openness of reasoning** is handled via:\n * open protocol and data model (CC BY-SA),\n * open documentation (CC BY-SA),\n * AGPL for core reasoning components,\n * and explicit transparency rules.\nThe decision to implement this hybrid model should be made explicitly before the first public release.\n=== 3.4 Structured Data & Curation Artefacts ===\nStructured data, curated knowledge artefacts and derived datasets are licensed under:\n* **Open Database License (ODbL)**\n**Note on ODbL:** The Open Database License includes a share-alike requirement, ensuring derivative databases remain open. This aligns with FactHarbor's commitment to openness and prevents proprietary capture of community-curated data.\nPrinciples:\n* data used for public reasoning should be:\n * reusable and remixable,\n * properly attributed,\n * versioned and traceable,\n * kept open through share-alike.\n* privacy, safety, and legal constraints may require:\n * partial publication or anonymity,\n * stronger access control around certain datasets.\nConcrete exceptions and more restrictive handling must be **documented at dataset level**.\n=== 3.5 Attribution Guidelines (Non-Mandatory but Recommended) ===\nFactHarbor encourages, but generally does not require beyond the base licenses, that:\n* user interfaces show a short line such as:\n`Powered by FactHarbor (open documentation, open protocol, open data)`\nIntent:\n* strengthen **brand recognition** and trust,\n* keep attribution light-weight and compatible with open licenses,\n* avoid creating extra legal conditions beyond the existing licenses.\n== 4. Licensing Goals and Principles ==\nEarlier \"Open Source Model & Licensing\" drafts contained valuable reasoning about **why** strong open-source protections might be needed. The core goals remain relevant, even though the exact license mix has evolved.\nFactHarbor's licensing aims to:\n* **Protect openness of reasoning**\n * Users must be able to understand how conclusions were reached.\n * Code and documentation that materially affect user-visible behaviour should be inspectable or clearly described.\n* **Discourage hostile or misleading forks**\n * Avoid \"closed clones\" that keep the FactHarbor name or appearance while hiding important changes.\n * Forks that significantly diverge should use their own branding and not pretend to be official FactHarbor instances.\n* **Make modifications traceable**\n * Substantial changes to code, specs, or governance documents should be documented and versioned.\n * Users interacting with a service based on FactHarbor should be able to see **which version or fork** they are using.\n* **Support long-term sustainability and legal clarity**\n * Licenses and governance must be enforceable in practice.\n * The organisation should have clear standing to protect the project if needed.\n== 5. Contributors, Governance & CLA ==\n=== 5.1 Contributor Journey (from licensing perspective) ===\nThe contributor journey (Visitor  New Contributor  Contributor  Trusted Contributor  Contributor  Moderator  Trusted Contributor) is defined in more detail in the **Contributor Processes** and **Organisation** pages.\nFrom a *licensing* perspective, the key points are:\n* All contributions must be compatible with the chosen licenses (CC, MIT, AGPL, ODbL, etc.).\n* Contributors confirm that they have the right to contribute the material under these licenses.\n* Higher-trust roles (Trusted Contributors, Contributors, Moderators) help enforce licensing and attribution rules when reviewing changes.\nFor full role definitions, see the **Organisation / Contributor Processes** documentation.\n=== 5.2 Contributor License Agreement (CLA) ===\nTo keep the legal situation clear and enforceable, FactHarbor uses a **Contributor License Agreement (CLA)**.\nSee [[Contributor License Agreement>>FactHarbor.Organisation.Legal and Compliance.CLA.WebHome]].\n==== 5.2.1 Dual Contributor Model ====\nFactHarbor distinguishes between two contributor types with different copyright arrangements:\n**Unpaid Contributors (Volunteers)**:\n* **Retain copyright** of their contributions\n* Grant FactHarbor a perpetual, royalty-free license to use and distribute\n* Enable the project to enforce licenses on their behalf\n* Maintain attribution in version control and documentation\n**Paid Contributors (Employees, Contractors)**:\n* **Transfer copyright** to FactHarbor Organisation\n* Ensures clear ownership for sponsored work\n* Simplifies long-term governance\n* Still receive attribution for their contributions\nThis dual model:\n* Respects volunteer contributions while preserving their rights\n* Provides clarity for commercially sponsored work\n* Ensures FactHarbor can effectively maintain and defend the project\n* Maintains transparency about contribution sources\n==== 5.2.2 Core Intent (All Contributors) ====\nRegardless of contributor type, the CLA ensures:\n* Contributors grant the **FactHarbor organisation**:\n * a **perpetual, worldwide, irrevocable license** to use, modify, and redistribute their contributions under the project's chosen licenses (CC BY-SA, MIT, AGPL, ODbL, etc.), and\n * the **express right to enforce** those licenses and **pursue legal action** against infringers on their behalf.\nThis ensures that:\n* the organisation has **clear standing** to defend the project legally,\n* individual contributors do not have to act alone against infringements,\n* licensing remains enforceable even if contributors become inactive.\n==== 5.2.3 Determining Contributor Type ====\n* **Default**: Contributors are considered unpaid volunteers unless they have a written agreement specifying paid status.\n* **Paid Status Indicators**: Employment contract, written contracting agreement, or grant/sponsorship agreement.\n* **Transparency**: Contributor type should be disclosed where applicable.\nSee [[Contributor License Agreement>>FactHarbor.Organisation.Legal and Compliance.CLA.WebHome]] for complete terms.\n== 6. AI Models and Licensing (AKEL) ==\nAKEL (AI Knowledge Extraction Layer) may rely on different types of models. Licensing and transparency rules are crucial here.\n=== 6.1 Open vs Proprietary Models ===\nAKEL may use:\n* **Open-source models (preferred)**:\n * weights and code are openly available under compatible licenses,\n * prompts, evaluation logic and integration code are made public where licenses permit.\n* **Proprietary / hosted models (allowed but constrained)**:\n * used only when necessary for quality or feasibility,\n * must be clearly **disclosed to the user** at point of use,\n * AKEL must label which parts of its output derive from proprietary tools,\n * surrounding **integration logic remains open** (MIT/AGPL or compatible) and is documented.\nRules:\n* No deployment may suggest \"fully open\" AI if proprietary models are used without disclosure.\n* For high-impact reasoning (e.g. health, politics, safety-critical topics), **open, auditable models** are preferred wherever feasible.\n* Where proprietary models are unavoidable, additional care is taken to:\n * document limitations,\n * avoid overstating certainty,\n * and keep reasoning layers as transparent as possible.\n=== 6.2 Prompts, Pipelines and Integration Code ===\n* Orchestration code, pipelines and evaluation logic around AKEL are treated as part of the **open FactHarbor codebase** (MIT or AGPL).\n* Where prompts or model configurations are licensed in a way that restricts publication, this must be documented clearly, and safe abstractions should be used in public documentation.\n=== 6.3 AI Prompts and Orchestration ===\n* Prompts, system instructions, and orchestration code are considered **Code** and licensed under **MIT** or **AGPL** (depending on component).\n* They must be visible in the repository to ensure the system is not a 'black box'.\n* If a proprietary model requires a prompt that cannot be shared (e.g. contractual restriction), that component cannot be part of the open core.\n== 7. Third-Party Libraries and Components ==\nFactHarbor depends on third-party libraries under:\n* permissive licenses (MIT, Apache-2.0, BSD), and/or\n* other compatible open-source licenses.\nRequirements:\n* All dependencies must be **license-compatible** with:\n * the MIT/AGPL-licensed code,\n * and the overall FactHarbor licensing strategy.\n* License information is documented in:\n * `/LICENSE` and, where applicable, `/NOTICE`,\n * and a dedicated \"Third-Party Licenses\" section in project documentation.\nFactHarbor actively avoids dependencies that:\n* restrict redistribution in ways incompatible with open-source norms,\n* prevent network users from accessing the relevant source,\n* or conflict with the project's transparency and licensing goals.\n== 8. Repository Standards ==\nEach official FactHarbor repository must follow a minimum standard.\n=== 8.1 Required Files ===\nEach repository should contain at least:\n* `README`  purpose, scope, status, and how to use it.\n* `LICENSE`  the applicable license(s) for the repository.\n* `CONTRIBUTING`  how to propose changes; coding/writing guidelines.\n* `CODEOWNERS`  who is responsible for which parts.\n* `CHANGELOG`  human-readable log of important changes.\n* `SECURITY` (or `SECURITY.md`)  how to report vulnerabilities and how they are handled.\n=== 8.2 Prohibited Content ===\nFactHarbor repositories must **not** contain:\n* purely ideological advocacy texts unrelated to the project's purpose,\n* opaque binaries or artefacts that cannot reasonably be inspected or reproduced,\n* embedded secrets (API keys, passwords, private tokens),\n* content that materially contradicts the stated licenses or governance rules.\n== 9. Historical Licensing Option: AGPLv3 for Core Engine (Non-Normative Background) ==\nEarlier versions of this page explored a **strong copyleft** option for the core software based on **GNU Affero General Public License v3 (AGPLv3)**.\nThose drafts argued that:\n* AGPLv3, as a **network-copyleft license**, would:\n * require modified network services to publish their source to users,\n * prevent closed forks of the core reasoning engine,\n * ensure that any public \"FactHarbor-like\" service stays inspectable.\nThey also defined:\n* the scope of AGPLv3 coverage (backend services, AKEL logic, frontend),\n* expectations for forks (must remain AGPLv3, must declare they are forks),\n * and the same CLA principles now adapted to the current license mix.\nThese AGPLv3 considerations have been **partially adopted** in the hybrid licensing model (section 3.3), where AGPL-3.0 is recommended for core reasoning components.\nThey are preserved here as **design background** and may be revisited for specific components or future arrangements.\n== 10. Organisational Transparency ==\nFactHarbor is committed to exceptional transparency in all aspects of its operations, governance, and finances. This commitment is essential to build trust in a system claiming to support well-grounded judgments.\n=== 10.1 Financial Transparency ===\nWe commit to publishing annually:\n* Complete financial statements (audited where possible)\n* Swiss tax filings (annual statements per Swiss law)\n* Income sources in aggregate (grants, donations, sponsorships)\n* Expense breakdown by category\n* Compensation ranges for staff roles (not individual salaries)\n* Major funding relationships and partnerships\n=== 10.2 Governance Transparency ===\nWe commit to publishing:\n* All governance documents (bylaws, policies, procedures)\n* Governing Team composition and meeting schedules\n* Governing Team meeting minutes (with narrow exceptions for privacy, security, or legal matters)\n* Policy changes with rationale and effective dates\n* Decision-making process documentation\n* Conflict of interest policies and disclosures\n=== 10.3 Operational Transparency ===\nWe commit to publishing:\n* Transparency reports (published twice yearly)\n* Content moderation statistics and practices\n* AKEL performance metrics and audit results\n* Risk tier assignment statistics\n* Partnership agreements and funding relationships\n* Incident reports (security, moderation, governance)\n* System uptime and performance data\n=== 10.4 Privacy Protection ===\nWhile maintaining organisational transparency, we protect:\n* Individual user privacy and personal data\n* Security vulnerabilities (until patched, typically 30-90 days)\n* Personnel matters and personal information\n* Ongoing legal matters (until resolved)\n* Whistleblower and abuse reports\n* Authentication credentials and sensitive operational details\n=== 10.5 Review and Oversight ===\n* Annual review of all information marked \"private\"\n* Public reporting on transparency compliance\n* Community input opportunities on transparency policies\n* Appeals process for information requests\n* Independent transparency audits (when feasible)\nSee [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]] for complete details.\n== 11. Privacy and Data Protection ==\nFactHarbor is committed to protecting user privacy while maintaining transparency in operations and governance.\n=== 11.1 Data Collection Principles ===\n* **Data minimization**: Collect only what is necessary for functionality\n* **Purpose limitation**: Use data only for stated purposes\n* **Short retention**: Delete data when no longer needed\n* **User control**: Provide access, correction, and deletion rights\n=== 11.2 User Rights ===\nUsers have the right to:\n* Access their personal data\n* Correct inaccurate information\n* Delete their accounts and associated data\n* Export their data (portability)\n* Object to certain processing\n* Lodge complaints with supervisory authorities\n=== 11.3 What We Collect ===\nFor specific details on data collection practices, retention periods, and processing purposes, see [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]].\nIn general:\n* **Public contributions**: Permanently public and attributed (essential for transparency)\n* **Account information**: Email, username (minimal required data)\n* **Technical data**: IP addresses, user agents (short retention, logged out users)\n* **Usage data**: Aggregated, anonymized analytics\n=== 11.4 What We Never Do ===\n* Sell or rent user data\n* Share personal data with third parties for marketing\n* Track users across unrelated sites\n* Use personal data for purposes beyond stated scope\n* Keep personal data longer than necessary\n=== 11.5 Security Measures ===\n* Encryption in transit (TLS/HTTPS)\n* Encryption at rest for sensitive data\n* Access controls and authentication\n* Regular security audits\n* Incident response procedures\n* Vulnerability disclosure program\n* **Data Protection Impact Assessments (DPIA)** for high-risk processing (required by FADP Article 22)\nSee [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]] for complete details.\n=== 11.6 Data Protection Officer (DPO) ===\n**If we serve users in the European Union**, we will appoint a Data Protection Officer (DPO) as required by EU GDPR Article 37.\nThe DPO will:\n* Advise on data protection compliance\n* Monitor FADP and GDPR compliance\n* Serve as contact point for Swiss FDPIC and EU data protection authorities\n* Conduct privacy audits and DPIAs\n* Handle data subject requests\nContact (if appointed): [DPO contact to be established if needed]\n**Note:** Swiss law (FADP) does not require a DPO for organizations of our size. However, EU GDPR Article 37 requires a DPO for:\n* Large-scale systematic monitoring of data subjects\n* Large-scale processing of sensitive personal data (including political opinions, health information)\nGiven that FactHarbor processes claims containing political opinions and uses AI for systematic evaluation, we commit to appointing a DPO if we process personal data of EU residents.\n== 12. Exceptions and Appeals ==\n=== 12.1 Requesting Information ===\nIf you believe FactHarbor should disclose specific organisational information:\n1. Submit a written request to [Transparency contact to be established]\n2. Specify the information requested and rationale\n3. Expect initial response promptly\n=== 12.2 Appeals Process ===\nIf a transparency request is denied:\n1. Appeal to the Transparency Committee (if established)\n2. Provide additional context or rationale\n3. Expect appeal decision promptly\n4. Final appeals may be escalated to the Governing Team\n=== 12.3 Exception Criteria ===\nInformation may be withheld only if disclosure would:\n* Violate individual privacy rights\n* Compromise security (vulnerability, credential)\n* Violate legal obligations (court order, attorney-client privilege)\n* Enable abuse or harm (expose victim, enable attack)\n* Breach fiduciary duty (ongoing confidential negotiations)\nAll exceptions are time-limited and reviewed annually.\n", "Organisation.Legal and Compliance.Reimbursable Expenses.WebHome": "= Reimbursable Expense Report =\n== 1. Purpose ==\nThis page tracks significant funding sources and expenses to ensure financial transparency. It is updated by the Finance & Compliance Lead.\n== 2. Expenses (Phase 0) ==\n(% class=\"box infomessage\" %)\n(((\n**Note:** In Phase 0, small infrastructure costs are covered by Founder self-funding. No external donations have been accepted yet.\n)))\n|=Date|=Type|=Description|=Amount (Approx)|=Source/Recipient|=Notes|\n//Full detailed records are available to the Governing Team for audit.//\n", "Organisation.Legal and Compliance.WebHome": "= Legal and Compliance =\n\nLegal frameworks, licensing, contributor agreements, and financial governance for FactHarbor.\n\n== Legal Foundations ==\n\n* [[Legal Framework>>FactHarbor.Organisation.Legal and Compliance.Legal Framework.WebHome]]  Organisational form, IP, data protection, and contracts\n* [[Open Source Model and Licensing>>FactHarbor.Organisation.Legal and Compliance.Open Source Model and Licensing.WebHome]]  Licence choices, transparency commitments, and repository standards\n\n== Contributor Agreements ==\n\n* [[Contributor License Agreement (CLA)>>FactHarbor.Organisation.Legal and Compliance.CLA.WebHome]]  Copyright, licence grants, and contributor types\n\n== Financial Governance ==\n\n* [[Finance and Compliance>>FactHarbor.Organisation.Legal and Compliance.Finance and Compliance.WebHome]]  Funding principles, budgeting, and internal controls\n* [[Large Donations Policy>>FactHarbor.Organisation.Legal and Compliance.Large Donations Policy.WebHome]]  The 20% rule, editorial firewall, and donation governance\n* [[Reimbursable Expenses>>FactHarbor.Organisation.Legal and Compliance.Reimbursable Expenses.WebHome]]  Expense tracking and transparency ledger\n\n== Related ==\n\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]  Organisational model and decision processes\n* [[Transparency Policy>>FactHarbor.Organisation.How-We-Work-Together.Transparency-Policy]]  Openness and accountability commitments\n", "Organisation.Strategy.Automation Philosophy.WebHome": "= Automation Philosophy =\n**Core Principle**: AKEL is primary. Humans monitor, improve, and handle exceptions.\n== 1. The Principle ==\n**FactHarbor is AI-first, not AI-assisted.**\nThis is not:\n*  \"AI helps humans make better decisions\"\n*  \"Humans review AI recommendations\"\n*  \"AI drafts, humans approve\"\nThis is:\n*  \"AI makes decisions, humans improve the AI\"\n*  \"Humans monitor metrics, not individual outputs\"\n*  \"Fix the system, not the data\"\n== 2. Why This Matters ==\n=== 2.1 Scalability ===\n**Human review doesn't scale**:\n* 1 person can review ~100 claims/day carefully\n* FactHarbor aims for millions of claims\n* Would need 10,000+ reviewers\n* Impossible to maintain consistency\n**Algorithmic processing scales**:\n* AKEL processes 1 claim or 1 million claims with same consistency\n* Cost per claim approaches zero at scale\n* Quality improves with more data\n* 24/7 availability\n=== 2.2 Consistency ===\n**Human judgment varies**:\n* Different reviewers apply criteria differently\n* Same reviewer makes different decisions on different days\n* Influenced by fatigue, mood, recent examples\n* Unconscious biases affect decisions\n**Algorithmic processing is consistent**:\n* Same input  same output, always\n* Rules applied uniformly\n* No mood, fatigue, or bias\n* Predictable behavior\n=== 2.3 Transparency ===\n**Human judgment is opaque**:\n* \"I just know\" - hard to explain\n* Expertise in human head\n* Can't audit thought process\n* Difficult to improve systematically\n**Algorithmic processing is transparent**:\n* Code can be audited\n* Parameters are documented\n* Decision logic is explicit\n* Changes are tracked\n* Can test \"what if\" scenarios\n=== 2.4 Improvement ===\n**Improving human judgment**:\n* Train each person individually\n* Hope training transfers consistently\n* Subjective quality assessment\n* Slow iteration\n**Improving algorithms**:\n* Change code once, affects all decisions\n* Test on historical data before deploying\n* Measure improvement objectively\n* Rapid iteration (deploy multiple times per week)\n== 3. The Human Role ==\nHumans in FactHarbor are **system architects**, not **content judges**.\n=== 3.1 What Humans Do ===\n**Monitor** system performance:\n* Watch dashboards showing aggregate metrics\n* Identify when metrics fall outside acceptable ranges\n* Spot patterns in errors or edge cases\n* Track user feedback trends\n**Improve** algorithms and policies:\n* Analyze systematic errors\n* Propose algorithm improvements\n* Update policies based on learning\n* Test changes before deployment\n* Document learnings\n**Handle** exceptions:\n* Items AKEL explicitly flags for review\n* System gaming attempts\n* Abuse and harassment\n* Legal/safety emergencies\n**Govern** the system:\n* Set risk tier policies\n* Define acceptable performance ranges\n* Allocate resources\n* Make strategic decisions\n=== 3.2 What Humans Do NOT Do ===\n**Review** individual claims for correctness:\n*  \"Let me check if this verdict is right\"\n*  \"I'll approve these before publication\"\n*  \"This needs human judgment\"\n**Override** AKEL decisions routinely:\n*  \"AKEL got this wrong, I'll fix it\"\n*  \"I disagree with this verdict\"\n*  \"This source should be rated higher\"\n**Act as** approval gates:\n*  \"All claims must be human-approved\"\n*  \"High-risk claims need review\"\n*  \"Quality assurance before publication\"\n**Why not?** Because this defeats the purpose and doesn't scale.\n== 4. When Humans Intervene ==\n=== 4.1 Legitimate Interventions ===\n**Humans should intervene when**:\n==== AKEL explicitly flags for review ====:\n* AKEL's confidence is too low\n* Detected potential manipulation\n* Unusual pattern requiring human judgment\n* Clear policy: \"Flag if confidence <X\"\n==== System metrics show problems ====:\n* Processing time suddenly increases\n* Error rate jumps\n* Confidence distribution shifts\n* User feedback becomes negative\n==== Systematic bias detected ====:\n* Metrics show pattern of unfairness\n* Particular domains consistently scored oddly\n* Source types systematically mis-rated\n==== Legal/safety emergency ====:\n* Legal takedown required\n* Imminent harm to individuals\n* Security breach\n* Compliance violation\n=== 4.2 Illegitimate Interventions ===\n**Humans should NOT intervene for**:\n==== \"I disagree with this verdict\" ====:\n* Problem: Your opinion vs AKEL's analysis\n* Solution: If AKEL is systematically wrong, fix the algorithm\n* Action: Gather data, propose algorithm improvement\n==== \"This source should rank higher\" ====:\n* Problem: Subjective preference\n* Solution: Fix scoring rules systematically\n* Action: Analyze why AKEL scored it lower, adjust scoring algorithm if justified\n==== \"Manual quality gate\" ====:\n* Problem: Creates bottleneck, defeats automation\n* Solution: Improve AKEL's quality to not need human gate\n* Action: Set quality thresholds in algorithm, not human review\n==== \"I know better than the algorithm\" ====:\n* Problem: Doesn't scale, introduces bias\n* Solution: Teach the algorithm what you know\n* Action: Update training data, adjust parameters, document expertise in policy\n== 5. Fix the System, Not the Data ==\n**Fundamental principle**: When AKEL makes mistakes, improve AKEL, don't fix individual outputs.\n=== 5.1 Why? ===\n**Fixing individual outputs**:\n* Doesn't prevent future similar errors\n* Doesn't scale (too many outputs)\n* Creates inconsistency\n* Hides systematic problems\n**Fixing the system**:\n* Prevents future similar errors\n* Scales automatically\n* Maintains consistency\n* Surfaces and resolves root causes\n=== 5.2 Process ===\n**When you see a \"wrong\" AKEL decision**:\n==== Document it ====:\n* What was the claim?\n* What did AKEL decide?\n* What should it have decided?\n* Why do you think it's wrong?\n==== Investigate ====:\n* Is this a one-off, or a pattern?\n* Check similar claims - same issue?\n* What caused AKEL to decide this way?\n* What rule/parameter needs changing?\n==== Propose systematic fix ====:\n* Algorithm change?\n* Policy clarification?\n* Training data update?\n* Parameter adjustment?\n==== Test the fix ====:\n* Run on historical data\n* Does it fix this case?\n* Does it break other cases?\n* What's the overall impact?\n==== Deploy and monitor ====:\n* Gradual rollout\n* Watch metrics closely\n* Gather feedback\n* Iterate if needed\n== 6. Balancing Automation and Human Values ==\n=== 6.1 Algorithms Embody Values ===\n**Important**: Automation doesn't mean \"value-free\"\n**Algorithms encode human values**:\n* Which evidence types matter most?\n* How much weight to peer review?\n* What constitutes \"high risk\"?\n* When to flag for human review?\n**These are human choices**, implemented in code.\n=== 6.2 Human Governance of Automation ===\n**Humans set**:\n*  Risk tier policies (what's high-risk?)\n*  Evidence weighting (what types of evidence matter?)\n*  Source scoring criteria (what makes a source credible?)\n*  Moderation policies (what's abuse?)\n*  Bias mitigation strategies\n**AKEL applies**:\n*  These policies consistently\n*  At scale\n*  Transparently\n*  Without subjective variation\n=== 6.3 Continuous Value Alignment ===\n**Ongoing process**:\n* Monitor: Are outcomes aligned with values?\n* Analyze: Where do values and outcomes diverge?\n* Adjust: Update policies or algorithms\n* Test: Validate alignment improved\n* Repeat: Values alignment is never \"done\"\n== 7. Cultural Implications ==\n=== 7.1 Mindset Shift Required ===\n**From**: \"I'm a content expert who reviews claims\"\n**To**: \"I'm a system architect who improves algorithms\"\n**From**: \"Good work means catching errors\"\n**To**: \"Good work means preventing errors systematically\"\n**From**: \"I trust my judgment\"\n**To**: \"I make my judgment codifiable and testable\"\n=== 7.2 New Skills Needed ===\n**Less emphasis on**:\n* Individual content judgment\n* Manual review skills\n* Subjective expertise application\n**More emphasis on**:\n* Data analysis and metrics interpretation\n* Algorithm design and optimization\n* Policy formulation\n* Testing and validation\n* Documentation and knowledge transfer\n=== 7.3 Job Satisfaction Sources ===\n**Satisfaction comes from**:\n*  Seeing metrics improve after your changes\n*  Building systems that help millions\n*  Solving systematic problems elegantly\n*  Continuous learning and improvement\n*  Transparent, auditable impact\n**Not from**:\n*  Being the expert who makes final call\n*  Manual review and approval\n*  Gatekeeping\n*  Individual heroics\n== 8. Trust and Automation ==\n=== 8.1 Building Trust in AKEL ===\n**Users trust AKEL when**:\n* Transparent: How decisions are made is documented\n* Consistent: Same inputs  same outputs\n* Measurable: Performance metrics are public\n* Improvable: Clear process for getting better\n* Governed: Human oversight of policies, not outputs\n=== 8.2 What Trust Does NOT Mean ===\n**Trust in automation **:\n*  \"Never makes mistakes\" (impossible)\n*  \"Better than any human could ever be\" (unnecessary)\n*  \"Beyond human understanding\" (must be understandable)\n*  \"Set it and forget it\" (requires continuous improvement)\n**Trust in automation =**:\n*  Mistakes are systematic, not random\n*  Mistakes can be detected and fixed systematically\n*  Performance continuously improves\n*  Decision process is transparent and auditable\n== 9. Edge Cases and Exceptions ==\n=== 9.1 Some Things Still Need Humans ===\n**AKEL flags for human review when**:\n* Confidence below threshold\n* Detected manipulation attempt\n* Novel situation not seen before\n* Explicit policy requires human judgment\n**Humans handle**:\n* Items AKEL flags\n* Not routine review\n=== 9.2 Learning from Exceptions ===\n**When humans handle an exception**:\n1. Resolve the immediate case\n2. Document: What made this exceptional?\n3. Analyze: Could AKEL have handled this?\n4. Improve: Update AKEL to handle similar cases\n5. Monitor: Did exception rate decrease?\n**Goal**: Fewer exceptions over time as AKEL learns.\n---\n**Remember**: AKEL is primary. You improve the SYSTEM. The system improves the CONTENT.\n== 10. Related Pages ==\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - How AKEL is governed\n* [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] - How to improve the system\n* [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]] - Team structure and roles\n* [[System Performance Metrics>>FactHarbor.Product Development.Specification.System-Performance-Metrics]] - What we monitor\n", "Organisation.Strategy.Competitive Analysis.WebHome": "= FactHarbor Competitive Analysis =\n\n== Market Landscape & Gap Identification ==\n\n**Date:** December 31, 2025\n**Purpose:** Identify competitive positioning and market gaps for FactHarbor\n\n----\n\n== Executive Summary ==\n\nThe fact-checking landscape in 2025 is experiencing significant disruption. Meta's withdrawal from third-party fact-checking, funding challenges from USAID cuts, and the shift toward crowdsourced models (Community Notes) have created both challenges and opportunities. FactHarbor's **Evidence Model approach** addresses fundamental gaps that existing solutions fail to fill.\n\n**Key Finding:** No current solution provides transparent, scenario-based, probabilistic fact-checking with explicit assumptionsFactHarbor's core differentiation.\n\n----\n\n== 1. Competitive Landscape Overview ==\n\n=== 1.1 Traditional Fact-Checking Organizations ===\n\n|=Organization|=Approach|=Limitations\n|**PolitiFact**|Human expert review, Truth-O-Meter (6-point scale)|Binary/scalar verdicts; no explicit assumptions; criticized for subjective selection; ~21% \"Half True\" verdicts show nuance difficulty\n|**Snopes**|Human review, 5-point scale + special categories|Context-dependent ratings inconsistent; no structured reasoning transparency\n|**FactCheck.org**|Academic/journalistic review|Limited scale; no scenario-based analysis\n|**Full Fact (UK)**|Human + AI tools for claim detection|Tools support humans, don't produce structured models\n\n**Market Data:**\n\n* 443 active fact-checking projects globally (down 2% from 2024)\n* ~160 projects relied on Meta partnerships (now at risk)\n* Fact-check volume down 6% in 2025 (38,000 vs 40,500 ClaimReview-tagged articles)\n\n{{info}}\n**Gap Identified:** All traditional fact-checkers produce **verdicts without transparent reasoning chains**. They answer \"what\" but not \"under what assumptions\" or \"in which contexts.\"\n{{/info}}\n\n----\n\n=== 1.2 Automated/AI-Powered Fact-Checking ===\n\n|=Tool|=Capability|=Limitations\n|**ClaimBuster**|Claim detection and prioritization from text|Does NOT verify claims; only identifies check-worthiness; Squash (verification) was shut down as \"not ready\"\n|**Google Fact Check Tools**|ClaimReview aggregation, fact-check markup|Aggregates existing verdicts; doesn't produce new analysis\n|**Full Fact AI**|Real-time monitoring, claim detection|Detection-focused; still requires human verdict\n|**Winston AI / Originality.AI**|AI content detection|Focus on AI-generated content, not factual verification\n|**LLM-based systems**|GPT/Claude for fact-checking|Poor calibration; overconfident; lack citation grounding\n\n**Academic Research Shows:**\n\n* \"Holy grail\" of fully automated fact-checking remains elusive\n* Key obstacles: \"elusive nature of truth claims, rigidity of binary epistemology, data scarcity, algorithmic deficiencies, transparency issues\"\n* Squash (ClaimBuster-based) shut down\"making too many mistakes\"\n* LLMs show 73% confidence scores but are \"overconfident and unreliable\"\n\n{{info}}\n**Gap Identified:** Automated tools either **detect claims only** (no verdicts) or produce **ungrounded, overconfident verdicts**. None generate structured Evidence Models with scenario-based analysis.\n{{/info}}\n\n----\n\n=== 1.3 Crowdsourced Fact-Checking ===\n\n|=Platform|=Model|=Limitations\n|**X Community Notes**|Crowd-contributed context with bridging algorithm|Slow (delays during fast-moving events); 74% of election misinformation posts never received notes; susceptible to gaming; no systematic methodology\n|**Meta Community Notes** (announced 2025)|Planned X-style system|Untested; Meta's previous fact-checking exit raises reliability concerns\n|**Wikipedia model**|Collective editing|Not designed for real-time claims; verification challenges\n\n**Research Findings:**\n\n* Community Notes posts 32% more likely to be deleted by authors (effective for retraction)\n* But: \"too slow to effectively reduce engagement with misinformation in the early (and most viral) stage\"\n* Only 8.5% of created notes ever displayed\n* Gaza conflict: 68% of top misinformation posts never received notes\n\n{{info}}\n**Gap Identified:** Crowdsourced systems are **reactive, slow, and inconsistent**. They lack systematic methodology and don't produce structured, citable analysis.\n{{/info}}\n\n----\n\n=== 1.4 Emerging AI Approaches (Research Stage) ===\n\n|=Approach|=Status|=Relevance\n|**CLUE (Uncertainty Explanation)**|Research paper|First to explain sources of uncertainty in multi-evidence fact-checkingaligns with FactHarbor philosophy\n|**AmbiFC Dataset**|Academic|Recognizes ambiguous claims need nuanced handling\n|**Climinator (Climate)**|Domain-specific|Multi-source debating framework for climate claims\n|**AVeriTeC**|Research project|Evidence-based verification with justifications\n\n{{info}}\n**Gap Identified:** Academic research validates the need for **uncertainty communication, evidence-based justifications, and nuanced verdicts**, but no production-ready tool implements this.\n{{/info}}\n\n----\n\n== 2. Critical Market Gaps ==\n\n=== Gap 1: Binary Epistemology Problem ===\n\n* **Current State:** 95%+ of fact-checking produces True/False or scalar verdicts\n* **Problem:** Complex claims have context-dependent truth values\n* **FactHarbor Solution:** Scenario-based analysis showing \"true under X assumptions, false under Y\"\n\n=== Gap 2: Transparency Deficit ===\n\n* **Current State:** Verdicts are pronouncements; reasoning hidden\n* **Problem:** Users must \"trust the checker\" without inspecting logic\n* **FactHarbor Solution:** Evidence Models expose all reasoning chains, assumptions, and confidence bases\n\n=== Gap 3: No Probabilistic Verdicts ===\n\n* **Current State:** Even nuanced scales (6-point) are categorical\n* **Problem:** Doesn't communicate confidence or uncertainty\n* **FactHarbor Solution:** Explicit probability ranges (0.65-0.84 = \"Likely\") with confidence factors\n\n=== Gap 4: Missing Contradiction Search ===\n\n* **Current State:** Evidence gathering often confirms pre-existing view\n* **Problem:** Creates filter bubbles in fact-checking itself\n* **FactHarbor Solution:** Mandatory contradiction search as quality gate\n\n=== Gap 5: No Ecosystem Infrastructure ===\n\n* **Current State:** Each organization's verdicts are siloed\n* **Problem:** No interoperability, no standard for structured fact-check data\n* **FactHarbor Solution:** Open-source Evidence Models + ClaimReview integration + federation capability\n\n=== Gap 6: Scalability vs. Quality Trade-off ===\n\n* **Current State:** Human review = quality but doesn't scale; AI = scale but unreliable\n* **Problem:** Neither approach works for the volume of misinformation\n* **FactHarbor Solution:** AI-generated with quality gates + risk-based publication tiers + human escalation for high-risk\n\n=== Gap 7: Real-Time Verification ===\n\n* **Current State:** Traditional fact-checks take hours/days\n* **Problem:** Misinformation spreads faster than corrections\n* **FactHarbor Solution:** 10-30 second analysis target for POC; structured output for immediate use\n\n----\n\n== 3. Competitor Weaknesses to Exploit ==\n\n=== 3.1 PolitiFact/Snopes Weaknesses ===\n\n* ~30% of matching claims receive different ratings (pre-adjustment)\n* \"Half True\" and \"Mixture\" verdicts used 17-21% of time, indicating methodology struggles with nuance\n* Perceived political bias undermines trust (both sides claim bias)\n* No machine-readable output beyond ClaimReview tags\n\n{{success}}\n**Opportunity:** FactHarbor can partner with/enhance these organizations, not compete\n{{/success}}\n\n=== 3.2 ClaimBuster Weakness ===\n\n* \"The first-ever end-to-end fact-checking system\" claim misleadingverification component (Squash) failed\n* Limited to claim detection; no verdict production\n* Text-only (no multimodal)\n\n{{success}}\n**Opportunity:** FactHarbor can integrate ClaimBuster's claim detection API as input source\n{{/success}}\n\n=== 3.3 Community Notes Weaknesses ===\n\n* \"Not really scalable for the amount of media being consumed\"\n* Bridging algorithm creates delays\n* No structured data output\n* Highly variable quality\n\n{{success}}\n**Opportunity:** FactHarbor provides systematic methodology that crowdsourced contributors lack\n{{/success}}\n\n=== 3.4 LLM-Based Tools Weaknesses ===\n\n* \"Overconfident and unreliable\" confidence estimates\n* Hallucination risk\n* No grounding in retrievable evidence\n* Black-box reasoning\n\n{{success}}\n**Opportunity:** FactHarbor's Evidence Model makes AI reasoning inspectable and citable\n{{/success}}\n\n----\n\n== 4. FactHarbor's Unique Positioning ==\n\n=== What Makes FactHarbor Different ===\n\n|=Feature|=Traditional|=Automated|=Crowdsourced|=**FactHarbor**\n|**Verdict Type**|Categorical|Categorical|Text note|**Probabilistic + Scenario-based**\n|**Transparency**|Article explains|Black box|Varies|**Full reasoning chain**\n|**Assumptions**|Implicit|None|None|**Explicit in each scenario**\n|**Confidence**|None|Uncalibrated|None|**Stated with factors**\n|**Contradiction Check**|Sometimes|Rarely|Never|**Mandatory**\n|**Output Format**|Article|Score|Free text|**Structured Evidence Model**\n|**Scalability**|Low|High|Medium|**High (AI + quality gates)**\n|**Open Source**|No|Partial|Yes (X)|**Yes**\n\n=== Key Differentiators ===\n\n1. **Scenario-Based Analysis:** A claim isn't just \"true\" or \"false\"it's \"true under these assumptions, false under those\"\n1. **Evidence Landscape:** Shows where a claim holds, fails, and where reasonable disagreement exists\n1. **Transparent Reasoning:** Every step from claim  scenario  evidence  verdict is visible\n1. **Probabilistic Verdicts:** Not just labels, but likelihood ranges with explicit uncertainty factors\n1. **Versioned Knowledge:** Updates tracked; evidence evolution visible\n1. **Federated Model:** No single entity controls the narrative; nodes can synchronize\n\n----\n\n== 5. Strategic Recommendations ==\n\n=== 5.1 Positioning Strategy ===\n\n{{warning}}\n**Don't position as \"another fact-checker\"position as:**\n\n* \"Fact-checking infrastructure\"\n* \"Evidence Model platform\"\n* \"Transparency layer for claims\"\n{{/warning}}\n\n=== 5.2 Partnership Opportunities ===\n\n|=Partner Type|=Value Proposition|=Examples\n|Fact-checking orgs|Provide structured methodology + scale|Full Fact, IFCN members\n|Academic institutions|Research platform + novel approach|ETH Zurich, Duke Reporters' Lab\n|Media organizations|API integration for embedded fact-checking|News publishers\n|Educators|Critical thinking curriculum|Universities, schools\n\n=== 5.3 Competitive Moats to Build ===\n\n1. **ClaimReview Integration:** First Evidence Model producer with full ClaimReview export\n1. **Federation Protocol:** Enable decentralized fact-checking network\n1. **Quality Data Set:** Well-labeled Evidence Models for AI training\n1. **Domain Expertise:** Build deep capability in high-risk domains (health, finance, elections)\n\n=== 5.4 Market Timing Advantages ===\n\n* Meta exit creates demand for alternatives\n* USAID cuts reduce funding for traditional approaches  need for efficient solutions\n* AI reliability concerns  transparency value increases\n* Growing awareness that binary verdicts don't work for complex claims\n\n----\n\n== 6. Competitive Threats to Monitor ==\n\n|=Threat|=Risk Level|=Mitigation\n|**Full Fact expands AI**|Medium|Partner early; our scenario approach is more advanced\n|**Google enhances Fact Check Tools**|Medium|Focus on production capability, not just aggregation\n|**Academic tools productionize**|Low-Medium|Move faster; POC demonstrates viability\n|**Community Notes improves**|Low|Different value prop (systematic vs. crowdsourced)\n|**New AI fact-checker startup**|Medium|Open source moat; methodology transparency\n\n----\n\n== 7. Conclusion ==\n\n=== Market Gaps Summary ===\n\n1. No existing tool provides **scenario-based, probabilistic fact-checking**\n1. Transparency in reasoning is universally missing\n1. Automated tools fail at reliable verification; humans can't scale\n1. The \"Holy Grail\" remains unfilled because everyone pursues binary answers\n\n=== FactHarbor's Opportunity ===\n\nFactHarbor is **uniquely positioned** to fill the gap between:\n\n* Human fact-checkers (high quality, low scale)\n* Automated systems (low quality, high scale)\n* Crowdsourced systems (variable quality, medium scale)\n\nBy producing **structured Evidence Models with explicit scenarios, assumptions, and probabilistic verdicts**, FactHarbor offers something no competitor provides: **transparent reasoning at scale**.\n\n=== Recommended Next Steps ===\n\n1. **POC Validation:** Demonstrate Evidence Model quality with 30-article test set\n1. **IFCN/EFCSN Outreach:** Present methodology to fact-checking community\n1. **ClaimReview Export:** Ensure Evidence Models generate valid ClaimReview for ecosystem integration\n1. **Academic Partnership:** Engage ETH Zurich or similar for methodology validation\n1. **Differentiation Messaging:** \"Not another verdicta truth landscape\"\n\n----\n\n{{box title=\"Analysis Metadata\"}}\n* **Analysis Date:** December 31, 2025\n* **Sources:** Web research, FactHarbor specification documents\n* **Author:** Claude (AI Assistant)\n{{/box}}\n", "Organisation.Strategy.Cooperation Opportunities.WebHome": "= FactHarbor Cooperation Opportunities Report =\n\n\n\n**Prepared for:**FactHarbor Project\n\n**Date:**December 21, 2025\n\n**Research Scope:**Global fact-checking ecosystem, funding organizations, academic institutions, technology platforms, and civic tech networks\n\n\n\n----\n\n\n\n== Executive Summary ==\n\n\n\nFactHarbor's innovative Evidence Model approach and automation-first philosophy position it uniquely in the fact-checking ecosystem. This report identifies**120+ specific cooperation opportunities**across seven strategic categories, prioritized by alignment with FactHarbor's mission, technical architecture, and Swiss nonprofit status.\n\n\n\n**Key Findings:**\n\n\n\n* **443 active fact-checking organizations globally**provide a vast network for collaboration\n\n* **170+ IFCN-verified signatories**represent quality partners for methodology validation\n\n* **61 EFCSN members**offer European collaboration opportunities, particularly important given FactHarbor's Swiss base\n\n* **Multiple funding sources**available totaling potential access to $500M+ in grants\n\n* **Strong academic interest**in automated fact-checking with established research centers\n\n* **Technology integration paths**through ClaimReview schema and Google/Meta partnerships\n\n\n\n----\n\n\n\n== 1. FACT-CHECKING NETWORKS & ORGANIZATIONS ==\n\n\n\n=== 1.1 International Networks (Priority: HIGH) ===\n\n\n\n==== **International Fact-Checking Network (IFCN)** ====\n\n\n\n* **Organization:**Poynter Institute\n\n* **Network Size:**170+ verified signatories across 57 countries\n\n* **Annual Conference:**#GlobalFact (yearly gathering of fact-checkers)\n\n* **Cooperation Potential:**\n\n** Apply for IFCN signatory status to gain credibility\n\n** Participate in annual #GlobalFact conference\n\n** Access training resources and best practices\n\n** Network with 170+ verified organizations\n\n** Potential partnership on automated tools development\n\n* **Contact:**factchecknet@poynter.org\n\n* **Website:**https:~/~/www.poynter.org/ifcn/\n\n* **Next Steps:**\n\n*1. Review IFCN Code of Principles compliance (FactHarbor already aligns well)\n\n*1. Apply for signatory status once POC demonstrates capability\n\n*1. Engage with IFCN's Global Fact Check Fund partnership opportunities\n\n\n\n==== **European Fact-Checking Standards Network (EFCSN)** ====\n\n\n\n* **Members:**61 organizations (as of April 2025)\n\n* **Geographic Scope:**Council of Europe member states + Kosovo, Belarus, Russia\n\n* **Standards:**European Code of Standards for Independent Fact-Checking Organisations\n\n* **Cooperation Potential:**\n\n** Apply for EFCSN membership (particularly relevant for Swiss nonprofit)\n\n** Biennial certification demonstrates compliance with European standards\n\n** Access to European funding programs\n\n** Collaboration with quality European fact-checkers\n\n* **Key Members to Engage:**\n\n** Maldita.es (Spain) - consortium lead\n\n** AFP (France)\n\n** Correctiv (Germany)\n\n** Demagog (Poland)\n\n** Pagella Politica/Facta (Italy)\n\n* **Website:**https:~/~/efcsn.com/\n\n* **Next Steps:**Prepare membership application highlighting automated transparency approach\n\n\n\n=== 1.2 Leading Fact-Checking Organizations (Priority: HIGH) ===\n\n\n\n==== **Full Fact (UK)** ====\n\n\n\n* **Type:**Independent charity, IFCN & EFCSN verified\n\n* **Innovation:**Full Fact AI used by 40+ organizations\n\n* **Technology:**BERT-based claim detection, live monitoring\n\n* **Cooperation Potential:**\n\n** **Technology Partnership:**Integrate Full Fact AI for claim detection\n\n** **Methodology Exchange:**Share Evidence Model approach for their evaluation\n\n** **Academic Collaboration:**They work with academic partners on claim detection research\n\n** **Open Source:**They have WordPress ClaimReview plugin - could integrate\n\n* **Contact:**Via website contact form\n\n* **Website:**https:~/~/fullfact.org/\n\n* **Academic Partners:**University College London, University of Sheffield\n\n* **Why This Matters:**Full Fact is the leading UK fact-checker with proven AI tools that FactHarbor could integrate or collaborate with\n\n\n\n==== **dpa Fact-Checking (Germany/Austria/Switzerland)** ====\n\n\n\n* **Coverage:**DACH region (Germany, Austria, Switzerland)\n\n* **Team Size:**~~30 people\n\n* **Languages:**German, French, Dutch\n\n* **Cooperation Potential:**\n\n** **Direct Swiss Relevance:**Already fact-checks content from Switzerland\n\n** **German/French Language:**Matches Swiss language needs\n\n** **IFCN & EFCSN Verified:**Quality standards alignment\n\n** **Training Services:**They offer verification training in DACH region\n\n** **Technology Integration:**Possible integration of FactHarbor's automated analysis\n\n* **Contact:**factchecking@dpa.com\n\n* **Website:**https:~/~/www.dpa.com/en/services/fact-checking\n\n* **Key Contact:**Teresa Dapp, Head of Fact-Checking Editorial\n\n* **Next Steps:**Reach out to discuss Swiss market collaboration and technology integration\n\n\n\n==== **AFP Fact Check** ====\n\n\n\n* **Scope:**Global, multiple languages\n\n* **Network:**Part of Agence France-Presse news agency\n\n* **IFCN Status:**Verified signatory\n\n* **Cooperation Potential:**\n\n** **French Language:**Critical for Swiss French speakers\n\n** **Global Reach:**Could amplify FactHarbor analyses\n\n** **Technology Partnership:**They use AI tools and may benefit from Evidence Model approach\n\n* **Website:**https:~/~/factcheck.afp.com/\n\n\n\n==== **FactCheck.org (USA)** ====\n\n\n\n* **Institution:**Annenberg Public Policy Center, University of Pennsylvania\n\n* **Funding Model:**Foundation-supported, transparent\n\n* **Educational Arm:**FactCheckEd.org for schools\n\n* **Cooperation Potential:**\n\n** **Educational Partnership:**Adapt Evidence Model for educational use\n\n** **Methodology Sharing:**Their rigorous 4-layer review process vs. FactHarbor's automated QA\n\n** **Research Collaboration:**University of Pennsylvania partnership potential\n\n* **Website:**https:~/~/www.factcheck.org/\n\n\n\n=== 1.3 Regional & Swiss Organizations (Priority: HIGH) ===\n\n\n\n==== **EBU Eurovision News Spotlight** ====\n\n\n\n* **Members:**SRF (Switzerland), ORF (Austria), ZDF/Deutsche Welle (Germany), RTBF (Belgium), France Tlvisions, BBC\n\n* **Focus:**Collaborative fact-checking and OSINT for public service media\n\n* **Launched:**April 2025\n\n* **Cooperation Potential:**\n\n** **SRF Partnership:**Swiss public broadcaster is a member\n\n** **OSINT Tools:**They're building fact-checking infrastructure for European broadcasters\n\n** **Technology Integration:**FactHarbor could provide backend analysis for their fact-checks\n\n** **Swiss Media Access:**Direct connection to Swiss media ecosystem\n\n* **Contact:**Through EBU headquarters in Geneva\n\n* **Website:**https:~/~/www.ebu.ch/\n\n* **Next Steps:**Contact EBU's Social Newsgathering team about technology partnership\n\n\n\n==== **Mimikama (Austria)** ====\n\n\n\n* **Focus:**Facebook hoaxes, German/Dutch language\n\n* **Geographic Reach:**Austria, Germany, Switzerland\n\n* **Cooperation Potential:**\n\n** **Language Match:**German-speaking region\n\n** **Social Media Focus:**Complements FactHarbor's text analysis\n\n** **Regional Knowledge:**Understanding of DACH region misinformation patterns\n\n* **Website:**https:~/~/www.mimikama.at/\n\n\n\n==== **SRF (Swiss Radio and Television)** ====\n\n\n\n* **Type:**Swiss public broadcaster\n\n* **EBU Member:**Part of Eurovision News Spotlight\n\n* **Cooperation Potential:**\n\n** **Direct Swiss Market:**National public broadcaster\n\n** **Trusted Source:**High credibility in Switzerland\n\n** **Technology Adoption:**May be interested in automated fact-checking tools\n\n** **Pilot Partnership:**Could test FactHarbor on Swiss political content\n\n* **Website:**https:~/~/www.srf.ch/\n\n* **Next Steps:**Identify fact-checking lead at SRF news division\n\n\n\n----\n\n\n\n== 2. ACADEMIC & RESEARCH PARTNERSHIPS ==\n\n\n\n=== 2.1 Leading Research Centers (Priority: HIGH) ===\n\n\n\n==== **University of Texas at Arlington - IDIR Lab** ====\n\n\n\n* **Project:**ClaimBuster (first automated fact-checking system)\n\n* **PI:**Dr. Chengkai Li (Associate Professor, Computer Science)\n\n* **Funding:**NSF grants totaling $500K+\n\n* **Technology:**API available for claim detection\n\n* **Cooperation Potential:**\n\n** **API Integration:**Integrate ClaimBuster API for claim spotting\n\n** **Research Partnership:**Joint research on Evidence Models vs. binary verdicts\n\n** **Dataset Sharing:**Access to 23,533+ labeled claims\n\n** **Publication Collaboration:**Co-author papers on automated fact-checking approaches\n\n* **Contact:**cli@uta.edu\n\n* **Website:**https:~/~/idir.uta.edu/claimbuster/\n\n* **Key Publications:**VLDB, KDD, IJCNN conferences\n\n* **Next Steps:**Email Dr. Li about Evidence Model methodology and potential collaboration\n\n\n\n==== **Duke University Reporters' Lab** ====\n\n\n\n* **Project:**Tech & Check Cooperative\n\n* **Director:**Bill Adair (Pulitzer Prize winner, PolitiFact founder)\n\n* **Funding:**Knight Foundation ($800K+), Facebook partnership\n\n* **Products:**Share the Facts widget, FactStream\n\n* **Cooperation Potential:**\n\n** **ClaimReview Integration:**They develop ClaimReview tools\n\n** **Innovation Hub:**Tech & Check Cooperative connects fact-checking tool developers\n\n** **Annual Meetings:**Regular meetups for innovators in fact-checking\n\n** **FactStream Partnership:**Could integrate FactHarbor analyses\n\n* **Contact:**reporters-lab@duke.edu\n\n* **Website:**https:~/~/reporterslab.org/\n\n* **Key Contact:**Erica Ryan (FactStream coordinator)\n\n* **Next Steps:**Apply to present at Tech & Check annual meeting\n\n\n\n==== **Harvard Kennedy School - Shorenstein Center** ====\n\n\n\n* **Project:**First Draft (now part of Shorenstein Center)\n\n* **Focus:**Media verification, misinformation research\n\n* **Network:**100+ organizations for real-time verification\n\n* **Cooperation Potential:**\n\n** **Research Partnership:**Academic validation of Evidence Model approach\n\n** **Network Access:**Connection to 100+ verification organizations\n\n** **Graduate Research:**Potential for student projects on FactHarbor\n\n** **Training Resources:**They develop training for newsrooms\n\n* **Website:**https:~/~/shorensteincenter.org/\n\n\n\n==== **ETH Zurich / University of Zurich (Switzerland)** ====\n\n\n\n* **Departments:**Computer Science, Computational Social Science\n\n* **Swiss Advantage:**Local Swiss institution\n\n* **Cooperation Potential:**\n\n** **Swiss Academic Partnership:**Local credibility and funding access\n\n** **Student Projects:**Master's theses on Evidence Model methodology\n\n** **Research Validation:**Academic papers on FactHarbor approach\n\n** **BRIDGE Funding:**Joint applications for Swiss research grants\n\n* **Next Steps:**Identify NLP/computational journalism researchers at ETH Zurich\n\n\n\n==== **Stanford Internet Observatory** ====\n\n\n\n* **Focus:**Disinformation research, internet abuse\n\n* **Expertise:**Large-scale analysis of information operations\n\n* **Cooperation Potential:**\n\n** **Research Partnership:**Study effectiveness of Evidence Models\n\n** **Dataset Access:**Large-scale misinformation datasets\n\n** **Methodology Validation:**Academic credibility for FactHarbor approach\n\n* **Website:**https:~/~/cyber.fsi.stanford.edu/io\n\n\n\n=== 2.2 European Research Networks (Priority: MEDIUM) ===\n\n\n\n==== **EDMO - European Digital Media Observatory** ====\n\n\n\n* **Function:**EU-funded research and fact-checking coordination\n\n* **Hubs:**Regional hubs across Europe\n\n* **Cooperation Potential:**\n\n** **EU Funding Access:**Connection to Horizon Europe programs\n\n** **Research Collaboration:**Academic network for validation\n\n** **Data Sharing:**Access to EDMO fact-check repository\n\n** **Protection Scheme:**Access to fact-checker protection resources\n\n* **Website:**https:~/~/edmo.eu/\n\n* **Funding:**European Commission's Digital Europe programme\n\n\n\n==== **European Journalism Training Association (EJTA)** ====\n\n\n\n* **Project:**EUfactcheck platform\n\n* **Focus:**Training journalism students in fact-checking\n\n* **Cooperation Potential:**\n\n** **Educational Tools:**FactHarbor as training platform\n\n** **Student Network:**Access to next-generation fact-checkers\n\n** **Erasmus Integration:**European student mobility for FactHarbor projects\n\n* **Website:**https:~/~/eufactcheck.eu/\n\n\n\n----\n\n\n\n== 3. FUNDING OPPORTUNITIES ==\n\n\n\n=== 3.1 Major Foundation Grants (Priority: HIGH) ===\n\n\n\n==== **Knight Foundation - Trust, Media & Democracy Initiative** ====\n\n\n\n* **Budget:**$50M+ committed to journalism and fact-checking\n\n* **Focus Areas:**\n\n** Technology for newsrooms ($20K grants for publishing platforms)\n\n** Innovation in journalism\n\n** Combating misinformation\n\n** Local news sustainability\n\n* **Previous Fact-Checking Grants:**\n\n** Duke Reporters' Lab: $800K\n\n** AP Fact-Checking expansion: $245K\n\n** First Draft: $250K\n\n* **Cooperation Potential:**\n\n** **Technology Grants:**Apply for digital platform support\n\n** **Knight Prototype Fund:**$50K grants for innovative projects (20 awarded in recent round)\n\n** **Research Grants:**For studies on automated fact-checking effectiveness\n\n* **Application:**Rolling applications, check https:~/~/knightfoundation.org/apply/\n\n* **Next Steps:**Monitor Knight Prototype Fund announcements, prepare application highlighting FactHarbor's innovative Evidence Model\n\n\n\n==== **Google News Initiative - Global Fact Check Fund** ====\n\n\n\n* **Budget:**$12M over three years (partnership with IFCN)\n\n* **Tiers:**\n\n** BUILD: Up to $25K\n\n** GROW: Up to $50K\n\n** ENGAGE: Up to $100K\n\n* **Eligibility:**\n\n** IFCN verified signatories (primary)\n\n** Non-signatories endorsed by IFCN members (secondary)\n\n* **Focus:**\n\n** Fact-checking capacity building\n\n** Technology development\n\n** Audience engagement\n\n* **Cooperation Potential:**\n\n** **GROW Track:**Scale FactHarbor platform with $50K grant\n\n** **Technology Partnership:**Google Fact Check Tools integration\n\n** **ClaimReview Implementation:**Critical for Google search visibility\n\n* **Next Steps:**\n\n*1. Achieve IFCN signatory status OR get endorsement from existing signatory\n\n*1. Apply for GROW or ENGAGE funding round\n\n* **Contact:**Through IFCN (factchecknet@poynter.org)\n\n\n\n==== **Meta (Facebook) Fact-Checking Program** ====\n\n\n\n* **Status:**Transitioning away from IFCN model (2025)\n\n* **Historical Budget:**Funded 160+ fact-checking organizations globally\n\n* **Current State:**Moving to \"Community Notes\" model similar to X/Twitter\n\n* **Cooperation Potential:**\n\n** **Uncertain:**May not be available as traditional funding source\n\n** **Alternative:**Position FactHarbor as community notes infrastructure\n\n* **Note:**Monitor Meta's evolving approach to fact-checking\n\n\n\n=== 3.2 Swiss Funding Sources (Priority: HIGH) ===\n\n\n\n==== **Innosuisse - Swiss Innovation Agency** ====\n\n\n\n* **Programs:**\n\n** **Innovation Projects:**Up to CHF 500K annually\n\n** **Start-up Funding:**For science-based startups\n\n** **BRIDGE Proof of Concept:**CHF 150K-850K (with SNSF)\n\n* **Requirements:**\n\n** Swiss UID number (company/nonprofit)\n\n** Research partner (university)\n\n** Market potential and innovation\n\n** 10% co-funding from applicant\n\n* **Cooperation Potential:**\n\n** **Perfect Fit:**FactHarbor's automated fact-checking = science-based innovation\n\n** **University Partnership:**Partner with ETH Zurich or University of Zurich\n\n** **Sustainable Development:**Fact-checking contributes to SDGs (info integrity, democracy)\n\n* **Application:**Via Innolink platform https:~/~/www.innosuisse.admin.ch/\n\n* **Timeline:**Rolling applications, 3-6 month review\n\n* **Next Steps:**\n\n*1. Identify research partner (ETH Zurich NLP group)\n\n*1. Develop project proposal highlighting innovation (Evidence Model)\n\n*1. Prepare budget and market analysis\n\n\n\n==== **Gebert Rf Stiftung** ====\n\n\n\n* **Focus:**Innovation for Swiss economy and society\n\n* **Budget:**CHF 15M annually\n\n* **Programs:**\n\n** **InnoBooster:**CHF 150K for deep tech with market potential\n\n** **Closed Pilot Projects:**(now BRIDGE handles this)\n\n* **Requirements:**\n\n** Scientific excellence\n\n** Innovation potential\n\n** Endorsement from funding program (Innosuisse, BRIDGE, etc.)\n\n* **Cooperation Potential:**\n\n** **InnoBooster Program:**After achieving Innosuisse or Venture Kick milestone\n\n** **Deep Tech Focus:**FactHarbor's AI-driven approach qualifies\n\n* **Next Steps:**First secure Innosuisse or BRIDGE funding, then apply for InnoBooster\n\n\n\n==== **Prototype Fund Switzerland** ====\n\n\n\n* **Focus:**Public interest tech, open source\n\n* **Grant Size:**Up to CHF 100K\n\n* **Current Theme:**Sustainable digitalization, digital sufficiency\n\n* **Requirements:**\n\n** Work permit in Switzerland\n\n** Open source project\n\n** Public interest focus\n\n* **Cooperation Potential:**\n\n** **Perfect Alignment:**FactHarbor is open source, public interest, Swiss-based\n\n** **Sustainability Angle:**Fact-checking prevents information pollution\n\n** **Civic Tech:**Democracy and transparency focus\n\n* **Application:**https:~/~/prototypefund.opendata.ch/\n\n* **Next Steps:**Monitor next funding round (periodic), prepare application\n\n\n\n==== **Swiss National Science Foundation (SNSF)** ====\n\n\n\n* **Programs:**\n\n** **BRIDGE Discovery:**Young researchers, proof of concept\n\n** **BRIDGE Proof of Concept:**CHF 150K-850K\n\n** **National Research Programs (NRP):**Thematic research\n\n* **Cooperation Potential:**\n\n** **Academic Partnership Required:**Must partner with Swiss university\n\n** **Research Focus:**Study effectiveness of Evidence Models in fact-checking\n\n* **Website:**http:~/~/www.snf.ch/\n\n\n\n=== 3.3 European Union Funding (Priority: MEDIUM) ===\n\n\n\n==== **European Network of Factcheckers (EU Grant)** ====\n\n\n\n* **Budget:**Multi-million EUR (exact amount not specified)\n\n* **Focus:**\n\n** Fact-checker protection scheme\n\n** Repository of fact-checks with API\n\n** Emergency response capacity\n\n** Training and capacity building\n\n* **Eligibility:**EU Member States + associated countries (Switzerland potentially included)\n\n* **Cooperation Potential:**\n\n** **API Integration:**FactHarbor could power the central repository\n\n** **Protection Scheme:**Access to legal, cybersecurity support\n\n** **Emergency Response:**FactHarbor's automation fits rapid response needs\n\n* **Timeline:**Active call (check EuroAccess for updates)\n\n* **Next Steps:**Monitor call status, consider consortium partnership\n\n\n\n==== **Horizon Europe** ====\n\n\n\n* **Programs:**\n\n** Media & Democracy calls\n\n** AI for Good initiatives\n\n** Civic engagement projects\n\n* **Budget:**Billions EUR across all programs\n\n* **Cooperation Potential:**\n\n** **Consortium Member:**Join EU research consortia\n\n** **Swiss Participation:**Switzerland has partial association\n\n* **Note:**Requires consortium partnership, complex application\n\n\n\n=== 3.4 Other Foundations (Priority: MEDIUM) ===\n\n\n\n==== **MacArthur Foundation** ====\n\n\n\n* **Initiative:**Press Forward ($500M over 5 years for local news)\n\n* **Focus:**Democracy, trust in institutions\n\n* **Cooperation Potential:**Chicago chapter active, potential expansion\n\n\n\n==== **Democracy Fund** ====\n\n\n\n* **Focus:**Democratic governance, civic participation\n\n* **Partnership:**Previous NewsMatch partnership with Knight Foundation\n\n* **Cooperation Potential:**Aligned with democracy protection mission\n\n\n\n==== **Mozilla Foundation** ====\n\n\n\n* **Focus:**Internet health, AI transparency, trustworthy AI\n\n* **Cooperation Potential:**\n\n** FactHarbor's open-source approach aligns\n\n** Transparency in AI decision-making (Evidence Models)\n\n* **Website:**https:~/~/foundation.mozilla.org/\n\n\n\n==== **Open Society Foundations** ====\n\n\n\n* **Focus:**Democracy, human rights, information integrity\n\n* **Cooperation Potential:**Global disinformation programs\n\n\n\n----\n\n\n\n== 4. TECHNOLOGY PARTNERSHIPS & INTEGRATIONS ==\n\n\n\n=== 4.1 Platform Integration (Priority: HIGH) ===\n\n\n\n==== **Google - Fact Check Tools** ====\n\n\n\n* **Products:**\n\n** Fact Check Explorer\n\n** Fact Check Markup Tool\n\n** ClaimReview API (Read/Write)\n\n* **Cooperation Potential:**\n\n** **ClaimReview Integration:**CRITICAL for search visibility\n\n** **API Access:**Contribute fact-checks to Google's repository\n\n** **Search Ranking:**ClaimReview markup improves discoverability\n\n** **Data Commons:**Fact-checks feed into public data infrastructure\n\n* **Requirements:**\n\n** Google Search Console access\n\n** Adherence to Google News Guidelines\n\n** Transparency and accountability standards\n\n** Multiple marked-up pages (no political entities)\n\n* **Implementation:**\n\n*1. Set up Search Console for FactHarbor domain\n\n*1. Implement ClaimReview schema on all fact-check pages\n\n*1. Apply for Fact Check Markup Tool access\n\n*1. Submit sitemap to Google\n\n* **Technical Docs:**https:~/~/developers.google.com/fact-check/\n\n* **Next Steps:**Priority implementation in POC\n\n\n\n==== **ClaimReview Project - Duke Reporters' Lab** ====\n\n\n\n* **Standard:**Schema.org ClaimReview markup\n\n* **Adoption:**200K+ fact-checks globally\n\n* **Tools:**\n\n** WordPress plugin (Full Fact)\n\n** FactStream integration\n\n* **Cooperation Potential:**\n\n** **Standard Compliance:**Implement ClaimReview for all FactHarbor reports\n\n** **FactStream Partnership:**Distribute FactHarbor analyses\n\n** **Plugin Development:**Create FactHarborClaimReview export tool\n\n* **Website:**https:~/~/www.claimreviewproject.com/\n\n* **Next Steps:**Study ClaimReview schema, plan implementation\n\n\n\n=== 4.2 AI & NLP Technology Partners (Priority: MEDIUM) ===\n\n\n\n==== **Anthropic (Claude)** ====\n\n\n\n* **Current:**FactHarbor already uses Claude for analysis\n\n* **Cooperation Potential:**\n\n** **Case Study:**FactHarbor as showcase for responsible AI in fact-checking\n\n** **API Partnership:**Potential preferred pricing or technical support\n\n** **Research Collaboration:**Study effectiveness of LLMs in structured fact-checking\n\n* **Note:**FactHarbor demonstrates responsible LLM use with transparency + quality gates\n\n\n\n==== **OpenAI / Mistral / Other LLM Providers** ====\n\n\n\n* **Cooperation Potential:**\n\n** **Redundancy:**Test multiple LLMs to reduce single-provider risk\n\n** **Benchmarking:**Compare Evidence Model quality across providers\n\n** **Cost Optimization:**Evaluate pricing vs. quality tradeoffs\n\n\n\n=== 4.3 Media Verification Tools (Priority: MEDIUM) ===\n\n\n\n==== **Sensity AI / Reality Defender** ====\n\n\n\n* **Focus:**Deepfake detection\n\n* **Cooperation Potential:**\n\n** **API Integration:**Add multimedia verification to FactHarbor\n\n** **Partnership:**Integrate their detection as evidence source\n\n* **Gap Identified:**Research report notes FactHarbor lacks deepfake detection\n\n\n\n==== **InVID/WeVerify** ====\n\n\n\n* **Focus:**Video verification\n\n* **EU Funded:**Horizon 2020 project\n\n* **Cooperation Potential:**Integrate for video claim analysis\n\n\n\n==== **TinEye / Google Lens** ====\n\n\n\n* **Focus:**Reverse image search\n\n* **Cooperation Potential:**Integrate for image verification\n\n\n\n----\n\n\n\n== 5. CIVIC TECH & OPEN SOURCE COMMUNITIES ==\n\n\n\n=== 5.1 Civic Tech Networks (Priority: MEDIUM) ===\n\n\n\n==== **Code for All** ====\n\n\n\n* **Network:**Global civic tech organizations (50+ countries)\n\n* **Projects:**Cofacts (Taiwan), mySociety (UK), Code for America\n\n* **Cooperation Potential:**\n\n** **Network Membership:**Join as Swiss civic tech organization\n\n** **Best Practices:**Learn from Cofacts' participatory fact-checking\n\n** **Global Reach:**Access to international civic tech community\n\n* **Example:**Cofacts handles 90%+ LINE messaging app fact-checks in Taiwan\n\n* **Website:**https:~/~/codeforall.org/\n\n* **Next Steps:**Contact Code for All about membership/partnership\n\n\n\n==== **mySociety (UK)** ====\n\n\n\n* **Projects:**TheyWorkForYou, WhatDoTheyKnow, EveryPolitician\n\n* **Focus:**Democratic accountability, open data\n\n* **Cooperation Potential:**\n\n** **Democratic Commons:**FactHarbor contributes to verified information commons\n\n** **Open Data:**Share FactHarbor analyses as open data\n\n** **Technology Sharing:**Learn from 20+ years of civic tech experience\n\n* **Website:**https:~/~/www.mysociety.org/\n\n\n\n==== **DemocracyLab** ====\n\n\n\n* **Project:**Civic Tech Index\n\n* **Cooperation Potential:**\n\n** **Visibility:**List FactHarbor in Civic Tech Index\n\n** **Volunteer Network:**Access to volunteer developers\n\n** **Community:**Connect with other civic tech projects\n\n* **Website:**https:~/~/www.democracylab.org/\n\n\n\n==== **Decidim / Participer.ge.ch (Geneva)** ====\n\n\n\n* **Type:**Participatory democracy platform (open source)\n\n* **Swiss Connection:**Active in Geneva\n\n* **Cooperation Potential:**\n\n** **Swiss Civic Tech:**Connect with Swiss participatory democracy community\n\n** **Technology Exchange:**Learn from successful Swiss civic tech deployment\n\n* **Contact:**Through Octree (Geneva implementation partner)\n\n\n\n=== 5.2 Open Source Communities (Priority: MEDIUM) ===\n\n\n\n==== **GitHub Civic Tech Community** ====\n\n\n\n* **Size:**Thousands of civic tech projects\n\n* **Tags:**#civic-tech, #fact-checking, #democracy\n\n* **Cooperation Potential:**\n\n** **Open Source Release:**Publish FactHarbor on GitHub\n\n** **Community Contributions:**Accept pull requests, build contributor base\n\n** **Discoverability:**Tag properly for discovery\n\n* **Next Steps:**Create FactHarbor GitHub organization, publish code\n\n\n\n==== **Open Knowledge Foundation** ====\n\n\n\n* **Focus:**Open data, open knowledge\n\n* **Projects:**CKAN, OpenSpending\n\n* **Cooperation Potential:**\n\n** **Open Data:**Publish FactHarbor analyses as open data\n\n** **Community:**Connect with open knowledge advocates\n\n* **Website:**https:~/~/okfn.org/\n\n\n\n----\n\n\n\n== 6. EDUCATIONAL INSTITUTIONS ==\n\n\n\n=== 6.1 Media Literacy Programs (Priority: HIGH) ===\n\n\n\n==== **MediaWise - Poynter Institute** ====\n\n\n\n* **Program:**Teen Fact-Checking Network (TFCN)\n\n* **Scale:**10+ countries, including TFCN Europe\n\n* **Partners:**dpa (Germany), Verificat (Spain), FactCheck.bg\n\n* **Cooperation Potential:**\n\n** **Educational Platform:**FactHarbor as teaching tool for teens\n\n** **Swiss Program:**Launch TFCN Switzerland with FactHarbor technology\n\n** **Curriculum Integration:**Evidence Model methodology for media literacy\n\n** **Peer Learning:**Teen fact-checkers learn structured analysis\n\n* **Contact:**MediaWise team via Poynter\n\n* **Website:**https:~/~/www.poynter.org/mediawise/\n\n* **Next Steps:**Propose Swiss TFCN chapter powered by FactHarbor\n\n\n\n==== **European Journalism Training Association (EJTA)** ====\n\n\n\n* **Program:**EUfactcheck (student fact-checking)\n\n* **Institutions:**70+ journalism schools across Europe\n\n* **Swiss Members:**Check for Swiss journalism schools\n\n* **Cooperation Potential:**\n\n** **Curriculum Tool:**FactHarbor for journalism students\n\n** **Student Projects:**Class assignments using FactHarbor\n\n** **Research:**Student theses on Evidence Model approach\n\n* **Website:**https:~/~/eufactcheck.eu/\n\n\n\n==== **FactCheckEd.org** ====\n\n\n\n* **Institution:**Annenberg Public Policy Center (University of Pennsylvania)\n\n* **Focus:**High school civics teachers\n\n* **Cooperation Potential:**\n\n** **Teacher Resources:**Adapt FactHarbor for classroom use\n\n** **Evidence Model Curriculum:**Teach structured thinking\n\n* **Website:**https:~/~/www.factchecked.org/\n\n\n\n=== 6.2 Swiss Universities (Priority: HIGH) ===\n\n\n\n==== **ETH Zurich** ====\n\n\n\n* **Relevant Departments:**\n\n** Computer Science (NLP, AI)\n\n** Computational Social Science\n\n** Data Science\n\n* **Cooperation Potential:**\n\n** **Research Partnership:**Joint publications on Evidence Models\n\n** **Student Projects:**Master's/PhD theses on FactHarbor\n\n** **Innosuisse Applications:**Required research partner for grants\n\n** **Technology Transfer:**ETH's innovation ecosystem\n\n* **Next Steps:**Identify specific professors in NLP/computational journalism\n\n\n\n==== **University of Zurich** ====\n\n\n\n* **Relevant Departments:**\n\n** Communication and Media Research\n\n** Computational Linguistics\n\n** Political Science\n\n* **Cooperation Potential:**\n\n** **Social Science Research:**Study impact of Evidence Models on public understanding\n\n** **User Studies:**Research how people interact with nuanced verdicts\n\n** **Swiss Context:**Research Swiss misinformation patterns\n\n\n\n==== **EPFL (Lausanne)** ====\n\n\n\n* **Relevant:**Data Science, Digital Humanities\n\n* **Cooperation Potential:**Similar to ETH, focus on francophone Switzerland\n\n\n\n==== **Graduate Institute Geneva** ====\n\n\n\n* **Focus:**International relations, governance\n\n* **Cooperation Potential:**\n\n** **Policy Research:**Study role of fact-checking in democracy\n\n** **International Relations:**Cross-border misinformation\n\n\n\n----\n\n\n\n== 7. MEDIA & JOURNALISM ORGANIZATIONS ==\n\n\n\n=== 7.1 Swiss Media (Priority: HIGH) ===\n\n\n\n==== **Swiss Broadcasting Corporation (SRG SSR)** ====\n\n\n\n* **Divisions:**SRF (German), RTS (French), RSI (Italian)\n\n* **Status:**Part of EBU Eurovision News Spotlight\n\n* **Cooperation Potential:**\n\n** **Technology Adoption:**FactHarbor backend for SRG fact-checking\n\n** **Swiss Elections:**Partnership for election coverage\n\n** **Multi-language:**Test FactHarbor in German, French, Italian\n\n** **Public Service Mandate:**Aligned with information integrity mission\n\n* **Contact:**Innovation/digital departments of SRF, RTS, RSI\n\n* **Next Steps:**Prepare pitch for SRG digital innovation team\n\n\n\n==== **Tamedia** ====\n\n\n\n* **Publications:**Tages-Anzeiger, Basler Zeitung, 24 heures, Tribune de Genve\n\n* **Scale:**Largest Swiss newspaper publisher\n\n* **Cooperation Potential:**\n\n** **Fact-Checking Service:**Offer FactHarbor as tool for newsroom\n\n** **Pilot Program:**Test on political coverage\n\n* **Website:**https:~/~/www.tamedia.ch/\n\n\n\n==== **NZZ (Neue Zrcher Zeitung)** ====\n\n\n\n* **Type:**Quality newspaper, international reach\n\n* **Cooperation Potential:**\n\n** **Technology Partnership:**FactHarbor for NZZ reporting\n\n** **Innovation:**NZZ has digital innovation focus\n\n\n\n=== 7.2 International Media (Priority: MEDIUM) ===\n\n\n\n==== **Reuters Institute (University of Oxford)** ====\n\n\n\n* **Focus:**Journalism research, digital news\n\n* **Reports:**Regular publications on fact-checking and misinformation\n\n* **Cooperation Potential:**\n\n** **Research Partnership:**Study effectiveness of Evidence Models\n\n** **Case Study:**FactHarbor as subject of research\n\n** **Visibility:**Reports reach global journalism community\n\n* **Website:**https:~/~/reutersinstitute.politics.ox.ac.uk/\n\n\n\n==== **ProPublica / The Markup** ====\n\n\n\n* **Focus:**Investigative journalism, transparency\n\n* **Cooperation Potential:**\n\n** **Methodology Alignment:**Transparent, evidence-based reporting\n\n** **Technology Story:**FactHarbor could be subject of investigative story\n\n\n\n----\n\n\n\n== 8. STRATEGIC PRIORITY MATRIX ==\n\n\n\n=== Tier 1: Foundation ===\n\n\n\n**Critical for POC success and credibility:**\n\n\n\n1. (((\n\n**ClaimReview Implementation**(Google/Duke)\n\n\n\n* Technical: Essential for search visibility\n\n* Scope: Moderate development effort\n\n* Impact: HIGH - enables discovery by users\n\n)))\n\n1. (((\n\n**IFCN Signatory Application Prep**\n\n\n\n* Strategic: Industry credibility\n\n* Timeline: Review requirements, prepare docs\n\n* Impact: HIGH - unlocks funding and partnerships\n\n)))\n\n1. (((\n\n**ETH Zurich Research Contact**\n\n\n\n* Purpose: Innosuisse grant application\n\n* Timeline: Identify faculty, initial meeting\n\n* Impact: HIGH - enables Swiss funding\n\n)))\n\n1. (((\n\n**dpa Fact-Checking Outreach**\n\n\n\n* Purpose: Swiss market collaboration\n\n* Timeline: Email introduction\n\n* Impact: MEDIUM - Swiss language fact-checking partner\n\n)))\n\n\n\n=== Tier 2: Growth ===\n\n\n\n**Build partnerships and apply for funding:**\n\n\n\n1. (((\n\n**Innosuisse Grant Application**\n\n\n\n* Requirement: ETH partnership established\n\n* Timeline: 3-6 month review process\n\n* Funding: Up to CHF 500K\n\n* Impact: HIGH - major funding for development\n\n)))\n\n1. (((\n\n**Knight Prototype Fund Application**\n\n\n\n* Application: Monitor for open calls\n\n* Timeline: Rolling\n\n* Funding: $50K\n\n* Impact: MEDIUM - international credibility\n\n)))\n\n1. (((\n\n**Duke Reporters' Lab Engagement**\n\n\n\n* Purpose: Tech & Check Cooperative membership\n\n* Timeline: Next annual meeting\n\n* Impact: MEDIUM - network access\n\n)))\n\n1. (((\n\n**Full Fact Technology Discussion**\n\n\n\n* Purpose: Explore AI tool integration\n\n* Timeline: After POC demonstration\n\n* Impact: MEDIUM - proven technology partnership\n\n)))\n\n\n\n=== Tier 3: Scale ===\n\n\n\n**Scale and expand reach:**\n\n\n\n1. (((\n\n**EFCSN Membership Application**\n\n\n\n* Requirement: Demonstrated fact-checking track record\n\n* Timeline: After sufficient operational track record\n\n* Impact: MEDIUM - European credibility\n\n)))\n\n1. (((\n\n**Google News Initiative Grant**\n\n\n\n* Requirement: IFCN signatory or endorsement\n\n* Timeline: Next funding round\n\n* Funding: $25K-$100K\n\n* Impact: HIGH - major technology development funding\n\n)))\n\n1. (((\n\n**MediaWise TFCN Switzerland**\n\n\n\n* Purpose: Launch Swiss teen fact-checking network\n\n* Timeline: Academic year planning\n\n* Impact: MEDIUM - educational reach\n\n)))\n\n1. (((\n\n**SRG SSR Pilot Partnership**\n\n\n\n* Purpose: Swiss public broadcasting integration\n\n* Timeline: After POC proves capability\n\n* Impact: HIGH - major Swiss media partnership\n\n)))\n\n\n\n=== Tier 4: Long-Term Strategic (12+ Months) ===\n\n\n\n**Establish FactHarbor as ecosystem infrastructure:**\n\n\n\n1. (((\n\n**EU EDMO Partnership**\n\n\n\n* Purpose: European fact-checking infrastructure\n\n* Timeline: Once established operations\n\n* Impact: HIGH - pan-European reach\n\n)))\n\n1. (((\n\n**Academic Research Publications**\n\n\n\n* Purpose: Validate Evidence Model approach\n\n* Timeline: 1-2 years for peer review\n\n* Impact: MEDIUM - academic credibility\n\n)))\n\n1. (((\n\n**Open Source Community Growth**\n\n\n\n* Purpose: Developer contributions\n\n* Timeline: Ongoing\n\n* Impact: MEDIUM - sustainability\n\n)))\n\n\n\n----\n\n\n\n== 9. GEOGRAPHIC COOPERATION STRATEGY ==\n\n\n\n=== Switzerland Focus (HOME BASE) ===\n\n\n\n**Build local credibility first:**\n\n\n\n* ETH Zurich / University of Zurich (research partners)\n\n* SRG SSR / SRF (media partners)\n\n* Innosuisse / Gebert Rf (funding)\n\n* Prototype Fund Switzerland (public interest tech)\n\n* dpa Switzerland coverage (existing fact-checker)\n\n\n\n**Why This Matters:**\n\n\n\n* Swiss nonprofit status enables local funding\n\n* Swiss academic partnerships required for grants\n\n* Swiss media provides use cases and validation\n\n* Local credibility transfers to international partnerships\n\n\n\n=== DACH Region (EXPANSION) ===\n\n\n\n**Leverage German-language network:**\n\n\n\n* dpa Fact-Checking (Germany, Austria, Switzerland)\n\n* ORF (Austrian public broadcasting)\n\n* Correctiv (German EFCSN member)\n\n* Mimikama (Austrian fact-checker)\n\n\n\n**Why This Matters:**\n\n\n\n* Common language (German)\n\n* Cultural similarity\n\n* Existing fact-checking infrastructure\n\n* Large market for scaling\n\n\n\n=== Europe (INTEGRATION) ===\n\n\n\n**Connect to European ecosystem:**\n\n\n\n* EFCSN membership\n\n* EDMO partnership\n\n* EBU Eurovision News Spotlight\n\n* EU funding programs\n\n\n\n**Why This Matters:**\n\n\n\n* Swiss alignment with European standards (even without EU membership)\n\n* Access to EU research funding\n\n* European fact-checking network (61 EFCSN members)\n\n\n\n=== Global (INFLUENCE) ===\n\n\n\n**Build international credibility:**\n\n\n\n* IFCN signatory status\n\n* Knight Foundation funding\n\n* Google News Initiative partnership\n\n* Academic publications (English)\n\n\n\n**Why This Matters:**\n\n\n\n* Methodology validation\n\n* Technology standard setting\n\n* Funding diversity\n\n* Global impact\n\n\n\n----\n\n\n\n== 10. PARTNERSHIP VALUE PROPOSITION ==\n\n\n\n=== What FactHarbor Offers Partners ===\n\n\n\n==== For Fact-Checking Organizations: ====\n\n\n\n* **Technology:**Automated analysis with transparent Evidence Models\n\n* **Scale:**Handle high-volume claim verification\n\n* **Methodology:**Structured approach reducing bias\n\n* **Open Source:**Customize and deploy independently\n\n\n\n==== For Academic Institutions: ====\n\n\n\n* **Research Platform:**Test automated fact-checking hypotheses\n\n* **Dataset:**Real-world fact-checking data\n\n* **Innovation:**Novel Evidence Model approach\n\n* **Publications:**Co-authored research papers\n\n\n\n==== For Funders: ====\n\n\n\n* **Innovation:**Unique scenario-based analysis (not binary verdicts)\n\n* **Transparency:**Open source, open methodology\n\n* **Impact:**Scalable solution to misinformation\n\n* **Swiss Base:**Stable, neutral jurisdiction\n\n\n\n==== For Media Organizations: ====\n\n\n\n* **Efficiency:**Automated initial analysis saves journalist time\n\n* **Quality:**Structured evidence gathering\n\n* **Transparency:**Show evidence clearly to readers\n\n* **Integration:**API-based, easy to embed\n\n\n\n==== For Educators: ====\n\n\n\n* **Pedagogy:**Teaches structured critical thinking\n\n* **Evidence-Based:**Shows how to evaluate claims systematically\n\n* **Engagement:**Interactive platform for students\n\n* **Curriculum:**Ready-made lesson plans (to be developed)\n\n\n\n----\n\n\n\n== 11. RISKS & MITIGATION ==\n\n\n\n=== Partnership Risks ===\n\n\n\n==== Risk: IFCN/EFCSN Rejection ====\n\n\n\n* **Likelihood:**MEDIUM (high standards, unproven track record)\n\n* **Impact:**HIGH (limits funding and credibility)\n\n* **Mitigation:**\n\n** Build sufficient quality fact-check track record before applying\n\n** Get endorsement from existing member (dpa, Full Fact)\n\n** Demonstrate all 5 IFCN principles clearly\n\n** Start with EFCSN (potentially easier for Swiss org)\n\n\n\n==== Risk: Swiss University Uninterested ====\n\n\n\n* **Likelihood:**LOW (fact-checking is hot research topic)\n\n* **Impact:**MEDIUM (delays Innosuisse funding)\n\n* **Mitigation:**\n\n** Target multiple universities (ETH, UZH, EPFL)\n\n** Emphasize novel research contributions\n\n** Offer paid research collaboration\n\n** Connect through personal network\n\n\n\n==== Risk: ClaimReview Implementation Complexity ====\n\n\n\n* **Likelihood:**LOW (well-documented standard)\n\n* **Impact:**HIGH (blocks Google visibility)\n\n* **Mitigation:**\n\n** Use existing tools (Full Fact WordPress plugin as reference)\n\n** Consult Duke Reporters' Lab\n\n** Hire contractor if needed\n\n\n\n==== Risk: Funding Competition ====\n\n\n\n* **Likelihood:**HIGH (many orgs seeking fact-checking funding)\n\n* **Impact:**MEDIUM (delays growth)\n\n* **Mitigation:**\n\n** Emphasize unique value (Evidence Models vs binary verdicts)\n\n** Target multiple funding sources simultaneously\n\n** Build strong Swiss base first (less competitive)\n\n** Demonstrate traction before major applications\n\n\n\n=== Market Risks ===\n\n\n\n==== Risk: Automated Fact-Checking Skepticism ====\n\n\n\n* **Likelihood:**MEDIUM (legitimate AI concerns)\n\n* **Impact:**MEDIUM (slow adoption)\n\n* **Mitigation:**\n\n** Emphasize transparency and quality gates\n\n** Position as \"augmentation not replacement\"\n\n** Build partnership with established fact-checkers\n\n** Publish methodology openly\n\n\n\n----\n\n\n\n== 12. RECOMMENDED ACTION PLAN ==\n\n\n\n=== Phase 1: Foundation ===\n\n\n\n**Goal: Establish credibility and core partnerships**\n\n\n\n1.  Implement ClaimReview schema in POC\n\n1.  Create GitHub organization and publish initial code\n\n1.  Draft IFCN signatory application (don't submit yet)\n\n\n\n1.  Email Dr. Chengkai Li (UTA ClaimBuster) about research collaboration\n1.  Contact dpa Fact-Checking about Swiss partnership\n1.  Reach out to ETH Zurich NLP researchers\n\n\n1.  Prepare Innosuisse application materials (with ETH partner)\n1.  Contact Duke Reporters' Lab about Tech & Check\n1.  Monitor Knight Prototype Fund for next call\n\n\n1.  Submit Innosuisse application (if ETH partnership confirmed)\n1.  Contact Full Fact about technology integration\n1.  Begin documenting fact-checks for IFCN application\n\n\n=== Phase 2: Growth ===\n\n\n\n**Goal: Secure funding and expand partnerships**\n\n\n\n1.  Outreach to SRF about pilot partnership\n1.  Contact MediaWise about Swiss TFCN chapter\n1.  Apply for Prototype Fund Switzerland (if call open)\n\n\n1.  Submit EFCSN membership application\n1.  Contact EDMO about research partnership\n1.  Attend #GlobalFact conference (if timing aligns)\n\n\n1.  Submit IFCN signatory application (after sufficient operational track record)\n1.  Knight Prototype Fund application (if call open)\n1.  Publish first research findings/case studies\n\n\n=== Phase 3: Scale ===\n\n\n\n**Goal: Establish FactHarbor as infrastructure**\n\n\n\n1.  Google News Initiative grant application (after IFCN signatory)\n1.  Expand media partnerships (Tamedia, NZZ)\n1.  Develop educational curriculum with EJTA\n\n\n1.  EU EDMO partnership discussion\n1.  Academic publication submission\n1.  Strategic review and next-phase planning\n\n\n----\n\n\n\n== 13. SUCCESS METRICS ==\n\n\n\n=== Partnership Metrics ===\n\n\n\n* Number of active partnerships: Target 10+\n\n* IFCN/EFCSN membership: Achieved\n\n* Academic publications: 1+ submitted\n\n* Media integrations: 2+ pilot partnerships\n\n\n\n=== Funding Metrics ===\n\n\n\n* Total funding secured: Target CHF 500K+\n\n* Funding source diversity: 3+ different funders\n\n* Swiss funding: Innosuisse or Gebert Rf secured\n\n\n\n=== Technical Metrics ===\n\n\n\n* ClaimReview implementation: Phase 1\n\n* API integrations: 2+\n\n* Open source contributors: 5+\n\n\n\n=== Impact Metrics ===\n\n\n\n* Fact-checks published: 100+, scaling to 500+\n\n* User reach: 10K+ users\n\n* Media citations: 5+\n\n\n\n----\n\n\n\n== 14. CONCLUSION ==\n\n\n\nFactHarbor enters a mature but evolving fact-checking ecosystem with a unique value proposition:**Evidence Models that provide nuanced, transparent analysis instead of binary verdicts**. This innovation, combined with Swiss nonprofit status and open-source commitment, creates strong cooperation potential across seven strategic categories.\n\n\n\n**Highest Priority Actions:**\n\n\n\n1. **Implement ClaimReview**(technical prerequisite)\n\n1. **Establish ETH Zurich partnership**(funding prerequisite)\n\n1. **Engage dpa Fact-Checking**(Swiss market entry)\n\n1. **Apply for Innosuisse funding**(major grant opportunity)\n\n1. **Pursue IFCN signatory status**(industry credibility)\n\n\n\n**Strategic Advantages:**\n\n\n\n* **Swiss base:**Access to Swiss funding + European networks\n\n* **Open source:**Attracts civic tech and academic communities\n\n* **Innovation:**Evidence Models differentiate from binary fact-checking\n\n* **Timing:**2025 sees increased demand for transparent AI in fact-checking\n\n\n\n**Ecosystem Positioning:**FactHarbor should position as**infrastructure provider**rather than competitor to existing fact-checkers. Partner with established organizations (Full Fact, dpa, IFCN) to provide automated analysis backend, while they maintain editorial oversight and audience relationships.\n\n\n\n**Next Steps:**Review this report with FactHarbor leadership, prioritize 5-10 partnerships for immediate outreach, and execute Phase 1 of the action plan within 90 days.\n\n\n\n----\n\n\n\n== APPENDIX A: CONTACT DIRECTORY ==\n\n\n\n=== Fact-Checking Organizations ===\n\n\n\n* **IFCN:**factchecknet@poynter.org\n\n* **Full Fact:**Via website contact form\n\n* **dpa Fact-Checking:**factchecking@dpa.com (Teresa Dapp)\n\n* **Duke Reporters' Lab:**reporters-lab@duke.edu (Erica Ryan)\n\n\n\n=== Academic Institutions ===\n\n\n\n* **ClaimBuster/UTA:**cli@uta.edu (Dr. Chengkai Li)\n\n* **ETH Zurich:**(To be identified - NLP research group)\n\n* **University of Zurich:**(To be identified - Media research)\n\n\n\n=== Funding Organizations ===\n\n\n\n* **Knight Foundation:**Via website application portal\n\n* **Innosuisse:**https:~/~/www.innosuisse.admin.ch/\n\n* **Gebert Rf:**https:~/~/www.grstiftung.ch/\n\n* **Prototype Fund CH:**https:~/~/prototypefund.opendata.ch/\n\n\n\n=== Technology Platforms ===\n\n\n\n* **Google Fact Check Tools:**Via Google Search Console\n\n* **ClaimReview Project:**https:~/~/www.claimreviewproject.com/\n\n\n\n=== Networks ===\n\n\n\n* **Code for All:**https:~/~/codeforall.org/\n\n* **EDMO:**https:~/~/edmo.eu/\n\n* **EBU:**https:~/~/www.ebu.ch/\n\n\n\n----\n\n\n\n== APPENDIX B: FUNDING SUMMARY TABLE ==\n\n\n\n|=Funding Source|=Amount|=Requirements|=Priority\n\n|Innosuisse Innovation|CHF 500K/year|Swiss entity, research partner, 10% co-funding|**HIGH**\n\n|BRIDGE Proof of Concept|CHF 150-850K|Research partnership (SNSF + Innosuisse)|**HIGH**\n\n|Gebert Rf InnoBooster|CHF 150K|Prior funding proof, market potential|**MEDIUM**\n\n|Prototype Fund CH|CHF 100K|Open source, public interest, Swiss work permit|**HIGH**\n\n|Knight Prototype Fund|$50K|Innovation focus|**MEDIUM**\n\n|Google News Initiative|$25-100K|IFCN signatory or endorsement|**HIGH**\n\n|EFCSN/EU Grants|Varies|European focus, consortium|**MEDIUM**\n\n\n\n**Total Potential:**CHF 500K-1M+ / $550K-1.1M+\n\n\n\n----\n\n\n\n**Report Prepared By:**Claude (Anthropic)\n\n**Date:**December 21, 2025\n\n**Based On:**\n\n\n\n* FactHarbor V0.9.69 Specification\n\n* Comprehensive Fact-Checking Research Report (50+ sources)\n\n* 60+ web search results across 7 categories\n\n* Analysis of 443 global fact-checking organizations\n\n\n\n**Confidentiality:**Internal use - may be shared with potential partners as needed", "Organisation.Strategy.Core Problems FactHarbor Solves.WebHome": "= Core Problems FactHarbor Solves =\n(% class=\"box infomessage\" %)\n(((\n**Our Mission**\nFactHarbor brings clarity and transparency to a world full of unclear, contested, and misleading information by shedding light on the context, assumptions, and evidence behind claims.\n)))\n== 1. Core Problems ==\n=== 1.1 Problem 1  Misinformation & Manipulation ===\nFalsehoods and distortions spread rapidly through:\n* Political propaganda\n* Social media amplification\n* Coordinated influence networks\n* AI-generated fake content\nUsers need a structured system that resists manipulation and makes reasoning transparent.\n=== 1.2 Problem 2  Missing Context Behind Claims ===\nMost claims change meaning drastically depending on:\n* Definitions\n* Assumptions\n* Boundaries\n* Interpretation\nFactHarbor reveals and compares these variations.\n=== 1.3 Problem 3  \"Binary Fact Checks\" Fail ===\nMost fact-checking simplifies complex claims into:\n* True\n* Mostly True\n* False\nThis hides legitimate contextual differences.\nFactHarbor replaces binary judgment with scenario-based, likelihood-driven evaluation.\n=== 1.4 Problem 4  Good Evidence Is Hard to Find ===\nHigh-quality evidence exists  but users often cannot:\n* Locate it\n* Assess its reliability\n* Understand how it fits into a scenario\n* Compare it with competing evidence\nFactHarbor aggregates, assesses, and organizes evidence with full transparency.\n=== 1.5 Problem 5  Claims Evolve Over Time ===\nResearch and understanding change:\n* New studies emerge\n* Old studies are retracted\n* Consensus shifts\nFactHarbor provides:\n* Full entity versioning\n* Verdict timelines\n* Automatic re-evaluation when inputs change\n=== 1.6 Problem 6  Users Cannot See Why People Disagree ===\nPeople often assume others are ignorant or dishonest, when disagreements typically arise from:\n* Different definitions\n* Different implicit assumptions\n* Different evidence\n* Different contexts\nFactHarbor exposes these underlying structures so disagreements become understandable, not divisive.\n== 2. Core Concepts ==\n=== 2.1 Claim ===\nA user- or AI-submitted statement whose meaning is often ambiguous and requires structured interpretation.\nA claim does not receive a single verdict  it branches into scenarios that clarify its meaning.\n=== 2.2 Scenario ===\nA structured interpretation that clarifies what the claim means under a specific set of:\n* Boundaries\n* Definitions\n* Assumptions\n* Contextual conditions\nMultiple scenarios allow claims to be understood fairly and without political or ideological bias.\n=== 2.3 Evidence ===\nInformation that supports or contradicts a scenario.\nEvidence includes empirical studies, experimental data, expert consensus, historical records, contextual background, and absence-of-evidence signals.\nEvidence evolves through versioning and includes reliability assessment.\n=== 2.4 Verdict ===\nA likelihood estimate for a claim within a specific scenario based on evidence quality, quantity, methodology, uncertainty factors, and comparison with competing scenarios.\nEach verdict is versioned and includes a historical timeline.\n=== 2.5 AI Knowledge Extraction Layer (AKEL) ===\nThe AI subsystem that interprets claims, proposes scenario drafts, retrieves evidence, classifies sources, drafts verdicts, detects contradictions, and triggers re-evaluation when inputs change.\nAKEL outputs follow risk-based publication model with quality gates and audit oversight.\n=== 2.6 Decentralized Federation Model ===\nFactHarbor supports a decentralized, multi-node architecture where each node stores its own data and synchronizes via federation protocol.\nThis increases resilience, autonomy, and scalability.\n== 3. Vision for Impact ==\nFactHarbor aims to:\n* **Reduce polarization** by revealing legitimate grounds for disagreement\n* **Combat misinformation** by providing structured, transparent evaluation\n* **Empower users** to make informed judgments based on evidence\n* **Support deliberative democracy** by clarifying complex policy questions\n* **Enable federated knowledge** so no single entity controls the truth\n* **Resist manipulation** through transparent reasoning and quality oversight\n* **Evolve with research** by continuously improving analysis through UCM configuration\n== 4. Related Pages ==\n* [[Requirements (Roles)>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[AKEL (AI Knowledge Extraction Layer)>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]\n* [[Functional Requirements>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[Federation & Decentralization>>FactHarbor.Product Development.Specification.Federation & Decentralization.WebHome]]", "Organisation.Strategy.Ideal Customer Profile.WebHome": "= Ideal Customer Profile =\n\n\n\nThis page defines FactHarbor's ideal customer segments and partner profiles to guide product development, marketing, and partnership strategy.\n\n\n\n== 1. Purpose ==\n\n\n\nUnderstanding who benefits most from FactHarbor helps us:\n\n* **Product Development**: Prioritize features that serve core user needs\n\n* **Marketing**: Communicate value effectively to target audiences\n\n* **Partnerships**: Identify and cultivate strategic relationships\n\n* **Resource Allocation**: Focus limited resources on highest-impact activities\n\n\n\n**Philosophy**: FactHarbor serves users who want to **understand**, not just believe. Our ideal customers share a frustration with binary \"true/false\" verdicts and value transparent reasoning they can inspect.\n\n\n\n== 2. Primary User Segments ==\n\n\n\n=== 2.1 Journalists & Newsrooms ===\n\n\n\n**Profile**:\n\n* Working journalists at news organizations (local to international)\n\n* Fact-checkers and verification specialists\n\n* Editorial teams producing investigative or political content\n\n\n\n**Core Needs** (from User Needs documentation):\n\n* **UN-4**: Fast social media fact-checking (15 seconds to initial verdict)\n\n* **UN-14**: API integration into professional workflows\n\n* **UN-5/UN-6**: Source provenance and publisher reliability tracking\n\n* **UN-7**: Evidence transparency for editorial review\n\n\n\n**Key Pain Points**:\n\n* Time pressure with breaking news and viral content\n\n* Need to verify claims quickly without sacrificing accuracy\n\n* Difficulty tracing claims to original sources\n\n* Binary fact-check verdicts lack nuance for complex stories\n\n\n\n**Value Proposition**:\n\nFactHarbor provides structured, scenario-based analysis that reveals **how** conclusions are reached, saving time while providing the context needed for accurate reporting.\n\n\n\n**Success Indicators**:\n\n* Reduced time spent on claim verification\n\n* Ability to cite FactHarbor analyses in published work\n\n* Improved editorial confidence in complex stories\n\n\n\n=== 2.2 Researchers & Academics ===\n\n\n\n**Profile**:\n\n* University researchers (political science, communications, media studies)\n\n* Think tank analysts\n\n* PhD students studying misinformation\n\n* Data scientists working on verification systems\n\n\n\n**Core Needs**:\n\n* **UN-7**: Complete evidence transparency\n\n* **UN-9**: Methodology transparency (auditable reasoning)\n\n* **UN-13**: Ability to cite FactHarbor verdicts in academic work\n\n* **UN-15**: Verdict evolution timeline (how assessments change with new evidence)\n\n\n\n**Key Pain Points**:\n\n* Existing fact-checks are methodologically opaque\n\n* Need structured data for quantitative analysis\n\n* Difficulty comparing how claims are assessed across sources\n\n* Binary verdicts hide important uncertainty\n\n\n\n**Value Proposition**:\n\nFactHarbor provides **transparent, structured methodology** that can be cited, analyzed, and built upon. The Evidence Model approach creates reusable data for academic research.\n\n\n\n**Success Indicators**:\n\n* Academic papers citing FactHarbor methodology\n\n* Researchers using FactHarbor data in studies\n\n* Methodology validation by academic institutions\n\n\n\n=== 2.3 Educators ===\n\n\n\n**Profile**:\n\n* University professors (media literacy, critical thinking, journalism)\n\n* High school teachers (civics, social studies, media studies)\n\n* Librarians and information literacy specialists\n\n* Corporate trainers (media literacy programs)\n\n\n\n**Core Needs**:\n\n* **UN-3**: Article summaries with FactHarbor analysis for teaching materials\n\n* **UN-8**: Understanding disagreement and consensus (why experts differ)\n\n* **UN-9**: Methodology transparency for pedagogical purposes\n\n* **UN-7**: Evidence transparency to teach source evaluation\n\n\n\n**Key Pain Points**:\n\n* Fact-checks don't show reasoning process for teaching\n\n* Hard to teach critical thinking with black-box verdicts\n\n* Need tools that demonstrate **how** to evaluate claims\n\n* Limited resources for curriculum development\n\n\n\n**Value Proposition**:\n\nFactHarbor teaches the **process** of evidence evaluation, not just the answer. Students see explicit assumptions, multiple scenarios, and how confidence levels are determined.\n\n\n\n**Success Indicators**:\n\n* Educators integrating FactHarbor into curricula\n\n* Student engagement with evidence exploration features\n\n* Educational institution partnerships\n\n\n\n=== 2.4 Policy Analysts ===\n\n\n\n**Profile**:\n\n* Government policy advisors\n\n* NGO research staff\n\n* Legislative aides\n\n* Regulatory analysts\n\n\n\n**Core Needs**:\n\n* **UN-2/UN-3**: Context-dependent analysis (claims true under some conditions, false under others)\n\n* **UN-8**: Understanding why reasonable people disagree\n\n* **UN-1**: Trust assessment with explicit confidence ranges\n\n* **UN-17**: In-article claim highlighting for briefing documents\n\n\n\n**Key Pain Points**:\n\n* Policy questions rarely have simple true/false answers\n\n* Need to understand stakeholder perspectives and their evidence\n\n* Difficulty synthesizing information from multiple sources\n\n* Risk of appearing biased when presenting controversial topics\n\n\n\n**Value Proposition**:\n\nFactHarbor's **scenario-based analysis** explicitly maps how conclusions depend on assumptions, enabling policy analysts to present balanced, well-sourced briefings.\n\n\n\n**Success Indicators**:\n\n* Policy briefs citing FactHarbor analyses\n\n* Repeat usage for complex policy questions\n\n* Feedback on improved briefing quality\n\n\n\n=== 2.5 Content Consumers (General Public) ===\n\n\n\n**Profile**:\n\n* Social media users seeking to verify viral claims\n\n* Engaged citizens following news and politics\n\n* People making decisions based on contested information\n\n* Anyone who has been frustrated by oversimplified fact-checks\n\n\n\n**Core Needs**:\n\n* **UN-1**: Trust assessment at a glance (immediate visual understanding)\n\n* **UN-4**: Fast social media fact-checking\n\n* **UN-12**: Ability to submit unchecked claims\n\n* **UN-17**: In-article claim highlighting when reading content\n\n\n\n**Key Pain Points**:\n\n* Don't trust fact-checkers' authority\n\n* Want to understand reasoning, not just accept verdicts\n\n* Time-constrained but want to make informed decisions\n\n* Frustrated by partisan accusations about fact-checkers\n\n\n\n**Value Proposition**:\n\nFactHarbor shows **reasoning you can inspect**. Trust comes from transparent methodology, not authority. You can form your own judgment based on visible evidence.\n\n\n\n**Success Indicators**:\n\n* User retention (return visits)\n\n* Time spent exploring evidence details\n\n* Claims submitted for verification\n\n* User satisfaction with transparency\n\n\n\n== 3. B2B Partner Segments ==\n\n\n\n=== 3.1 Media Organizations ===\n\n\n\n**Priority**: HIGH (Tier 1)\n\n\n\n**Target Partners**:\n\n* Swiss Broadcasting (SRG SSR, SRF, RTS, RSI)\n\n* Major newspapers (Tamedia, NZZ)\n\n* Regional news organizations\n\n* Digital-first news outlets\n\n\n\n**Partnership Value**:\n\n* **For Partners**: Automated initial analysis saves journalist time; structured evidence for reader transparency\n\n* **For FactHarbor**: Validation, use cases, credibility, potential funding\n\n\n\n**Engagement Model**:\n\n* API integration for newsroom tools\n\n* Embedded analysis widgets\n\n* Co-branded fact-checking initiatives\n\n* Pilot programs for election coverage\n\n\n\n=== 3.2 Fact-Checking Organizations ===\n\n\n\n**Priority**: HIGH (Tier 1)\n\n\n\n**Target Partners**:\n\n* IFCN (International Fact-Checking Network) members\n\n* EFCSN (European Fact-Checking Standards Network) members\n\n* dpa Fact-Checking (DACH region)\n\n* Correctiv (Germany)\n\n* Full Fact (UK)\n\n\n\n**Partnership Value**:\n\n* **For Partners**: Technology platform, scalability, methodology alignment\n\n* **For FactHarbor**: Credibility, network access, ecosystem integration\n\n\n\n**Engagement Model**:\n\n* Open-source technology sharing\n\n* ClaimReview schema collaboration\n\n* Joint methodology development\n\n* Cross-referencing and data sharing\n\n\n\n=== 3.3 Academic Institutions ===\n\n\n\n**Priority**: HIGH (Tier 1)\n\n\n\n**Target Partners**:\n\n* ETH Zurich / University of Zurich (Swiss, research collaboration)\n\n* Duke Reporters' Lab (ClaimReview, Tech & Check)\n\n* Harvard Shorenstein Center (network access)\n\n* Stanford Internet Observatory (misinformation research)\n\n* Oxford Reuters Institute (journalism research)\n\n\n\n**Partnership Value**:\n\n* **For Partners**: Research platform, real-world data, novel methodology to study\n\n* **For FactHarbor**: Academic validation, grant access (Innosuisse), publications\n\n\n\n**Engagement Model**:\n\n* Research partnerships\n\n* Student thesis projects\n\n* Co-authored publications\n\n* Conference presentations\n\n* Joint grant applications\n\n\n\n=== 3.4 Funding Organizations ===\n\n\n\n**Priority**: MEDIUM (Tier 2)\n\n\n\n**Target Partners**:\n\n* Knight Foundation (journalism innovation)\n\n* Google News Initiative (fact-checking fund)\n\n* Swiss Innosuisse (research/innovation grants)\n\n* Gebert Rf Foundation (Swiss innovation)\n\n* Prototype Fund Switzerland\n\n\n\n**Partnership Value**:\n\n* **For Partners**: Support innovative, transparent approach to misinformation\n\n* **For FactHarbor**: Operational funding, validation, network access\n\n\n\n**Engagement Model**:\n\n* Grant applications\n\n* Progress reporting\n\n* Impact documentation\n\n* Network participation\n\n\n\n== 4. Common Customer Characteristics ==\n\n\n\n=== 4.1 Unifying Frustrations ===\n\n\n\nAll ideal customers share frustration with:\n\n* Binary \"true/false\" verdicts that hide complexity\n\n* Opaque methodology (\"trust us\" authority model)\n\n* Lack of explicit assumptions and confidence ranges\n\n* Inability to see evidence and reasoning process\n\n* No way to understand why experts disagree\n\n\n\n=== 4.2 Unifying Values ===\n\n\n\nAll ideal customers value:\n\n* **Transparency**: Visible reasoning chains and methodology\n\n* **Nuance**: Context-dependent truth (scenarios)\n\n* **Independence**: Forming own judgment from evidence\n\n* **Integrity**: Non-profit, open-source, no hidden agenda\n\n* **Accessibility**: Understanding without specialized expertise\n\n\n\n=== 4.3 Decision Criteria ===\n\n\n\nWhen evaluating fact-checking tools, ideal customers prioritize:\n\n1. **Methodology Transparency**: Can I see how conclusions are reached?\n\n2. **Evidence Quality**: Are sources traceable and credible?\n\n3. **Nuance Handling**: Does it acknowledge complexity?\n\n4. **Speed & Usability**: Can I use it in my workflow?\n\n5. **Trust & Independence**: Is there hidden bias or agenda?\n\n\n\n== 5. Customer Journey ==\n\n\n\n=== 5.1 Awareness ===\n\n\n\n**How they find us**:\n\n* Academic publications citing FactHarbor\n\n* Referrals from fact-checking organizations\n\n* Search engine results (ClaimReview schema visibility)\n\n* Media coverage of misinformation topics\n\n* Social media discussions about fact-checking\n\n\n\n=== 5.2 Evaluation ===\n\n\n\n**What they assess**:\n\n* Methodology documentation (open and detailed?)\n\n* Sample analyses (quality and transparency?)\n\n* Open-source code (auditable?)\n\n* Non-profit status (trustworthy?)\n\n* User experience (usable?)\n\n\n\n=== 5.3 Adoption ===\n\n\n\n**How they start**:\n\n* Submit a claim they're curious about\n\n* Explore an existing analysis in depth\n\n* Review methodology documentation\n\n* Test with a known case to validate quality\n\n* Integrate API into existing workflow\n\n\n\n=== 5.4 Retention ===\n\n\n\n**Why they return**:\n\n* Consistent quality and transparency\n\n* Time savings in verification workflow\n\n* Unique value (scenario analysis not available elsewhere)\n\n* Trust in methodology\n\n* Community participation\n\n\n\n== 6. Anti-Personas (Not Our Target) ==\n\n\n\n=== 6.1 Confirmation Seekers ===\n\n\n\n**Profile**: Users who want verdicts that confirm their existing beliefs\n\n\n\n**Why Not Ideal**:\n\n* Will be frustrated by nuanced, scenario-based analysis\n\n* May reject conclusions that don't match expectations\n\n* Not looking for transparent reasoninglooking for validation\n\n\n\n**How to Handle**:\n\n* Don't compromise methodology to satisfy them\n\n* The transparency may eventually convert some\n\n\n\n=== 6.2 Speed-Only Users ===\n\n\n\n**Profile**: Users who only want instant answers, no interest in evidence\n\n\n\n**Why Not Ideal**:\n\n* Don't value FactHarbor's core differentiator (transparency)\n\n* Would be better served by simpler binary fact-checkers\n\n* Won't engage with evidence or scenarios\n\n\n\n**How to Handle**:\n\n* Provide quick summary views (UN-1: trust at a glance)\n\n* Make deeper exploration available but not required\n\n\n\n=== 6.3 Bad-Faith Actors ===\n\n\n\n**Profile**: Users seeking to game or manipulate the system\n\n\n\n**Why Not Ideal**:\n\n* Waste resources\n\n* Damage system integrity\n\n* Not genuine users\n\n\n\n**How to Handle**:\n\n* AKEL detection of manipulation patterns\n\n* Moderation for flagged escalations\n\n* Transparent ban policies\n\n\n\n== 7. Metrics and Validation ==\n\n\n\n=== 7.1 Segment Metrics ===\n\n\n\nTrack for each segment:\n\n* **Acquisition**: How many from each segment?\n\n* **Activation**: Do they complete first analysis?\n\n* **Engagement**: Do they explore evidence?\n\n* **Retention**: Do they return?\n\n* **Referral**: Do they recommend others?\n\n\n\n=== 7.2 Segment-Specific Success Indicators ===\n\n\n\n| Segment | Key Success Metric |\n\n|---------|-------------------|\n\n| Journalists | API calls per newsroom; time saved per verification |\n\n| Researchers | Papers citing FactHarbor; data downloads |\n\n| Educators | Curricula integrations; student engagement |\n\n| Policy Analysts | Briefings citing FactHarbor; repeat usage |\n\n| Content Consumers | Retention rate; evidence exploration rate |\n\n\n\n=== 7.3 Partnership Metrics ===\n\n\n\n| Partner Type | Success Metric |\n\n|-------------|----------------|\n\n| Media | Integration count; co-published analyses |\n\n| Fact-Checkers | Data sharing volume; methodology alignment |\n\n| Academic | Papers published; grants received |\n\n| Funders | Grants awarded; renewal rate |\n\n\n\n== 8. Related Pages ==\n\n\n\n* [[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]] - Detailed user need definitions\n\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] - How user needs map to requirements\n\n* [[Cooperation Opportunities>>FactHarbor.Organisation.Strategy.Cooperation Opportunities.WebHome]] - Partnership opportunity details\n\n* [[Large Donations Policy>>FactHarbor.Organisation.Legal and Compliance.Large Donations Policy.WebHome]] - Funding sources and contacts\n\n* [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]] - How FactHarbor is structured\n\n", "Organisation.Strategy.WebHome": "= Strategy =\n\nStrategic context, market positioning, and operating philosophy for FactHarbor.\n\n== Mission & Problem Space ==\n\n* [[Core Problems FactHarbor Solves>>FactHarbor.Organisation.Strategy.Core Problems FactHarbor Solves.WebHome]]  The six core problems and FactHarbor's vision for impact\n\n== Operating Philosophy ==\n\n* [[Automation Philosophy>>FactHarbor.Organisation.Strategy.Automation Philosophy.WebHome]]  AI-first principles: AKEL is primary, humans improve the system\n\n== Market & Positioning ==\n\n* [[Competitive Analysis>>FactHarbor.Organisation.Strategy.Competitive Analysis.WebHome]]  Market landscape, gaps, and FactHarbor's unique positioning\n* [[Ideal Customer Profile>>FactHarbor.Organisation.Strategy.Ideal Customer Profile.WebHome]]  User segments, partner profiles, and customer journey\n\n== Partnerships ==\n\n* [[Cooperation Opportunities>>FactHarbor.Organisation.Strategy.Cooperation Opportunities.WebHome]]  Partnership models, outreach targets, and collaboration frameworks\n\n== Related ==\n\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]  Organisational model and decision processes\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]  How strategy maps to product requirements\n", "Organisation.WebHome": "= Organisation =\n\nThis section describes **how FactHarbor operates** as an organization  governance, culture, legal framework, and strategy.\n\n{{include reference=\"FactHarbor.Organisation.Diagrams.Governance Structure.WebHome\"/}}\n\n== Governance ==\n\nStructure, roles, decision processes, and operational readiness.\n\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]]  Organisational model, roles, decision processes, transition plan, and operational readiness\n\n== How We Work Together ==\n\nCulture, values, guidelines, policies, and contributor processes.\n\n* [[How We Work Together>>FactHarbor.Organisation.How-We-Work-Together.WebHome]]  Workplace culture, consent-based decisions, global rules, policies, and contributor processes\n\n== Legal and Compliance ==\n\nLegal foundations, licensing, contributor agreements, and financial governance.\n\n* [[Legal and Compliance>>FactHarbor.Organisation.Legal and Compliance.WebHome]]  Legal framework, open source licensing, CLA, finance, and donations policy\n\n== Strategy ==\n\nMission context, operating philosophy, market positioning, and partnerships.\n\n* [[Strategy>>FactHarbor.Organisation.Strategy.WebHome]]  Core problems, automation philosophy, competitive analysis, ICP, and cooperation opportunities\n\n== Diagrams ==\n\nVisual reference diagrams for governance, domains, and workflows.\n\n* [[Diagrams>>FactHarbor.Organisation.Diagrams.WebHome]]  Governance Structure, Domain Interaction Map, Claim and Scenario Lifecycle\n\n---\n\n== Key Principles ==\n\n==== Automation First ====\n* AKEL makes content decisions\n* Humans improve algorithms\n* Scale through code, not people\n\n==== Continuous Improvement ====\n* Measure everything\n* Test before deploying\n* Learn from failures\n* Iterate rapidly\n\n==== Transparency ====\n* Open decisions\n* Public metrics\n* Documented rationale\n* Auditable code\n\n==== Empowerment ====\n* Clear domains\n* Autonomous decisions\n* Support and resources\n* Trust by default\n\n==== User Focus ====\n* Serve users' needs\n* Protect privacy\n* Build trust\n* Enable informed decisions\n", "Product Development.DevOps.Deployment.WebHome": "= Deployment =\n\nHosting, CI/CD, and environment configuration for deploying FactHarbor. Current strategy targets zero-cost or near-zero-cost hosting using free-tier cloud services during pre-release phases.\n\n== Hosting ==\n\n* [[Zero-Cost Hosting Implementation Guide>>FactHarbor.Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide.WebHome]] - Complete Fly.io deployment guide for pre-release beta ($0-5/month)\n", "Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide.WebHome": "= Zero-Cost Hosting Implementation Guide =\n\n\n\n**Document Version:** 1.0\\\\\n\n**Date:** 2026-01-02\\\\\n\n**Status:** Approved - Ready for Implementation\\\\\n\n**Target:** Pre-release beta with logged-in users, $0-5/month budget\n\n\n\n\n\n{{info}}\n\nFactHarbor is currently in **local POC development**. This guide is a **future deployment plan** and is not required for the current phase.\n\n{{/info}}\n\n----\n\n\n\n== Executive Summary ==\n\n\n\nThis guide provides a complete implementation plan for hosting FactHarbor's pre-release beta version with **near-zero infrastructure costs** ($0-5/month) while supporting 10-50 active beta users.\n\n\n\n**Key Strategy:**\\\\\n\n* Leverage generous free tiers from modern cloud providers\n\n* Implement aggressive cost controls (rate limiting, caching, tiered LLM models)\n\n* Use separated architecture to reduce AI costs by 70%\n\n* Scale infrastructure costs only when revenue/funding is secured\n\n\n\n----\n\n\n\n== Recommended Architecture: Fly.io Stack ==\n\n\n\n=== Why Fly.io? ===\n\n\n\n* **True $0/month possible** with generous free tier\n\n* **Works with any tech stack** (Docker-based)\n\n* **Global deployment** in seconds\n\n* **Includes PostgreSQL + Redis** in free tier\n\n* **Auto-suspend when idle** (saves compute)\n\n* **Easy migration path** to paid tier when ready\n\n\n\n----\n\n\n\n== Complete Stack Overview ==\n\n\n\n{{code}}\n\n\n\n                    USER BROWSER                          \n\n\n\n                     \n\n                      HTTPS\n\n                     \n\n\n\n           Cloudflare Pages (Frontend)                    \n\n            React/Vue/Svelte SPA                         \n\n            FREE: Unlimited bandwidth                    \n\n            Global CDN                                   \n\n\n\n                     \n\n                      REST API\n\n                     \n\n\n\n           Fly.io App (Backend API)                       \n\n            Node.js/Python/Go/.NET                       \n\n            FREE: 3 shared-cpu VMs (256MB each)          \n\n            Auto-suspend when idle                       \n\n\n\n                           \n\n                           \n\n  \n\n Fly.io    Upstash    Anthropic       \n\nPostgres    Redis     Claude API      \n\n                                      \n\n FREE:     FREE:      PAY-PER-USE:    \n\n 3GB       10k        ~$2-5/mo with   \n\n storage   cmds/day   optimizations   \n\n  \n\n{{/code}}\n\n\n\n**Total Monthly Cost: $0-5**\n\n\n\n----\n\n\n\n== Implementation Steps ==\n\n\n\n=== Step 1: Set Up Fly.io Account and Infrastructure ===\n\n\n\n==== 1.1 Create Fly.io Account ====\n\n\n\n{{code language=\"bash\"}}\n\n# Install flyctl CLI\n\n# Windows (PowerShell)\n\niwr https://fly.io/install.ps1 -useb | iex\n\n\n\n# macOS/Linux\n\ncurl -L https://fly.io/install.sh | sh\n\n\n\n# Sign up (credit card required but NOT charged for free tier)\n\nfly auth signup\n\n\n\n# Or log in if you have an account\n\nfly auth login\n\n{{/code}}\n\n\n\n==== 1.2 Create PostgreSQL Database ====\n\n\n\n{{code language=\"bash\"}}\n\n# Create a new Postgres cluster (uses free tier)\n\nfly postgres create \\\n\n  --name factharbor-db \\\n\n  --region ord \\\n\n  --vm-size shared-cpu-1x \\\n\n  --volume-size 3 \\\n\n  --initial-cluster-size 1\n\n\n\n# Save the connection string displayed (you'll need it)\n\n# Format: postgres://user:password@factharbor-db.internal:5432/dbname\n\n{{/code}}\n\n\n\n==== 1.3 Create Redis Cache (Upstash) ====\n\n\n\n{{code language=\"bash\"}}\n\n# Sign up at https://upstash.com (separate service, better free tier)\n\n# Free tier: 10,000 commands/day, 256MB storage\n\n\n\n# Create database via Upstash console\n\n# Select: Global, REST API enabled\n\n# Save connection details:\n\n# - UPSTASH_REDIS_REST_URL\n\n# - UPSTASH_REDIS_REST_TOKEN\n\n{{/code}}\n\n\n\n----\n\n\n\n=== Step 2: Containerize Your Application ===\n\n\n\n==== 2.1 Create Dockerfile (Node.js Example) ====\n\n\n\n{{code language=\"dockerfile\"}}\n\n# Dockerfile\n\nFROM node:20-alpine AS builder\n\n\n\nWORKDIR /app\n\n\n\n# Copy package files\n\nCOPY package*.json ./\n\n\n\n# Install dependencies\n\nRUN npm ci --only=production\n\n\n\n# Copy source code\n\nCOPY . .\n\n\n\n# Build if needed (for TypeScript, etc.)\n\nRUN npm run build\n\n\n\n# Production image\n\nFROM node:20-alpine\n\n\n\nWORKDIR /app\n\n\n\n# Copy built app\n\nCOPY --from=builder /app/dist ./dist\n\nCOPY --from=builder /app/node_modules ./node_modules\n\nCOPY --from=builder /app/package*.json ./\n\n\n\n# Expose port\n\nEXPOSE 8080\n\n\n\n# Start app\n\nCMD [\"node\", \"dist/server.js\"]\n\n{{/code}}\n\n\n\n==== 2.2 Create Dockerfile (.NET Example) ====\n\n\n\n{{code language=\"dockerfile\"}}\n\n# Dockerfile\n\nFROM mcr.microsoft.com/dotnet/sdk:8.0 AS build\n\nWORKDIR /src\n\n\n\n# Copy csproj and restore dependencies\n\nCOPY [\"FactHarbor.API/FactHarbor.API.csproj\", \"FactHarbor.API/\"]\n\nRUN dotnet restore \"FactHarbor.API/FactHarbor.API.csproj\"\n\n\n\n# Copy everything else and build\n\nCOPY . .\n\nWORKDIR \"/src/FactHarbor.API\"\n\nRUN dotnet build \"FactHarbor.API.csproj\" -c Release -o /app/build\n\n\n\n# Publish\n\nFROM build AS publish\n\nRUN dotnet publish \"FactHarbor.API.csproj\" -c Release -o /app/publish\n\n\n\n# Runtime image\n\nFROM mcr.microsoft.com/dotnet/aspnet:8.0\n\nWORKDIR /app\n\nCOPY --from=publish /app/publish .\n\n\n\nEXPOSE 8080\n\nENTRYPOINT [\"dotnet\", \"FactHarbor.API.dll\"]\n\n{{/code}}\n\n\n\n==== 2.3 Test Locally ====\n\n\n\n{{code language=\"bash\"}}\n\n# Build image\n\ndocker build -t factharbor-api .\n\n\n\n# Run locally\n\ndocker run -p 8080:8080 \\\n\n  -e DATABASE_URL=\"your-connection-string\" \\\n\n  -e REDIS_URL=\"your-redis-url\" \\\n\n  -e ANTHROPIC_API_KEY=\"your-api-key\" \\\n\n  factharbor-api\n\n\n\n# Test\n\ncurl http://localhost:8080/health\n\n{{/code}}\n\n\n\n----\n\n\n\n=== Step 3: Deploy to Fly.io ===\n\n\n\n==== 3.1 Initialize Fly App ====\n\n\n\n{{code language=\"bash\"}}\n\n# Create fly.toml config\n\nfly launch \\\n\n  --name factharbor-api \\\n\n  --region ord \\\n\n  --no-deploy\n\n\n\n# This creates fly.toml - edit it:\n\n{{/code}}\n\n\n\n==== 3.2 Configure fly.toml ====\n\n\n\n{{code language=\"toml\"}}\n\n# fly.toml\n\napp = \"factharbor-api\"\n\nprimary_region = \"ord\"\n\n\n\n[build]\n\n\n\n[env]\n\n  PORT = \"8080\"\n\n  NODE_ENV = \"production\"\n\n\n\n[http_service]\n\n  internal_port = 8080\n\n  force_https = true\n\n  auto_stop_machines = true  # Auto-suspend when idle (saves $)\n\n  auto_start_machines = true\n\n  min_machines_running = 0   # Can scale to 0 when idle\n\n\n\n  [[http_service.checks]]\n\n    interval = \"10s\"\n\n    timeout = \"2s\"\n\n    grace_period = \"5s\"\n\n    method = \"GET\"\n\n    path = \"/health\"\n\n\n\n[[vm]]\n\n  memory = '256mb'\n\n  cpu_kind = 'shared'\n\n  cpus = 1\n\n\n\n[[statics]]\n\n  guest_path = \"/app/public\"\n\n  url_prefix = \"/static\"\n\n{{/code}}\n\n\n\n==== 3.3 Set Secrets ====\n\n\n\n{{code language=\"bash\"}}\n\n# Set environment variables (encrypted)\n\nfly secrets set \\\n\n  DATABASE_URL=\"postgres://user:pass@factharbor-db.internal:5432/db\" \\\n\n  REDIS_URL=\"your-upstash-redis-url\" \\\n\n  REDIS_TOKEN=\"your-upstash-token\" \\\n\n  ANTHROPIC_API_KEY=\"your-claude-api-key\" \\\n\n  JWT_SECRET=\"$(openssl rand -base64 32)\"\n\n{{/code}}\n\n\n\n==== 3.4 Deploy ====\n\n\n\n{{code language=\"bash\"}}\n\n# Deploy to Fly.io\n\nfly deploy\n\n\n\n# Check status\n\nfly status\n\n\n\n# View logs\n\nfly logs\n\n\n\n# Open in browser\n\nfly open\n\n{{/code}}\n\n\n\n----\n\n\n\n=== Step 4: Set Up Frontend on Cloudflare Pages ===\n\n\n\n==== 4.1 Build Frontend ====\n\n\n\n{{code language=\"bash\"}}\n\n# Example with React\n\ncd frontend\n\n\n\n# Build for production\n\nnpm run build\n\n# Output: dist/ or build/ folder\n\n{{/code}}\n\n\n\n==== 4.2 Deploy to Cloudflare Pages ====\n\n\n\n{{code language=\"bash\"}}\n\n# Install Wrangler CLI\n\nnpm install -g wrangler\n\n\n\n# Login to Cloudflare\n\nwrangler login\n\n\n\n# Deploy\n\nwrangler pages deploy dist \\\n\n  --project-name factharbor \\\n\n  --branch main\n\n\n\n# Configure environment variables in Cloudflare dashboard:\n\n# - VITE_API_URL=https://factharbor-api.fly.dev\n\n{{/code}}\n\n\n\n**Alternative: Use Cloudflare Pages Git Integration**\n\n\n\n# Push frontend to GitHub\n\n# Go to Cloudflare Dashboard  Pages  Create Project\n\n# Connect GitHub repo\n\n# Configure build:\n\n#* Framework preset: React/Vue/Svelte\n\n#* Build command: {{code}}npm run build{{/code}}\n\n#* Build output: {{code}}dist{{/code}}\n\n# Add environment variable: {{code}}VITE_API_URL{{/code}}\n\n# Deploy automatically on every git push\n\n\n\n----\n\n\n\n=== Step 5: Implement Cost Control Measures ===\n\n\n\n==== 5.1 Rate Limiting (Critical!) ====\n\n\n\n{{code language=\"typescript\"}}\n\n// rate-limiter.ts\n\nimport { RateLimiterRedis } from 'rate-limiter-flexible';\n\nimport Redis from 'ioredis';\n\n\n\nconst redis = new Redis(process.env.REDIS_URL);\n\n\n\n// Per-user limits\n\nexport const userRateLimiter = new RateLimiterRedis({\n\n  storeClient: redis,\n\n  keyPrefix: 'rl:user',\n\n  points: 10,           // 10 analyses\n\n  duration: 86400,      // per day\n\n  blockDuration: 86400  // block for 1 day if exceeded\n\n});\n\n\n\n// Global limits (safety net)\n\nexport const globalRateLimiter = new RateLimiterRedis({\n\n  storeClient: redis,\n\n  keyPrefix: 'rl:global',\n\n  points: 100,          // 100 total analyses\n\n  duration: 86400       // per day\n\n});\n\n\n\n// Middleware\n\nexport async function rateLimitMiddleware(req, res, next) {\n\n  try {\n\n    const userId = req.user?.id || req.ip;\n\n\n\n    // Check user limit\n\n    await userRateLimiter.consume(userId);\n\n\n\n    // Check global limit\n\n    await globalRateLimiter.consume('global');\n\n\n\n    next();\n\n  } catch (err) {\n\n    res.status(429).json({\n\n      error: 'Rate limit exceeded',\n\n      message: 'You have reached your daily analysis limit. Please try again tomorrow.'\n\n    });\n\n  }\n\n}\n\n{{/code}}\n\n\n\n==== 5.2 Budget Alerts (Anthropic API) ====\n\n\n\n{{code language=\"typescript\"}}\n\n// budget-monitor.ts\n\nconst DAILY_BUDGET = 0.50;  // $0.50/day = ~$15/month\n\nconst MONTHLY_BUDGET = 10.00;\n\n\n\nlet dailySpend = 0;\n\nlet monthlySpend = 0;\n\n\n\nexport function trackAIUsage(tokensUsed: number, model: string) {\n\n  const cost = calculateCost(tokensUsed, model);\n\n\n\n  dailySpend += cost;\n\n  monthlySpend += cost;\n\n\n\n  if (dailySpend > DAILY_BUDGET) {\n\n    console.error(`  DAILY BUDGET EXCEEDED: $${dailySpend.toFixed(2)}`);\n\n    // Disable AI processing until tomorrow\n\n    throw new Error('Daily budget exceeded');\n\n  }\n\n\n\n  if (monthlySpend > MONTHLY_BUDGET) {\n\n    console.error(` MONTHLY BUDGET EXCEEDED: $${monthlySpend.toFixed(2)}`);\n\n    // Send alert email\n\n    sendAlertEmail('Budget exceeded!');\n\n  }\n\n}\n\n\n\nfunction calculateCost(tokens: number, model: string): number {\n\n  const pricing = {\n\n    'claude-sonnet-4.5': { input: 0.003, output: 0.015 },\n\n    'claude-haiku-4': { input: 0.0008, output: 0.004 }\n\n  };\n\n\n\n  // Simplified: average of input/output\n\n  const avgPrice = (pricing[model].input + pricing[model].output) / 2;\n\n  return (tokens / 1000000) * avgPrice;\n\n}\n\n{{/code}}\n\n\n\n==== 5.3 Tiered Model Routing (40% Cost Savings) ====\n\n\n\n{{code language=\"typescript\"}}\n\n// llm-router.ts\n\nexport class LLMRouter {\n\n  async routeRequest(task: AITask): Promise<string> {\n\n    switch (task.type) {\n\n      case 'EXTRACT_CLAIMS':\n\n        // Simple extraction - use Haiku (cheap)\n\n        return 'claude-haiku-4';\n\n\n\n      case 'EXTRACT_EVIDENCE':\n\n        // Simple extraction - use Haiku (cheap)\n\n        return 'claude-haiku-4';\n\n\n\n      case 'UNDERSTAND_ARTICLE':\n\n        // Complex reasoning - use Sonnet\n\n        return 'claude-sonnet-4.5';\n\n\n\n      case 'GENERATE_VERDICT':\n\n        // Complex synthesis - use Sonnet\n\n        return 'claude-sonnet-4.5';\n\n\n\n      default:\n\n        return 'claude-sonnet-4.5';\n\n    }\n\n  }\n\n}\n\n\n\n// Usage:\n\nconst model = await llmRouter.routeRequest({ type: 'EXTRACT_CLAIMS' });\n\nconst result = await anthropic.messages.create({\n\n  model: model,\n\n  max_tokens: 1024,\n\n  messages: [...]\n\n});\n\n{{/code}}\n\n\n\n==== 5.4 Claim Caching (70% Cost Savings) ====\n\n\n\n{{warning}}This feature is planned but not implemented in current code.{{/warning}}\n\n\n\nSee \"Separated Architecture Implementation Guide\" for full details.\n\n\n\n{{code language=\"typescript\"}}\n\n// Quick example:\n\nasync function analyzeClaimWithCache(claim: string): Promise<Verdict> {\n\n  const cached = await cache.get(claim);\n\n  if (cached) {\n\n    return cached; // Save 100% of cost for this claim\n\n  }\n\n\n\n  const verdict = await analyzeClaimFull(claim);\n\n  await cache.set(claim, verdict, 7); // 7-day TTL\n\n  return verdict;\n\n}\n\n{{/code}}\n\n\n\n----\n\n\n\n=== Step 6: Authentication for Beta Users ===\n\n\n\n==== 6.1 Simple JWT-based Auth ====\n\n\n\n{{code language=\"typescript\"}}\n\n// auth.ts\n\nimport jwt from 'jsonwebtoken';\n\nimport bcrypt from 'bcrypt';\n\n\n\nconst JWT_SECRET = process.env.JWT_SECRET;\n\n\n\n// Beta user allowlist (stored in DB)\n\nconst BETA_USERS = [\n\n  { email: 'user1@example.com', password: '$2b$10$...' },\n\n  { email: 'user2@example.com', password: '$2b$10$...' }\n\n];\n\n\n\nexport async function login(email: string, password: string) {\n\n  const user = await db.query(\n\n    'SELECT * FROM users WHERE email = $1 AND is_beta_user = true',\n\n    [email]\n\n  );\n\n\n\n  if (!user.rows[0]) {\n\n    throw new Error('Not authorized for beta');\n\n  }\n\n\n\n  const valid = await bcrypt.compare(password, user.rows[0].password_hash);\n\n  if (!valid) {\n\n    throw new Error('Invalid credentials');\n\n  }\n\n\n\n  const token = jwt.sign(\n\n    { userId: user.rows[0].id, email: user.rows[0].email },\n\n    JWT_SECRET,\n\n    { expiresIn: '7d' }\n\n  );\n\n\n\n  return { token, user: user.rows[0] };\n\n}\n\n\n\nexport function authenticateToken(req, res, next) {\n\n  const token = req.headers.authorization?.split(' ')[1];\n\n\n\n  if (!token) {\n\n    return res.status(401).json({ error: 'No token provided' });\n\n  }\n\n\n\n  try {\n\n    const decoded = jwt.verify(token, JWT_SECRET);\n\n    req.user = decoded;\n\n    next();\n\n  } catch (err) {\n\n    return res.status(403).json({ error: 'Invalid token' });\n\n  }\n\n}\n\n{{/code}}\n\n\n\n==== 6.2 Beta User Signup (Manual Approval) ====\n\n\n\n{{code language=\"typescript\"}}\n\n// POST /api/beta-signup\n\nexport async function betaSignup(req, res) {\n\n  const { email, name, reason } = req.body;\n\n\n\n  // Store request for manual review\n\n  await db.query(`\n\n    INSERT INTO beta_signup_requests (email, name, reason, status)\n\n    VALUES ($1, $2, $3, 'pending')\n\n  `, [email, name, reason]);\n\n\n\n  res.json({\n\n    message: 'Thank you! Your request has been submitted for review.'\n\n  });\n\n\n\n  // Notify admin\n\n  await sendEmail({\n\n    to: 'admin@factharbor.org',\n\n    subject: 'New beta signup request',\n\n    body: `${name} (${email}) requested beta access: ${reason}`\n\n  });\n\n}\n\n\n\n// Admin approves via /admin/beta-requests\n\nexport async function approveBetaUser(req, res) {\n\n  const { requestId } = req.params;\n\n  const { approved } = req.body;\n\n\n\n  if (approved) {\n\n    // Create user account\n\n    const tempPassword = generateRandomPassword();\n\n    const passwordHash = await bcrypt.hash(tempPassword, 10);\n\n\n\n    await db.query(`\n\n      INSERT INTO users (email, password_hash, is_beta_user)\n\n      SELECT email, $1, true\n\n      FROM beta_signup_requests\n\n      WHERE id = $2\n\n    `, [passwordHash, requestId]);\n\n\n\n    // Send welcome email with temp password\n\n    const request = await db.query(\n\n      'SELECT email FROM beta_signup_requests WHERE id = $1',\n\n      [requestId]\n\n    );\n\n\n\n    await sendEmail({\n\n      to: request.rows[0].email,\n\n      subject: 'Welcome to FactHarbor Beta!',\n\n      body: `Your temporary password: ${tempPassword}\\n\\nPlease log in and change it.`\n\n    });\n\n  }\n\n\n\n  // Update request status\n\n  await db.query(\n\n    'UPDATE beta_signup_requests SET status = $1 WHERE id = $2',\n\n    [approved ? 'approved' : 'rejected', requestId]\n\n  );\n\n\n\n  res.json({ success: true });\n\n}\n\n{{/code}}\n\n\n\n----\n\n\n\n=== Step 7: Monitoring and Alerts ===\n\n\n\n==== 7.1 Health Check Endpoint ====\n\n\n\n{{code language=\"typescript\"}}\n\n// GET /health\n\nexport async function healthCheck(req, res) {\n\n  const checks = {\n\n    api: 'ok',\n\n    database: await checkDatabase(),\n\n    redis: await checkRedis(),\n\n    budgetStatus: await checkBudget()\n\n  };\n\n\n\n  const allHealthy = Object.values(checks).every(v => v === 'ok');\n\n\n\n  res.status(allHealthy ? 200 : 503).json({\n\n    status: allHealthy ? 'healthy' : 'degraded',\n\n    checks,\n\n    timestamp: new Date().toISOString()\n\n  });\n\n}\n\n\n\nasync function checkDatabase(): Promise<string> {\n\n  try {\n\n    await db.query('SELECT 1');\n\n    return 'ok';\n\n  } catch (err) {\n\n    return 'error';\n\n  }\n\n}\n\n\n\nasync function checkBudget(): Promise<string> {\n\n  const today = await db.query(`\n\n    SELECT SUM(cost) as total\n\n    FROM ai_usage_log\n\n    WHERE date = CURRENT_DATE\n\n  `);\n\n\n\n  const spent = today.rows[0]?.total || 0;\n\n  if (spent > DAILY_BUDGET) return 'exceeded';\n\n  if (spent > DAILY_BUDGET * 0.8) return 'warning';\n\n  return 'ok';\n\n}\n\n{{/code}}\n\n\n\n==== 7.2 Daily Budget Report ====\n\n\n\n{{code language=\"typescript\"}}\n\n// Run daily via cron job or scheduled task\n\nexport async function sendDailyReport() {\n\n  const stats = await db.query(`\n\n    SELECT\n\n      COUNT(*) as total_analyses,\n\n      COUNT(DISTINCT user_id) as active_users,\n\n      SUM(cost) as total_cost,\n\n      AVG(processing_time_ms) as avg_processing_time\n\n    FROM analysis_log\n\n    WHERE date = CURRENT_DATE - INTERVAL '1 day'\n\n  `);\n\n\n\n  const cacheStats = await db.query(`\n\n    SELECT\n\n      COUNT(*) as total_claims,\n\n      SUM(access_count - 1) as cache_hits\n\n    FROM ClaimVerdict\n\n    WHERE created_at >= CURRENT_DATE - INTERVAL '1 day'\n\n  `);\n\n\n\n  await sendEmail({\n\n    to: 'admin@factharbor.org',\n\n    subject: `FactHarbor Daily Report - ${new Date().toLocaleDateString()}`,\n\n    body: `\n\n      Total Analyses: ${stats.rows[0].total_analyses}\n\n      Active Users: ${stats.rows[0].active_users}\n\n      AI Cost: $${stats.rows[0].total_cost.toFixed(2)}\n\n      Cache Hits: ${cacheStats.rows[0].cache_hits}\n\n      Avg Processing Time: ${stats.rows[0].avg_processing_time}ms\n\n    `\n\n  });\n\n}\n\n{{/code}}\n\n\n\n==== 7.3 Set Up Fly.io Monitoring ====\n\n\n\n{{code language=\"bash\"}}\n\n# View metrics\n\nfly dashboard\n\n\n\n# Set up alerts (in Fly.io dashboard)\n\n# Alert if:\n\n# - Response time > 2s\n\n# - Error rate > 5%\n\n# - Memory usage > 200MB\n\n{{/code}}\n\n\n\n----\n\n\n\n== Cost Breakdown Analysis ==\n\n\n\n=== Infrastructure Costs ===\n\n\n\n|= Service |= Free Tier |= Usage Estimate |= Monthly Cost\n\n| **Fly.io App** | 3 VMs (256MB each) | 1 VM used | **$0**\n\n| **Fly.io Postgres** | 3GB storage | 1GB used | **$0**\n\n| **Upstash Redis** | 10k cmds/day | ~5k/day | **$0**\n\n| **Cloudflare Pages** | Unlimited | Frontend hosting | **$0**\n\n| **Domain (optional)** | N/A | factharbor.org | ~$12/year\n\n\n\n**Total Infrastructure: $0/month** (or $1/month if you count domain)\n\n\n\n----\n\n\n\n=== AI Costs (Claude API) ===\n\n\n\n**Scenario:** 50 beta users, 10 analyses/user/month = 500 total analyses\n\n\n\n**With All Optimizations:**\\\\\n\n* Claim caching (70% hit rate after 1 week)\n\n* Tiered models (Haiku for extraction, Sonnet for reasoning)\n\n\n\n|= Stage |= Model |= Tokens/Analysis |= Cost/1M tokens |= Cost/Analysis |= Total (500)\n\n| Extract Claims | Haiku | 2,000 | $0.80 | $0.0016 | $0.80\n\n| Extract Evidence | Haiku | 5,000 | $0.80 | $0.0040 | $2.00\n\n| Generate Verdict | Sonnet | 3,000 | $3.00 | $0.0090 | $4.50\n\n\n\n**Before caching:** $7.30/month\\\\\n\n**After 70% cache hit rate:** $7.30  0.30 = **$2.19/month**\n\n\n\n----\n\n\n\n=== Total Monthly Cost: $2-3 ===\n\n\n\n**Best case:** $2.19 (with high cache hit rate)\\\\\n\n**Worst case:** $7.30 (no caching, month 1)\\\\\n\n**Realistic:** $3-5 (moderate caching)\n\n\n\n----\n\n\n\n== Scaling Plan ==\n\n\n\n=== When to Upgrade? ===\n\n\n\n|= Metric |= Free Tier Limit |= Action When Reached\n\n| **Users** | 50-100 | Stay on free tier, add waitlist\n\n| **Analyses/day** | 100 | Increase rate limits slowly\n\n| **Database size** | 3GB | Archive old data, or upgrade to $7/mo tier\n\n| **Memory usage** | 256MB | Optimize code, or add 1 more free VM\n\n| **Monthly AI cost** | $10 | Seek funding/donations before scaling\n\n\n\n=== Upgrade Path ===\n\n\n\n**Phase 1: Free Tier (Current)**\\\\\n\n* 0-50 users\n\n* $0-5/month\n\n* Manual beta approvals\n\n\n\n**Phase 2: Hobby Tier ($10-20/month)**\\\\\n\n* 50-200 users\n\n* Upgrade Fly.io to 512MB VMs ($5/mo)\n\n* Upgrade Upstash to paid tier ($10/mo)\n\n* AI costs: $5-10/month\n\n\n\n**Phase 3: Growth Tier ($50-100/month)**\\\\\n\n* 200-1000 users\n\n* Add CDN, monitoring, backups\n\n* Consider sponsorships/donations\n\n\n\n----\n\n\n\n== Deployment Checklist ==\n\n\n\n=== Pre-Deployment ===\n\n\n\n* [ ] Backend API containerized and tested locally\n\n* [ ] Frontend built and tested\n\n* [ ] Database schema created\n\n* [ ] Environment variables documented\n\n* [ ] Rate limiting implemented\n\n* [ ] Budget monitoring implemented\n\n* [ ] Authentication system tested\n\n\n\n=== Fly.io Deployment ===\n\n\n\n* [ ] Fly.io account created\n\n* [ ] PostgreSQL database created\n\n* [ ] Upstash Redis created\n\n* [ ] Secrets configured ({{code}}fly secrets set{{/code}})\n\n* [ ] {{code}}fly.toml{{/code}} configured\n\n* [ ] Health check endpoint working\n\n* [ ] Deployed ({{code}}fly deploy{{/code}})\n\n* [ ] Logs reviewed ({{code}}fly logs{{/code}})\n\n\n\n=== Cloudflare Pages Deployment ===\n\n\n\n* [ ] Frontend repo pushed to GitHub\n\n* [ ] Cloudflare Pages connected to repo\n\n* [ ] Build settings configured\n\n* [ ] Environment variables set\n\n* [ ] Custom domain configured (optional)\n\n* [ ] HTTPS enabled\n\n\n\n=== Post-Deployment ===\n\n\n\n* [ ] Health check returns 200\n\n* [ ] Frontend loads correctly\n\n* [ ] API requests work\n\n* [ ] Authentication works\n\n* [ ] Rate limiting works\n\n* [ ] Budget alerts configured\n\n* [ ] Daily reports configured\n\n* [ ] Backup strategy defined\n\n\n\n----\n\n\n\n== Troubleshooting ==\n\n\n\n=== Issue: Fly.io App Not Starting ===\n\n\n\n{{code language=\"bash\"}}\n\n# Check logs\n\nfly logs\n\n\n\n# Common issues:\n\n# - Wrong PORT (must be 8080)\n\n# - Missing environment variables\n\n# - Database connection failed\n\n\n\n# Debug locally:\n\nfly ssh console\n\n{{/code}}\n\n\n\n=== Issue: Database Connection Failed ===\n\n\n\n{{code language=\"bash\"}}\n\n# Verify database is running\n\nfly postgres list\n\n\n\n# Check connection string\n\nfly postgres connect -a factharbor-db\n\n\n\n# Test from app\n\nfly ssh console -a factharbor-api\n\n# Inside container:\n\npsql $DATABASE_URL\n\n{{/code}}\n\n\n\n=== Issue: Rate Limits Not Working ===\n\n\n\n{{code language=\"typescript\"}}\n\n// Verify Redis connection\n\nimport Redis from 'ioredis';\n\nconst redis = new Redis(process.env.REDIS_URL);\n\n\n\nredis.ping().then(() => {\n\n  console.log('Redis connected!');\n\n}).catch(err => {\n\n  console.error('Redis error:', err);\n\n});\n\n{{/code}}\n\n\n\n=== Issue: High AI Costs ===\n\n\n\n# Check cache hit rate:\n\n{{code language=\"sql\"}}\n\n   SELECT\n\n     COUNT(*) as total_claims,\n\n     AVG(access_count) as avg_reuses\n\n   FROM ClaimVerdict;\n\n{{/code}}\n\n\n\n# Verify tiered model routing:\n\n{{code language=\"typescript\"}}\n\n   // Log every LLM call\n\n   console.log(`AI Request: ${task.type}  ${model}  ${tokens} tokens  $${cost}`);\n\n{{/code}}\n\n\n\n# Implement hard budget limit:\n\n{{code language=\"typescript\"}}\n\n   if (dailySpend > DAILY_BUDGET) {\n\n     throw new Error('Budget exceeded - AI disabled for today');\n\n   }\n\n{{/code}}\n\n\n\n----\n\n\n\n== Alternative: Even Cheaper Option (Vercel + Supabase) ==\n\n\n\nIf Fly.io seems too complex:\n\n\n\n{{code}}\n\nFrontend: Vercel (free, better DX than Cloudflare)\n\nBackend: Vercel Serverless Functions (free tier: 100GB-hrs)\n\nDatabase: Supabase (free tier: 500MB, 2 projects)\n\nRedis: Upstash (same as above)\n\n\n\nPros: Even simpler deployment (git push)\n\nCons: Less control, harder to migrate later\n\n{{/code}}\n\n\n\n----\n\n\n\n== Security Considerations ==\n\n\n\n=== 1. Protect API Keys ===\n\n\n\n{{code language=\"bash\"}}\n\n# NEVER commit secrets to git\n\necho \".env\" >> .gitignore\n\necho \"fly.toml\" >> .gitignore  # Contains secrets\n\n\n\n# Use fly secrets instead\n\nfly secrets set ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n{{/code}}\n\n\n\n=== 2. Enable CORS Properly ===\n\n\n\n{{code language=\"typescript\"}}\n\nimport cors from 'cors';\n\n\n\napp.use(cors({\n\n  origin: [\n\n    'https://factharbor.pages.dev',\n\n    'https://factharbor.org'\n\n  ],\n\n  credentials: true\n\n}));\n\n{{/code}}\n\n\n\n=== 3. Rate Limit All Endpoints ===\n\n\n\n{{code language=\"typescript\"}}\n\n// Not just /analyze, but also /login, /signup\n\napp.use('/api/login', rateLimitMiddleware);\n\napp.use('/api/analyze', rateLimitMiddleware);\n\napp.use('/api/beta-signup', rateLimitMiddleware);\n\n{{/code}}\n\n\n\n=== 4. Validate All Inputs ===\n\n\n\n{{code language=\"typescript\"}}\n\nimport { z } from 'zod';\n\n\n\nconst AnalyzeRequestSchema = z.object({\n\n  url: z.string().url(),\n\n  userId: z.string().uuid().optional()\n\n});\n\n\n\napp.post('/api/analyze', async (req, res) => {\n\n  const result = AnalyzeRequestSchema.safeParse(req.body);\n\n  if (!result.success) {\n\n    return res.status(400).json({ error: result.error });\n\n  }\n\n\n\n  // Process validated data\n\n  const { url } = result.data;\n\n  // ...\n\n});\n\n{{/code}}\n\n\n\n----\n\n\n\n== Success Metrics ==\n\n\n\nTrack these weekly:\n\n\n\n* [ ] **Uptime:** &gt;99% (check Fly.io status)\n\n* [ ] **Response time:** &lt;2s average (check logs)\n\n* [ ] **Daily cost:** &lt;$0.50 (check budget monitor)\n\n* [ ] **Cache hit rate:** &gt;40% after week 2\n\n* [ ] **Active users:** Growing steadily\n\n* [ ] **Error rate:** &lt;1%\n\n\n\n----\n\n\n\n== Next Steps After Beta ==\n\n\n\nWhen ready to scale:\n\n\n\n# **Seek funding/donations** before increasing usage limits\n\n# **Add payment system** (Stripe) if going subscription model\n\n# **Upgrade infrastructure** gradually based on metrics\n\n# **Implement CDN** for faster global access\n\n# **Add monitoring** (Sentry, DataDog, etc.)\n\n# **Hire DevOps** if growing beyond 1000 users\n\n\n\n----\n\n\n\n== Resources ==\n\n\n\n* **Fly.io Docs:** https://fly.io/docs\n\n* **Upstash Docs:** https://docs.upstash.com\n\n* **Cloudflare Pages:** https://pages.cloudflare.com\n\n* **Anthropic Pricing:** https://www.anthropic.com/pricing\n\n* **Rate Limiter Library:** https://github.com/animir/node-rate-limiter-flexible\n\n\n\n----\n\n\n\n== Support Contacts ==\n\n\n\n* **Fly.io Community:** https://community.fly.io\n\n* **Fly.io Support:** support@fly.io (for paying customers)\n\n* **This Guide Author:** Claude Code (2026-01-02)\n\n\n\n----\n\n\n\n== Change Log ==\n\n\n\n|= Version |= Date |= Author |= Changes\n\n| 1.0 | 2026-01-02 | Claude Code | Initial hosting guide for zero-cost beta\n\n", "Product Development.DevOps.Guidelines.Coding Guidelines.WebHome": "= Coding Guidelines for FactHarbor =\n\nThis document provides guidelines for developers working on FactHarbor. These principles should be followed to maintain code quality, consistency, and adherence to the project's goals.\n\n----\n\n== Core Principles ==\n\n=== 1. Generic by Design ===\n\n**Rule**: Code, prompts, and logic must work for ANY topic, not just specific domains.\n\n**Do NOT**:\n* Hardcode domain-specific keywords like ##['bolsonaro', 'trump', 'vaccine']##\n* Use topic-specific prompts or conditions\n* Create special cases for particular events or people\n\n**Example violations to avoid**:\n{{code language=\"typescript\"}}\n// BAD - Domain-specific keywords\nconst keywords = ['trial', 'judgment', 'PERSON_A', 'PERSON_B', 'PERSON_C'];\n\n// BAD - Specific outcome detection\nconst isHighImpactOutcome =\n    hay.includes(\"sentenced\") ||\n    hay.includes(\"convicted\") ||\n    hay.includes(\"27-year sentence\");\n\n// GOOD - Generic patterns\nconst hasLegalTerminology = detectDomainPatterns(text, 'legal');\nconst hasSpecificOutcome = extractNumericClaims(text);\n{{/code}}\n\n**Instead**:\n* Use parameterization and configuration\n* Detect patterns generically\n* Let the LLM identify domain-specific elements dynamically\n\n=== 2. Input Neutrality ===\n\n**Rule**: Input phrasing must NOT affect analysis depth, research, or verdict. A yes/no phrased input (\"Was X...?\") must behave the same as the equivalent statement (\"X was ...\").\n\n**Hard requirements**:\n* **No separate analysis paths**: There must be **zero branching** in analysis logic based on whether the user wrote a question or a statement.\n* **LLM-first equivalence**: The LLM handles question/statement equivalence naturally without deterministic text transformation. Raw user input (question or statement) is passed directly to ##understandClaim()##.\n* **Single analysis string**: All analysis steps (context detection, research, evidence extraction, verdict generation, weighting, quality gates, inversion correction) use the same input text.\n* **No question-specific metadata**: Do not emit or persist flags/fields such as ##wasQuestionInput##, ##questionBeingAsked##, ##questionIntent##, ##QuestionAnswer##, ##calculateQuestionTruthPercentage##, etc.\n* **UI neutrality**: UI must not show question badges/labels or question-specific layouts. The same components and result fields must be used regardless of phrasing.\n* **Equivalence guarantee**: For meaning-equivalent inputs, verdict and reasoning must be effectively identical; any drift should be **4%** (tolerance increased from <1% due to LLM variability).\n\n**Implementation guidance**:\n* Pass raw user input to the LLM with minimal preprocessing (whitespace normalization only).\n* Trust the LLM to understand semantic equivalence between \"Was X fair?\" and \"X was fair\".\n* Store the original input in a dedicated display-only field (e.g., ##originalInputDisplay##) for UI rendering.\n\n=== 3. Pipeline Integrity ===\n\n**Rule**: All pipeline stages must execute, no skipping allowed.\n\n**Required Stages**:\n1. **Understand**  Extract claims, detect context, assign risk tiers\n1. **Research**  Web search, fetch sources, extract evidence\n1. **Verdict**  Generate verdicts, apply quality gates\n1. **Summary**  Format results\n1. **Report**  Generate markdown\n\n**Do NOT**:\n* Skip research phase for \"easy\" claims\n* Skip quality gates for \"obvious\" facts\n* Short-circuit verdict generation\n\n=== 4. Evidence Transparency ===\n\n**Rule**: Every verdict must cite supporting or opposing facts.\n\n**Requirements**:\n* Track ##supportingEvidenceIds## for each verdict\n* Include counter-evidence (criticism category)\n* Show evidence sources with excerpts\n* Display quality gate decisions with reasons\n\n**Implementation**:\n{{code language=\"typescript\"}}\n// GOOD - Track evidence explicitly\nverdict.supportingEvidenceIds = evidenceItems\n  .filter(f => supportsVerdict(f, claim))\n  .map(f => f.id);\n\n// Count counter-evidence\nconst counterEvidence = facts.filter(f =>\n  f.category === \"criticism\" &&\n  relatesToClaim(f, claim)\n).length;\n{{/code}}\n\n=== 5. Context Detection ===\n\n**Rule**: Detect and analyze distinct contexts independently.\n\n**Context Definition**:\n* A bounded analytical frame with:\n** Defined boundaries (what's included/excluded)\n** Methodology (how analysis is done)\n** Temporal period (when)\n** Subject matter (what)\n\n**Examples of Distinct Contexts**:\n* Different legal proceedings (Trial A vs. Trial B)\n* Different methodological approaches (Study 1 vs. Study 2)\n* Different temporal periods (2020 events vs. 2024 events)\n* Different regulatory frameworks (US law vs. EU law)\n\n**NOT Distinct Contexts**:\n* Different perspectives on same event (\"US view\" vs. \"Brazil view\")\n* Different claims within same event\n* Different sources reporting same facts\n\n----\n\n== Code Quality Standards ==\n\n=== Testing Requirements ===\n\n**After each significant change**:\n1. Run automated tests without manual intervention\n1. Re-test recent analysis inputs (regression tests)\n1. Use Swagger to investigate API behavior\n1. Review ##apps/web/debug-analyzer.log## for issues\n1. Analyze reports for correctness\n1. Check that earlier issues are not re-introduced\n1. Search for other issues, flaws, inconsistencies\n1. Report findings to ##.md## file\n1. Fix issues automatically where possible\n\n**Test Cases to Always Verify**:\n* Bolsonaro trial inputs (question and statement forms)\n* Hydrogen vs Electric Cars article\n* PDF article analysis\n* Venezuela article\n* Any input with recent events (date awareness)\n\n=== Logging and Debugging ===\n\n**Local Development**:\n* Debug code and detailed logging are acceptable\n* Use environment checks for debug-only code:\n{{code language=\"typescript\"}}\nif (process.env.NODE_ENV === 'development') {\n  // Debug code here\n}\n{{/code}}\n\n**Production**:\n* Remove or disable debug endpoints\n* Remove debug logging calls\n* No hardcoded debug URLs (e.g., ##fetch('http://127.0.0.1:7242')##)\n\n=== Performance ===\n\n**Responsiveness Requirements**:\n* Job queuing should not block UI\n* Analyze button should not be disabled for long periods\n* Even with 3-4 running jobs, new submissions should queue quickly\n* Progress updates should appear in real-time\n\n**Optimization**:\n* Use parallel fetching where possible\n* Implement timeout limits (5s per source fetch)\n* Use exponential backoff for retries\n* Cache evaluated data (source reliability scores via SR Service)\n\n----\n\n== UI/UX Consistency ==\n\n=== Labeling Standards ===\n\n**Rule**: Titles and labels must match underlying content.\n\n**Requirements**:\n* \"Article Summary\"  Must contain actual article summary\n* \"Verdict\"  Must show verdict name (according to Scale Mapping) and verdict value\n* \"Confidence\"  Must be confidence in the verdict (0-100%)\n* Labels should be human-readable (not code identifiers)\n\n**Format Consistency**:\n{{code}}\n// GOOD - Unified format\n Mostly True 82% (80% confidence)\n\n// BAD - Inconsistent format\nVerdict: Mostly True\nConfidence: 80\nTruth: 82%\n{{/code}}\n\n=== Layout Standards ===\n\n**Article Box Contents** (all in one box):\n* Article Summary (the actual summary text)\n* Verdict (7-point scale + truth % + confidence %)\n* Key Factors (if applicable)\n* Assessment (overall analysis)\n\n**Never Show These**:\n* \"Implied Claim\" (redundant with summary)\n* \"Question asked\" (redundant with Input)\n* Methodology validation claims (low value)\n\n**Unified Naming**:\n* Use \"Verdict\" everywhere (not \"Overall Verdict\" or \"Article Verdict\")\n* Use \"Boundaries\" (ClaimAssessmentBoundaries) instead of \"Boundary-by-Boundary Analysis\" or \"Scope-by-Scope Analysis\"\n* Use \"Confidence\" (not \"Conf\" or \"Certainty\")\n\n=== Multi-Context Display ===\n\n**Rule**: Use same layout for single-context and multi-context reports.\n\n**Implementation**:\n* Multi-context: Show contexts in rows (not columns)\n* Single-context: Use same structure, omit context headers\n* Support 2+ contexts (not just 2)\n\n----\n\n== Data Model Standards ==\n\n=== Verdict Scale ===\n\n**Use**: 7-point scale with numeric truth percentage (0-100%)\n\n**Scale Mapping**:\n* **TRUE**: 86-100% (Score +3)\n* **MOSTLY-TRUE**: 72-85% (Score +2)\n* **LEANING-TRUE**: 58-71% (Score +1)\n* **MIXED**: 43-57% with confidence >=60% (Score 0)\n* **UNVERIFIED**: 43-57% with confidence <60% (Score 0)\n* **LEANING-FALSE**: 29-42% (Score -1)\n* **MOSTLY-FALSE**: 15-28% (Score -2)\n* **FALSE**: 0-14% (Score -3)\n\n**Do NOT use**: Old 4-scale (WELL-SUPPORTED, PARTIALLY-SUPPORTED, UNCERTAIN, REFUTED)\n\n=== Centrality Detection ===\n\n**Central Claims**:\n* Core factual assertion that article depends on\n* Removing it would invalidate the article's thesis\n\n**NOT Central Claims**:\n* Attribution claims (\"Source S said X\")\n* Background context claims\n\n**Irrelevant Claims** (do not use at all):\n* Claims that validate Validation Methodology (e.g. \"methodology provides an accurate framework for\")\n\n=== Claim Deduplication ===\n\n**Rule**: Near-duplicate claims should not fully influence verdict multiple times.\n\n**Detection**:\n* Detect extremely similar claims (>80% semantic overlap)\n* Flag as duplicates\n* Weight duplicates proportionally in aggregation\n\n**Example Duplicates**:\n* \"Venezuelan takeover constituted theft of American property\"\n* \"Venezuelan oil seizure was one of largest thefts of American property\"\n\n----\n\n== Temporal Awareness ==\n\n=== Current Date Handling ===\n\n**Rule**: LLM must be aware of current date.\n\n**Requirements**:\n* Inject current date into all prompts\n* Detect and sanitize temporal errors\n* Flag claims about \"future\" events that are actually past\n* Use web search for very recent information (<6 months)\n\n**Error Sanitization**:\n{{code language=\"typescript\"}}\n// Remove false \"temporal error\" comments\nfunction sanitizeTemporalErrors(text: string): string {\n  return text\n    .replace(/temporal error/gi, '')\n    .replace(/in the future from the current date/gi, '')\n    .replace(/date discrepancy/gi, '');\n}\n{{/code}}\n\n----\n\n== Prompt Engineering ==\n\n=== Generic Prompts ===\n\n**Rule**: Prompts must work for any topic, not specific domains.\n\n**Do**:\n* Use examples from diverse domains\n* Describe patterns, not specific instances\n* Let LLM identify domain dynamically\n* Use abstract placeholders in examples (e.g., \"Entity A did X\", \"Event E occurred\")\n\n**Don't hardcode in prompts or source code**:\n* specific person names (Bolsonaro, Trump, etc.)\n* specific event types (trials, elections, etc.)\n* specific outcomes (\"27-year sentence\", etc.)\n\n**CRITICAL: No Test-Case Terms in Prompt Examples**\n\nPrompt examples must NEVER contain terms, phrases, or patterns from known test cases or verification inputs. This prevents \"teaching to the test\" and ensures genuine generalization.\n\n**Forbidden**:\n* Using words/phrases from regression test inputs in prompt examples\n* Creating examples that mirror specific verification scenarios\n* Adding domain-specific terms that match test cases (e.g., \"built the industry\", \"stole through force\", \"trial was fair\")\n\n**Required**:\n* Use abstract variables: \"Entity A\", \"Event E\", \"Action X\", \"Characterization Y\"\n* Describe structural patterns, not content patterns\n* If concrete examples are needed, use completely unrelated domains from the test suite\n\n=== Prompt Structure ===\n\n**Best Practices**:\n* Use clear section headers\n* Use bullet points for lists\n* Include 2-3 diverse examples\n* State expected output format explicitly\n* Include edge cases and non-examples\n\n**Format**:\n{{code}}\n# Task\n\n[Clear description]\n\n# Requirements\n\n- Requirement 1\n- Requirement 2\n- Requirement 3\n\n# Examples\n\n**Example 1** (Domain A):\n[Input]  [Output]\n\n**Example 2** (Domain B):\n[Input]  [Output]\n\n# Edge Cases\n\n- [Edge case 1]: [How to handle]\n- [Edge case 2]: [How to handle]\n\n# Output Format\n\n[JSON schema or structure description]\n{{/code}}\n\n----\n\n== Configuration Management ==\n\n**Use UCM for runtime configuration:**\n* Add new config fields to appropriate UCM schema (pipeline, search, calc, SR, lexicons)\n* Update schema version when adding/removing fields\n* Add validation rules in config-schemas.ts\n* Do not add hardcoded config constants\n* Do not use environment variables for business logic config\n\n**Environment variables are only for:**\n* API keys and secrets (ANTHROPIC_API_KEY, etc.)\n* Deployment settings (PORT, NODE_ENV, etc.)\n* Infrastructure config (DATABASE_URL, etc.)\n\n**For new config fields:**\n1. Add to appropriate schema in ##apps/web/src/lib/config-schemas.ts##\n1. Update default file in ##apps/web/configs/*.default.json##\n1. Increment schema version if breaking change\n1. Document in user guides\n\n----\n\n== Configuration and Environment ==\n\n=== Environment Variables ===\n\n**Required**:\n* LLM provider API keys (ANTHROPIC_API_KEY, etc.)\n* Search provider keys (SERPAPI_API_KEY, etc.)\n* Internal keys (FH_ADMIN_KEY, FH_INTERNAL_RUNNER_KEY)\n\n**Optional**:\n* Feature flags (UCM: ##pipeline.deterministic##, ##search.enabled##)\n* Limits (FH_RUNNER_MAX_CONCURRENCY)\n* Domain whitelist (UCM: ##search.domainWhitelist##)\n\n**Security**:\n* Never commit secrets to git\n* Use ##.env.local## for local development\n* Use environment-specific configs for deployment\n* Validate all environment variables at startup\n\n----\n\n== Documentation Standards ==\n\n=== Code Comments ===\n\n**When to Comment**:\n* Complex algorithms or calculations\n* Non-obvious business logic\n* Workarounds for known issues\n* Security-critical sections\n* Performance-critical sections\n\n**When NOT to Comment**:\n* Obvious code (##// Increment counter##)\n* Redundant with function names\n* Outdated information (remove instead)\n\n=== Documentation Files ===\n\n**Keep Updated**:\n* Architecture diagrams when structure changes\n* Calculation formulas when logic changes\n* Environment variable lists when adding new vars\n* API endpoint lists when adding routes\n\n**Archive**:\n* Version-specific fix documents\n* Historical analysis reports\n* Deprecated design decisions\n\n----\n\n== References ==\n\n* **AGENTS.md**: Core rules for AI coding assistants (project root)\n* **Docs/ARCHITECTURE/Calculations.md**: Verdict calculation methodology\n* **Docs/STATUS/Current_Status.md**: Current implementation status\n* **[[Getting Started>>FactHarbor.Product Development.DevOps.Guidelines.Getting Started.WebHome]]**: Setup and first run\n\n----\n\n**Last Updated**: February 2, 2026", "Product Development.DevOps.Guidelines.Getting Started.WebHome": "= Getting Started with FactHarbor =\n\n== Purpose ==\n\nThis guide will help you set up and run FactHarbor on your local machine from scratch. It combines installation prerequisites with first-run configuration to ensure a smooth setup experience.\n\nFor a quick summary, see [[CONTRIBUTING.md>>https://github.com/robertschaub/FactHarbor/blob/main/CONTRIBUTING.md]] in the GitHub repository.\n\n----\n\n== Table of Contents ==\n\n* [[Prerequisites>>||anchor=\"HPrerequisites\"]]\n* [[Installation>>||anchor=\"HInstallation\"]]\n* [[Repository Setup>>||anchor=\"HRepositorySetup\"]]\n* [[Environment Configuration>>||anchor=\"HEnvironmentConfiguration\"]]\n* [[Database Initialization>>||anchor=\"HDatabaseInitialization\"]]\n* [[Starting Services>>||anchor=\"HStartingServices\"]]\n* [[Health Verification>>||anchor=\"HHealthVerification\"]]\n* [[First Analysis>>||anchor=\"HFirstAnalysis\"]]\n* [[Troubleshooting>>||anchor=\"HTroubleshooting\"]]\n* [[Next Steps>>||anchor=\"HNextSteps\"]]\n\n----\n\n== Prerequisites ==\n\n=== System Requirements ===\n\n* **Operating System**: Windows 10 or 11\n* **Permissions**: Administrator rights for installation\n\n=== Required Tools ===\n\n==== 1. Version Control ====\n\n**Git for Windows**\n* Download from: [[https:~~/~~/git-scm.com/download/win>>https://git-scm.com/download/win]]\n* Verify installation:\n\n{{code language=\"bash\"}}\ngit --version\n{{/code}}\n\n==== 2. Runtime & Build Tools ====\n\n**Node.js (LTS)**\n* Download from: [[https:~~/~~/nodejs.org>>https://nodejs.org]]\n* Install the LTS (Long Term Support) version\n* Verify installation:\n\n{{code language=\"bash\"}}\nnode -v\nnpm -v\n{{/code}}\n\n**.NET SDK 8.x**\n* Download from: [[https:~~/~~/dotnet.microsoft.com/download>>https://dotnet.microsoft.com/download]]\n* Install the SDK (not just runtime)\n* Verify installation:\n\n{{code language=\"bash\"}}\ndotnet --info\n{{/code}}\n\n==== 3. Development Editors ====\n\n**Visual Studio 2022**\n* Download from: [[https:~~/~~/visualstudio.microsoft.com>>https://visualstudio.microsoft.com]]\n* Required workload: **ASP.NET and web development**\n* Used for: Backend API development\n\n**Cursor or VS Code**\n* Cursor: [[https:~~/~~/cursor.sh>>https://cursor.sh]]\n* VS Code: [[https:~~/~~/code.visualstudio.com>>https://code.visualstudio.com]]\n* Required extensions:\n** TypeScript\n** ESLint\n** EditorConfig\n* Used for: Frontend web development\n\n=== Recommended Tools ===\n\n**Browser with DevTools**\n* Chrome or Edge (for debugging)\n\n**API Testing Client**\n* Postman ([[https:~~/~~/www.postman.com>>https://www.postman.com]]) or\n* Insomnia ([[https:~~/~~/insomnia.rest>>https://insomnia.rest]])\n\n**Database Inspection (Optional)**\n* SQLite browser for read-only database inspection\n\n=== Tools to Defer ===\n\n**Do NOT install these yet** (not needed for local development):\n* Docker Desktop\n* PostgreSQL\n* Azure or Vercel CLIs\n\n----\n\n== Installation ==\n\n=== Verification ===\n\nAfter installing all prerequisites, verify everything is working:\n\n{{code language=\"bash\"}}\ngit --version\nnode -v\nnpm -v\ndotnet --info\n{{/code}}\n\nAll commands must succeed before continuing.\n\n=== Installation Hints ===\n\n* Avoid preview or nightly runtime versions\n* Use stable LTS releases for Node.js and .NET\n* Use one editor per role:\n** Visual Studio 2022 for API (.NET)\n** Cursor/VS Code for Web (TypeScript)\n\n----\n\n== Repository Setup ==\n\n=== 1. Clone the Repository ===\n\n{{code language=\"bash\"}}\ngit clone https://github.com/robertschaub/FactHarbor.git\ncd factharbor\n{{/code}}\n\n=== 2. Open in Editors ===\n\n* **API**: Open ##apps/api## folder in Visual Studio 2022\n* **Web**: Open root folder in Cursor or VS Code\n\n**Important**: Do not change configuration files yet!\n\n----\n\n== Environment Configuration ==\n\n=== Web Configuration (##apps/web##) ===\n\n1. Navigate to ##apps/web##\n1. Copy the example environment file:\n\n{{code language=\"bash\"}}\ncopy .env.example .env.local\n{{/code}}\n\n1. Edit ##.env.local## and configure your API keys and internal secrets:\n\n=== Required API Keys ===\n\nFactHarbor requires API keys from external AI and search providers. You must register with these services before running analyses.\n\n|= Provider |= Key |= Required? |= Sign Up\n| **Anthropic** (Claude) | ##ANTHROPIC_API_KEY## | **Yes** | [[console.anthropic.com>>https://console.anthropic.com/]]\n| **OpenAI** | ##OPENAI_API_KEY## | **Yes** | [[platform.openai.com>>https://platform.openai.com/]]\n| **Google Custom Search** | ##GOOGLE_CSE_API_KEY## + ##GOOGLE_CSE_ID## | **Yes** | [[programmablesearchengine.google.com>>https://programmablesearchengine.google.com/]]\n| Google Gemini | ##GOOGLE_GENERATIVE_AI_API_KEY## | Optional | [[aistudio.google.com>>https://aistudio.google.com/]]\n| Mistral | ##MISTRAL_API_KEY## | Optional | [[console.mistral.ai>>https://console.mistral.ai/]]\n| SerpAPI | ##SERPAPI_API_KEY## | Optional (fallback search) | [[serpapi.com>>https://serpapi.com/]]\n\n{{info}}\n**Cost note:** These are commercial services with pay-per-use pricing. Both Anthropic and OpenAI offer free trial credits for new accounts. Google CSE provides 100 free queries/day. Typical development usage costs single-digit $/month. See each provider's website for current pricing and free tier details.\n\n**Licensing:** By using these API keys you agree to each provider's terms of service. FactHarbor itself is open source, but the external AI and search services it depends on are commercial products with their own licensing terms.\n{{/info}}\n\n=== Example .env.local ===\n\n{{code language=\"bash\"}}\n# LLM providers (both required)\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\n\n# Web search (Google CSE recommended)\nGOOGLE_CSE_API_KEY=...\nGOOGLE_CSE_ID=...\n\n# Internal keys (must match API settings)\nFH_API_BASE_URL=http://localhost:5000\nFH_ADMIN_KEY=your-secure-admin-key\nFH_INTERNAL_RUNNER_KEY=your-secure-runner-key\n{{/code}}\n\n=== API Configuration (##apps/api##) ===\n\n1. Navigate to ##apps/api##\n1. Copy the example settings file:\n\n{{code language=\"bash\"}}\ncopy appsettings.Development.example.json appsettings.Development.json\n{{/code}}\n\n1. Edit ##appsettings.Development.json## and verify:\n\n{{code language=\"json\"}}\n{\n  \"Admin\": {\n    \"Key\": \"your-secure-admin-key\"  // Must match FH_ADMIN_KEY\n  },\n  \"Runner\": {\n    \"RunnerKey\": \"your-secure-runner-key\"  // Must match FH_INTERNAL_RUNNER_KEY\n  },\n  \"Db\": {\n    \"Provider\": \"sqlite\"\n  }\n}\n{{/code}}\n\n=== Critical Configuration ===\n\n**Key mismatches are the #1 cause of setup failures!**\n\nEnsure these match exactly:\n* ##FH_ADMIN_KEY## (web) = ##Admin:Key## (API)\n* ##FH_INTERNAL_RUNNER_KEY## (web) = ##Runner:RunnerKey## (API)\n\n=== Configuration Management (UCM) ===\n\nFactHarbor uses **Unified Config Management (UCM)** for runtime configuration:\n\n* **API keys**  Environment variables (##.env.local##)\n* **Provider selection & analysis settings**  UCM Admin UI\n\nAfter first run, configure your LLM provider:\n1. Open http://localhost:3000/admin/config\n1. Go to **Pipeline** config\n1. Set ##llmProvider## to your preferred provider (##anthropic##, ##openai##, ##google##, ##mistral##)\n1. Save and activate\n\n**Note:** The deprecated ##LLM_PROVIDER## environment variable is no longer used. Provider selection is now managed via UCM.\n\nSee [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]] for detailed configuration options.\n\n----\n\n== Database Initialization ==\n\n**POC behavior:** The API creates the SQLite database automatically on startup (no ##dotnet ef## step required).\n\n**Expected result on first run:**\n* ##apps/api/factharbor.db## file is created automatically\n* API starts without errors\n\nIf you already have a corrupted DB and want to reset it (local dev only):\n\n{{code language=\"powershell\"}}\ncd apps/api\ndel factharbor.db\n{{/code}}\n\n----\n\n== Starting Services ==\n\n=== Option 1: Automated Setup (Recommended) ===\n\nUse the first-run script:\n\n{{code language=\"powershell\"}}\npowershell -ExecutionPolicy Bypass -File scripts/first-run.ps1\n{{/code}}\n\nThis script will:\n1. Install npm dependencies\n1. Start the API server (port 5000) (creates the SQLite DB automatically if missing)\n1. Start the Web UI (port 3000)\n\n=== Option 2: Manual Setup ===\n\n**Terminal 1 - API**:\n\n{{code language=\"bash\"}}\ncd apps/api\ndotnet watch run\n{{/code}}\n\n**Terminal 2 - Web**:\n\n{{code language=\"bash\"}}\ncd apps/web\nnpm install\nnpm run dev\n{{/code}}\n\n----\n\n== Health Verification ==\n\nBefore running your first analysis, verify both services are healthy:\n\n=== API Health Check ===\n\nOpen in browser: http://localhost:5000/health\n\n**Expected response:**\n\n{{code language=\"json\"}}\n{\n  \"ok\": true\n}\n{{/code}}\n\n=== Web Health Check ===\n\nOpen in browser: http://localhost:3000/api/health\n\n**Expected response:**\n\n{{code language=\"json\"}}\n{\n  \"ok\": true\n}\n{{/code}}\n\n=== Swagger UI (Optional) ===\n\nYou can also test the API directly at: http://localhost:5000/swagger\n\n**Do not proceed until both health checks return 200 OK**\n\n----\n\n== First Analysis ==\n\n=== Run Your First Fact-Check ===\n\n1. **Open the Web UI**: http://localhost:3000\n1. **Submit a test analysis**:\n** Start with simple text (avoid URLs for your first test)\n** Example: \"Organization A announced Policy B on Date C\"\n** Click **\"Run Analysis\"**\n1. **Observe progress**:\n** Job will show \"QUEUED\"  \"RUNNING\"  \"SUCCEEDED\"\n** Progress updates appear in real-time\n1. **View results**:\n** Report tab: Human-readable markdown\n** JSON tab: Structured analysis data\n** If you see a \"Classification Fallbacks\" warning, see Evidence Quality Filtering - Classification Fallbacks\n\n=== Verification ===\n\nYour setup is successful when:\n* Analysis completes without errors\n* Results are displayed in both tabs\n* Refreshing the page shows saved results\n* ##/jobs## page lists your analysis\n\n**Congratulations! FactHarbor is now operational.**\n\n----\n\n== Troubleshooting ==\n\n=== Common Failure Patterns ===\n\n|= Symptom |= Likely Cause |= Solution\n| **Job stuck in QUEUED** | Runner key mismatch | Check ##FH_INTERNAL_RUNNER_KEY## matches ##Runner:RunnerKey##\n| **Job fails immediately** | Missing LLM API key | Add ##ANTHROPIC_API_KEY## and ##OPENAI_API_KEY## to ##.env.local## (both required)\n| **No progress updates** | Admin key mismatch | Check ##FH_ADMIN_KEY## matches ##Admin:Key##\n| **API not starting** | Database error | The DB is auto-created on startup; check API logs, then (local dev) delete ##apps/api/factharbor.db## and restart\n| **Port already in use** | Previous instance running | Stop services with ##scripts/stop-services.ps1##\n\n=== Health Check Fails ===\n\n**API Health Check (port 5000) fails:**\n\n{{code language=\"bash\"}}\n# Check if API is running\ncd apps/api\ndotnet watch run\n# Look for errors in console output\n{{/code}}\n\n**Web Health Check (port 3000) fails:**\n\n{{code language=\"bash\"}}\n# Check if Web is running\ncd apps/web\nnpm run dev\n# Look for errors in console output\n{{/code}}\n\n=== API Key Issues ===\n\n**\"Unauthorized\" or \"Invalid API key\" errors:**\n1. Verify your key is valid (test at provider's website)\n1. Check for spaces or quotes in the key\n1. Ensure the key is for the correct provider:\n** OpenAI keys start with ##sk-##\n** Anthropic keys start with ##sk-ant-##\n\n=== Database Issues ===\n\n**\"Cannot open database\" error:**\n\n{{code language=\"bash\"}}\ncd apps/api\n# Delete old database if corrupted\ndel factharbor.db\n# Restart API to recreate the DB automatically\n{{/code}}\n\n=== Port Conflicts ===\n\n**\"Port already in use\" error:**\n\n{{code language=\"bash\"}}\n# Stop all services\n.\\scripts\\stop-services.ps1\n\n# Or manually find and kill processes\nnetstat -ano | findstr \":5000\"\nnetstat -ano | findstr \":3000\"\n# Note the PID and use Task Manager to end the process\n{{/code}}\n\n----\n\n== Next Steps ==\n\nNow that FactHarbor is running:\n\n1. **Configure via UCM Admin**: http://localhost:3000/admin/config\n** Set your LLM provider (Pipeline config)\n** Configure search settings (Search config)\n** Adjust analysis parameters (Calculation config)\n1. **Test Your Configuration**: http://localhost:3000/admin/test-config\n** Verify all API keys are valid\n** Test LLM and search provider connectivity\n1. **Read the Documentation**:\n** [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]]  Runtime configuration\n** [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]] - Configure AI providers\n** ##Docs/ARCHITECTURE/Calculations.md## - How verdicts are calculated\n1. **Customize Your Setup**:\n** Configure search providers via UCM (Admin  Config  Search)\n** Enable source reliability scoring (Admin  Config  Source Reliability)\n** Adjust verdict thresholds (Admin  Config  Calculation)\n\n----\n\n== Additional Resources ==\n\n* **Project Repository**: [[https:~~/~~/github.com/robertschaub/FactHarbor>>https://github.com/robertschaub/FactHarbor]]\n* **Documentation**: See ##Docs/## folder\n* **Scripts Reference**: See ##scripts/## folder for automation tools\n* **AGENTS.md**: Guidelines for AI coding assistants (in project root)\n\n----\n\n== Getting Help ==\n\nIf you encounter issues not covered in this guide:\n\n1. Check the **Troubleshooting** section above\n1. Review error messages in the console output\n1. Check ##apps/web/debug-analyzer.log## for detailed logs\n1. Search existing GitHub issues\n1. Create a new issue with:\n** Your operating system\n** Node.js and .NET versions\n** Error messages and logs\n** Steps to reproduce\n\n----\n\n**Last Updated**: February 2, 2026", "Product Development.DevOps.Guidelines.Scope Definition Guidelines.WebHome": "= Boundary Definition Guidelines: EvidenceScope vs ClaimBoundary =\n\n**Version**: 3.0.0-cb\n**Date**: 2026-02-16\n**Audience**: Developers, Prompt Engineers\n**Related**: [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]], AGENTS.md\n\n----\n\n== Table of Contents ==\n\n1. [[Quick Reference>>||anchor=\"H1.QuickReference\"]]\n1. [[EvidenceScope>>||anchor=\"H2.EvidenceScope\"]]\n1. [[ClaimBoundary>>||anchor=\"H3.ClaimBoundary\"]]\n1. [[Decision Tree>>||anchor=\"H4.DecisionTree\"]]\n1. [[Common Mistakes>>||anchor=\"H5.CommonMistakes\"]]\n1. [[Code Examples>>||anchor=\"H6.CodeExamples\"]]\n\n----\n\n== 1. Quick Reference ==\n\n=== TL;DR ===\n\n|= Concept |= Purpose |= Scope |= Cardinality\n| **EvidenceScope** | Source methodology metadata | Per-evidence-item | Required (1 per item)\n| **ClaimBoundary** | Evidence-emergent grouping | Per-analysis | 1-5 per analysis\n\n**Rule of Thumb**:\n* **EvidenceScope** = \"What methodology/boundaries does THIS source use?\"\n* **ClaimBoundary** = \"Which evidence items can be clustered together by compatible EvidenceScopes?\"\n\n=== When in Doubt ===\n\n**Use EvidenceScope** (always):\n* EVERY evidence item MUST have an EvidenceScope\n* Describes source's methodology, temporal period, boundaries, geography\n* Mandatory fields: ##methodology##, ##temporal##\n\n**Use ClaimBoundary** (determined by pipeline):\n* ClaimBoundaries emerge from clustering compatible EvidenceScopes (Stage 3)\n* NOT pre-created from user input\n* Grouping is based on congruence assessment (methodology + temporal + boundaries + geographic compatibility)\n\n----\n\n== 2. EvidenceScope ==\n\n=== 2.1 Definition ===\n\n**EvidenceScope** = Per-evidence-item metadata describing the source's **methodology, temporal period, boundaries, and geography**.\n\n**Location**: Attached to every ##EvidenceItem## object (MANDATORY, not optional)\n\n{{code language=\"typescript\"}}\ninterface EvidenceItem {\n  id: string;                               // EV_001, EV_002, ...\n  statement: string;                        // The evidence statement\n  category: string;\n  sourceId: string;\n  sourceUrl: string;\n  sourceExcerpt: string;\n\n  // REQUIRED: Source methodology metadata\n  evidenceScope: {\n    name: string;                           // E.g., \"Methodology A Analysis\"\n    methodology: string;                    // REQUIRED - The analytical approach\n    temporal: string;                       // REQUIRED - Time period of source data\n    boundaries?: string;                    // Optional - What is included/excluded\n    geographic?: string;                    // Optional - Geographic scope\n    sourceType?: SourceType;                // Optional - peer_reviewed_study, etc.\n    additionalDimensions?: Record<string, string>; // Optional - domain-specific data\n  };\n\n  scopeQuality: \"complete\" | \"partial\" | \"incomplete\"; // LLM-assessed\n  claimBoundaryId?: string;                 // Assigned during CLUSTER stage\n  relevantClaimIds: string[];               // Which claims this evidence relates to\n  isDerivative: boolean;                    // Derives from another source's study\n  derivedFromSourceUrl?: string;            // URL of original source (optional)\n}\n{{/code}}\n\n=== 2.2 Purpose ===\n\n**Primary Use**: Congruence assessment for ClaimBoundary clustering\n\nWhen evidence is extracted during Stage 2: RESEARCH, each EvidenceItem carries its source's EvidenceScope. During Stage 3: CLUSTER BOUNDARIES, compatible EvidenceScopes are grouped into ClaimBoundaries.\n\n**Secondary Use**: Source reliability calibration and verdict explanation\n\nEvidenceScope helps users understand WHY evidence was weighted a certain way and which methodological frame it belongs to.\n\n=== 2.3 When to Use EvidenceScope ===\n\n**ALWAYS use EvidenceScope** (it's mandatory):\n\n|= Scenario |= Example (Generic)\n| **Source uses specific methodology** | \"Standard S analysis\" (vs \"Framework F analysis\")\n| **Source has temporal boundaries** | \"Period P data\" (vs \"Period Q historical data\")\n| **Source has system boundaries** | \"Full system boundary\" (vs \"Subsystem only\")\n| **Source has geographic boundaries** | \"Jurisdiction J regulations\" (vs \"Region R standards\")\n| **Source type affects reliability** | Peer-reviewed study vs organization report\n\n**Every evidence source has**:\n* A methodology (even an opinion uses \"editorial commentary\")\n* A temporal anchor (at minimum the publication date)\n* Optional boundaries and geographic scope\n\n=== 2.4 EvidenceScope Examples ===\n\n==== Example 1: Methodological Scope ====\n\n**Input**: \"Process X is more effective than Process Y\"\n\n**Evidence with EvidenceScope**:\n{{code language=\"typescript\"}}\n{\n  id: \"EV_001\",\n  statement: \"Study using Methodology A found Process X efficiency at Value V1\",\n  category: \"statistic\",\n  sourceUrl: \"https://source.example/study-1\",\n  sourceExcerpt: \"Using Methodology A including phases 1-3...\",\n  evidenceScope: {\n    name: \"Methodology A Analysis\",\n    methodology: \"Standard S with Approach A\",\n    temporal: \"Period P data (Year-Year)\",\n    boundaries: \"Phases 1-3 included\",\n    sourceType: \"peer_reviewed_study\"\n  },\n  scopeQuality: \"complete\",\n  claimBoundaryId: \"CB_01\",  // Assigned during clustering\n  relevantClaimIds: [\"AC_01\"],\n  isDerivative: false\n}\n{{/code}}\n\n**Why EvidenceScope?**:\n* Methodology (Standard S) is metadata about the source\n* Helps cluster with other evidence using same/compatible methodology\n* Mandatory  cannot skip\n\n==== Example 2: Geographic Regulatory Boundaries ====\n\n**Input**: \"Entity A violated regulations\"\n\n**Evidence with EvidenceScope**:\n{{code language=\"typescript\"}}\n{\n  id: \"EV_002\",\n  statement: \"Entity A's metric exceeded permitted limit by N%\",\n  category: \"statistic\",\n  sourceUrl: \"https://regulatory-body.example/report\",\n  sourceExcerpt: \"Agency audit found Entity A exceeded the Jurisdiction J limit...\",\n  evidenceScope: {\n    name: \"Jurisdiction J Regulations\",\n    methodology: \"Regulatory audit, Framework F\",\n    temporal: \"Audit conducted Period P\",\n    boundaries: \"Jurisdiction J only\",\n    geographic: \"Region R\",\n    sourceType: \"government_report\"\n  },\n  scopeQuality: \"complete\"\n}\n{{/code}}\n\n**Why EvidenceScope?**:\n* Geography (Jurisdiction J) affects interpretation\n* Clusters with other Jurisdiction J evidence\n* Separates from Jurisdiction K evidence (different regulatory framework)\n\n==== Example 3: Temporal Data Boundaries ====\n\n**Input**: \"Trend T is accelerating\"\n\n**Evidence with EvidenceScope**:\n{{code language=\"typescript\"}}\n{\n  id: \"EV_003\",\n  statement: \"Metric M increased by N units from Period P to Period Q\",\n  category: \"statistic\",\n  sourceUrl: \"https://data.example/records\",\n  sourceExcerpt: \"Data records show N-unit increase over Period P-Q...\",\n  evidenceScope: {\n    name: \"Period P-Q Data Records\",\n    methodology: \"Measurement Standard S\",\n    temporal: \"Period P to Period Q timeframe\",\n    boundaries: \"Metric M measurement boundary\",\n    sourceType: \"government_report\"\n  },\n  scopeQuality: \"complete\"\n}\n{{/code}}\n\n**Why EvidenceScope?**:\n* Temporal boundary (Period P-Q) is metadata about the measurement\n* Helps assess temporal consistency across evidence\n* Mandatory field\n\n----\n\n== 3. ClaimBoundary ==\n\n=== 3.1 Definition ===\n\n**ClaimBoundary** = An **evidence-emergent grouping** of compatible EvidenceScopes created AFTER research by clustering evidence with congruent methodology, temporal, boundaries, and geographic dimensions.\n\n**Location**: Top-level in analysis result, created during Stage 3: CLUSTER BOUNDARIES\n\n{{code language=\"typescript\"}}\ninterface ClaimBoundary {\n  id: string;                    // E.g., \"CB_01\", \"CB_02\"\n  name: string;                  // E.g., \"Methodology A Studies\"\n  shortName: string;             // E.g., \"Method A\"\n  description: string;           // What this boundary represents\n  methodology?: string;          // Dominant methodology (if applicable)\n  boundaries?: string;           // Scope boundaries\n  geographic?: string;           // Geographic scope\n  temporal?: string;             // Temporal scope\n  internalCoherence: number;     // 0-1: consistency of evidence within\n  evidenceCount: number;         // Number of evidence items\n}\n{{/code}}\n\n=== 3.2 Purpose ===\n\n**Primary Use**: Organize evidence for verdict generation\n\nClaimBoundaries provide analytical structure for Stage 4: VERDICT. Each ClaimVerdict includes per-boundary findings (##BoundaryFinding[]##) showing how evidence within each boundary supports or contradicts each claim.\n\n**Key principle**: Evidence tells us what boundaries exist. We don't guess.\n\n=== 3.3 When ClaimBoundaries Are Created ===\n\n**Boundaries emerge from evidence** (NOT from user input):\n\n|= Scenario |= Result\n| **Compatible EvidenceScopes** | Cluster into single ClaimBoundary\n| **Incompatible methodologies** | Separate ClaimBoundaries (e.g., \"Method A\" vs \"Method B\")\n| **Different jurisdictions** | Separate ClaimBoundaries (e.g., \"Jurisdiction J\" vs \"Jurisdiction K\")\n| **Different temporal periods** | May separate if temporal is primary subject\n| **Low internal coherence** | May split if evidence contradicts due to scope differences\n\n**Clustering criteria** (LLM-evaluated during Stage 3):\n\n| Factor | Merge if... | Separate if... |\n|--------|------------|---------------|\n| Methodology | Same or compatible methodology | Fundamentally different approach |\n| Boundaries | Overlapping scope boundaries | Non-overlapping system boundaries |\n| Geographic | Same or overlapping regions | Distinct jurisdictions with different rules |\n| Temporal | Overlapping time periods | Non-overlapping periods with policy changes |\n| Contradiction | Low contradiction between items | High contradiction driven by scope differences |\n\n=== 3.4 ClaimBoundary Examples ===\n\n==== Example 1: Methodological Split ====\n\n**Input**: \"Process X is more effective than Process Y\"\n\n**EvidenceScopes** (after research):\n* 5 evidence items using \"Methodology A, Full system\"\n* 3 evidence items using \"Methodology B, Subsystem only\"\n\n**ClaimBoundaries** (after clustering):\n{{code language=\"typescript\"}}\n[\n  {\n    id: \"CB_01\",\n    name: \"Methodology A Studies (Full System)\",\n    shortName: \"Method A\",\n    description: \"Evidence using Methodology A with full system boundary\",\n    methodology: \"Methodology A\",\n    boundaries: \"Full system boundary\",\n    internalCoherence: 0.92,\n    evidenceCount: 5\n  },\n  {\n    id: \"CB_02\",\n    name: \"Methodology B Studies (Subsystem)\",\n    shortName: \"Method B\",\n    description: \"Evidence using Methodology B with subsystem boundary\",\n    methodology: \"Methodology B\",\n    boundaries: \"Subsystem boundary\",\n    internalCoherence: 0.88,\n    evidenceCount: 3\n  }\n]\n{{/code}}\n\n**Why separate ClaimBoundaries?**:\n* **Different methodologies** (A vs B)\n* **Different system boundaries** (full vs subsystem)\n* **Cannot meaningfully combine** into single verdict without losing information\n* **Each boundary gets its own finding** in ClaimVerdict.boundaryFindings[]\n\n==== Example 2: Geographic Split ====\n\n**Input**: \"Entity A violated regulations\"\n\n**EvidenceScopes** (after research):\n* 4 evidence items from \"Jurisdiction J, Framework F\"\n* 2 evidence items from \"Jurisdiction K, Framework G\"\n\n**ClaimBoundaries** (after clustering):\n{{code language=\"typescript\"}}\n[\n  {\n    id: \"CB_01\",\n    name: \"Jurisdiction J Proceedings\",\n    shortName: \"Jurisdiction J\",\n    description: \"Evidence from Jurisdiction J regulatory framework\",\n    methodology: \"Framework F\",\n    geographic: \"Jurisdiction J\",\n    internalCoherence: 0.95,\n    evidenceCount: 4\n  },\n  {\n    id: \"CB_02\",\n    name: \"Jurisdiction K Proceedings\",\n    shortName: \"Jurisdiction K\",\n    description: \"Evidence from Jurisdiction K regulatory framework\",\n    methodology: \"Framework G\",\n    geographic: \"Jurisdiction K\",\n    internalCoherence: 0.90,\n    evidenceCount: 2\n  }\n]\n{{/code}}\n\n**Why separate ClaimBoundaries?**:\n* **Different jurisdictions** (J vs K)\n* **Different regulatory frameworks** (F vs G)\n* **May have different outcomes** (violation in J, not in K)\n* **Need separate per-boundary findings**\n\n==== Example 3: Single Boundary (Compatible Evidence) ====\n\n**Input**: \"Trend T is accelerating\"\n\n**EvidenceScopes** (after research):\n* All 8 evidence items use \"Standard S, Period P-Q data, Measurement Boundary B\"\n\n**ClaimBoundaries** (after clustering):\n{{code language=\"typescript\"}}\n[\n  {\n    id: \"CB_01\",\n    name: \"General Evidence (Standard S)\",\n    shortName: \"General\",\n    description: \"All evidence uses compatible Standard S methodology with Period P-Q data\",\n    methodology: \"Standard S\",\n    temporal: \"Period P-Q\",\n    boundaries: \"Measurement Boundary B\",\n    internalCoherence: 0.93,\n    evidenceCount: 8\n  }\n]\n{{/code}}\n\n**Why single ClaimBoundary?**:\n* **All evidence compatible** (same standard, same period, same boundaries)\n* **No meaningful split needed**\n* **Default case for most analyses**\n\n----\n\n== 4. Decision Tree ==\n\n{{code}}\nEvidence Item Extracted\n\n ALWAYS assign EvidenceScope (MANDATORY)\n   methodology: \"Standard S with Approach A\" (REQUIRED)\n   temporal: \"Period P data\" (REQUIRED)\n   boundaries: \"Full system\" (optional)\n   geographic: \"Region R\" (optional)\n\n After ALL evidence extracted  Stage 3: CLUSTER BOUNDARIES\n   \n    Are EvidenceScopes COMPATIBLE?\n      YES (same/compatible methodology + temporal + boundaries)\n         Cluster into SINGLE ClaimBoundary\n        Examples:\n        - All sources use Standard S, Period P\n        - Compatible methodologies with overlapping boundaries\n     \n      NO (incompatible methodology OR boundaries OR temporal OR geographic)\n          Create SEPARATE ClaimBoundaries\n         Examples:\n         - Standard S (full system) vs Standard T (subsystem)\n         - Jurisdiction J vs Jurisdiction K\n         - Period P vs Period Q (if temporal is primary subject)\n{{/code}}\n\n=== Decision Checklist ===\n\n**For EvidenceScope (every evidence item)**:\n* Did I assign methodology? (REQUIRED)\n* Did I assign temporal? (REQUIRED)\n* Are boundaries/geographic relevant for this source? (optional but recommended)\n* Is scopeQuality \"complete\", \"partial\", or \"incomplete\"?\n\n**For ClaimBoundary (pipeline-determined)**:\n* Let the LLM cluster compatible EvidenceScopes (don't pre-create boundaries from input)\n* Trust the congruence assessment (methodology + temporal + boundaries + geographic)\n* Expect 1-3 boundaries (typical), max 5 (rare)\n\n----\n\n== 5. Common Mistakes ==\n\n=== Mistake 1: Skipping EvidenceScope ===\n\n**Wrong**:\n{{code language=\"typescript\"}}\n// Evidence without EvidenceScope\nevidenceItems: [\n  {\n    id: \"EV_001\",\n    statement: \"Source found Value V\",\n    // evidenceScope: undefined   MISSING - will fail validation\n  }\n]\n{{/code}}\n\n**Problem**: EvidenceScope is MANDATORY in ClaimBoundary pipeline. Cannot skip.\n\n**Correct**:\n{{code language=\"typescript\"}}\nevidenceItems: [\n  {\n    id: \"EV_001\",\n    statement: \"Source found Value V\",\n    evidenceScope: {\n      name: \"Source Methodology\",\n      methodology: \"Standard S\",\n      temporal: \"Period P data\",\n    },\n    scopeQuality: \"complete\"\n  }\n]\n{{/code}}\n\n=== Mistake 2: Pre-Creating ClaimBoundaries from Input ===\n\n**Wrong**:\n{{code language=\"typescript\"}}\n// Input: \"Process X vs Process Y\"\n// Manually creating ClaimBoundaries BEFORE research\nclaimBoundaries: [\n  { name: \"Process X Analysis\" },   DON'T pre-create\n  { name: \"Process Y Analysis\" },   DON'T pre-create\n]\n{{/code}}\n\n**Problem**: ClaimBoundaries are NOT pre-created. They emerge from evidence EvidenceScopes during Stage 3.\n\n**Correct**:\n{{code language=\"typescript\"}}\n// Input: \"Process X vs Process Y\"\n// Extract AtomicClaims during Stage 1\natomicClaims: [\n  { statement: \"Process X achieves Metric M at Value V1\" },\n  { statement: \"Process Y achieves Metric M at Value V2\" }\n]\n\n// Research during Stage 2 (gather evidence with EvidenceScopes)\nevidenceItems: [\n  {\n    statement: \"Study using Method A found Process X at V1\",\n    evidenceScope: { methodology: \"Method A\", temporal: \"Period P\" },\n    relevantClaimIds: [\"AC_01\"]\n  },\n  {\n    statement: \"Study using Method B found Process Y at V2\",\n    evidenceScope: { methodology: \"Method B\", temporal: \"Period P\" },\n    relevantClaimIds: [\"AC_02\"]\n  }\n]\n\n// Cluster boundaries during Stage 3 (IF methodologies are incompatible)\nclaimBoundaries: [\n  { id: \"CB_01\", name: \"Method A Studies\", evidenceCount: ...},\n  { id: \"CB_02\", name: \"Method B Studies\", evidenceCount: ...}\n]\n// OR cluster into single boundary if methodologies are compatible\n{{/code}}\n\n=== Mistake 3: Using Source Name as EvidenceScope ===\n\n**Wrong**:\n{{code language=\"typescript\"}}\n// Putting source name in EvidenceScope\nevidenceScope: {\n  name: \"According to Source Entity\",   This is just attribution\n  methodology: \"Source Entity study\"   Source name is not methodology\n}\n{{/code}}\n\n**Correct**:\n{{code language=\"typescript\"}}\n// EvidenceScope is for METHODOLOGY, not source name\nevidenceScope: {\n  name: \"Peer-Reviewed Assessment using Standard S\",   Methodology-focused\n  methodology: \"Standard S with Approach A\",   The analytical approach\n  temporal: \"Period P data\",\n  sourceType: \"peer_reviewed_study\"\n}\n\n// Source name goes in sourceTitle\nsourceTitle: \"Source Entity Publication\"\n{{/code}}\n\n=== Mistake 4: Conflating EvidenceScope with ClaimBoundary ===\n\n**Wrong thinking**:\n\"Each EvidenceScope becomes a ClaimBoundary\"\n\n**Problem**: Multiple evidence items share the same EvidenceScope, and compatible EvidenceScopes cluster into a SINGLE ClaimBoundary.\n\n**Correct thinking**:\n\"Compatible EvidenceScopes cluster into ClaimBoundaries\"\n\n**Example**:\n* 10 evidence items all have EvidenceScope: ##{ methodology: \"Standard S\", temporal: \"Period P\" }##\n* These 10 items cluster into **1 ClaimBoundary** (not 10)\n* ClaimBoundary name: \"Standard S Studies (Period P)\"\n\n----\n\n== 6. Code Examples ==\n\n=== 6.1 Creating EvidenceScope (Stage 2: RESEARCH) ===\n\n{{code language=\"typescript\"}}\nimport type { EvidenceItem, EvidenceScope } from \"@/lib/analyzer/types\";\n\nfunction extractEvidenceWithScope(source: Source, relevantClaims: AtomicClaim[]): EvidenceItem[] {\n  const items: EvidenceItem[] = [];\n\n  // Extract evidence statement\n  const statement = extractStatementFromSource(source);\n\n  // REQUIRED: Extract EvidenceScope metadata from source\n  const scope: EvidenceScope = {\n    name: detectScopeName(source),           // E.g., \"Methodology A Analysis\"\n    methodology: detectMethodology(source),  // REQUIRED - never empty\n    temporal: detectTemporal(source),        // REQUIRED - at minimum publication date\n    boundaries: detectBoundaries(source),    // Optional - what's included/excluded\n    geographic: detectGeographic(source),    // Optional - geographic scope\n    sourceType: classifySourceType(source),  // Optional - peer_reviewed_study, etc.\n  };\n\n  // Assess scope quality\n  const scopeQuality = assessScopeQuality(scope);\n\n  items.push({\n    id: generateId(source.id),\n    statement: statement,\n    category: categorizeEvidence(statement),\n    sourceId: source.id,\n    sourceUrl: source.url,\n    sourceExcerpt: source.excerpt,\n    evidenceScope: scope,  // MANDATORY\n    scopeQuality: scopeQuality,\n    relevantClaimIds: relevantClaims.map(c => c.id),\n    isDerivative: detectDerivative(source),\n  });\n\n  return items;\n}\n{{/code}}\n\n=== 6.2 Clustering ClaimBoundaries (Stage 3: CLUSTER BOUNDARIES) ===\n\n{{code language=\"typescript\"}}\nimport type { ClaimBoundary, EvidenceItem, EvidenceScope } from \"@/lib/analyzer/types\";\n\nasync function clusterBoundaries(\n  evidenceItems: EvidenceItem[],\n  atomicClaims: AtomicClaim[]\n): Promise<ClaimBoundary[]> {\n  // Collect all unique EvidenceScopes\n  const scopes: EvidenceScope[] = collectUniqueScopes(evidenceItems);\n\n  // LLM call: Cluster compatible EvidenceScopes into ClaimBoundaries\n  const clusteringResult = await llm.clusterEvidenceScopes({\n    scopes: scopes,\n    evidenceItems: evidenceItems,\n    atomicClaims: atomicClaims,\n    congruenceGuidance: getCongruenceGuidance(),  // From architecture doc 11.5\n  });\n\n  // Assign evidence items to ClaimBoundaries\n  for (const item of evidenceItems) {\n    const boundaryId = clusteringResult.scopeToBoundaryMapping[item.evidenceScope.name];\n    item.claimBoundaryId = boundaryId;\n  }\n\n  return clusteringResult.claimBoundaries;\n}\n\nfunction collectUniqueScopes(items: EvidenceItem[]): EvidenceScope[] {\n  const uniqueScopes = new Map<string, EvidenceScope>();\n  for (const item of items) {\n    const scopeKey = `${item.evidenceScope.methodology}|${item.evidenceScope.temporal}|${item.evidenceScope.boundaries || ''}`;\n    if (!uniqueScopes.has(scopeKey)) {\n      uniqueScopes.set(scopeKey, item.evidenceScope);\n    }\n  }\n  return Array.from(uniqueScopes.values());\n}\n{{/code}}\n\n=== 6.3 Using ClaimBoundaries in Verdicts (Stage 4: VERDICT) ===\n\n{{code language=\"typescript\"}}\nimport type { ClaimVerdict, BoundaryFinding } from \"@/lib/analyzer/types\";\n\nasync function generateVerdicts(\n  claims: AtomicClaim[],\n  evidenceItems: EvidenceItem[],\n  boundaries: ClaimBoundary[]\n): Promise<ClaimVerdict[]> {\n  const verdicts: ClaimVerdict[] = [];\n\n  for (const claim of claims) {\n    // Group evidence by ClaimBoundary for this claim\n    const evidenceByBoundary = groupEvidenceByBoundary(\n      evidenceItems.filter(e => e.relevantClaimIds.includes(claim.id)),\n      boundaries\n    );\n\n    // LLM call: Generate verdict with per-boundary findings\n    const verdictResult = await llm.advocateVerdict({\n      claim: claim,\n      evidenceByBoundary: evidenceByBoundary,\n      boundaries: boundaries,\n    });\n\n    verdicts.push({\n      id: generateVerdictId(claim.id),\n      claimId: claim.id,\n      truthPercentage: verdictResult.truthPercentage,\n      verdict: verdictResult.verdict,\n      confidence: verdictResult.confidence,\n      reasoning: verdictResult.reasoning,\n      boundaryFindings: verdictResult.boundaryFindings,  // Per-boundary assessments\n      // ...\n    });\n  }\n\n  return verdicts;\n}\n{{/code}}\n\n----\n\n== Appendix: Real-World Examples (Genericized) ==\n\n=== Example A: Methodology Split ===\n\n**Input**: \"Process X is more effective than Process Y\"\n\n**Correct Approach**:\n{{code language=\"typescript\"}}\n// TWO ClaimBoundaries (incompatible methodologies)\nclaimBoundaries: [\n  { id: \"CB_01\", name: \"Method A Studies (Full System)\", methodology: \"Method A\" },\n  { id: \"CB_02\", name: \"Method B Studies (Subsystem)\", methodology: \"Method B\" },\n]\n\n// Evidence with EvidenceScope (drives clustering)\nevidenceItems: [\n  {\n    id: \"EV_001\",\n    statement: \"Study using Method A found Process X effectiveness at Value V1\",\n    evidenceScope: { methodology: \"Method A\", temporal: \"Period P\", boundaries: \"Full system\" },\n    claimBoundaryId: \"CB_01\",  // Assigned during clustering\n    relevantClaimIds: [\"AC_01\"]\n  },\n  {\n    id: \"EV_002\",\n    statement: \"Study using Method B found Process Y effectiveness at Value V2\",\n    evidenceScope: { methodology: \"Method B\", temporal: \"Period P\", boundaries: \"Subsystem\" },\n    claimBoundaryId: \"CB_02\",  // Assigned during clustering\n    relevantClaimIds: [\"AC_02\"]\n  },\n]\n{{/code}}\n\n=== Example B: No Split Needed (Compatible Evidence) ===\n\n**Input**: \"Entity A's action was appropriate\"\n\n**Correct Approach**:\n{{code language=\"typescript\"}}\n// ONE ClaimBoundary (all evidence compatible)\nclaimBoundaries: [\n  { id: \"CB_01\", name: \"General Evidence\", methodology: \"Standard S\", evidenceCount: 5 },\n]\n\n// Evidence with compatible EvidenceScopes (cluster into single boundary)\nevidenceItems: [\n  {\n    id: \"EV_001\",\n    statement: \"Source A assessed action as appropriate using Standard S\",\n    evidenceScope: { methodology: \"Standard S\", temporal: \"Period P\" },\n    claimBoundaryId: \"CB_01\"\n  },\n  {\n    id: \"EV_002\",\n    statement: \"Source B confirmed appropriateness using Standard S\",\n    evidenceScope: { methodology: \"Standard S\", temporal: \"Period P\" },\n    claimBoundaryId: \"CB_01\"\n  },\n  // All 5 items cluster into CB_01\n]\n{{/code}}\n\n----\n\n**Document Version**: 3.0.0-cb\n**Last Updated**: 2026-02-16\n**Next Review**: After ClaimBoundary pipeline UI implementation\n**Maintained by**: Technical Writer\n", "Product Development.DevOps.Guidelines.Testing Strategy.WebHome": "= Testing Strategy =\n\n== Overview ==\n\nFactHarbor's testing strategy focuses on measurement-driven quality improvement:\n\n1. **Baseline Establishment**: 30 diverse test cases covering all critical scenarios\n1. **A/B Testing**: Compare old vs optimized prompts with real LLM calls\n1. **Regression Prevention**: Automated checks ensure improvements don't degrade quality\n1. **Continuous Validation**: Metrics tracking for every analysis\n\n== Test Case Suite ==\n\n=== Coverage (30 cases total) ===\n\n* **Simple Factual** (5 cases): Basic well-established facts\n* **Multi-Context** (5 cases): Distinct analytical frames requiring separation\n* **Comparative** (5 cases): Comparisons with rating direction challenges\n* **Attribution Separation** (5 cases): WHO said vs WHAT was said\n* **Temporal** (5 cases): Recent claims requiring current data\n* **Pseudoscience** (3 cases): Debunked claims requiring detection\n* **Methodology** (2 cases): Methodology-specific scopes\n\n=== Difficulty Distribution ===\n\n* **Easy** (10 cases): Well-established facts, obvious outcomes\n* **Medium** (15 cases): Requires analysis, multiple scopes, attribution\n* **Hard** (5 cases): Recent data, complex comparisons, methodology detection\n\n== Baseline Execution ==\n\n=== Process ===\n\n1. **Setup**: Configure environment with production settings\n1. **Execution**: Run all 30 test cases with current system\n1. **Collection**: Record metrics for each run\n1. **Analysis**: Calculate baseline performance\n\n=== Metrics Collected ===\n\n**Per Test Case:**\n* Verdict accuracy (truth percentage vs expected range)\n* Token usage (prompt + completion)\n* Duration (total ms)\n* Cost (estimated $)\n* Schema compliance (boolean)\n* Gate 1 pass rate (%)\n* Gate 4 confidence distribution\n\n**Summary Statistics:**\n* Average duration\n* Average cost\n* Average tokens\n* Schema compliance rate (target: >95%)\n* Verdict accuracy rate (target: >80% within expected range)\n\n=== Baseline Script ===\n\n{{code language=\"bash\"}}\n# Run baseline test suite\nnpm run test:baseline\n\n# Output: baseline-results-{date}.json\n{{/code}}\n\n== A/B Testing ==\n\n=== Configuration ===\n\n{{code language=\"typescript\"}}\nconst abConfig = {\n  testCases: BASELINE_TEST_CASES,\n  variants: ['inline-prompts', 'optimized-prompts'],\n  providers: ['anthropic', 'openai', 'google'],\n  runsPerVariant: 3, // For statistical significance\n};\n{{/code}}\n\n=== Execution ===\n\n{{code language=\"bash\"}}\n# Run A/B test (WARNING: Expensive!)\nnpm run test:ab\n\n# Quick test (10 cases, 1 provider, 2 runs)\nnpm run test:ab:quick\n{{/code}}\n\n=== Comparison Metrics ===\n\n**Per Test Case:**\n* Token reduction (%)\n* Speed improvement (%)\n* Cost reduction (%)\n* Schema compliance change (percentage points)\n* Verdict accuracy change (percentage points)\n\n**Overall:**\n* Average token reduction (target: 30-40%)\n* Average speed improvement (target: may vary)\n* Average cost reduction (target: follows token reduction)\n* Schema improvement rate (% of tests with better compliance)\n* Verdict accuracy maintained (within +/-5%)\n\n=== Success Criteria ===\n\n**Pass**:\n* Token reduction >=30%\n* Schema compliance maintained or improved\n* Verdict accuracy within +/-5% of baseline\n* No catastrophic failures\n\n**Fail**:\n* Token reduction <20%\n* Schema compliance degrades >10%\n* Verdict accuracy degrades >5%\n* >10% failure rate\n\n== Regression Testing ==\n\n=== Automated Checks ===\n\nAfter every significant change:\n\n1. **Smoke Test**: Run 5 representative cases\n1. **Quality Check**: Compare to baseline thresholds\n1. **Alert on Degradation**: >5% drop in any metric\n\n=== Regression Suite ===\n\nThe baseline test cases become the permanent regression suite:\n\n{{code language=\"typescript\"}}\n// Run before merging any PR\nnpm test:regression\n\n// Checks:\n// - All cases produce valid output\n// - Verdicts within 10% of baseline\n// - Schema compliance baseline rate\n{{/code}}\n\n== Integration Tests ==\n\n=== Real LLM Calls (E2E) ===\n\n{{code language=\"typescript\"}}\ndescribe('End-to-End Analysis', () => {\n  it('should analyze simple factual claim', async () => {\n    const result = await analyze({\n      inputType: 'text',\n      inputValue: 'The Earth orbits the Sun',\n    });\n\n    expect(result.articleTruthPercentage).toBeGreaterThan(85);\n    expect(result.articleVerdict).toBe('TRUE');\n  }, 60000); // 60s timeout\n});\n{{/code}}\n\n=== Cost Guards ===\n\n{{code language=\"typescript\"}}\n// Prevent runaway costs\nbeforeAll(() => {\n  process.env.TEST_MODE = 'true';\n  process.env.MAX_TEST_COST = '5.00'; // $5 limit\n});\n{{/code}}\n\n== Performance Testing ==\n\n=== Load Testing ===\n\nNot applicable for POC (single-user local deployment).\n\nFor production: Use k6 or Artillery to test:\n* Concurrent analyses\n* Database query performance\n* API rate limits\n\n=== Benchmark Tests ===\n\n{{code language=\"typescript\"}}\ndescribe('Performance Benchmarks', () => {\n  it('should complete 10-claim analysis in <60s', async () => {\n    const start = Date.now();\n    await analyze({ /* 10 claims */ });\n    const duration = Date.now() - start;\n    expect(duration).toBeLessThan(60000);\n  });\n});\n{{/code}}\n\n== Test Environments ==\n\n=== Local Development ===\n* SQLite database\n* Mock LLM calls (unit tests)\n* Real LLM calls (integration tests - mark with ##@e2e##)\n\n=== CI/CD ===\n* Unit tests only (no LLM calls)\n* Fast feedback (<2 minutes)\n\n=== Manual Testing ===\n* Baseline execution (manual trigger)\n* A/B testing (manual trigger + approval)\n* Requires API budget allocation\n\n== Cost Management ==\n\n=== Test Budget ===\n\n|= Test Type |= Estimated Cost\n| Baseline (30 cases) | $20-50\n| A/B Test (full) | $100-200\n| A/B Test (quick) | $10-20\n| Regression (5 cases) | $3-8\n\n=== Cost Control ===\n\n1. **Budget Approval Required**: All expensive tests require explicit approval\n1. **Cost Tracking**: Log estimated cost before execution\n1. **Abort on Threshold**: Stop if cost exceeds budget\n1. **Use Budget Models**: For non-critical test validation\n\n== Continuous Monitoring ==\n\n=== Production Metrics ===\n\nEvery production analysis automatically records:\n* Duration, cost, tokens\n* Schema compliance\n* Quality gate statistics\n* Verdict distribution\n\n=== Alerts ===\n\nConfigure alerts for:\n* Schema compliance <90%\n* Average cost >2x baseline\n* Average duration >2x baseline\n* Gate 1 pass rate <50%\n\n== Test Data ==\n\n=== Synthetic vs Real ===\n\n**Synthetic** (30 baseline cases):\n* Controlled, reproducible\n* Cover edge cases\n* Enable regression detection\n\n**Real** (production analyses):\n* Uncontrolled, varied\n* True user scenarios\n* Enable drift detection\n\n=== Privacy ===\n\nTest cases must NOT include:\n* Real user data\n* Sensitive information\n* Copyrighted material\n\n== Reporting ==\n\n=== Baseline Report ===\n\n{{code language=\"markdown\"}}\n# Baseline Results - {date}\n\n## Summary\n- Total cases: 30\n- Completed: 29\n- Failed: 1\n- Avg duration: 42s\n- Avg cost: $0.18\n- Schema compliance: 96.5%\n\n## By Category\n- Simple Factual: 5/5 \n- Multi-Context: 4/5 \n- Comparative: 5/5 \n...\n{{/code}}\n\n=== A/B Report ===\n\n{{code language=\"markdown\"}}\n# A/B Test Results - {date}\n\n## Overall Improvements\n- Token reduction: 38%\n- Speed improvement: 12%\n- Cost reduction: 38%\n- Schema compliance: +2.5%\n- Verdict accuracy: -1.2% \n\n## Recommendation\n APPROVE - Optimized prompts show significant improvements\n{{/code}}\n\n== Related Documentation ==\n\n* [[Metrics Schema>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema.WebHome]]\n* Baseline Results (BASELINE_RESULTS.md)\n* A/B Test Results (AB_TEST_RESULTS.md)", "Product Development.DevOps.Guidelines.WebHome": "= Development Guidelines =\n\nDevelopment standards for FactHarbor contributors and AI agents. Covers code style, naming conventions, how to define and use EvidenceScope in claims, and the testing strategy (unit tests, promptfoo LLM testing, quality gates).\n\n== Getting Started ==\n\n* [[Getting Started>>FactHarbor.Product Development.DevOps.Guidelines.Getting Started.WebHome]]  Quick start guide for setup and first run\n\n== Guidelines ==\n\n* [[Coding Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.Coding Guidelines.WebHome]]  Code style, naming conventions, and best practices\n* [[Scope Definition Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.Scope Definition Guidelines.WebHome]]  How to define and use EvidenceScope in claims\n* [[Testing Strategy>>FactHarbor.Product Development.DevOps.Guidelines.Testing Strategy.WebHome]]  Test pyramid, promptfoo testing, and quality assurance\n* [[When to Add Complexity>>FactHarbor.Product Development.DevOps.Guidelines.When to Add Complexity.WebHome]]  Data-driven triggers for adding deferred features\n", "Product Development.DevOps.Guidelines.When to Add Complexity.WebHome": "= When to Add Complexity =\nFactHarbor starts simple and adds complexity **only when metrics prove it's necessary**. This page defines clear triggers for adding deferred features.\n**Philosophy**: Let data and user feedback drive complexity, not assumptions about future needs.\n== 1. Add Elasticsearch ==\n**Current**: PostgreSQL full-text search\n**Add Elasticsearch when**:\n*  PostgreSQL search queries consistently >500ms\n*  Search accounts for >20% of total database load\n*  Users complain about search speed\n*  Search index size >50GB\n**Metrics to monitor**:\n* Search query response time (P95, P99)\n* Database CPU usage during search\n* User search abandonment rate\n* Search result relevance scores\n**Before adding**:\n* Try PostgreSQL search optimization (indexes, pg_trgm, GIN indexes)\n* Profile slow queries\n* Consider query result caching\n* Estimate Elasticsearch costs\n**Implementation effort**: ~\n== 2. Add TimescaleDB ==\n**Current**: PostgreSQL with time-series data in regular tables\n**Add TimescaleDB when**:\n*  Metrics queries consistently >1 second\n*  Metrics tables >100GB\n*  Need for time-series specific features (continuous aggregates, data retention policies)\n*  Dashboard loading noticeably slow\n**Metrics to monitor**:\n* Metrics query response time\n* Metrics table size growth rate\n* Dashboard load time\n* Time-series query patterns\n**Before adding**:\n* Try PostgreSQL optimization (partitioning, materialized views)\n* Implement query result caching\n* Consider data aggregation strategies\n* Profile slow metrics queries\n**Implementation effort**: ~\n== 3. Add Federation ==\n**Current**: Single-node deployment with read replicas\n**Add Federation when**:\n*  10,000+ users on single node\n*  Users explicitly request ability to run own instances\n*  Geographic latency becomes significant problem (>200ms)\n*  Censorship/control concerns emerge\n*  Community demands decentralization\n**Metrics to monitor**:\n* Total active users\n* Geographic distribution of users\n* Single-node performance limits\n* User feature requests\n* Community sentiment\n**Before adding**:\n* Exhaust vertical scaling options\n* Add read replicas in multiple regions\n* Implement CDN for static content\n* Survey users about federation interest\n**Implementation effort**: ~ (major undertaking)\n== 4. Add Advanced User Management ==\n**Current**: Simple manual roles (Reader, User, UCM Administrator, Moderator)\n**Add Advanced User Management when**:\n*  Submission volume requires automated quota management\n*  Manual role management becomes bottleneck (>5 hours/week)\n*  Clear patterns of abuse require automated detection\n*  Multiple UCM Administrators need coordinated workflows\n**Metrics to monitor**:\n* Number of registered users and submission volume\n* Time spent on manual role management\n* Abuse incident rate\n* Submission quality distribution\n* Community feedback on roles\n**Before adding**:\n* Document current manual process thoroughly\n* Identify most time-consuming tasks\n* Prototype automated user management features\n* Get community feedback on proposal\n**Implementation effort**: ~\n== 5. Add Many-to-Many Scenarios ==\n**Current**: Scenarios belong to single claims (one-to-many)\n**Add Many-to-Many Scenarios when**:\n*  Users request \"apply this scenario to other claims\"\n*  Clear use cases for scenario reuse emerge\n*  Scenario duplication becomes significant storage issue\n*  Cross-claim scenario analysis requested\n**Metrics to monitor**:\n* Scenario duplication rate\n* User feature requests\n* Storage costs of scenarios\n* Query patterns involving scenarios\n**Before adding**:\n* Analyze scenario duplication patterns\n* Design junction table schema\n* Plan data migration strategy\n* Consider query performance impact\n**Implementation effort**: ~\n== 6. Add Full UCM Config Versioning System ==\n**Current**: Simple UCM config audit trail (immutable blobs, activation tracking)\n**Add Full Config Versioning when**:\n*  UCM Administrators request \"see complete config version history\"\n*  UCM Administrators request \"restore to specific previous config version\"\n*  Need for config branching and merging emerges (e.g., A/B testing configs)\n*  Collaborative config management requires conflict resolution\n**Metrics to monitor**:\n* UCM Administrator feature requests for config versioning\n* Manual config rollback frequency\n* Config change conflict rate\n* Storage costs of full config history\n**Before adding**:\n* Design config branching/merging strategy\n* Plan storage optimization (delta compression for config blobs)\n* Consider UI/UX for config version history\n* Estimate storage and performance impact\n**Implementation effort**: ~\n== 7. Add Graph Database ==\n**Current**: Relational data model in PostgreSQL\n**Add Graph Database when**:\n*  Complex relationship queries become common\n*  Need for multi-hop traversals (friend-of-friend, citation chains)\n*  PostgreSQL recursive queries too slow\n*  Graph algorithms needed (PageRank, community detection)\n**Metrics to monitor**:\n* Relationship query patterns\n* Recursive query performance\n* Use cases requiring graph traversals\n* Query complexity growth\n**Before adding**:\n* Try PostgreSQL recursive CTEs\n* Consider graph extensions for PostgreSQL\n* Profile slow relationship queries\n* Evaluate Neo4j vs alternatives\n**Implementation effort**: ~\n== 8. Add Collaborative UCM Configuration Management ==\n**Current**: Single UCM Administrator manages configuration via direct access\n**Add Collaborative UCM Management when**:\n*  Multiple UCM Administrators need concurrent config access\n*  Config change coordination becomes frequent issue\n*  Need for live updates during configuration sessions\n*  Collaborative review of config changes common\n**Metrics to monitor**:\n* Config change conflict frequency\n* UCM Administrator feature requests\n* Collaborative config review patterns\n* Average config management session duration\n**Before adding**:\n* Design config change conflict resolution strategy\n* Consider WebSocket infrastructure for live config preview\n* Plan UI/UX for collaborative config management\n* Estimate server resource requirements\n**Implementation effort**: ~\n== 9. Add Machine Learning Pipeline ==\n**Current**: Rule-based quality scoring and LLM-based analysis\n**Add ML Pipeline when**:\n*  Need for custom models beyond LLM APIs\n*  Opportunity for specialized fine-tuning\n*  Cost savings from specialized models\n*  Real-time learning from user feedback\n**Metrics to monitor**:\n* LLM API costs\n* Need for domain-specific models\n* Quality improvement opportunities\n* User feedback patterns\n**Before adding**:\n* Collect training data (user feedback, corrections)\n* Experiment with fine-tuning approaches\n* Estimate cost savings vs infrastructure costs\n* Consider model hosting options\n**Implementation effort**: ~\n== 10. Add Blockchain/Web3 Integration ==\n**Current**: Traditional database with audit logs\n**Add Blockchain when**:\n*  Need for immutable public audit trail\n*  Decentralized verification demanded\n*  Token economics would add value\n*  Community governance requires voting\n*  Cross-organization trust is critical\n**Metrics to monitor**:\n* User requests for blockchain features\n* Need for external verification\n* Governance participation rate\n* Trust/verification requirements\n**Before adding**:\n* Evaluate real vs perceived benefits\n* Consider costs (gas fees, infrastructure)\n* Design token economics carefully\n* Study successful Web3 content platforms\n**Implementation effort**: ~\n== Decision Framework ==\n**For any complexity addition, ask**:\n==== Do we have data? ====\n* Metrics showing current system inadequate?\n* User requests documenting need?\n* Performance problems proven?\n==== Have we exhausted simpler options? ====\n* Optimization of current system?\n* Configuration tuning?\n* Simple workarounds?\n==== Do we understand the cost? ====\n* Implementation time realistic?\n* Ongoing maintenance burden?\n* Infrastructure costs?\n* Technical debt implications?\n==== Is the timing right? ====\n* Core product stable?\n* Team capacity available?\n* User demand strong enough?\n**If all four answers are YES**: Proceed with complexity addition\n**If any answer is NO**: Defer and revisit later\n== Monitoring Dashboard ==\n**Recommended metrics to track**:\n**Performance**:\n* P95/P99 response times for all major operations\n* Database query performance\n* AKEL processing time\n* Search performance\n**Usage**:\n* Active users (by period)\n* Claims processed (by period)\n* Search queries (by period)\n* Contribution rate\n**Costs**:\n* Infrastructure costs per user\n* LLM API costs per claim\n* Storage costs per GB\n* Total operational costs\n**Quality**:\n* Confidence score distribution\n* Evidence completeness\n* Source reliability trends\n* User satisfaction (surveys)\n**Community**:\n* Active users\n* Moderation workload\n* Feature requests by category\n* Abuse incident rate\n== Periodic Review Process ==\n**Regularly review**:\n1. **Metrics dashboard**: Are any triggers close to thresholds?\n2. **User feedback**: What features are most requested?\n3. **Performance**: What's slowing down?\n4. **Costs**: What's most expensive?\n5. **Team capacity**: Can we handle new complexity?\n**Decision**: Prioritize complexity additions based on:\n* Urgency (current pain vs future optimization)\n* Impact (user benefit vs internal efficiency)\n* Effort (quick wins vs major projects)\n* Dependencies (prerequisites needed)\n== Related Pages ==\n* [[Design Decisions>>FactHarbor.Product Development.Specification.Design-Decisions]]\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]\n## Remember\n**Build what you need now. Measure everything. Add complexity only when data proves it's necessary.**\nThe best architecture is the simplest one that works for current needs. ", "Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome": "= FactHarbor Admin GUI Guide =\n\n== Overview ==\n\nThe FactHarbor Admin GUI provides web-based interfaces for administrators to:\n* Manage unified configuration (pipeline, search, calculation, prompts, SR)\n* View and manage source reliability cache\n* Test and validate API keys and service configurations\n\n== Admin Pages ==\n\n|= Page |= URL |= Purpose\n| **Unified Configuration** | ##/admin/config## | Manage all configuration types with version history\n| **Source Reliability** | ##/admin/source-reliability## | View/manage cached source reliability scores\n| **Test Config** | ##/admin/test-config## | Validate API keys and service connectivity\n\n----\n\n== Unified Configuration Management ==\n\nNavigate to: ##http:~~/~~/localhost:3000/admin/config##\n\nThe Unified Configuration Management system provides a single interface for managing all FactHarbor configurations with version history, validation, and export capabilities.\n\n=== Configuration Types ===\n\n|= Type |= Profile Keys |= Description\n| **Pipeline** | ##default## | LLM provider selection, model routing, budgets, feature flags\n| **Search** | ##default## | Web search provider settings\n| **Calculation** | ##default## | Verdict aggregation and weighting parameters\n| **Source Reliability** | ##default## | SR service thresholds, caching, filtering (separate SR domain)\n| **Prompt** | ##orchestrated##, ##monolithic-dynamic##, ##source-reliability##, ##text-analysis-input##, ##text-analysis-evidence##, ##text-analysis-context##, ##text-analysis-verdict## | LLM prompt templates (pipeline + text-analysis stages)\n\n=== Tabs ===\n\n* **Active**: View currently active configuration with version info\n* **Edit**: Modify configuration using form-based editors (JSON) or text editor (prompts)\n* **History**: Browse all saved versions with activation status\n* **Effective** (non-prompt): See resolved config (UCM is the source of truth; env overrides are disabled for analysis settings)\n\n=== Features ===\n\n==== Version Control ====\n\n* Every saved configuration is immutable and content-addressed (SHA-256 hash)\n* Full version history with timestamps and activation tracking\n* One-click rollback to any previous version\n\n==== Validation ====\n\n* Real-time JSON schema validation for search/calculation configs\n* Syntax highlighting and error markers for prompt editing\n* Pre-save validation prevents invalid configurations\n\n==== Export/Import ====\n\n* Export any configuration as JSON or ##.prompt.md## file\n* Import configurations from files with schema validation\n\n=== Environment Variables ===\n\nAnalysis settings (pipeline/search/calculation/SR) are managed in UCM. Environment variables are used\nonly for infrastructure and secrets (API keys, endpoints, cache paths). The **Effective** tab shows\nthe resolved config from UCM (no env overrides for analysis behavior).\n\n----\n\n== Configuration Test Dashboard ==\n\nNavigate to: ##http:~~/~~/localhost:3000/admin/test-config##\n\n== Security (POC note) ==\n\nThe admin test UI calls ##GET /api/admin/test-config##, which (currently) **does not require authentication** and can trigger **paid LLM/search API calls** if keys are configured.\n\n* Keep this endpoint **local-only** during POC development.\n* Before any public exposure, protect it with ##FH_ADMIN_KEY## (and add rate limiting/cost quotas).\n\n== Configuration Test Dashboard ==\n\nThe Configuration Test Dashboard allows you to verify that all required API keys and services are working correctly.\n\n=== Running Tests ===\n\n1. Click the **\"Run All Tests\"** button\n1. Wait for all tests to complete\n1. Review the results\n\n=== Test Results Summary ===\n\nAfter running tests, you'll see a summary showing:\n\n|= Metric |= Description\n| **Total Tests** | Total number of configuration tests run\n| **Passed** | Services that are properly configured and working\n| **Failed** | Services with invalid or broken configurations\n| **Not Configured** | Services missing required environment variables\n| **Skipped** | Services not selected as the current provider\n\n=== Test Statuses ===\n\nEach service test returns one of the following statuses:\n\n* **Success** - The service is properly configured and responding\n* **Error** - The API key is invalid, expired, or the service is unreachable\n* **Not Configured** - Required environment variables are missing\n* **Skipped** - The service is not selected as the active provider\n\n== Services Tested ==\n\n=== FactHarbor Core Services ===\n\n|= Service |= Environment Variables |= Description\n| FH API Base URL | ##FH_API_BASE_URL## | Tests connectivity to the FactHarbor API backend\n| FH Admin Key | ##FH_ADMIN_KEY## | Validates the admin authentication key\n| FH Internal Runner Key | ##FH_INTERNAL_RUNNER_KEY## | Validates the internal job runner key\n\n=== LLM Providers ===\n\nOnly the currently selected LLM provider (via the pipeline config ##llmProvider##) is actively tested. Others are marked as \"Skipped\".\n\n|= Provider |= Environment Variables |= Config URL\n| OpenAI | ##OPENAI_API_KEY## | [[https:~~/~~/platform.openai.com/api-keys>>https://platform.openai.com/api-keys]]\n| Anthropic | ##ANTHROPIC_API_KEY## | [[https:~~/~~/console.anthropic.com/settings/keys>>https://console.anthropic.com/settings/keys]]\n| Google Generative AI | ##GOOGLE_GENERATIVE_AI_API_KEY## | [[https:~~/~~/aistudio.google.com/app/apikey>>https://aistudio.google.com/app/apikey]]\n| Mistral AI | ##MISTRAL_API_KEY## | [[https:~~/~~/console.mistral.ai/api-keys>>https://console.mistral.ai/api-keys]]\n\n**Setting the LLM Provider:**\nUse **Admin  Config  Pipeline** and set ##llmProvider## (anthropic/openai/google/mistral).\n\n=== Search Providers ===\n\nSearch providers are tested when search is enabled in UCM and the provider is selected or auto-detection is enabled.\n\n|= Provider |= Environment Variables |= Config URL\n| SerpAPI | ##SERPAPI_API_KEY## | [[https:~~/~~/serpapi.com/manage-api-key>>https://serpapi.com/manage-api-key]]\n| Google Custom Search | ##GOOGLE_CSE_API_KEY##, ##GOOGLE_CSE_ID## | [[https:~~/~~/developers.google.com/custom-search/v1/introduction>>https://developers.google.com/custom-search/v1/introduction]]\n\n**Setting the Search Provider:**\nUse **Admin  Config  Web Search** and set ##enabled## + ##provider##.\n\n== Troubleshooting ==\n\n=== Common Issues ===\n\n==== API Key contains placeholder text ====\n\n**Problem:** The test shows \"contains placeholder text\" error.\n\n**Solution:** Replace placeholder values like ##PASTE_YOUR_KEY_HERE##, ##sk-...##, or ##AIza...## with actual API keys from the provider.\n\n==== Service not configured ====\n\n**Problem:** Required environment variables are missing.\n\n**Solution:**\n1. Add the required environment variables to ##apps/web/.env.local##\n1. Restart the development server (##npm run dev##)\n\n==== Service skipped ====\n\n**Problem:** The service shows as \"Skipped\".\n\n**Solution:** This is normal if you're using a different provider. Update pipeline ##llmProvider## or the UCM search provider if you want to use this service instead.\n\n==== Connection errors ====\n\n**Problem:** The service shows connection timeout or network errors.\n\n**Solution:**\n1. Check your internet connection\n1. Verify the API key is valid by visiting the provider's configuration URL\n1. Ensure no firewall or proxy is blocking the connection\n\n=== Environment Variables Reference ===\n\nCreate or edit ##apps/web/.env.local## with the following variables:\n\n{{code language=\"bash\"}}\n# FactHarbor Core\nFH_API_BASE_URL=http://localhost:8080\nFH_ADMIN_KEY=your-admin-key\nFH_INTERNAL_RUNNER_KEY=your-runner-key\n\n# LLM Provider API keys (provider selected in UCM pipeline config)\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGOOGLE_GENERATIVE_AI_API_KEY=AIza...\nMISTRAL_API_KEY=...\n\n# Search Provider keys (provider selected in UCM search config)\nSERPAPI_API_KEY=...\nGOOGLE_CSE_API_KEY=...\nGOOGLE_CSE_ID=...\n{{/code}}\n\n== API Endpoint ==\n\nThe admin test functionality is also available via API:\n\n{{code}}\nGET /api/admin/test-config\n{{/code}}\n\n**Response:**\n\n{{code language=\"json\"}}\n{\n  \"summary\": {\n    \"total\": 10,\n    \"success\": 5,\n    \"error\": 1,\n    \"not_configured\": 2,\n    \"skipped\": 2\n  },\n  \"results\": [\n    {\n      \"service\": \"Anthropic\",\n      \"status\": \"success\",\n      \"message\": \"Anthropic API key is valid\",\n      \"details\": \"Test response: OK\",\n      \"configUrl\": \"https://console.anthropic.com/settings/keys\"\n    }\n  ],\n  \"timestamp\": \"2026-01-03T12:00:00.000Z\"\n}\n{{/code}}\n\n== Security Notes ==\n\n* The admin pages should be protected in production environments\n* API keys are never displayed in the UI, only validation status\n* Test responses are minimal to avoid exposing sensitive data\n", "Product Development.DevOps.Subsystems and Components.LLM Configuration.Debate Role Configuration.WebHome": "= Debate Role Configuration =\n\n== Overview ==\n\nThis page explains how verdict debate roles are configured in FactHarbor.\n\nRole structure is fixed:\n* ##advocate##\n* ##selfConsistency##\n* ##challenger##\n* ##reconciler##\n* ##validation##\n\nBehavior is configurable per role (tier and provider) through UCM PipelineConfig.\n\n=== Recommended Starting Point (post-baseline) ===\n\nAfter baseline measurement, recommended first default candidate:\n* ##debateProfile = \"cross-provider\"##\n\nWhy this is the best starting point:\n* Introduces structural independence where it matters most (challenger role)\n* Keeps blast radius small (only challenger provider changes)\n* Preserves current debate flow and tier strategy\n* Directly supports C1/C16 risk reduction goals\n\nOperational note:\n* Requires both ##ANTHROPIC_API_KEY## and ##OPENAI_API_KEY##\n* If OpenAI credentials are missing, runtime falls back and emits ##debate_provider_fallback##\n\n----\n\n== How Debate Works (Diagram) ==\n\nThe verdict stage runs a fixed 5-step debate pattern per claim.\n\n{{mermaid}}\nflowchart LR\n    E[Evidence + ClaimBoundary] --> A[Step 1: Advocate Verdict]\n    A --> S[Step 2: Self-Consistency x3]\n    A --> C[Step 3: Challenger Critique]\n    S --> R[Step 4: Reconciliation]\n    C --> R\n    R --> V[Step 5: Validation]\n    V --> O[Final Claim Verdict]\n{{/mermaid}}\n\nRole-to-step mapping:\n* ##advocate## -> Step 1\n* ##selfConsistency## -> Step 2\n* ##challenger## -> Step 3\n* ##reconciler## -> Step 4\n* ##validation## -> Step 5\n\n----\n\n== Worked Example ==\n\n=== Example config ===\n\n{{code language=\"json\"}}\n{\n  \"llmProvider\": \"anthropic\",\n  \"debateProfile\": \"cross-provider\",\n  \"debateModelTiers\": {\n    \"challenger\": \"sonnet\",\n    \"validation\": \"haiku\"\n  }\n}\n{{/code}}\n\n=== What this executes as ===\n\nAssuming default model mappings:\n\n|= Role |= Tier |= Provider |= Sub-model used\n| ##advocate## | sonnet | anthropic (inherited) | ##claude-sonnet-4-5-20250929##\n| ##selfConsistency## | sonnet | anthropic (inherited) | ##claude-sonnet-4-5-20250929##\n| ##challenger## | sonnet | openai (from ##cross-provider##) | ##gpt-4.1##\n| ##reconciler## | sonnet | anthropic (inherited) | ##claude-sonnet-4-5-20250929##\n| ##validation## | haiku | anthropic (inherited) | ##claude-haiku-4-5-20251001##\n\nExecution order:\n1. Advocate generates initial verdict.\n1. Self-consistency runs 3 parallel verdict samples.\n1. Challenger critiques the advocate verdict.\n1. Reconciler merges advocate + self-consistency + challenger outputs.\n1. Validation runs grounding/direction checks and returns final verdict package.\n\n----\n\n== Where to Configure ==\n\nConfigure in Admin -> Config -> Pipeline:\n\n* ##debateProfile##\n* ##debateModelTiers##\n* ##debateModelProviders##\n* ##llmProvider## (global fallback provider)\n\nAll of these are runtime config values (no code change required).\n\n----\n\n== Resolution Precedence ==\n\nEffective role config is resolved in this order:\n\n1. Explicit per-role overrides:\n** ##debateModelTiers##\n** ##debateModelProviders##\n1. Profile preset:\n** ##debateProfile##\n1. Hardcoded defaults:\n** debate tiers: sonnet for debate roles, haiku for validation\n** providers: inherit global ##llmProvider##\n\n----\n\n== Built-in Profiles ==\n\n|= debateProfile |= Role Intent |= Notes\n| ##baseline## | All roles Anthropic, same-tier debate | Default behavior\n| ##tier-split## | Challenger on haiku tier (same provider) | Lower cost\n| ##cross-provider## | Challenger provider = OpenAI | Provider diversity\n| ##max-diversity## | Challenger = OpenAI, selfConsistency = Google | Maximum provider diversity\n\nExample:\n\n{{code language=\"json\"}}\n{\n  \"debateProfile\": \"cross-provider\"\n}\n{{/code}}\n\n----\n\n== Per-Role Override Example ==\n\nYou can use a profile and still override one role, or define the full role map explicitly.\n\n{{code language=\"json\"}}\n{\n  \"llmProvider\": \"anthropic\",\n  \"debateModelTiers\": {\n    \"advocate\": \"sonnet\",\n    \"selfConsistency\": \"sonnet\",\n    \"challenger\": \"haiku\",\n    \"reconciler\": \"sonnet\",\n    \"validation\": \"haiku\"\n  },\n  \"debateModelProviders\": {\n    \"advocate\": \"anthropic\",\n    \"selfConsistency\": \"google\",\n    \"challenger\": \"openai\",\n    \"reconciler\": \"anthropic\",\n    \"validation\": \"anthropic\"\n  }\n}\n{{/code}}\n\n----\n\n== Tier to Model Mapping ==\n\nFor verdict-stage role calls, tier is mapped to task routing:\n* ##sonnet## tier -> ##verdict## task model\n* ##haiku## tier -> ##understand## task model\n\nCurrent provider defaults:\n\n|= Provider |= sonnet tier |= haiku tier\n| Anthropic | ##modelVerdict## (default: ##claude-sonnet-4-5-20250929##) | ##modelUnderstand## (default: ##claude-haiku-4-5-20251001##)\n| OpenAI | ##gpt-4.1## | ##gpt-4.1-mini##\n| Google | ##gemini-2.5-pro## | ##gemini-2.5-flash##\n| Mistral | ##mistral-large-latest## | ##mistral-small-latest##\n\nNote: For Anthropic, effective model IDs depend on current PipelineConfig values of ##modelVerdict## and ##modelUnderstand##.\n\n----\n\n== Credential Fallback Behavior ==\n\nIf a role has provider override but that provider key is missing, runtime falls back to global ##llmProvider## and emits a warning:\n\n* ##debate_provider_fallback## in ##analysisWarnings##\n\nIf all 4 debate roles use the same tier and same provider intent, runtime emits:\n\n* ##all_same_debate_tier##\n\nBoth warnings are expected signals for calibration/governance review.\n\n----\n\n== Verification ==\n\n=== Quick verification (UI) ===\n\n1. Set PipelineConfig values in Admin -> Config -> Pipeline\n1. Run a job or calibration\n1. Inspect ##analysisWarnings## in result JSON for:\n** ##all_same_debate_tier##\n** ##debate_provider_fallback##\n\n=== Effective runtime dump (terminal) ===\n\n{{code language=\"powershell\"}}\ncd apps/web\nnpx tsx -e \"import { loadPipelineConfig, loadCalcConfig } from './src/lib/config-loader.ts'; import { buildVerdictStageConfig } from './src/lib/analyzer/claimboundary-pipeline.ts'; (async () => { const p = await loadPipelineConfig('default'); const c = await loadCalcConfig('default'); const v = buildVerdictStageConfig(p.config as any, c.config as any); console.log(JSON.stringify({ llmProvider: (p.config as any).llmProvider, debateProfile: (p.config as any).debateProfile ?? null, debateModelTiers: v.debateModelTiers, debateModelProviders: v.debateModelProviders }, null, 2)); })();\"\n{{/code}}\n\n----\n\n== Related ==\n\n* [[LLM Configuration Guide>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]]\n* [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]]\n* [[LLM Model Tiering (Diagram)>>FactHarbor.Product Development.Diagrams.LLM Model Tiering.WebHome]]\n", "Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome": "= LLM Configuration Guide =\n\n== Overview ==\n\nFactHarbor supports multiple LLM (Large Language Model) providers and search providers. This guide explains how to configure, optimize, and switch between providers to balance cost, performance, and quality.\n\n----\n\n== Table of Contents ==\n\n* [[Supported Providers>>||anchor=\"HSupportedProviders\"]]\n* [[Provider Selection>>||anchor=\"HProviderSelection\"]]\n* [[Debate Role Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.Debate Role Configuration.WebHome]]\n* [[Environment Configuration>>||anchor=\"HEnvironmentConfiguration\"]]\n* [[Provider-Specific Optimization>>||anchor=\"HProvider-SpecificOptimization\"]]\n* [[Search Provider Configuration>>||anchor=\"HSearchProviderConfiguration\"]]\n* [[Cost Optimization>>||anchor=\"HCostOptimization\"]]\n* [[Troubleshooting>>||anchor=\"HTroubleshooting\"]]\n\n----\n\n== Supported Providers ==\n\n=== LLM Providers ===\n\n|= Provider |= Models |= Best For |= Status\n| **Anthropic** | ##claude-sonnet-4-20250514## | Complex reasoning, analysis | Supported\n| **OpenAI** | ##gpt-4o## | General purpose | Supported\n| **Google** | ##gemini-1.5-pro## | Long context | Supported\n| **Mistral** | ##mistral-large-latest## | Cost-effective | Supported\n\n=== Search Providers ===\n\n|= Provider |= API |= Best For |= Status\n| **Google Custom Search (CSE)** | Free tier available | General web search | Supported\n| **SerpAPI** | Pay-per-use | Reliable Google results | Supported\n| **Gemini Grounded Search** | Built-in | When using Gemini | Experimental (requires ##pipeline.llmProvider=google## + search ##mode=grounded##; only counts as \"grounded\" when the provider returns grounding metadata/citations)\n| **Brave Search** | API required | Privacy-focused | Not implemented (no adapter in code)\n| **Tavily** | API required | AI-optimized | Not implemented (no adapter in code)\n| **Bing Search** | API required | General web search | Not implemented (no adapter in code)\n\n----\n\n== Provider Selection ==\n\n=== Setting the LLM Provider ===\n\n**Current (UCM):** LLM provider is configured via UCM pipeline config.\n\nTo change provider:\n1. Navigate to Admin  Config  Pipeline\n1. Update ##llmProvider## field (values: ##anthropic##, ##openai##, ##google##, ##mistral##)\n1. Save and activate\n\n**Deprecated:** The ##LLM_PROVIDER## environment variable is no longer used (removed 2026-02-02).\nIf present in your ##.env## file, it will be ignored.\n\n**Migration:** If you previously set ##LLM_PROVIDER=openai##, update your pipeline config to\n##\"llmProvider\": \"openai\"## via the admin UI.\n\n=== Debate Role Configuration (Verdict Stage) ===\n\nDebate roles are configured in PipelineConfig via:\n* ##debateProfile##\n* ##debateModelTiers##\n* ##debateModelProviders##\n\nSee the dedicated guide:\n[[Debate Role Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.Debate Role Configuration.WebHome]]\n\n=== Provider API Keys ===\n\n**Important:** Provider selection is configured in UCM (Admin  Config  Pipeline), but API keys remain environment-based for security.\n\n**Anthropic Claude** (Recommended):\n\n{{code language=\"bash\"}}\nANTHROPIC_API_KEY=sk-ant-...\n{{/code}}\n\n* Get key: [[https:~~/~~/console.anthropic.com/settings/keys>>https://console.anthropic.com/settings/keys]]\n* Legacy default model (tiering off): ##claude-sonnet-4-20250514##\n* Best for: Complex analysis, high-quality reasoning\n\n**OpenAI**:\n\n{{code language=\"bash\"}}\nOPENAI_API_KEY=sk-...\n{{/code}}\n\n* Get key: [[https:~~/~~/platform.openai.com/api-keys>>https://platform.openai.com/api-keys]]\n* Legacy default model (tiering off): ##gpt-4o##\n* Best for: General purpose, established ecosystem\n\n**Google Gemini**:\n\n{{code language=\"bash\"}}\nGOOGLE_GENERATIVE_AI_API_KEY=AIza...\n{{/code}}\n\n* Get key: [[https:~~/~~/aistudio.google.com/app/apikey>>https://aistudio.google.com/app/apikey]]\n* Legacy default model (tiering off): ##gemini-1.5-pro##\n* Best for: Long context, multimodal inputs\n\n**Mistral AI**:\n\n{{code language=\"bash\"}}\nMISTRAL_API_KEY=...\n{{/code}}\n\n* Get key: [[https:~~/~~/console.mistral.ai/api-keys>>https://console.mistral.ai/api-keys]]\n* Legacy default model (tiering off): ##mistral-large-latest##\n* Best for: Cost-conscious deployments\n\n----\n\n== Environment Configuration ==\n\n=== Complete ##.env.local## Example ===\n\nProvider selection and analysis behavior are configured in UCM (Admin  Config). The environment only\nstores API keys and infrastructure settings.\n\n{{code language=\"bash\"}}\n# API Keys (configure the providers you intend to use)\nANTHROPIC_API_KEY=sk-ant-your-key-here\n# OPENAI_API_KEY=sk-your-key-here\n# GOOGLE_GENERATIVE_AI_API_KEY=AIza-your-key-here\n# MISTRAL_API_KEY=your-key-here\n\n# Search API Keys (search provider selected in UCM)\nSERPAPI_API_KEY=your-serpapi-key\n# Or Google Custom Search:\n# GOOGLE_CSE_API_KEY=your-cse-key\n# GOOGLE_CSE_ID=your-cse-id\n{{/code}}\n\n=== Configuration Validation ===\n\nTest your configuration at: http://localhost:3000/admin/test-config\n\nThis admin interface will:\n* Validate all API keys\n* Test LLM provider connectivity\n* Test search provider connectivity\n* Show which services are active\n\n----\n\n== Provider-Specific Optimization ==\n\n> **Comprehensive Guide**: See [[Provider-Specific Formatting>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting.WebHome]] for detailed documentation on v2.8.0 prompt architecture.\n\n=== Overview (v2.8.0+) ===\n\nFactHarbor uses **provider-specific prompt variants** to optimize performance across different LLMs. Each provider has unique strengths and preferred prompt structures:\n\n|= Provider |= Format |= Strengths |= Optimizations\n| **Anthropic Claude** | XML-structured | Nuanced reasoning, boundary clustering | XML tags, thinking blocks, prefill technique\n| **OpenAI GPT-4** | Markdown | General purpose, schema adherence | Clear headings, numbered lists, code blocks\n| **Google Gemini** | Example-heavy | Long context, visual formatting | Emojis, bullets, repetition, multiple examples\n| **Mistral** | Formal academic | Cost-effective, bilingual | Explicit reasoning chains, French examples\n\n=== General Optimization Principles ===\n\n1. **Prompt Composition**:\n** Base prompt (universal logic)\n** Provider variant (format optimization)\n** Config adaptation (tiering, knowledge mode)\n1. **Temperature Settings**:\n** Use ##FH_DETERMINISTIC=true## for reproducible results (temperature = 0.0)\n** Override per-provider if needed via PromptConfig\n1. **Token Limits**:\n** Claude: 200k tokens\n** GPT-4: 128k tokens\n** Gemini: 2M tokens\n** Mistral: 128k tokens\n\n=== Configuration ===\n\nProvider-specific settings are configured via **PromptConfig** in the Unified Config Management system:\n\n{{code language=\"json\"}}\n{\n  \"provider\": \"anthropic\",\n  \"model\": \"claude-sonnet-4\",\n  \"temperature\": 0.2,\n  \"maxTokens\": 8000,\n  \"tier\": \"standard\"\n}\n{{/code}}\n\nSee: [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]] for profile management\n\n----\n\n== Search Provider Configuration ==\n\n=== Google Custom Search (Free Tier) ===\n\n1. **Get API Key**: [[https:~~/~~/developers.google.com/custom-search/v1/introduction>>https://developers.google.com/custom-search/v1/introduction]]\n1. **Create Custom Search Engine**: [[https:~~/~~/cse.google.com/cse/>>https://cse.google.com/cse/]]\n1. **Configure in UCM** (Admin  Config  Web Search):\n** ##enabled##: ##true##\n** ##provider##: ##google-cse##\n\n**Environment keys**:\n\n{{code language=\"bash\"}}\nGOOGLE_CSE_API_KEY=your-api-key\nGOOGLE_CSE_ID=your-cse-id\n{{/code}}\n\n**Limits**: 100 queries/day (free tier)\n\n=== SerpAPI (Pay-per-use) ===\n\n1. **Sign up**: [[https:~~/~~/serpapi.com>>https://serpapi.com]]\n1. **Get API Key**: [[https:~~/~~/serpapi.com/manage-api-key>>https://serpapi.com/manage-api-key]]\n1. **Configure in UCM** (Admin  Config  Web Search):\n** ##enabled##: ##true##\n** ##provider##: ##serpapi##\n\n**Environment key**:\n\n{{code language=\"bash\"}}\nSERPAPI_API_KEY=your-api-key\n{{/code}}\n\n**Pricing**: ~$0.002 per search\n\n=== Auto Mode (Recommended) ===\n\nLet FactHarbor choose the best available provider:\n\nSet the search ##provider## to ##auto## in UCM (Admin  Config  Web Search).\n\n**Behavior**:\n* Tries Google CSE first (if configured)\n* Falls back to SerpAPI for remaining slots\n* Uses Gemini Grounded Search when ##pipeline.llmProvider=google## and search ##mode=grounded## (experimental). If grounding metadata/citations are missing, FactHarbor should fall back to standard search rather than treating model synthesis as evidence.\n\n=== Domain Whitelist ===\n\nRestrict searches to trusted domains:\n\n{{code language=\"json\"}}\n{\n  \"domainWhitelist\": [\"who.int\", \"cdc.gov\", \"nih.gov\", \"nature.com\", \"science.org\"]\n}\n{{/code}}\n\nThis improves:\n* Source reliability\n* Result relevance\n* Analysis quality\n\n----\n\n== Cost Optimization ==\n\n=== Prompt Optimization Baseline (v2.8.0) ===\n\n**Automatic estimated 20-30% cost reduction** across all providers and models:\n\nAs of February 2026 (version 2.8.0), all prompts have been optimized to reduce token usage by an estimated 20-30% while maintaining quality. This optimization:\n* Applies automatically to all LLM providers (Anthropic, OpenAI, Google, Mistral)\n* Works with both tiered and non-tiered model configurations\n* Reduces costs by an estimated ~20-30% per analysis compared to previous versions\n* Maintains analytical quality and accuracy\n\n**No configuration required** - all users benefit from these savings automatically.\n\nSee Prompt Architecture v2.8.0 for technical details.\n\n----\n\n=== Multi-Tier Model Strategy ===\n\nUse cheaper models for simple tasks, premium models for complex reasoning:\n\n|= Task |= Recommended Model |= Cost Saving (vs Sonnet baseline)\n| Claim extraction | Claude Haiku | ~75% cheaper (70% model + estimated 20-30% prompt optimization)\n| Evidence extraction | Claude Haiku | ~75% cheaper (70% model + estimated 20-30% prompt optimization)\n| Understanding | Claude Haiku | ~75% cheaper (70% model + estimated 20-30% prompt optimization)\n| Verdict generation | Claude Sonnet | estimated 20-30% cheaper (prompt optimization only)\n\nNote: Prompt optimization savings (estimated 20-30%) apply to ALL models and providers automatically.\n\nTiered model routing is configured via the **pipeline config** in Unified Configuration Management (UCM). Adjust the tiering toggle and per-task model names in the pipeline config editor (Admin UI  Config  Pipeline).\n\nWhen enabled, FactHarbor routes models per pipeline task:\n* **Understand**: cheaper/faster model (provider default)\n* **Extract evidence**: cheaper/faster model (provider default)\n* **Verdict**: higher-quality model (provider default)\n\nDefault per-provider routing (unless overridden in pipeline config):\n* **Anthropic**: understand/extract  ##claude-3-5-haiku-20241022##, verdict  ##claude-sonnet-4-20250514##\n* **OpenAI**: understand/extract  ##gpt-4o-mini##, verdict  ##gpt-4o##\n* **Google**: understand/extract  ##gemini-1.5-flash##, verdict  ##gemini-1.5-pro##\n* **Mistral**: understand/extract  ##mistral-small-latest##, verdict  ##mistral-large-latest##\n\n==== Per-task model overrides (optional) ====\n\nOverride the model names per task via the pipeline config (UCM), not environment variables. See [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]] for editing pipeline profiles.\n\n=== Tuning analysis depth (implemented) ===\n\nFactHarbor's primary \"cost vs quality\" knob is still **analysis mode**; tiered model routing is optional and can further reduce cost by using cheaper models for extraction-style steps:\n\n{{code language=\"json\"}}\n{\n  \"analysisMode\": \"quick\"\n}\n{{/code}}\n\nThis affects limits like max iterations and max total sources (see ##apps/web/src/lib/analyzer/config.ts##).\n\n=== Cost Control Settings ===\n\n{{code language=\"json\"}}\n{\n  \"deterministic\": true,\n  \"allowModelKnowledge\": false\n}\n{{/code}}\n\n{{code language=\"json\"}}\n{\n  \"dateRestrict\": \"y\"\n}\n{{/code}}\n\n=== Cost Monitoring ===\n\nTrack costs via the admin dashboard (when implemented):\n* LLM token usage\n* Search API calls\n* Estimated monthly spend\n\n----\n\n== Troubleshooting ==\n\n=== LLM Provider Issues ===\n\n**\"Invalid API key\" error:**\n1. Verify key format:\n** Anthropic: starts with ##sk-ant-##\n** OpenAI: starts with ##sk-##\n** Google: starts with ##AIza##\n1. Test key at provider's console\n1. Check for spaces/quotes in ##.env.local##\n1. Restart web server after changes\n\n**\"Rate limit exceeded\":**\n* Check your API plan limits\n* Wait for rate limit reset\n* Consider upgrading to paid tier\n\n**\"Model not found\":**\n* Verify model name matches provider's API\n* Check if you have access to the model\n* Some models require special access approval\n\n=== Search Provider Issues ===\n\n**\"No search results\":**\n1. Verify search is enabled in UCM (Search config)\n1. Check search provider API key is valid\n1. Verify domain whitelist isn't too restrictive\n1. Check search provider status page\n\n**\"Quota exceeded\" (Google CSE):**\n* Free tier: 100 queries/day\n* Upgrade to paid tier or switch to SerpAPI\n* Use ##provider=auto## in UCM for automatic fallback\n\n**\"Search provider timeout\":**\n* Network connectivity issues\n* Provider service disruption\n* Try alternative provider\n\n**\"No sources were fetched\" (All pipelines including Dynamic):**\n\nBoth pipelines (Orchestrated and Monolithic Dynamic) require search provider credentials to perform web searches. Without them, the pipeline will:\n1. Generate search queries (correctly)\n1. Attempt searches (loop runs but returns empty)\n1. Continue without external sources (LLM uses only internal knowledge)\n\n**Verify search providers are configured:**\n\n{{code language=\"bash\"}}\n# Check logs for this message:\n[Search] NO SEARCH PROVIDERS CONFIGURED! Set SERPAPI_API_KEY or GOOGLE_CSE_API_KEY+GOOGLE_CSE_ID\n{{/code}}\n\n**Solution**: Configure at least one search provider:\n\n{{code language=\"bash\"}}\n# Option 1: SerpAPI (simpler setup)\nSERPAPI_API_KEY=your-serpapi-key\n\n# Option 2: Google CSE (free tier available)\nGOOGLE_CSE_API_KEY=your-google-api-key\nGOOGLE_CSE_ID=your-custom-search-engine-id\n{{/code}}\n\n**Verify configuration is working:**\nLook for successful search logs:\n\n{{code language=\"bash\"}}\n[Search] Available providers: Google CSE=true, SerpAPI=true\n[Search] Google CSE returned 4 results, total now: 4\n{{/code}}\n\n=== Configuration Validation ===\n\n**Test configuration without running analysis:**\n\nVisit: http://localhost:3000/admin/test-config\n\n**Manual API test:**\n\n{{code language=\"bash\"}}\n# Test Anthropic\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"content-type: application/json\" \\\n  -d '{\"model\":\"claude-sonnet-4\",\"max_tokens\":10,\"messages\":[{\"role\":\"user\",\"content\":\"Hi\"}]}'\n\n# Test OpenAI\ncurl https://api.openai.com/v1/chat/completions \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\":\"gpt-4\",\"messages\":[{\"role\":\"user\",\"content\":\"Hi\"}],\"max_tokens\":10}'\n{{/code}}\n\n----\n\n== Provider Comparison ==\n\n=== Quality vs Cost Trade-offs ===\n\n|= Provider |= Quality |= Speed |= Cost |= Best Use Case\n| **Claude Sonnet** | 5/5 | 4/5 | $$$ | Production, complex analysis\n| **Claude Haiku** | 3/5 | 5/5 | $ | Simple extraction tasks\n| **GPT-4** | 4/5 | 3/5 | $$$$ | High-quality general purpose\n| **Gemini Pro** | 4/5 | 4/5 | $$ | Long context, multimodal\n| **Mistral Large** | 3/5 | 4/5 | $$ | Cost-conscious deployments\n\n=== Recommended Provider by Use Case ===\n\n* **Production fact-checking**: Anthropic Claude Sonnet\n* **Development/testing**: Claude Haiku or Mistral\n* **Long articles (>10k words)**: Google Gemini\n* **Tight budget**: Mistral Large\n* **Maximum accuracy**: GPT-4 or Claude Sonnet\n\n----\n\n== Advanced Configuration ==\n\n=== Custom Model Selection ===\n\nFactHarbor supports task-tiered model routing. Configure tiering and per-task model overrides in the **pipeline config** (UCM) rather than environment variables. See [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]] for editing pipeline profiles.\n\n=== Fallback Configuration ===\n\nThe text-analysis pipeline is **LLM-only** and does not support heuristic or provider fallbacks. LLM provider selection and routing are controlled via the pipeline config.\n\n=== Knowledge Toggle ===\n\nControl whether the LLM can use background knowledge:\n\n{{code language=\"bash\"}}\nFH_ALLOW_MODEL_KNOWLEDGE=false  # Strict evidence-only mode (recommended)\nFH_ALLOW_MODEL_KNOWLEDGE=true   # Allow broader context\n{{/code}}\n\n> **Known Issue**: This toggle is **not fully respected** in all analysis phases. The Understanding step currently allows model knowledge regardless of this setting.\n\n----\n\n== Testing Recommendations ==\n\nWhen switching providers or configurations:\n\n1. **Run test analysis with known inputs**\n** Compare output quality\n** Check for parsing errors\n** Verify consistency\n1. **Test edge cases**\n** Long articles\n** Complex claims\n** Multi-language content\n1. **Monitor costs**\n** Track token usage\n** Compare pricing across providers\n** Optimize based on actual usage\n1. **Measure performance**\n** Response times\n** Success rates\n** Error frequency\n\n----\n\n== Getting Help ==\n\n=== Resources ===\n\n* **Anthropic Docs**: [[https:~~/~~/docs.anthropic.com>>https://docs.anthropic.com]]\n* **OpenAI Docs**: [[https:~~/~~/platform.openai.com/docs>>https://platform.openai.com/docs]]\n* **Google AI Docs**: [[https:~~/~~/ai.google.dev/docs>>https://ai.google.dev/docs]]\n* **Mistral Docs**: [[https:~~/~~/docs.mistral.ai>>https://docs.mistral.ai]]\n\n=== Support ===\n\n* Check ##apps/web/debug-analyzer.log## for detailed logs\n* Use admin test config to validate setup\n* Review provider status pages for outages\n* Search GitHub issues for similar problems\n\n----\n\n**Last Updated**: February 2, 2026\n", "Product Development.DevOps.Subsystems and Components.Source Reliability Export.WebHome": "= Source Reliability Export Guide =\n\n**Version**: 2.6.38\n**Last Updated**: January 26, 2026\n\n----\n\n== Overview ==\n\nThe Source Reliability admin page includes export functionality for source evaluation data. You can export individual source evaluations in multiple formats for documentation, review, or archival purposes.\n\n----\n\n== Accessing Export Features ==\n\n1. Navigate to ##/admin/source-reliability##\n1. Click the \"View\" button (eye icon) on any cached source entry\n1. The Source Evaluation Details modal opens with export buttons in the footer\n\n----\n\n== Export Formats ==\n\n=== Print / PDF ===\n\n* Opens browser's native print dialog\n* Can save as PDF from the print dialog\n* Optimized print layout hides navigation elements\n* Score badges preserve colors\n\n**Use case**: Quick printouts for meetings or filing\n\n=== HTML Export ===\n\n* Downloads a standalone HTML file\n* Embedded CSS for consistent appearance\n* All evaluation data included:\n** Domain and score with color-coded badge\n** Confidence level\n** Category and bias indicator\n** LLM reasoning\n** Evidence cited (parsed and formatted)\n** Evidence pack with source links\n** Model information and consensus status\n** Cache metadata\n\n**Filename format**: ##{domain}_source_eval_{YYYYMMDD_HHMMSS}.html##\n\n**Use case**: Sharing evaluations via email, archival\n\n=== Markdown Export ===\n\n* Downloads a formatted .md file\n* Tables for structured data (summary, models, cache info)\n* Blockquotes for LLM reasoning\n* Lists for evidence\n\n**Filename format**: ##{domain}_source_eval_{YYYYMMDD_HHMMSS}.md##\n\n**Use case**: Documentation, version control, wikis\n\n=== JSON Export ===\n\n* Downloads structured JSON data\n* Complete ##CachedScore## object\n* Parsed JSON fields (evidenceCited, evidencePack)\n* Export metadata included\n* Pretty-printed with 2-space indentation\n\n**Filename format**: ##{domain}_source_eval_{YYYYMMDD_HHMMSS}.json##\n\n**Use case**: Data analysis, API integration, backups\n\n----\n\n== Exported Data Fields ==\n\nAll export formats include:\n\n|= Field |= Description\n| Domain | The evaluated source domain\n| Score | Reliability score (0-100)\n| Confidence | LLM confidence in the evaluation\n| Category | Source category (news, academic, etc.)\n| Bias | Detected bias indicator\n| Reasoning | LLM's reasoning for the score\n| Evidence Cited | Specific facts the LLM used\n| Evidence Pack | Source URLs consulted\n| Primary Model | Main LLM used for evaluation\n| Secondary Model | Verification LLM (if multi-model enabled)\n| Consensus | Whether models agreed\n| Evaluated At | When the evaluation was performed\n| Expires At | When the cached evaluation expires\n\n----\n\n== Known Limitations ==\n\n1. Print functionality depends on browser's print dialog capabilities\n1. Score badge colors in print may vary based on browser's color handling\n1. Very long URLs in evidence pack may wrap in print view\n1. Export is limited to the currently selected entry (no bulk export)\n\n----\n\n== Related Documentation ==\n\n* [[Admin Interface>>FactHarbor.Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome]] - Overview of admin features\n* [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]] - Source reliability settings", "Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome": "= Unified Config Management (UCM) =\n\n== 1. Introduction ==\n\n=== What is UCM? ===\n\n**Unified Config Management (UCM)** is FactHarbor's system for managing analysis configurations through **reusable profiles**. Instead of tweaking settings every time you run an analysis, create named profiles for different scenarios.\n\n=== Why Use UCM? ===\n\n**Without UCM**:\n* Manually set search parameters, prompt templates, and calculation thresholds for each analysis\n* Inconsistent settings across analyses\n* No way to share configurations with team members\n\n**With UCM**:\n* Save configurations as named profiles (\"High-Stakes Fact-Checking\", \"Exploratory Research\")\n* Switch between profiles with one click\n* Import/export profiles for team sharing\n* Version control for configuration changes\n* Rollback to previous configurations easily\n\n=== Key Concepts ===\n\n**Profile**: A named configuration **per config type** (e.g., ##pipeline:default##, ##search:default##).\nThere is no single combined profile object; each config type is versioned independently.\n\n**Configuration Types**:\n* **PipelineConfig**  LLM provider selection, model routing, budgets, feature flags\n* **SearchConfig**  Web search behavior (provider, max results, timeouts)\n* **CalcConfig**  Calculation thresholds and verdict bands\n* **PromptConfig**  Prompt profiles (pipeline + text-analysis templates) stored in UCM\n* **Evidence Lexicon**  Evidence filtering patterns (UCM lexicon)\n* **Aggregation Lexicon**  Aggregation heuristics (UCM lexicon)\n* **Source Reliability (SR)**  SR service config (separate SR domain)\n\n**Default Profile**: Built-in profile used when no custom profile selected\n\n----\n\n== 2. Config Source Precedence (Alpha) ==\n\n=== 2.1 Config Loading Flow ===\n\nThe diagram below shows how FactHarbor loads, validates, and activates configurations at runtime:\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.UCM Config Precedence.WebHome\"/}}\n\n**Key Points**:\n* **Runtime Authority**: Database is the source of truth during execution\n* **Fresh DB Seeding**: On first startup, configs are loaded from JSON files\n* **Validation**: All configs are validated against Zod schemas before activation\n* **Fallback Chain**: Database  JSON files  Code constants\n* **Manual Updates**: Admin UI changes create new versions in the database\n* **Dev Mode**: Saving to files is restricted to development environments\n\n=== 2.2 Where Configs Live ===\n\n1. **Runtime Authority:** Database (active config in UCM)\n1. **Default Templates:** JSON files in ##apps/web/configs/##\n1. **Fallback:** Code constants in ##config-schemas.ts##\n\n=== 2.3 Update Scenarios ===\n\n|= Scenario |= Behavior |= Notes\n| Fresh DB | File  DB | On first startup, seed from files\n| File updated | **Manual reset required** | DB unchanged; admin must Reset to Default\n| DB updated via UI | File unchanged | Only affects DB; files stay as templates\n| Save-to-File invoked | DB  File | Overwrites file (dev mode only; not enabled in alpha)\n\n=== 2.4 Alpha Limitations ===\n\n* No automatic drift detection UI\n* No automatic merging of file updates\n* File changes require manual admin action\n\n=== 2.5 Save to File (Development Mode) ===\n\nIn development, you can save active DB configs back to default files:\n\n1. Navigate to Admin  Config  [config type]\n1. Make your changes and save to DB\n1. Click \"Save to File\" button (appears only in development)\n1. Preview with \"Preview Save to File\" before committing\n\n**Safety Features:**\n* Only works in ##NODE_ENV=development## or with ##FH_ALLOW_CONFIG_FILE_WRITE=true##\n* Creates ##.bak## backup before overwriting\n* Atomic writes (.tmp  rename)\n* Schema validation before write\n\n**Production:** Save-to-file is blocked by default in production for safety.\n\n----\n\n== 3. Configuration Profiles ==\n\n=== Profile Structure ===\n\nEach config type has its own profile:\n\n{{code language=\"typescript\"}}\ninterface ConfigProfile<T> {\n  configType: \"pipeline\" | \"search\" | \"calculation\" | \"prompt\" | \"sr\" | \"evidence-lexicon\" | \"aggregation-lexicon\";\n  profileKey: string;            // Usually \"default\"\n  content: T;                    // Config JSON or prompt content\n  createdAt: string;             // ISO 8601 timestamp\n  updatedAt: string;             // ISO 8601 timestamp\n}\n{{/code}}\n\n=== Profile List View ===\n\n**Location**: Navigate to ##/admin/config##\n\n**Features**:\n* View all available profiles\n* See which profile is active (highlighted)\n* Quick switch between profiles\n* Create new profile\n* Import/export profiles\n\n**Example Profiles**:\n\n|= Profile Name |= Description |= Use Case\n| **Default** | Standard balanced settings | General fact-checking\n| **High-Stakes** | Strict quality gates, conservative verdicts | Legal, medical, high-impact claims\n| **Exploratory** | Lenient filters, more evidence | Research, hypothesis generation\n| **Budget** | Minimal searches, token limits | Cost-sensitive analyses\n\n----\n\n== 4. Profile Management ==\n\n=== Creating a New Profile ===\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Click **\"Create New Profile\"** button\n1. Enter profile details:\n** **Name**: Descriptive name (e.g., \"Medical Claims Analysis\")\n** **Description**: Purpose and use case\n1. Configure settings:\n** **Search**: Provider, max results, timeouts, domain filters\n** **Prompts**: LLM provider, temperature, token limits\n** **Calculation**: Verdict bands, quality gates, evidence filters\n1. Click **\"Save Profile\"**\n1. Profile appears in profile list\n\n**Tips**:\n* Start from an existing profile (duplicate it first)\n* Use descriptive names that indicate the use case\n* Document any non-standard settings in the description\n\n=== Editing a Profile ===\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Click on profile name to open\n1. Modify settings in any of the config sections\n1. Click **\"Save Changes\"**\n1. System creates a new version (version history preserved)\n\n**Version Control**:\n* Each save creates a new version timestamp\n* Previous versions preserved for rollback\n* View version history via **\"History\"** button\n\n=== Duplicating a Profile ===\n\n**Use Case**: Create a variation of an existing profile\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Select profile to duplicate\n1. Click **\"Duplicate\"** button\n1. Enter new profile name\n1. System creates copy with suffix \" (Copy)\"\n1. Edit as needed\n\n=== Deleting a Profile ===\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Select profile to delete\n1. Click **\"Delete\"** button\n1. Confirm deletion\n\n**Restrictions**:\n* Cannot delete the Default profile\n* Cannot delete active profile (switch to another first)\n* Deletion is permanent (no undo)\n\n=== Importing a Profile ===\n\n**Use Case**: Load a profile shared by a team member or from backup\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Click **\"Import Profile\"** button\n1. Select profile JSON file\n1. System validates profile structure\n1. If valid, profile appears in list\n1. If invalid, error message shows what's wrong\n\n**Profile JSON Format**:\n\n{{code language=\"json\"}}\n{\n  \"name\": \"Medical Claims Analysis\",\n  \"description\": \"Strict settings for medical fact-checking\",\n  \"searchConfig\": { \"...\" : \"...\" },\n  \"promptConfig\": { \"...\" : \"...\" },\n  \"calcConfig\": { \"...\" : \"...\" }\n}\n{{/code}}\n\n=== Exporting a Profile ===\n\n**Use Case**: Share profile with team members or create backup\n\n**Steps**:\n1. Navigate to ##/admin/config##\n1. Select profile to export\n1. Click **\"Export\"** button\n1. System downloads JSON file (e.g., ##profile-medical-claims.json##)\n1. Share file via email, Slack, or version control\n\n**Best Practices**:\n* Export after major configuration changes (backup)\n* Include version number in filename (e.g., ##profile-medical-v2.json##)\n* Store exported profiles in version control (Git)\n\n----\n\n== 5. Configuration Types ==\n\n=== 5.1 SearchConfig ===\n\n**Purpose**: Controls web search behavior\n\n**Key Settings**:\n\n|= Setting |= Description |= Default |= Range\n| **provider** | Search provider (auto/google/bing/serper) | ##auto## | \n| **mode** | Search mode (standard/deep/quick) | ##standard## | \n| **maxResults** | Max search results per query | ##6## | 120\n| **maxSourcesPerIteration** | Max sources to fetch per iteration | ##4## | 110\n| **timeoutMs** | Search timeout in milliseconds | ##12000## | 100060000\n| **dateRestrict** | Filter by date (e.g., \"m6\" for 6 months) | ##null## | \n| **domainWhitelist** | Only search these domains (empty = all) | ##[]## | \n| **domainBlacklist** | Exclude these domains | ##[]## | \n\n**Example SearchConfig**:\n\n{{code language=\"json\"}}\n{\n  \"provider\": \"google\",\n  \"mode\": \"deep\",\n  \"maxResults\": 12,\n  \"maxSourcesPerIteration\": 6,\n  \"timeoutMs\": 20000,\n  \"dateRestrict\": \"y1\",\n  \"domainWhitelist\": [\"nih.gov\", \"cdc.gov\", \"nature.com\"],\n  \"domainBlacklist\": []\n}\n{{/code}}\n\n**Use Case**: Medical claims requiring peer-reviewed sources only\n\n=== 5.2 PromptConfig ===\n\n**Purpose**: Stores prompt text templates by profile. Runtime behavior fields (provider, model routing, budget controls) are in ##PipelineConfig##.\n\n**Supported Prompt Profiles**:\n\n|= Profile Key |= Used By |= Default File Path\n| ##orchestrated## | Orchestrated analysis pipeline | ##apps/web/prompts/orchestrated.prompt.md##\n| ##monolithic-dynamic## | Monolithic dynamic pipeline | ##apps/web/prompts/monolithic-dynamic.prompt.md##\n| ##source-reliability## | Source reliability evaluator | ##apps/web/prompts/source-reliability.prompt.md##\n| ##text-analysis-input## | Input classification stage | ##apps/web/prompts/text-analysis/text-analysis-input.prompt.md##\n| ##text-analysis-evidence## | Evidence-quality stage | ##apps/web/prompts/text-analysis/text-analysis-evidence.prompt.md##\n| ##text-analysis-context## | Context similarity stage | ##apps/web/prompts/text-analysis/text-analysis-context.prompt.md##\n| ##text-analysis-verdict## | Verdict-validation stage | ##apps/web/prompts/text-analysis/text-analysis-verdict.prompt.md##\n\n**Operational Notes**:\n* Prompt import/export/reseed APIs validate profile keys against the canonical allowlist.\n* For ##text-analysis-*## profiles, frontmatter pipeline is validated as ##text-analysis##.\n* Prompt text that influences analysis should be managed via UCM prompt profiles (not hardcoded in runtime code).\n\n=== 5.3 CalcConfig ===\n\n**Purpose**: Controls verdict calculation, evidence filtering, and aggregation\n\n==== Verdict Bands ====\n\n{{code language=\"typescript\"}}\nverdictBands: {\n  true: [86, 100],          // 86-100% confidence = \"True\"\n  mostlyTrue: [72, 85],     // 72-85% = \"Mostly True\"\n  leaningTrue: [58, 71],    // 58-71% = \"Leaning True\"\n  mixed: [43, 57],          // 43-57% = \"Mixed\"\n  leaningFalse: [29, 42],   // 29-42% = \"Leaning False\"\n  mostlyFalse: [15, 28],    // 15-28% = \"Mostly False\"\n  false: [0, 14],           // 0-14% = \"False\"\n}\n{{/code}}\n\n==== Aggregation Weights ====\n\n{{code language=\"typescript\"}}\naggregation: {\n  centralityWeights: { high: 3.0, medium: 2.0, low: 1.0 },\n  harmPotentialMultiplier: 1.5,\n  contestationWeights: { established: 0.3, disputed: 0.5, opinion: 1.0 },\n}\n{{/code}}\n\n==== probativeValue Weights ====\n\n{{code language=\"typescript\"}}\nprobativeValueWeights: {\n  high: 1.0,    // Well-attributed, specific evidence (100% weight)\n  medium: 0.8,  // Moderately specific evidence (80% weight)\n  low: 0.5,     // Vague evidence (50% weight, usually filtered)\n}\n{{/code}}\n\n==== sourceType Calibration ====\n\n{{code language=\"typescript\"}}\nsourceTypeCalibration: {\n  peer_reviewed_study: 1.0,    // Academic research (baseline)\n  fact_check_report: 1.05,     // Fact-checking orgs (+5% boost)\n  government_report: 1.0,      // Official gov sources (baseline)\n  legal_document: 1.0,         // Court rulings (baseline)\n  news_primary: 1.0,           // Primary journalism (baseline)\n  news_secondary: 0.95,        // News aggregation (-5%)\n  expert_statement: 0.9,       // Expert opinions (-10%)\n  organization_report: 0.95,   // NGO/think tank reports (-5%)\n  other: 0.8,                  // Unclassified sources (-20%)\n}\n{{/code}}\n\n==== Evidence Filter Rules ====\n\n{{code language=\"typescript\"}}\nevidenceFilter: {\n  minStatementLength: 20,           // Min characters for evidence statement\n  maxVaguePhraseCount: 2,           // Max vague phrases allowed\n  requireSourceExcerpt: true,       // Must have source excerpt\n  minExcerptLength: 30,             // Min excerpt characters\n  requireSourceUrl: true,           // Must have source URL\n  deduplicationThreshold: 0.85,     // Jaccard similarity (0-1)\n}\n{{/code}}\n\n**Use Cases**:\n* **High-Stakes**: Lower thresholds, stricter filters\n* **Exploratory**: Higher thresholds, lenient filters\n* **Budget**: Moderate settings, balance cost vs quality\n\n=== 5.4 PipelineConfig ===\n\n**Purpose**: Controls LLM provider selection, model routing, budgets, feature flags, and **claim filtering behavior**.\n\n**Key Settings**:\n\n|= Setting |= Description |= Default |= Range\n| **llmProvider** | Default LLM provider | ##anthropic## | anthropic/openai/google\n| **llmModel** | Default model (null = provider default) | ##null## | \n| **llmTemperature** | LLM temperature | ##0.2## | 0.01.0\n| **llmTiering** | Enable model tiering | ##false## | \n| **searchEnabled** | Enable search step | ##true## | \n\n==== Additional Admin-Exposed Fields (Current UI) ====\n\n|= Setting |= Description |= Default\n| **understandMaxChars** | Max input chars sent to UNDERSTAND | ##12000##\n| **understandLlmTimeoutMs** | LLM timeout for UNDERSTAND stage | ##600000##\n| **extractEvidenceLlmTimeoutMs** | LLM timeout for EXTRACT_EVIDENCE stage | ##300000##\n| **temporalPromptContractTemplate** | Temporal contract template injected into prompts | UCM default text\n| **temporalPromptKnowledgeRuleAllowed** | Knowledge rule when model knowledge is allowed | UCM default text\n| **temporalPromptKnowledgeRuleEvidenceOnly** | Knowledge rule for evidence-only runs | UCM default text\n| **temporalPromptRecencyRuleSensitive** | Recency rule for time-sensitive claims | UCM default text\n| **temporalPromptRecencyRuleGeneral** | Recency rule for general claims | UCM default text\n| **temporalPromptNoFreshEvidenceRule** | Guardrail when fresh evidence is missing | UCM default text\n| **temporalPromptRelativeTimeRule** | Relative-time wording guardrail | UCM default text\n| **gapResearchEnabled** | Enable post-pass gap-driven research | ##true##\n| **gapResearchMaxIterations** | Max gap-research iterations | ##2##\n| **gapResearchMaxQueries** | Max gap-research queries | ##8##\n\n==== Claim Filtering Options ====\n\nThese options control the multi-layer claim filtering system:\n\n|= Setting |= Description |= Default |= Range\n| **thesisRelevanceValidationEnabled** | Validate thesis relevance classifications | ##true## | \n| **thesisRelevanceLowConfidenceThreshold** | Flag low-confidence relevance (warn) | ##70## | 0100\n| **thesisRelevanceAutoDowngradeThreshold** | Auto-downgrade directtangential | ##60## | 0100\n| **minEvidenceForTangential** | Min evidence required for tangential claims | ##2## | 010\n| **tangentialEvidenceQualityCheckEnabled** | Extra quality check for tangential | ##false## | \n| **maxOpinionFactors** | Max opinion keyFactors (0=unlimited) | ##0## | 020\n| **opinionAccumulationWarningThreshold** | Warn if opinion % exceeds this | ##70## | 0100\n\n**Example PipelineConfig (Claim Filtering)**:\n\n{{code language=\"json\"}}\n{\n  \"thesisRelevanceValidationEnabled\": true,\n  \"thesisRelevanceLowConfidenceThreshold\": 70,\n  \"thesisRelevanceAutoDowngradeThreshold\": 60,\n  \"minEvidenceForTangential\": 2,\n  \"tangentialEvidenceQualityCheckEnabled\": false,\n  \"maxOpinionFactors\": 0,\n  \"opinionAccumulationWarningThreshold\": 70\n}\n{{/code}}\n\n**Behavior**:\n\n1. **Thesis Relevance Validation**: When enabled, validates LLM relevance classifications:\n** If confidence < ##thesisRelevanceLowConfidenceThreshold## (70): logs warning\n** If confidence < ##thesisRelevanceAutoDowngradeThreshold## (60): auto-downgrades \"direct\" to \"tangential\"\n1. **Tangential Claim Pruning**: Claims marked \"tangential\" require ##minEvidenceForTangential## (2) supporting facts to appear in the final report. This prevents baseless tangential claims from diluting the analysis.\n1. **Opinion Monitoring**: Tracks opinion-based keyFactors:\n** If ##maxOpinionFactors## > 0: prunes excess opinion factors\n** If opinion % exceeds ##opinionAccumulationWarningThreshold##: logs warning\n\n**Use Cases**:\n* **Strict Fact-Checking**: Set ##minEvidenceForTangential: 3##, ##thesisRelevanceAutoDowngradeThreshold: 70##\n* **Opinion-Tolerant**: Set ##maxOpinionFactors: 5##, ##opinionAccumulationWarningThreshold: 90##\n* **Development/Debug**: Set ##tangentialEvidenceQualityCheckEnabled: true## for extra validation\n\n----\n\n== 6. Common Use Cases ==\n\n=== 6.1 High-Stakes Fact-Checking ===\n\n**Scenario**: Verifying claims that could impact policy, health, or legal decisions\n\n**Profile Configuration**:\n\n**SearchConfig**:\n* Provider: ##google## (highest quality)\n* Mode: ##deep## (exhaustive search)\n* maxResults: ##12## (gather more evidence)\n* domainWhitelist: Trusted sources only (nih.gov, gov, .edu, nature.com, etc.)\n\n**PipelineConfig**:\n* llmProvider: ##anthropic## (strong reasoning on complex claims)\n* modelVerdict: high-quality reasoning model\n* deterministic: ##true## (reproducible outputs)\n* allowModelKnowledge: ##false## (evidence-first)\n\n**CalcConfig**:\n* Verdict bands: Conservative (shift thresholds higher)\n* probativeValueWeights: Strict  reduce medium weight to 0.7, low to 0.3\n* evidenceFilter: Strict  minStatementLength 30, maxVaguePhraseCount 1, minExcerptLength 50, deduplicationThreshold 0.90\n\n**Result**: Conservative verdicts with high confidence thresholds\n\n=== 6.2 Exploratory Research ===\n\n**Scenario**: Exploring a topic to generate hypotheses, not making definitive verdicts\n\n**Profile Configuration**:\n\n**SearchConfig**:\n* Provider: ##auto## (any available)\n* Mode: ##standard##\n* maxResults: ##10##\n* domainWhitelist: ##[]## (no restrictions)\n\n**PipelineConfig**:\n* llmProvider: ##google## (or preferred exploratory provider)\n* analysisMode: ##deep##\n* allowModelKnowledge: ##true## (broader context exploration)\n* llmTiering: ##true## (cost-balanced)\n\n**CalcConfig**:\n* Verdict bands: Standard (default)\n* probativeValueWeights: Lenient  medium 0.9, low 0.6\n* evidenceFilter: Lenient  minStatementLength 15, maxVaguePhraseCount 3, requireSourceExcerpt false, deduplicationThreshold 0.80\n\n**Result**: More evidence collected, broader perspective, exploratory verdicts\n\n=== 6.3 Budget-Constrained Analysis ===\n\n**Scenario**: Minimize API costs while maintaining acceptable quality\n\n**Profile Configuration**:\n\n**SearchConfig**:\n* Provider: ##bing## (lower cost than Google)\n* Mode: ##quick##\n* maxResults: ##4## (minimal)\n* maxSourcesPerIteration: ##2##\n* timeoutMs: ##8000## (shorter timeout)\n\n**PipelineConfig**:\n* llmTiering: ##true## (cheaper models for simpler stages)\n* modelExtractEvidence: lower-cost extraction model\n* maxTotalTokens: reduced budget cap\n* maxTokensPerCall: reduced per-call cap\n\n**CalcConfig**:\n* Verdict bands: Standard\n* probativeValueWeights: Standard (default)\n* evidenceFilter: Standard (default)\n\n**Result**: Lower cost (~40% reduction), acceptable quality for non-critical analyses\n\n=== 6.4 Domain-Specific Analyses ===\n\n**Scenario**: Legal claim analysis requiring case law and statutes\n\n**Profile Configuration**:\n\n**SearchConfig**:\n* domainWhitelist: ##[\"supremecourt.gov\", \"law.cornell.edu\", \"justia.com\", \"courtlistener.com\"]##\n* maxResults: ##8##\n* dateRestrict: ##null## (include historical cases)\n\n**PipelineConfig + Prompt Profiles**:\n* llmProvider: ##anthropic##\n* modelVerdict: high-quality reasoning model\n* Customize domain instructions in UCM prompt profile (usually ##prompt:orchestrated##)\n\n**CalcConfig**:\n* sourceTypeCalibration: Boost legal sources  legal_document 1.10, government_report 1.05, other 0.6\n* evidenceFilter: Require citations\n\n**Result**: Legal-focused evidence with proper citations\n\n----\n\n== 7. Troubleshooting ==\n\n=== Profile Not Saving ===\n\n**Symptoms**: Click \"Save\", but changes don't persist\n\n**Causes**:\n1. **Invalid JSON**: Config contains syntax errors\n1. **Missing required fields**: Essential fields (name, description) empty\n1. **Schema validation failure**: Values outside allowed ranges\n\n**Solutions**:\n1. Check browser console for validation errors (F12  Console tab)\n1. Verify all required fields have values\n1. Check value ranges: maxResults 120, contextDedupThreshold 0.51.0, timeoutMs 100060000\n\n=== Profile Import Fails ===\n\n**Symptoms**: Error message when importing profile JSON\n\n**Causes**:\n1. **Malformed JSON**: Syntax error in file\n1. **Incompatible version**: Profile from older/newer FactHarbor version\n1. **Missing fields**: Required fields not present\n\n**Solutions**:\n1. Validate JSON using an online validator\n1. Check FactHarbor version compatibility\n1. Compare with exported profile from current version to identify missing fields\n\n=== Analysis Using Wrong Profile ===\n\n**Symptoms**: Analysis results don't match expected profile settings\n\n**Causes**:\n1. **Profile not set as active**: Default profile still selected\n1. **Cache issue**: Browser cached old profile\n1. **Job queue used old profile**: Job created before profile switch\n\n**Solutions**:\n1. Verify active profile highlighted in profile list\n1. Hard refresh browser (Ctrl+Shift+R)\n1. Create new analysis job (don't reuse old job)\n\n=== Evidence Filter Too Strict ===\n\n**Symptoms**: Most evidence filtered out, verdicts unreliable due to insufficient evidence\n\n**Indicators**:\n* Filter stats show >50% filtered\n* FP rate >20% (filtering valid evidence)\n* Verdict confidence very low\n\n**Solutions**:\n1. Check evidenceFilter settings:\n** Increase ##minStatementLength## if filtering valid short statements\n** Increase ##maxVaguePhraseCount## if filtering contextually valid evidence\n** Decrease ##deduplicationThreshold## if filtering distinct evidence\n1. Review filter logs to identify most common filter reasons\n1. Consider using \"Exploratory\" profile template as starting point\n\n=== Evidence Filter Too Lenient ===\n\n**Symptoms**: Low-quality evidence passing through, verdicts unreliable\n\n**Indicators**:\n* Evidence contains many vague phrases\n* sourceExcerpts are LLM-generated (\"Based on...\")\n* Duplicate evidence items\n* FP rate <5% (filter not catching enough)\n\n**Solutions**:\n1. Check evidenceFilter settings:\n** Decrease ##minStatementLength## if allowing too-short statements\n** Decrease ##maxVaguePhraseCount## to catch vague evidence\n** Increase ##deduplicationThreshold## for more aggressive dedup\n1. Enable ##requireSourceExcerpt## and ##requireSourceUrl## if disabled\n1. Consider using \"High-Stakes\" profile template as starting point\n\n----\n\n== 8. Admin Tools (v2.10) ==\n\n=== 8.1 Active Config Dashboard ===\n\n**Location**: Top of ##/admin/config## page\n\n**Purpose**: Visual overview of all currently active configurations at a glance.\n\n**Features**:\n* Color-coded cards for each config type (Search, Calculation, Pipeline, Prompts, SR)\n* Shows version label for each active config\n* Displays activation timestamp\n* Grouped by profile key\n\n**Use Case**: Quickly understand current system state before making changes.\n\n=== 8.2 Config Search by Hash ===\n\n**Location**: Top of ##/admin/config## page\n\n**Purpose**: Find configs by content hash for debugging and traceability.\n\n**How to Use**:\n1. Enter full or partial hash (minimum 4 characters)\n1. Press Enter or click \"Search\"\n1. Results show matching configs across all types\n1. Click result to navigate directly to that config version\n\n**Use Case**: When debugging a job report that shows a config hash, quickly find the exact config that was used.\n\n=== 8.3 Version Comparison (Diff View) ===\n\n**Location**: History tab in ##/admin/config##\n\n**Purpose**: Compare any two config versions side-by-side.\n\n**How to Use**:\n1. Go to History tab\n1. Check boxes next to two versions to compare\n1. Click \"Compare Selected\"\n1. Diff view shows:\n** JSON configs: Field-by-field changes with added/removed/modified indicators\n** Prompts: Side-by-side text comparison\n\n**Color Coding**:\n* Green: Added fields/lines\n* Red: Removed fields/lines\n* Yellow: Modified fields\n\n**Use Case**: Understand what changed between versions when reviewing history.\n\n=== 8.4 Default Value Indicators ===\n\n**Location**: Active Config view (when viewing active config for JSON types)\n\n**Purpose**: Identify which settings are customized from defaults.\n\n**Visual Indicators**:\n* **Green banner**: \"Using default configuration\"  no customizations\n* **Yellow banner**: Shows count of customized fields with percentage\n\n**Expandable Details**: Click to see exact field paths that differ from defaults.\n\n**Use Case**: Quickly identify if a config has been customized and which fields changed.\n\n=== 8.5 Export All Configs ===\n\n**Location**: ##/admin## page (main admin dashboard)\n\n**Purpose**: Create complete backup of all active configurations.\n\n**How to Use**:\n1. Navigate to Admin Dashboard\n1. Click \"Export All Configurations\"\n1. Downloads JSON file with all configs\n\n**Export Format**:\n\n{{code language=\"json\"}}\n{\n  \"exportedAt\": \"2026-01-31T14:30:00Z\",\n  \"analyzerVersion\": \"2.10.0\",\n  \"schemaVersion\": \"1.0\",\n  \"configs\": {\n    \"pipeline\": { \"default\": {} },\n    \"search\": { \"default\": {} },\n    \"calculation\": { \"default\": {} },\n    \"prompt\": { \"default\": {} }\n  }\n}\n{{/code}}\n\n**Use Cases**:\n* Disaster recovery backup\n* Environment migration\n* Version control of configurations\n\n=== 8.6 Toast Notifications ===\n\n**Behavior**: All admin operations now use professional toast notifications instead of browser alerts.\n\n**Toast Types**:\n* **Success** (green): Operation completed successfully\n* **Error** (red): Operation failed  includes error details\n* **Info** (default): Informational feedback\n\n**Features**:\n* Non-blocking (can continue working while notification shows)\n* Auto-dismiss after 35 seconds\n* Errors persist until dismissed\n* Multiple toasts stack properly\n\n----\n\n== Appendix A: Default Profile Settings ==\n\n=== Default SearchConfig ===\n\n{{code language=\"json\"}}\n{\n  \"enabled\": true,\n  \"provider\": \"auto\",\n  \"mode\": \"standard\",\n  \"maxResults\": 6,\n  \"maxSourcesPerIteration\": 4,\n  \"timeoutMs\": 12000,\n  \"dateRestrict\": null,\n  \"domainWhitelist\": [],\n  \"domainBlacklist\": []\n}\n{{/code}}\n\n=== Default Prompt Profiles ===\n\nPrompt defaults are seeded from markdown files, one profile per prompt domain:\n\n|= Profile Key |= Default File\n| ##orchestrated## | ##apps/web/prompts/orchestrated.prompt.md##\n| ##monolithic-dynamic## | ##apps/web/prompts/monolithic-dynamic.prompt.md##\n| ##source-reliability## | ##apps/web/prompts/source-reliability.prompt.md##\n| ##text-analysis-input## | ##apps/web/prompts/text-analysis/text-analysis-input.prompt.md##\n| ##text-analysis-evidence## | ##apps/web/prompts/text-analysis/text-analysis-evidence.prompt.md##\n| ##text-analysis-context## | ##apps/web/prompts/text-analysis/text-analysis-context.prompt.md##\n| ##text-analysis-verdict## | ##apps/web/prompts/text-analysis/text-analysis-verdict.prompt.md##\n\n=== Default CalcConfig ===\n\n{{code language=\"json\"}}\n{\n  \"verdictBands\": {\n    \"true\": [86, 100],\n    \"mostlyTrue\": [72, 85],\n    \"leaningTrue\": [58, 71],\n    \"mixed\": [43, 57],\n    \"leaningFalse\": [29, 42],\n    \"mostlyFalse\": [15, 28],\n    \"false\": [0, 14]\n  },\n  \"aggregation\": {\n    \"centralityWeights\": { \"high\": 3.0, \"medium\": 2.0, \"low\": 1.0 },\n    \"harmPotentialMultiplier\": 1.5,\n    \"contestationWeights\": { \"established\": 0.3, \"disputed\": 0.5, \"opinion\": 1.0 }\n  },\n  \"probativeValueWeights\": {\n    \"high\": 1.0,\n    \"medium\": 0.8,\n    \"low\": 0.5\n  },\n  \"sourceTypeCalibration\": {\n    \"peer_reviewed_study\": 1.0,\n    \"fact_check_report\": 1.05,\n    \"government_report\": 1.0,\n    \"legal_document\": 1.0,\n    \"news_primary\": 1.0,\n    \"news_secondary\": 0.95,\n    \"expert_statement\": 0.9,\n    \"organization_report\": 0.95,\n    \"other\": 0.8\n  },\n  \"evidenceFilter\": {\n    \"minStatementLength\": 20,\n    \"maxVaguePhraseCount\": 2,\n    \"requireSourceExcerpt\": true,\n    \"minExcerptLength\": 30,\n    \"requireSourceUrl\": true,\n    \"deduplicationThreshold\": 0.85\n  }\n}\n{{/code}}\n\n----\n\n== Related ==\n\n* [[Admin Interface>>FactHarbor.Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome]]  Admin panel and GUI mechanics\n* [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]]  Provider configuration and cost optimization\n* [[Getting Started>>FactHarbor.Product Development.DevOps.Guidelines.Getting Started.WebHome]]  Installation and first-run setup\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture and key diagrams\n", "Product Development.DevOps.Subsystems and Components.WebHome": "= Subsystems and Components =\n\nGuides for using and configuring FactHarbor's key subsystems.\n\n== Administration ==\n\n* [[Admin Interface>>FactHarbor.Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome]]  Admin panel and management interface\n* [[Unified Config Management>>FactHarbor.Product Development.DevOps.Subsystems and Components.Unified Config Management.WebHome]]  Configuration profiles, versioning, and runtime settings\n* [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]]  Configuring LLM providers and models\n\n== Data Export ==\n\n* [[Source Reliability Export>>FactHarbor.Product Development.DevOps.Subsystems and Components.Source Reliability Export.WebHome]]  Exporting source reliability data\n\n== Related ==\n\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture and key diagrams\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]  Glossary of terms\n", "Product Development.DevOps.Tooling.1st Run Checklist.WebHome": "= 1st Run Checklist =\n\n== Purpose ==\n\nThis checklist is designed to ensure**POC1 runs end-to-end on the first attempt**. It focuses on the most common real-world failure points.\n\n----\n\n== Phase 1  Repository Setup ==\n\n* Clone the Git repository\n* Open:\n** API in Visual Studio 2022\n** Web app in Cursor or VS Code\n\n**Hint:**Do not change configuration yet.\n\n----\n\n== Phase 2  Environment Configuration ==\n\n=== Web (apps/web) ===\n\n* Copy.env.example.env.local\n* Set:\n** OPENAI_API_KEY\n** VerifyFH_API_BASE_URLpoints to the local API\n\n=== API (apps/api) ===\n\n* Copyappsettings.Development.example.jsonappsettings.Development.json\n* Verify:\n** Admin:KeymatchesFH_ADMIN_KEY\n** Runner:RunnerKeymatchesFH_INTERNAL_RUNNER_KEY\n** Db:Provideris set tosqlite\n\n**Important Hint:**\nKey mismatches are the most common cause of failure.\n\n----\n\n== Phase 3  Database Initialization ==\n\nRun:\n\n{{{cd apps/api\ndotnet ef database update\n}}}\n\nExpected result:\n\n* factharbor.dbfile is created\n* No errors reported\n\n----\n\n== Phase 4  Start Services ==\n\nRecommended:\n\n{{{powershell -ExecutionPolicy Bypass -File scripts/first-run.ps1\n}}}\n\nThis:\n\n* installs dependencies\n* applies migrations\n* starts API\n* starts Web UI\n\n----\n\n== Phase 5  Health Verification ==\n\nCheck:\n\n* API:http:~/~/localhost:5000/health\n* Web:http:~/~/localhost:3000/api/health\n\nExpected:\n\n* HTTP 200\n* ok: true\n\nDo not proceed until both are healthy.\n\n----\n\n== Phase 6  First Analysis ==\n\nSteps:\n\n1. Open the Web UI\n1. Paste a short text (avoid URL first)\n1. Click**Run Analysis**\n1. Observe progress\n1. Verify result is visible as:\n1*. Report\n1*. JSON\n\n----\n\n== Common Failure Patterns ==\n\n|=Symptom|=Likely Cause\n|Job stuck in QUEUED|Runner key mismatch\n|Job fails immediately|Missing OpenAI key\n|No progress updates|Admin key mismatch\n|API not starting|Database migrations missing\n\n----\n\n== Definition of Success ==\n\n* Analysis completes successfully\n* Refreshing the job page still shows results\n* /jobslists previous runs\n\nAt this point,**POC1 is operational**.", "Product Development.DevOps.Tooling.Documentation Viewer.WebHome": "= Documentation Viewer =\n\nFactHarbor documentation is written in xWiki 2.1 syntax and stored as ##.xwiki## files under ##Docs/xwiki-pages/FactHarbor/##. Three viewers let you browse the rendered documentation without an xWiki server.\n\n----\n\n== Table of Contents ==\n\n* [[Viewing Options>>||anchor=\"HViewingOptions\"]]\n* [[GitHub Pages (Online)>>||anchor=\"HGitHubPages28Online29\"]]\n* [[Local Browser Viewer>>||anchor=\"HLocalBrowserViewer\"]]\n* [[VS Code Extension>>||anchor=\"HVSCodeExtension\"]]\n* [[Viewer Features>>||anchor=\"HViewerFeatures\"]]\n* [[Custom Navigation Order (_sort files)>>||anchor=\"HCustomNavigationOrder28_sortfiles29\"]]\n* [[Rebuilding GitHub Pages>>||anchor=\"HRebuildingGitHubPages\"]]\n\n----\n\n== Viewing Options ==\n\n|= Method |= When to Use |= Requirements\n| **GitHub Pages** | Browse documentation online, share links | Web browser only\n| **Local Viewer** | Full-featured editing and preview | Git clone, Chrome/Edge\n| **VS Code Extension** | In-editor preview while coding | VS Code with extension installed\n\n----\n\n== GitHub Pages (Online) ==\n\nThe simplest way to read documentation. No installation needed.\n\n**URL:** [[https:~~/~~/robertschaub.github.io/FactHarbor/>>https://robertschaub.github.io/FactHarbor/]]\n\n=== Features ===\n\n* Full rendered documentation with tree navigation\n* Page search (filter by name)\n* Wiki link resolution and click-through navigation\n* Include macros rendered inline\n* Mermaid diagrams\n* Deep linking  share URLs to specific pages (e.g. ##~#Organisation.Governance.WebHome##)\n\n=== Deep Linking ===\n\nEvery page has a unique hash-based URL. When you navigate to a page, the browser URL updates automatically. Copy and share the URL to link directly to that page.\n\nExamples:\n* ##https:~~/~~/robertschaub.github.io/FactHarbor/~#Organisation.Governance.WebHome##\n* ##https:~~/~~/robertschaub.github.io/FactHarbor/~#Product Development.Specification.Architecture.WebHome##\n\n=== Limitations ===\n\n* **Read-only**  no source editing or file watching\n* **Snapshot**  reflects the last deployment, not live changes\n* **No source view**  only the rendered preview is available\n\n----\n\n== Local Browser Viewer ==\n\nFull-featured viewer with source editing, split view, and live file watching.\n\n=== Quick Start ===\n\nFrom the project root, run:\n\n{{code language=\"bash\"}}\nDocs\\xwiki-pages\\View.cmd\n{{/code}}\n\nThis starts a local HTTP server on ##localhost:8471## and opens Chrome (or Edge) with the folder picker. Select the ##FactHarbor## folder to load all pages.\n\n=== VS Code Tasks ===\n\nThe project's ##.vscode/tasks.json## includes viewer tasks. Use **Terminal > Run Task**:\n\n* **XWiki Viewer: Open**  starts server and opens Chrome\n* **XWiki Viewer: Open (Edge)**  starts server and opens Edge\n* **XWiki Viewer: Quick Open (no server)**  opens via ##file:~~/~~/## directly\n\n=== PowerShell Options ===\n\n{{code language=\"powershell\"}}\n# Default: Chrome on port 8471\n.\\Docs\\xwiki-pages\\viewer-impl\\Open-XWikiViewer.ps1\n\n# Custom port and browser\n.\\Docs\\xwiki-pages\\viewer-impl\\Open-XWikiViewer.ps1 -Port 9000 -Browser edge\n\n# Skip auto folder picker\n.\\Docs\\xwiki-pages\\viewer-impl\\Open-XWikiViewer.ps1 -NoAutoOpen\n{{/code}}\n\n=== Features ===\n\n* **View modes**: Preview (default), Split (source + preview side-by-side), Source\n* **Source editing**: Edit xWiki source directly in the browser, with ##Ctrl+Enter## to re-render\n* **File watching**: Detects when files change on disk and reloads automatically (requires File System Access API  Chrome or Edge)\n* **Tree navigation**: Sidebar with expandable folders, click to open pages\n* **Page search**: Filter pages by name in the sidebar search box\n* **Wiki links**: ##~[~[label>>Page~]~]## links resolve and navigate between pages\n* **Includes**: ##~{~{include reference=\"...\"~}~}## renders the included page inline with a header\n* **Mermaid diagrams**: ##~{~{mermaid~}~}...~{~{/mermaid~}~}## renders as SVG diagrams\n* **Back/Forward**: Browser-style navigation history between pages\n* **Breadcrumbs**: Show the current page path for orientation\n\n----\n\n== VS Code Extension ==\n\nPreview xWiki pages directly in VS Code, side-by-side with the source.\n\n=== Installation ===\n\nThe extension is located at ##tools/vscode-xwiki-preview/##. To install:\n\n{{code language=\"bash\"}}\ncd tools/vscode-xwiki-preview\nnpm install\nnpm run compile\n{{/code}}\n\nThen in VS Code, use **Run > Start Debugging** (F5) to launch an Extension Development Host with the extension loaded.\n\n=== Usage ===\n\n1. Open any ##.xwiki## file in VS Code\n1. The **XWiki Pages** tree appears in the sidebar (Explorer view)\n1. Click any page in the tree to open it\n1. The preview panel opens automatically beside the source\n\n=== Features ===\n\n* **Live preview**: Updates as you type\n* **Tree sidebar**: XWiki Pages tree view in the Explorer panel\n* **Wiki link resolution**: Links shown as resolved (solid underline) or unresolved (wavy underline)\n* **Include rendering**: Included pages are fetched and rendered inline\n* **Click navigation**: Click wiki links or tree items to navigate\n* **Mermaid diagrams**: Rendered in the preview panel\n\n----\n\n== Viewer Features ==\n\n=== Feature Comparison ===\n\n|= Feature |= GitHub Pages |= Local Viewer |= VS Code\n| Rendered preview | Yes | Yes | Yes\n| Source editing | No | Yes | Yes (native editor)\n| Split view | No | Yes | Yes (side-by-side)\n| File watching | No | Yes | Yes (automatic)\n| Tree navigation | Yes | Yes | Yes\n| Page search | Yes | Yes | VS Code search\n| Wiki links | Yes | Yes | Yes\n| Includes | Yes | Yes | Yes\n| Mermaid diagrams | Yes | Yes | Yes\n| Deep linking | Yes (hash URLs) | No | No\n| Offline use | No | Yes | Yes\n| Custom sort order | Yes | Yes | Yes\n\n=== Wiki Links ===\n\nWiki links follow xWiki syntax:\n\n{{code language=\"none\"}}\n[[Display Text>>Space.Page.WebHome]]\n[[Simple Link>>PageName]]\n{{/code}}\n\nIn all viewers, links are colour-coded:\n* **Resolved** (solid underline)  the target page exists; click to navigate\n* **Unresolved** (wavy underline, amber)  the target page was not found in the current tree\n\n=== Include Macros ===\n\nThe ##~{~{include~}~}## macro pulls content from another page and renders it inline:\n\n{{code language=\"none\"}}\n{{include reference=\"Organisation.Governance.WebHome\"/}}\n{{/code}}\n\nIncluded content appears in a bordered box with a header showing the source page name. Click the header to navigate to the original page. Includes resolve up to 5 levels deep; circular includes are detected and shown as an error.\n\n=== Mermaid Diagrams ===\n\nWrap Mermaid syntax in the ##~{~{mermaid~}~}## macro:\n\n{{code language=\"none\"}}\n{{mermaid}}\ngraph TD\n  A[Start] --> B{Decision}\n  B -->|Yes| C[Action]\n  B -->|No| D[End]\n{{/mermaid}}\n{{/code}}\n\nAll three viewers render Mermaid diagrams as interactive SVG.\n\n----\n\n== Custom Navigation Order (_sort files) ==\n\nBy default, folders appear before files and everything is sorted alphabetically. To define a custom order, place a ##_sort## file in any directory.\n\n=== Format ===\n\n{{code language=\"none\"}}\n# Lines starting with # are comments\nOrganisation\nProduct Development\nLicense and Disclaimer\n{{/code}}\n\n**Rules:**\n* One name per line  folder name or page filename without the ##.xwiki## extension\n* Items listed appear **first**, in the order given\n* Unlisted items follow after, sorted alphabetically\n* Folders always appear before files within each group\n* Empty lines and lines starting with ##~### are ignored\n\n=== Example ===\n\nThe top-level ##_sort## file at ##Docs/xwiki-pages/FactHarbor/_sort## sets this order:\n\n{{code language=\"none\"}}\n# Top-level navigation order\nOrganisation\nProduct Development\nLicense and Disclaimer\n{{/code}}\n\nWithout this file, the order would be alphabetical: License and Disclaimer, Organisation, Product Development. With the ##_sort## file, Organisation appears first, followed by Product Development, then License and Disclaimer.\n\n=== Where _sort Files Work ===\n\n* **GitHub Pages**  the build script reads ##_sort## files and bakes the order into ##pages.json##\n* **Local viewer**  reads ##_sort## files at runtime via the File System Access API\n* **VS Code extension**  reads ##_sort## files via the VS Code filesystem API\n\nYou can place a ##_sort## file in any subdirectory. Each directory manages its own order independently.\n\n=== Current _sort Files ===\n\n|= Directory |= Purpose\n| ##FactHarbor/## | Top-level section order\n| ##FactHarbor/Organisation/## | Organisation sub-pages order\n| ##FactHarbor/Product Development/## | Product Development section order\n| ##FactHarbor/Product Development/Specification/## | Specification sub-pages order\n| ##FactHarbor/Product Development/DevOps/## | DevOps sub-pages order\n\nTo add custom ordering to a new directory, create a ##_sort## text file listing the desired order.\n\n----\n\n== Rebuilding GitHub Pages ==\n\nAfter editing documentation, rebuild and deploy the GitHub Pages site:\n\n=== Build ===\n\n{{code language=\"bash\"}}\npython Docs/xwiki-pages/scripts/build_ghpages.py\n{{/code}}\n\nThis generates ##gh-pages-build/index.html## and ##gh-pages-build/pages.json##.\n\n=== Deploy ===\n\n{{code language=\"bash\"}}\n# Stash any uncommitted changes\ngit stash\n\n# Switch to gh-pages branch\ngit checkout gh-pages\n\n# Copy build output\ncopy gh-pages-build\\index.html .\ncopy gh-pages-build\\pages.json .\n\n# Commit and push\ngit add index.html pages.json .nojekyll\ngit commit -m \"docs: update gh-pages\"\ngit push origin gh-pages\n\n# Switch back\ngit checkout main\ngit stash pop\n{{/code}}\n\nThe site updates within a few minutes at [[https:~~/~~/robertschaub.github.io/FactHarbor/>>https://robertschaub.github.io/FactHarbor/]].\n\n=== Build Options ===\n\n{{code language=\"bash\"}}\n# Custom content directory\npython Docs/xwiki-pages/scripts/build_ghpages.py --content-dir path/to/pages\n\n# Custom output directory\npython Docs/xwiki-pages/scripts/build_ghpages.py -o my-output-dir\n\n# Custom viewer template\npython Docs/xwiki-pages/scripts/build_ghpages.py --viewer path/to/xwiki-viewer.html\n{{/code}}\n\n----\n\n**Last Updated**: February 8, 2026\n", "Product Development.DevOps.Tooling.Installation Checklist.WebHome": "= Installation Checklist =\n\n== Purpose ==\n\nThis checklist ensures a predictable and low-friction setup. Follow the phases in order and stop once your current phase works.\n\n----\n\n== Phase 0  Prerequisites ==\n\n* Windows 10 or 11\n* Administrator rights for installation\n\n----\n\n== Phase 1  Minimal POC1 Setup ==\n\n=== Required Tools ===\n\n==== Version Control ====\n\n* Git for Windows\n** Verify:git ~-~-version\n\n==== Runtime & Build ====\n\n* Node.js (LTS)\n** Verify:node -v,npm -v\n* .NET SDK 8.x\n** Verify:dotnet ~-~-info\n\n==== Editors ====\n\n* Visual Studio 2022\n** Workload: ASP.NET and web development\n* Cursor or VS Code\n** Extensions:\n*** TypeScript\n*** ESLint\n*** EditorConfig\n\n----\n\n== Phase 1  Recommended Additions ==\n\n=== Debugging & Inspection ===\n\n* Browser with DevTools (Chrome or Edge)\n* API client (Postman or Insomnia)\n\n=== Database Inspection ===\n\n* Optional: SQLite browser (read-only inspection)\n\n----\n\n== Phase 2  Deferred (Do Not Install Yet) ==\n\n* Docker Desktop\n* PostgreSQL\n* Azure or Vercel CLIs\n\n----\n\n== Hints to Avoid Problems ==\n\n* Avoid installing tools that are not required for local execution\n* Use one editor per role (VS2022 for API, Cursor/VS Code for Web)\n* Avoid preview or nightly runtime versions\n\n----\n\n== Verification ==\n\nRun once after installation:\n\n{{{git --version\nnode -v\nnpm -v\ndotnet --info\n}}}\n\nAll commands must succeed before continuing.", "Product Development.DevOps.Tooling.Promptfoo Testing.WebHome": "= Promptfoo Testing Guide for FactHarbor =\n\nThis guide explains how to use [[Promptfoo>>https://promptfoo.dev]] to test and evaluate FactHarbor's LLM prompts.\n\n== Overview ==\n\nPromptfoo is an open-source tool for testing LLM prompts. We use it to:\n\n* **Compare providers** - Test the same prompt across Claude, GPT-4o, etc.\n* **Validate behavior** - Ensure prompts produce correct outputs (score caps, ratings, etc.)\n* **Catch regressions** - Detect when prompt changes break expected behavior\n* **Optimize costs** - Compare quality vs. cost across different models\n\n== Installation ==\n\nPromptfoo is already installed as a dev dependency. If you need to reinstall:\n\n{{code language=\"bash\"}}\ncd apps/web\nnpm install promptfoo --save-dev\n{{/code}}\n\n== Prerequisites ==\n\n1. **API Keys** - You need valid API keys in ##apps/web/.env.local##:\n\n{{code language=\"bash\"}}\n# Copy from example if needed\ncp apps/web/.env.example apps/web/.env.local\n\n# Edit and add your keys\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\n{{/code}}\n\n1. **Verify setup**:\n\n{{code language=\"bash\"}}\nnpm run promptfoo:list\n{{/code}}\n\nYou should see available test configurations.\n\n== Running Tests ==\n\n=== From Repository Root ===\n\n{{code language=\"bash\"}}\n# Run source reliability tests (default)\nnpm run promptfoo:sr\n\n# Run verdict generation tests\nnpm run promptfoo:verdict\n\n# Run all prompt tests\nnpm run promptfoo:all\n\n# List available test configs\nnpm run promptfoo:list\n\n# View results in browser\nnpm run promptfoo:view\n{{/code}}\n\n=== Using the Script Directly ===\n\n{{code language=\"bash\"}}\nnode scripts/promptfoo-sr.js sr          # Source reliability\nnode scripts/promptfoo-sr.js verdict     # Verdict generation\nnode scripts/promptfoo-sr.js all         # All tests\nnode scripts/promptfoo-sr.js --list      # List configs\nnode scripts/promptfoo-sr.js sr --view   # Run then open viewer\n{{/code}}\n\n=== From apps/web Directory ===\n\n{{code language=\"bash\"}}\ncd apps/web\nnpm run promptfoo:sr\nnpm run promptfoo:view\n{{/code}}\n\n== Available Test Configurations ==\n\n|= Config |= Command |= Description |= Test Cases\n| ##sr## | ##npm run promptfoo:sr## | Source Reliability evaluation prompts | 7\n| ##verdict## | ##npm run promptfoo:verdict## | Verdict generation prompts | 5\n| ##text-analysis## | ##npm run promptfoo:text-analysis## | LLM Text Analysis Pipeline prompts (v2.9) | 26\n\n**Total Test Coverage**: 38 test cases across 6 prompts\n\n== Understanding Results ==\n\n=== Terminal Output ===\n\nAfter running tests, you'll see a table showing:\n\n* **Vars** - Input variables for each test case\n* **Provider outputs** - What each model returned\n* **Pass/Fail** - Whether assertions passed\n\n=== Web Viewer ===\n\nRun ##npm run promptfoo:view## to open an interactive dashboard showing:\n\n* Test results by provider\n* Pass rates and metrics\n* Full model outputs\n* Cost breakdown\n\n=== Results Files ===\n\nRaw results are saved to ##apps/web/prompts/promptfoo-results/##:\n\n{{code}}\nprompts/promptfoo-results/\n source-reliability.json   # Source reliability test results\n verdict.json              # Verdict generation test results\n text-analysis.json        # Text analysis pipeline test results\n{{/code}}\n\n== Test Configuration Files ==\n\n=== Location ===\n\n{{code}}\napps/web/\n promptfooconfig.yaml                      # Verdict tests\n promptfooconfig.source-reliability.yaml   # Source reliability tests\n promptfooconfig.text-analysis.yaml        # Text analysis pipeline tests (26 cases)\n prompts/promptfoo/\n     verdict-prompt.txt\n     source-reliability-prompt.txt\n     text-analysis-input-prompt.txt        # Input classification tests\n     text-analysis-evidence-prompt.txt     # Evidence quality tests\n     text-analysis-context-prompt.txt      # Context similarity tests\n     text-analysis-verdict-prompt.txt      # Verdict validation tests\n{{/code}}\n\n=== Config Structure ===\n\n{{code language=\"yaml\"}}\ndescription: \"Test description\"\n\n# Prompt template file\nprompts:\n  - file://prompts/promptfoo/my-prompt.txt\n\n# LLM providers to test\nproviders:\n  - id: anthropic:claude-3-5-haiku-20241022\n    label: Claude Haiku\n    config:\n      temperature: 0\n\n  - id: openai:gpt-4o\n    label: GPT-4o\n    config:\n      temperature: 0\n\n# Test cases\ntests:\n  - description: \"Test case description\"\n    vars:\n      domain: \"example.com\"\n      currentDate: \"2026-01-25\"\n      evidencePack: |\n        [E1] Evidence item 1\n        [E2] Evidence item 2\n    assert:\n      - type: is-json\n      - type: javascript\n        value: |\n          const data = JSON.parse(output);\n          return data.score >= 0.5;\n        metric: score_check\n{{/code}}\n\n== Adding New Test Cases ==\n\nEdit the relevant config file and add a new test under ##tests:##:\n\n{{code language=\"yaml\"}}\ntests:\n  # ... existing tests ...\n\n  - description: \"My new test case\"\n    vars:\n      domain: \"new-source.example\"\n      currentDate: \"2026-01-25\"\n      evidencePack: |\n        [E1] First evidence item\n        [E2] Second evidence item\n    assert:\n      - type: is-json\n      - type: javascript\n        value: |\n          const data = JSON.parse(output);\n          return data.score !== null && data.score >= 0.72;\n        metric: expected_reliable\n{{/code}}\n\n=== Assertion Types ===\n\n|= Type |= Description\n| ##is-json## | Output must be valid JSON\n| ##contains## | Output contains a string\n| ##javascript## | Custom JS function returns true\n\n=== JavaScript Assertions ===\n\n{{code language=\"yaml\"}}\n- type: javascript\n  value: |\n    const data = JSON.parse(output);\n    // Your validation logic\n    return data.score >= 0.5 && data.score <= 1.0;\n  metric: my_metric_name  # Shows in results\n{{/code}}\n\n== Adding New Prompt Configurations ==\n\n=== 1. Create the prompt template ===\n\n{{code language=\"bash\"}}\n# Create new prompt file\napps/web/prompts/promptfoo/my-new-prompt.txt\n{{/code}}\n\n=== 2. Create the config file ===\n\n{{code language=\"bash\"}}\n# Create new config\napps/web/promptfooconfig.my-feature.yaml\n{{/code}}\n\n=== 3. Register in the script ===\n\nEdit ##scripts/promptfoo-sr.js## and add to ##CONFIGS##:\n\n{{code language=\"javascript\"}}\nconst CONFIGS = {\n  'sr': { ... },\n  'verdict': { ... },\n  'myfeature': {\n    name: 'My Feature',\n    file: 'promptfooconfig.my-feature.yaml',\n    description: 'Tests my feature prompts'\n  }\n};\n{{/code}}\n\n=== 4. Add npm script (optional) ===\n\nEdit root ##package.json##:\n\n{{code language=\"json\"}}\n{\n  \"scripts\": {\n    \"promptfoo:myfeature\": \"node scripts/promptfoo-sr.js myfeature\"\n  }\n}\n{{/code}}\n\n== Source Reliability Test Cases ==\n\nThe source reliability tests (##promptfooconfig.source-reliability.yaml##) validate:\n\n|= Test |= What It Checks\n| Reliable Wire Service | Score 0.72-1.0, rating ##reliable##/##highly_reliable##\n| Unreliable Source | Score <=0.42, rating in unreliable category\n| State-Controlled Media | ##sourceType## = ##state_controlled_media##, score <=0.42\n| Propaganda Outlet | ##sourceType## = ##propaganda_outlet##, score <=0.14\n| Insufficient Data | ##score## = null, ##factualRating## = ##insufficient_data##\n| Biased but Reliable | Bias noted but score >=0.58 (not penalized)\n| Evidence Citation | All claims cite evidence IDs (E1, E2, etc.)\n\n== Text Analysis Pipeline Test Cases ==\n\nThe text analysis tests (##promptfooconfig.text-analysis.yaml##) validate the LLM Text Analysis Pipeline (v2.9) across four analysis points:\n\n=== Input Classification Tests (8 cases) ===\n\n|= Test |= What It Checks\n| Simple Factual Claim | ##isComparative=false##, ##isCompound=false##, ##claimType=factual##, ##complexity=simple##\n| Comparative Claim | ##isComparative=true## - detects \"more efficient than\" pattern\n| Compound (semicolon) | ##isCompound=true##, multiple ##decomposedClaims##\n| Compound (and) | Splits independent predicates joined by \"and\"\n| Non-compound (and) | Single predicate with multiple subjects NOT split\n| Evaluative Claim | ##claimType=evaluative## for opinions/judgments\n| Predictive Claim | ##claimType=predictive## for future predictions\n| Complex Claim | ##complexity=complex## for multi-context claims\n\n=== Evidence Quality Tests (5 cases) ===\n\n|= Test |= What It Checks\n| High Quality Stats | Specific numbers, sources  ##qualityAssessment=high##\n| Vague Attribution | \"Some say\", \"many believe\"  ##qualityAssessment=low/filter##\n| Medium Quality | Partial criteria met  ##qualityAssessment=medium##\n| Irrelevant Evidence | Off-topic evidence  ##qualityAssessment=filter##\n| Expert Quote Check | Named expert + credentials  ##high##; anonymous  ##low/filter##\n\n=== Context Similarity Tests (5 cases) ===\n\n|= Test |= What It Checks\n| Duplicate Contexts | Same entity  ##similarity>=0.85##, ##shouldMerge=true##\n| Different Jurisdictions | US vs EU  ##similarity<0.85##, ##shouldMerge=false##\n| Production Phase | Manufacturing contexts  ##phaseBucket=production##\n| Usage Phase | Operating contexts  ##phaseBucket=usage##\n| Mixed Phases | Production vs Usage  different phase buckets, no merge\n\n=== Verdict Validation Tests (8 cases) ===\n\n|= Test |= What It Checks\n| Inversion Detection | Reasoning contradicts verdict  ##isInverted=true##, correction suggested\n| Non-Inverted | Reasoning matches verdict  ##isInverted=false##\n| High Harm (death) | Death/injury keywords  ##harmPotential=high##\n| Medium Harm | Standard claims  ##harmPotential=medium##\n| Contested (evidence) | Documented counter-evidence  ##isContested=true##, ##factualBasis=established/disputed##\n| Opinion Only | Political criticism  ##factualBasis=opinion## or ##isContested=false##\n| Causal Contestation | Methodology criticism on causal claim  ##factualBasis=established##\n| Fraud High Harm | Fraud accusations  ##harmPotential=high##\n\n=== Configuration Note ===\n\nText analysis is **LLM-only** and no longer uses per-point env feature flags or heuristic fallback. Prompt tests always target the LLM text-analysis prompts configured in Unified Config Management.\n\n== Troubleshooting ==\n\n=== \"API key not set\" ===\n\nEnsure your ##.env.local## has valid API keys:\n\n{{code language=\"bash\"}}\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\n{{/code}}\n\n=== \"Config file not found\" ===\n\nRun from the repository root, not from ##apps/web##:\n\n{{code language=\"bash\"}}\ncd /path/to/FactHarbor\nnpm run promptfoo:sr\n{{/code}}\n\n=== JSON Parse Errors ===\n\nIf models wrap output in markdown code blocks, the config includes a transform to strip them. If you still see errors, check the prompt instructs \"no markdown\".\n\n=== Tests Failing on Confidence ===\n\nThe confidence calculation is mechanistic. If tests fail on confidence:\n1. Check ##independentAssessmentsCount## in evidence\n1. Verify evidence is marked as recent\n1. Consider lowering the threshold in assertions\n\n== Cost Considerations ==\n\nTypical costs per 100 source reliability evaluations:\n\n|= Provider |= Cost/100 calls\n| Claude Haiku + GPT-4o | ~$0.96\n| Claude Haiku + GPT-4o-mini | ~$0.32\n| Claude Haiku only | ~$0.28\n\nSet SR config ##openaiModel## in UCM (Admin  Config  Source Reliability) to switch between ##gpt-4o## (quality) and ##gpt-4o-mini## (cost savings).\n\n== Best Practices ==\n\n1. **Run before merging** - Run ##npm run promptfoo:all## before merging prompt changes\n1. **Add regression tests** - When fixing a bug, add a test case that would have caught it\n1. **Use descriptive metrics** - Name your assertion metrics clearly (e.g., ##score_capped_correctly##)\n1. **Keep evidence realistic** - Use evidence that mirrors real fact-checker outputs\n1. **Avoid hardcoded dates** - Use relative terms like \"last 6 months\" instead of \"2025\"\n\n== Further Reading ==\n\n* [[Promptfoo Documentation>>https://promptfoo.dev/docs/intro]]\n* [[Assertion Reference>>https://promptfoo.dev/docs/configuration/expected-outputs]]\n* [[Provider Configuration>>https://promptfoo.dev/docs/providers]]\n", "Product Development.DevOps.Tooling.Source Reliability Bundle.WebHome": "= Source Reliability Bundle (Archived) =\n\n{{warning}}\n**This page is archived.** The static Source Reliability Bundle approach was replaced by the **LLM-powered Source Reliability Service** in v2.6.\n\nSee: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]\n{{/warning}}\n\n== What Changed ==\n\n|= Aspect |= Old (Bundle) |= New (Service)\n| **Data Source** | Pre-seeded MBFC JSON file | LLM-powered on-demand evaluation\n| **Storage** | Static JSON file (##source-bundle.json##) | SQLite cache (##source-reliability.db##, 90-day TTL)\n| **Configuration** | ##FH_SOURCE_BUNDLE_PATH## env var | UCM SR config in database\n| **Evaluation** | Static pre-computed scores | Sequential refinement (Claude + OpenAI cross-check)\n| **Coverage** | ~50-9,500 pre-seeded sources | Any source, evaluated on first encounter\n| **Multi-language** | N/A | Automatic language detection + regional fact-checkers\n\n== Why ==\n\n* Static bundles quickly become stale\n* Limited to pre-seeded sources only\n* No cross-verification or confidence scoring\n* Could not handle sources not in the bundle\n\nThe new service evaluates any source on-demand, caches results, and provides confidence scores with multi-model cross-checking.\n", "Product Development.DevOps.Tooling.Tools Decisions.WebHome": "= Tools Decisions =\n\n== Purpose ==\n\nThis page documents the deliberate tool choices for FactHarbor from**POC1 to Beta**. The guiding principles are:\n\n* minimal setup and cognitive load for POC1\n* no rewrites when moving to Beta\n* clear, incremental upgrade paths\n\n----\n\n== Phase 1  POC1 (Local, Low Infrastructure) ==\n\n=== Principles ===\n\n* Local-first development\n* No mandatory container or cloud infrastructure\n* Same architectural shape as later phases\n\n=== Backend ===\n\n* (((\n**ASP.NET Core (.NET 8)**\n\n* Role: system of record (jobs, status, results, events)\n* Reason: strong typing, long-term stability, C# expertise\n)))\n* (((\n**SQLite (EF Core provider)**\n\n* Role: local persistence\n* Reason: zero setup, file-based, smooth transition to PostgreSQL\n)))\n\n=== Frontend & Orchestration ===\n\n* (((\n**Next.js (TypeScript)**\n\n* Role: UI and AI orchestration\n* Reason: fast iteration, strong ecosystem, good AI tooling\n)))\n* (((\n**Vercel AI SDK**\n\n* Role: LLM abstraction\n* Reason: OpenAI-first, Claude-ready, provider switching without rewrites\n)))\n\n=== AI Providers ===\n\n* OpenAI (default)\n* Anthropic (optional, later)\n\n=== Editors ===\n\n* Visual Studio 2022  primary for .NET API\n* Cursor or VS Code  primary for Next.js / TypeScript\n\n----\n\n== Phase 2  Transition to Beta ==\n\n=== Changes (No Rewrite) ===\n\n* SQLite  PostgreSQL\n* Local execution  hosted environments\n* Add authentication, quotas, persistence hardening\n\n=== Additional Tools Introduced ===\n\n* Docker (optional, mainly for DB parity)\n* Managed PostgreSQL\n* GitHub Actions (CI)\n* Hosting:\n** Next.js  Vercel\n** .NET API  Azure App Service or Container Apps\n\n----\n\n== Explicitly Deferred ==\n\n* Kubernetes\n* Microservice decomposition\n* Complex distributed queues\n* Multi-region deployment\n\nThese are intentionally deferred to reduce early risk.\n\n----\n\n== Summary ==\n\n* Architecture is stable from POC1 onward\n* Infrastructure is phased, not front-loaded\n* Tool choices prioritize clarity and durability over novelty", "Product Development.DevOps.Tooling.WebHome": "= Tooling =\n\nDevelopment environment setup, tool decisions, and testing/documentation tools for FactHarbor contributors.\n\n----\n\n== Setup ==\n\n* **[[1st Run Checklist>>FactHarbor.Product Development.DevOps.Tooling.1st Run Checklist.WebHome]]**  Step-by-step checklist to ensure POC1 runs end-to-end on the first attempt\n* **[[Installation Checklist>>FactHarbor.Product Development.DevOps.Tooling.Installation Checklist.WebHome]]**  Phased setup guide for a predictable, low-friction installation\n* **[[Tools Decisions>>FactHarbor.Product Development.DevOps.Tooling.Tools Decisions.WebHome]]**  Documented tool choices from POC1 to Beta with upgrade paths\n\n== Developer Tools ==\n\n* **[[Documentation Viewer>>FactHarbor.Product Development.DevOps.Tooling.Documentation Viewer.WebHome]]**  Browse documentation online, locally, or in VS Code\n* **[[Promptfoo Testing>>FactHarbor.Product Development.DevOps.Tooling.Promptfoo Testing.WebHome]]**  Prompt testing and evaluation with promptfoo\n\n== Archived ==\n\n* **[[Source Reliability Bundle>>FactHarbor.Product Development.DevOps.Tooling.Source Reliability Bundle.WebHome]]**  (Archived) Static SR bundle approach, replaced by the LLM-powered Source Reliability Service in v2.6\n", "Product Development.DevOps.WebHome": "= DevOps =\n\nDevelopment guidelines, tooling, deployment, and subsystem guides for FactHarbor contributors.\n\n----\n\n== [[Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.WebHome]] ==\n\nDevelopment standards for code, testing, and data modelling.\n\n* [[Coding Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.Coding Guidelines.WebHome]]  Code style, naming conventions, and best practices\n* [[Scope Definition Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.Scope Definition Guidelines.WebHome]]  How to define and use EvidenceScope in claims\n* [[Testing Strategy>>FactHarbor.Product Development.DevOps.Guidelines.Testing Strategy.WebHome]]  Test pyramid, promptfoo testing, and quality assurance\n* [[Getting Started>>FactHarbor.Product Development.DevOps.Guidelines.Getting Started.WebHome]]  Quick start guide\n\n== [[Tooling>>FactHarbor.Product Development.DevOps.Tooling.WebHome]] ==\n\nDevelopment environment setup, tool decisions, and developer tools.\n\n* [[1st Run Checklist>>FactHarbor.Product Development.DevOps.Tooling.1st Run Checklist.WebHome]]  End-to-end setup for first run\n* [[Installation Checklist>>FactHarbor.Product Development.DevOps.Tooling.Installation Checklist.WebHome]]  Phased setup guide\n* [[Tools Decisions>>FactHarbor.Product Development.DevOps.Tooling.Tools Decisions.WebHome]]  Documented tool choices from POC1 to Beta\n* [[Documentation Viewer>>FactHarbor.Product Development.DevOps.Tooling.Documentation Viewer.WebHome]]  Browse documentation online, locally, or in VS Code\n* [[Promptfoo Testing>>FactHarbor.Product Development.DevOps.Tooling.Promptfoo Testing.WebHome]]  Prompt testing and evaluation with promptfoo\n\n== [[Deployment>>FactHarbor.Product Development.DevOps.Deployment.WebHome]] ==\n\nHosting, CI/CD, and environment configuration.\n\n* [[Zero-Cost Hosting Implementation Guide>>FactHarbor.Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide.WebHome]]  Fly.io deployment for pre-release beta ($0-5/month)\n\n== [[Subsystems and Components>>FactHarbor.Product Development.DevOps.Subsystems and Components.WebHome]] ==\n\nGuides for using and configuring FactHarbor's key subsystems.\n\n* [[Admin Interface>>FactHarbor.Product Development.DevOps.Subsystems and Components.Admin Interface.WebHome]]  Admin panel and management interface\n* [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]]  Configuring LLM providers and models\n* [[Source Reliability Export>>FactHarbor.Product Development.DevOps.Subsystems and Components.Source Reliability Export.WebHome]]  Exporting source reliability data\n\n== Related ==\n\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture and key diagrams\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]  Glossary of terms\n", "Product Development.Diagrams.AKEL Analysis Pipeline.WebHome": "= AKEL Analysis Pipeline =\n\n{{mermaid}}\nflowchart TB\n    INPUT[\"User Input\\n(text or URL)\"] --> STEP1\n\n    subgraph STEP1[\"1. Understand\"]\n        CLAIMS[\"Extract claims\\nwith dependencies\"]\n        CONTEXTS[\"Detect analysis\\ncontexts\"]\n        FACTORS[\"Discover\\nKeyFactors\"]\n        GATE1{{\"Gate 1\\nClaim Validation\"}}\n        CLAIMS --> GATE1\n        CONTEXTS ~~~ FACTORS\n    end\n\n    STEP1 --> STEP2\n\n    subgraph STEP2[\"2. Research (iterative)\"]\n        QUERY[\"Generate\\nsearch queries\"]\n        FETCH[\"Fetch & parse\\nsources\"]\n        FILTER[\"Filter evidence\\n(7-layer defense)\"]\n        QUERY --> FETCH --> FILTER\n        FILTER -.->|\"gaps remain\"| QUERY\n    end\n\n    STEP2 --> STEP3\n\n    subgraph STEP3[\"3. Verdict\"]\n        PERCLAIM[\"Per-claim verdicts\\n(7-point scale)\"]\n        AGG[\"Aggregate weighted\\nverdicts\"]\n        GATE4{{\"Gate 4\\nConfidence Assessment\"}}\n        PERCLAIM --> AGG --> GATE4\n    end\n\n    STEP3 --> STEP4[\"4. Summary\"] --> STEP5[\"5. Report\"] --> RESULT[\"AnalysisResult\\n+ Markdown Report\"]\n\n    style STEP1 fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style STEP2 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style STEP3 fill:#fff9c4,stroke:#f9a825,color:#000\n    style STEP4 fill:#f3e5f5,stroke:#6a1b9a,color:#000\n    style STEP5 fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n\n//The 5-step AKEL analysis pipeline with quality gates. Step 2 (Research) is iterative. Gate 1 validates claims; Gate 4 assesses verdict confidence.//\n", "Product Development.Diagrams.AKEL Engine Overview.WebHome": "= AKEL Engine Overview =\n\n{{mermaid}}\nflowchart LR\n    subgraph Input[\"What goes in\"]\n        URL[\"URL\"]\n        TEXT[\"Text or Claim\"]\n    end\n\n    subgraph AKEL[\"AKEL Analysis Engine\"]\n        direction TB\n        U[\"Understand\\nclaims & contexts\"]\n        R[\"Research\\nevidence from web\"]\n        V[\"Evaluate\\nverdicts & confidence\"]\n        S[\"Report\\ntransparent output\"]\n        U --> R --> V --> S\n    end\n\n    subgraph Output[\"What comes out\"]\n        VERDICT[\"Structured Verdict\\nper claim + overall\"]\n        EVIDENCE[\"Evidence Trail\\nsources & excerpts\"]\n        REPORT[\"Analysis Report\\nhuman-readable\"]\n    end\n\n    Input --> AKEL --> Output\n\n    style AKEL fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Input fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Output fill:#fff9c4,stroke:#f9a825,color:#000\n{{/mermaid}}\n\n//AKEL takes user-submitted text or URLs, analyses the claims against web evidence, and produces structured verdicts with full evidence provenance.//\n", "Product Development.Diagrams.AKEL Pipeline Detail.WebHome": "= AKEL Pipeline Detail =\n\n{{mermaid}}\nflowchart TB\n    subgraph Input[\"Input\"]\n        URL[\"URL\"]\n        TEXT[\"Text / Claim\"]\n    end\n\n    subgraph Retrieval[\"Content Retrieval\"]\n        FETCH[\"Extract text\\nfrom URL\"]\n    end\n\n    subgraph Step1[\"Step 1: Understand\"]\n        UNDERSTAND[\"Detect input type\\nExtract claims & dependencies\\nIdentify analysis contexts\\nAssign risk tiers\"]\n        GATE1{{\"Gate 1\\nClaim Validation\"}}\n    end\n\n    subgraph Step2[\"Step 2: Research (Iterative)\"]\n        DECIDE[\"Generate search\\nqueries\"]\n        SEARCH[\"Web Search\\n(Google CSE / SerpAPI)\"]\n        EXTRACT[\"Fetch sources &\\nextract evidence\"]\n        CHECK{{\"Research\\ncomplete?\"}}\n    end\n\n    subgraph Step3[\"Step 3: Verdict\"]\n        VERDICT[\"Generate claim verdicts\\n(7-point scale)\"]\n        AGGREGATE[\"Aggregate into\\narticle verdict\"]\n        GATE4{{\"Gate 4\\nConfidence Assessment\"}}\n    end\n\n    subgraph Step4[\"Step 4: Summary\"]\n        SUMMARY[\"Build two-panel\\nsummary\"]\n    end\n\n    subgraph Step5[\"Step 5: Report\"]\n        REPORT[\"Generate markdown\\nreport\"]\n    end\n\n    subgraph Output[\"Output\"]\n        RESULT[\"AnalysisResult JSON\\n+ Markdown Report\"]\n    end\n\n    URL --> FETCH --> UNDERSTAND\n    TEXT --> UNDERSTAND\n    UNDERSTAND --> GATE1\n    GATE1 --> DECIDE\n    DECIDE --> SEARCH --> EXTRACT --> CHECK\n    CHECK -->|\"More research needed\"| DECIDE\n    CHECK -->|\"Complete\"| VERDICT\n    VERDICT --> AGGREGATE --> GATE4\n    GATE4 --> SUMMARY --> REPORT --> RESULT\n\n    style Step1 fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Step2 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Step3 fill:#fff9c4,stroke:#f9a825,color:#000\n    style Step4 fill:#f3e5f5,stroke:#6a1b9a,color:#000\n    style Step5 fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n\n//The AKEL pipeline processes input through 5 steps. Step 2 (Research) is iterative -- the system keeps searching until evidence is sufficient or the budget is exhausted. Quality gates validate claims (Gate 1) and verdict confidence (Gate 4).//\n", "Product Development.Diagrams.AKEL Quality Assurance.WebHome": "= AKEL Quality Assurance =\n\n{{mermaid}}\nflowchart LR\n    subgraph Gates[\"Quality Gates\"]\n        G1[\"Gate 1\\nClaim Validation\\n(UNDERSTAND phase)\"]\n        G4[\"Gate 4\\nVerdict Confidence\\n(VERDICT phase)\"]\n    end\n\n    subgraph Defense[\"Evidence Quality\"]\n        EF[\"7-Layer Defense\\nagainst low-quality\\nevidence\"]\n    end\n\n    subgraph Trust[\"Source Trust\"]\n        SR[\"Source Reliability\\nLLM-evaluated\\ncredibility scoring\"]\n    end\n\n    Gates ~~~ Defense ~~~ Trust\n\n    style Gates fill:#fff9c4,stroke:#f9a825,color:#000\n    style Defense fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Trust fill:#e8f5e9,stroke:#2e7d32,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.AKEL Shared Modules.WebHome": "= AKEL Shared Modules =\n\n{{mermaid}}\nflowchart TB\n    subgraph Pipelines[\"Pipeline Variants\"]\n        CB[\"ClaimAssessmentBoundary\"]\n        DYN[\"Dynamic\"]\n    end\n\n    subgraph Analysis[\"Analysis & Weighting\"]\n        AGG[\"aggregation.ts\\nVerdict weighting,\\ncontestation detection\"]\n        CLAIM_D[\"claim-decomposition.ts\\nClaim parsing,\\nnormalisation\"]\n    end\n\n    subgraph Quality[\"Quality & Filtering\"]\n        EF[\"evidence-filter.ts\\nProbative value filtering\"]\n        QG[\"quality-gates.ts\\nGate 1 + Gate 4\"]\n        VC[\"verdict-corrections.ts\\nVerdict inversion detection\"]\n    end\n\n    subgraph Evidence[\"Evidence Processing\"]\n        EN[\"evidence-normalization.ts\\nID migration, classification\"]\n        ER[\"evidence-recency.ts\\nTemporal analysis, dates\"]\n        EC[\"evidence-context-utils.ts\\nContext metadata utilities\"]\n        ED[\"evidence-deduplication.ts\\nDuplicate detection\"]\n    end\n\n    subgraph Trust[\"Trust & Scoring\"]\n        SR[\"source-reliability.ts\\nSource credibility scoring\"]\n        TS[\"truth-scale.ts\\n7-point verdict mapping\"]\n        BU[\"budgets.ts\\nToken/cost budgets\"]\n    end\n\n    CB --> Analysis\n    CB --> Quality\n    CB --> Evidence\n    CB --> Trust\n    DYN --> SR\n    DYN --> TS\n    DYN --> BU\n\n    style Pipelines fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Analysis fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Quality fill:#fff9c4,stroke:#f9a825,color:#000\n    style Evidence fill:#fce4ec,stroke:#c2185b,color:#000\n    style Trust fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n\n//Shared modules are grouped by function: Analysis (verdict weighting, claim parsing), Quality (evidence filtering, quality gates, verdict corrections), Evidence Processing (normalization, recency assessment, deduplication, context utilities), and Trust (source reliability, truth scale mapping, budget enforcement). The ClaimAssessmentBoundary pipeline uses all modules; the Monolithic Dynamic variant uses a subset.//\n", "Product Development.Diagrams.Analysis Entity Model ERD.WebHome": "{{info}}\n**Current Implementation (CB Pipeline v2.11.0+)**  The complete analysis entity hierarchy from input through verdict to overall assessment. Source of truth: ##apps/web/src/lib/analyzer/types.ts## (CB pipeline interfaces: ##CBClaimUnderstanding##, ##AtomicClaim##, ##CBClaimVerdict##, ##BoundaryFinding##, ##ClaimAssessmentBoundary##, ##EvidenceItem##, ##EvidenceScope##, ##FetchedSource##, ##OverallAssessment##, ##VerdictNarrative##, ##ConsistencyResult##, ##ChallengeResponse##, ##CoverageMatrix##, ##TriangulationScore##).\n\nUpdated 2026-02-22.\n{{/info}}\n\n= Analysis Entity Model (CB Pipeline v2.11.0+) =\n\n== Entity Relationship Diagram ==\n\n{{mermaid}}\n\nerDiagram\n    CB_CLAIM_UNDERSTANDING ||--o{ ATOMIC_CLAIM : \"extracts\"\n    CB_CLAIM_UNDERSTANDING ||--|| GATE_1_STATS : \"produces\"\n\n    ATOMIC_CLAIM ||--|| CB_CLAIM_VERDICT : \"receives\"\n\n    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : \"contains\"\n    CB_CLAIM_VERDICT ||--|| CONSISTENCY_RESULT : \"has\"\n    CB_CLAIM_VERDICT ||--o{ CHALLENGE_RESPONSE : \"has\"\n    CB_CLAIM_VERDICT ||--|| TRIANGULATION_SCORE : \"has\"\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"cites_supporting\"\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"cites_contradicting\"\n\n    EVIDENCE_ITEM }o--|| FETCHED_SOURCE : \"from\"\n    EVIDENCE_ITEM |o--o| EVIDENCE_SCOPE : \"has_scope\"\n\n    EVIDENCE_SCOPE }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"clusters_into\"\n    EVIDENCE_ITEM }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"assigned_to\"\n\n    BOUNDARY_FINDING }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"per_boundary\"\n\n    OVERALL_ASSESSMENT ||--o{ CB_CLAIM_VERDICT : \"aggregates\"\n    OVERALL_ASSESSMENT ||--o{ CLAIM_ASSESSMENT_BOUNDARY : \"presents\"\n    OVERALL_ASSESSMENT ||--|| VERDICT_NARRATIVE : \"has\"\n    OVERALL_ASSESSMENT ||--|| COVERAGE_MATRIX : \"has\"\n    OVERALL_ASSESSMENT ||--|| QUALITY_GATES : \"has\"\n\n    CB_CLAIM_UNDERSTANDING {\n        string detectedInputType \"claim_or_article\"\n        string impliedClaim \"LLM-extracted_thesis\"\n        string articleThesis \"Article_thesis\"\n        string backgroundDetails \"Broader_frame\"\n        string riskTier \"A_B_or_C\"\n        json distinctEvents \"name_date_description\"\n        json preliminaryEvidence \"sourceUrl_snippet_claimId\"\n    }\n\n    GATE_1_STATS {\n        int totalClaims\n        int passedOpinion\n        int passedSpecificity\n        int passedFidelity \"Fidelity_to_original_input\"\n        int filteredCount\n        boolean overallPass\n    }\n\n    ATOMIC_CLAIM {\n        string id_PK \"AC_01_AC_02\"\n        string statement \"Verifiable_assertion\"\n        string category \"factual_evaluative_procedural\"\n        string centrality \"high_medium\"\n        string harmPotential \"critical_high_medium_low\"\n        boolean isCentral \"Always_true_filtered\"\n        string claimDirection \"supports_thesis_contradicts_thesis_contextual\"\n        string verifiability \"high_medium_low_none_optional\"\n        string checkWorthiness \"high_medium\"\n        float specificityScore \"0-1_Gate1_min_0.6\"\n        string groundingQuality \"strong_moderate_weak_none\"\n        json keyEntities \"Named_entities_referenced\"\n        json expectedEvidenceProfile \"methodologies_metrics_sourceTypes\"\n    }\n\n    CB_CLAIM_VERDICT {\n        string id_PK\n        string claimId_FK \"FK_to_AtomicClaim\"\n        float truthPercentage \"0-100\"\n        string verdict \"7-point_scale_label\"\n        float confidence \"0-100\"\n        string reasoning \"LLM-generated_includes_challenges\"\n        string harmPotential \"critical_high_medium_low\"\n        boolean isContested \"Documented_counter-evidence\"\n        json supportingEvidenceIds_FK \"EvidenceItem_IDs\"\n        json contradictingEvidenceIds_FK \"EvidenceItem_IDs\"\n        json truthPercentageRange \"min_max_plausible_range\"\n        string misleadingness \"not_potentially_highly_optional\"\n        string misleadingnessReason \"Reason_optional\"\n    }\n\n    BOUNDARY_FINDING {\n        string boundaryId_FK \"FK_to_ClaimAssessmentBoundary\"\n        string boundaryName\n        float truthPercentage \"Per-boundary_0-100\"\n        float confidence \"Per-boundary_0-100\"\n        string evidenceDirection \"supports_contradicts_mixed_neutral\"\n        int evidenceCount\n    }\n\n    CONSISTENCY_RESULT {\n        string claimId_FK\n        json percentages \"Truth_pct_from_each_run\"\n        float average\n        float spread \"max_minus_min\"\n        boolean stable \"spread_below_threshold\"\n        boolean assessed \"false_if_skipped\"\n    }\n\n    TRIANGULATION_SCORE {\n        int boundaryCount \"Boundaries_with_evidence\"\n        int supporting \"Boundaries_supports_direction\"\n        int contradicting \"Boundaries_contradicts_direction\"\n        string level \"strong_moderate_weak_conflicted\"\n        float factor \"Multiplicative_weight_from_UCM\"\n    }\n\n    CHALLENGE_RESPONSE {\n        string challengeType \"assumption_missing_evidence_methodology_independence\"\n        string response \"Reconciler_response\"\n        boolean verdictAdjusted \"Whether_verdict_changed\"\n        json adjustmentBasedOnChallengeIds \"Challenge_point_IDs_optional\"\n    }\n\n    EVIDENCE_ITEM {\n        string id_PK \"EV_001_EV_002\"\n        string statement \"Extracted_evidence_text\"\n        string category \"statistic_expert_quote_event_legal_provision_etc\"\n        string specificity \"high_medium\"\n        string sourceId_FK\n        string sourceUrl\n        string sourceTitle\n        string sourceExcerpt\n        string claimDirection \"supports_contradicts_neutral\"\n        string probativeValue \"high_medium_low\"\n        float extractionConfidence \"0-100\"\n        string claimBoundaryId_FK \"Assigned_in_Stage_3\"\n        json relevantClaimIds \"Related_atomic_claims\"\n        string scopeQuality \"complete_partial_incomplete\"\n        string sourceType \"peer_reviewed_study_news_primary_etc\"\n        boolean isDerivative \"Cites_another_source_study\"\n        string derivedFromSourceUrl \"Original_source_URL\"\n        boolean derivativeClaimUnverified \"Original_source_not_fetched\"\n    }\n\n    EVIDENCE_SCOPE {\n        string name \"Short_label_WTW_TTW_EU-LCA\"\n        string methodology \"ISO_14040_EU_RED_II_etc\"\n        string temporal \"Source_data_time_period\"\n        string boundaries \"What_is_included_or_excluded\"\n        string geographic \"Source_data_geography\"\n        string sourceType \"peer_reviewed_study_government_report_etc\"\n        map additionalDimensions \"Domain-specific_scope_data\"\n    }\n\n    FETCHED_SOURCE {\n        string id_PK\n        string url\n        string title\n        float trackRecordScore \"0.0-1.0\"\n        float trackRecordConfidence \"0.0-1.0\"\n        boolean trackRecordConsensus\n        string category\n        boolean fetchSuccess\n        string fetchedAt\n    }\n\n    CLAIM_ASSESSMENT_BOUNDARY {\n        string id_PK \"CB_01_CB_02\"\n        string name \"Human-readable_label\"\n        string shortName \"Short_label_for_UI_tabs\"\n        string description \"What_this_boundary_represents\"\n        string methodology \"Dominant_methodology\"\n        string boundaries \"Scope_boundaries\"\n        string geographic \"Geographic_scope\"\n        string temporal \"Temporal_scope\"\n        json constituentScopes \"EvidenceScopes_composing_boundary\"\n        float internalCoherence \"0-1_consistency\"\n        int evidenceCount\n    }\n\n    VERDICT_NARRATIVE {\n        string headline \"One_sentence_finding\"\n        string evidenceBaseSummary \"Quantitative_summary\"\n        string keyFinding \"Main_synthesis_2-3_sentences\"\n        json boundaryDisagreements \"Where_boundaries_diverge\"\n        string limitations \"What_could_not_be_determined\"\n    }\n\n    COVERAGE_MATRIX {\n        json claims \"Claim_IDs_rows\"\n        json boundaries \"Boundary_IDs_columns\"\n        json counts \"Evidence_count_per_cell\"\n    }\n\n    QUALITY_GATES {\n        boolean passed \"Overall_pass\"\n        json gate1Stats \"Claim_validation_stats\"\n        json gate4Stats \"Confidence_assessment_stats\"\n        json summary \"totalEvidenceItems_totalSources_etc\"\n    }\n\n    OVERALL_ASSESSMENT {\n        float truthPercentage \"0-100_weighted\"\n        string verdict \"7-point_scale_label\"\n        float confidence \"0-100_weighted\"\n        boolean hasMultipleBoundaries\n        json truthPercentageRange \"min_max_plausible_range\"\n        json explanationQualityCheck \"B-8_quality_check_optional\"\n    }\n\n{{/mermaid}}\n\n//The CB pipeline analysis entity hierarchy. Stage 1 (Extract Claims) produces ##CB_CLAIM_UNDERSTANDING## with ##AtomicClaims## and ##GATE_1_STATS##. Stage 2 (Research) gathers ##EVIDENCE_ITEMs## with ##EVIDENCE_SCOPEs## from ##FETCHED_SOURCEs##. Stage 3 (Cluster) groups ##EVIDENCE_SCOPEs## into ##CLAIM_ASSESSMENT_BOUNDARYs##. Stage 4 (Verdict) generates ##CB_CLAIM_VERDICTs## with per-boundary ##BOUNDARY_FINDINGs##, ##CONSISTENCY_RESULTs## (self-consistency), ##TRIANGULATION_SCOREs## (cross-boundary agreement), and ##CHALLENGE_RESPONSEs## (adversarial debate). Stage 5 (Aggregate) produces ##OVERALL_ASSESSMENT## with ##VERDICT_NARRATIVE##, ##COVERAGE_MATRIX##, and ##QUALITY_GATES##.//\n\n== Entity Hierarchy ==\n\nThe CB pipeline produces a result structure organized as follows:\n\n1. **CBClaimUnderstanding** (Stage 1 output)  Top-level understanding of the input. Contains the extracted ##atomicClaims[]## array and ##gate1Stats## validation summary. Captures the article thesis, background details, risk tier, distinct events, and preliminary evidence gathered during the two-pass extraction process.\n\n1. **AtomicClaim**  A single verifiable assertion extracted from user input. Only central claims (high/medium centrality) survive the extraction filter. Each carries ##expectedEvidenceProfile## describing what evidence types and methodologies would verify or refute it. Fields ##verifiability## (B-6) and ##groundingQuality## assess fact-checkability and evidence anchoring.\n\n1. **CBClaimVerdict** (Stage 4 output)  One verdict per AtomicClaim, not per boundary. The verdict integrates evidence across all boundaries into a single ##truthPercentage## and 7-point ##verdict## label. Boundary-specific nuance is captured in ##boundaryFindings[]##. Includes ##consistencyResult## (self-consistency check), ##triangulationScore## (cross-boundary agreement), and ##challengeResponses[]## (adversarial debate reconciliation). Optional ##misleadingness## (B-7) provides an independent assessment that is output-only and not fed back into the debate.\n\n1. **BoundaryFinding**  Per-boundary quantitative signals within a CBClaimVerdict. Records ##truthPercentage##, ##confidence##, ##evidenceDirection##, and ##evidenceCount## for one ClaimAssessmentBoundary. Provides nuance when different methodological boundaries yield different conclusions about the same claim.\n\n1. **EvidenceItem** (Stage 2 output)  Extracted evidence from a source. This is unverified material to be evaluated against claims  not a verified fact. Linked to a ##FetchedSource## via ##sourceId## and carries an ##EvidenceScope## describing the source methodology. Assigned to a ##ClaimAssessmentBoundary## in Stage 3 via ##claimBoundaryId##.\n\n1. **EvidenceScope**  Per-evidence source metadata describing the methodology, boundaries, geography, and timeframe of the source data. Embedded within EvidenceItem (not a separate stored entity). Extensible via ##additionalDimensions## (Decision D4). Compatible scopes are clustered into ClaimAssessmentBoundaries in Stage 3.\n\n1. **ClaimAssessmentBoundary** (Stage 3 output)  Evidence-emergent grouping of compatible EvidenceScopes. Created after research by clustering scopes with compatible methodology, temporal, and geographic dimensions. Contains ##constituentScopes[]## and measures ##internalCoherence## (0-1).\n\n1. **OverallAssessment** (Stage 5 output)  Final aggregated result. Weighted average of ##truthPercentage## and ##confidence## across all CBClaimVerdicts. Contains ##verdictNarrative## (structured narrative), ##coverageMatrix## (claims x boundaries evidence distribution), ##qualityGates## (Gate 1 and Gate 4 results), ##claimBoundaries[]##, and ##claimVerdicts[]##. Optional ##explanationQualityCheck## (B-8) provides structural and rubric-based quality assessment of the narrative.\n\n== Key Implementation Notes ==\n\n**7-Point Verdict Scale:**\n* TRUE (86-100%) / MOSTLY-TRUE (72-85%) / LEANING-TRUE (58-71%)\n* MIXED (43-57%, confidence >= 40%) / UNVERIFIED (43-57%, confidence < 40%)\n* LEANING-FALSE (29-42%) / MOSTLY-FALSE (15-28%) / FALSE (0-14%)\n\n**EvidenceScope (mandatory core fields):** Per-evidence metadata describing the methodology and boundaries of the source data. ##methodology## and ##temporal## are the primary scope dimensions populated when available from the source. All fields except ##name## are optional in the TypeScript interface, but the extraction prompt targets methodology and temporal as mandatory when source data permits. Embedded in EvidenceItem, not a separate stored entity. Extensible via ##additionalDimensions## (Decision D4).\n\n**harmPotential (4-level, Decision D9):** ##critical## (1.5x weight) = death/injury allegations, ##high## (1.2x) = serious but not life-threatening, ##medium## (1.0x) = moderate, ##low## (1.0x) = minimal. Applied to both AtomicClaim and CBClaimVerdict.\n\n**claimDirection semantics (Decision D6):** AtomicClaim uses ##supports_thesis## / ##contradicts_thesis## / ##contextual## (contextual = relevant background without directional stance). EvidenceItem uses ##supports## / ##contradicts## / ##neutral## for backward compatibility.\n\n**Derivative evidence (CB pipeline):** Evidence items that cite another source's underlying study are flagged with ##isDerivative## and ##derivedFromSourceUrl##. If the original source was not fetched, ##derivativeClaimUnverified## = true. Derivative evidence receives reduced weight in aggregation.\n\n**VerdictNarrative (Decision D7):** Structured type with ##headline##, ##evidenceBaseSummary##, ##keyFinding##, ##boundaryDisagreements[]##, and ##limitations##. LLM-generated (Sonnet, 1 call) after weighted aggregation. Stored within OverallAssessment.\n\n**Self-Consistency (Stage 4 Step 2):** CBClaimVerdict includes ##consistencyResult## recording the spread of truth percentages across multiple LLM runs with temperature > 0. The ##stable## flag indicates whether spread is below the UCM-configured threshold. If ##assessed## is false, the check was skipped (disabled or deterministic mode).\n\n**Triangulation (Stage 4):** CBClaimVerdict includes ##triangulationScore## measuring cross-boundary agreement: ##strong## / ##moderate## / ##weak## / ##conflicted##. The ##factor## field provides a multiplicative weight adjustment derived from UCM thresholds.\n\n**Adversarial Challenge (Stage 4 Steps 3-4):** CBClaimVerdict includes ##challengeResponses[]## recording how each adversarial challenge point was addressed in reconciliation. Challenges must be evidence-backed to adjust verdicts; unsubstantiated objections do not reduce truth percentage. Each response records ##challengeType##, ##response##, ##verdictAdjusted##, and optional ##adjustmentBasedOnChallengeIds## for provenance tracking.\n\n**Misleadingness (B-7):** Optional independent assessment on CBClaimVerdict. Values: ##not_misleading##, ##potentially_misleading##, ##highly_misleading##. Output-only; not fed back into the debate.\n\n**TruthPercentageRange:** Plausible range (##min##, ##max##) computed from self-consistency spread and optionally widened by boundary variance. Present on both CBClaimVerdict and OverallAssessment.\n\n**Explanation Quality (B-8):** Optional ##explanationQualityCheck## on OverallAssessment. Two tiers: structural (deterministic  checks for cited evidence, verdict category, confidence statement, limitations) and rubric (LLM-powered  scores clarity, completeness, neutrality, evidence support, appropriate hedging on a 1-5 scale).\n\n**Storage:** All data stored as JSON blob in SQLite ##ResultJson## field. Schema version: ##3.0.0-cb##.\n\n**See Also:** [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]] for multi-view field-level detail. [[Quality Gates Flow>>FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome]] for Gate 1 and Gate 4 detail.\n", "Product Development.Diagrams.Architecture Roadmap.WebHome": "= Architecture Roadmap =\n\n{{mermaid}}\nflowchart LR\n    subgraph POC[\"POC (Current)\"]\n        P1[\"Single host\"]\n        P2[\"3x SQLite\\n(factharbor, config, SR)\"]\n        P3[\"Shared secrets\"]\n        P4[\"Manual ops\"]\n    end\n\n    subgraph ALPHA[\"Alpha\"]\n        A1[\"User auth\"]\n        A2[\"PostgreSQL\"]\n        A3[\"RBAC\"]\n        A4[\"Rate limiting\"]\n    end\n\n    subgraph BETA[\"Beta\"]\n        B1[\"Horizontal scaling\"]\n        B2[\"Redis caching\"]\n        B3[\"Prometheus + Grafana\"]\n        B4[\"Moderation tools\"]\n    end\n\n    subgraph V1[\"V1.0\"]\n        V1A[\"Multi-region\"]\n        V1B[\"OpenTelemetry\"]\n        V1C[\"Log aggregation\"]\n        V1D[\"Full DR\"]\n    end\n\n    subgraph V2[\"V2.0+\"]\n        V2A[\"Federation\"]\n        V2B[\"Vector DB\"]\n        V2C[\"Advanced analytics\"]\n    end\n\n    POC --> ALPHA --> BETA --> V1 --> V2\n\n    style POC fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style ALPHA fill:#fff9c4,stroke:#f9a825,color:#000\n    style BETA fill:#fff3e0,stroke:#e65100,color:#000\n    style V1 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style V2 fill:#f3e5f5,stroke:#7b1fa2,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Audit Trail ERD.WebHome": "{{info}}\n**Implementation Status (v2.10.2)**  Current implementation uses ##JobEvents## for job execution logs and UCM config tables for configuration versioning. Analysis output data is **immutable** and never edited. All audit tracking is on UCM configuration changes and job execution, not on data edits.\n{{/info}}\n\n= Current Implementation: Job Execution Audit =\n\n{{mermaid}}\n\nerDiagram\n    JOBS ||--o{ JOB_EVENTS : logs\n\n    JOBS {\n        int JobId_PK\n        string Status\n        datetime CreatedAt\n        datetime UpdatedAt\n        json ResultJson\n        text ReportMarkdown\n    }\n\n    JOB_EVENTS {\n        int Id_PK\n        int JobId_FK\n        datetime TsUtc\n        string Level\n        string Message\n    }\n\n{{/mermaid}}\n\n**Current audit capabilities:**\n* Job creation and completion timestamps\n* Job execution events (start, progress, errors)\n* Immutable analysis results (JSON blobs)\n* No user attribution needed (anonymous submission model)\n\n= UCM Configuration Audit Trail =\n\n{{mermaid}}\n\nerDiagram\n    CONFIG_BLOBS ||--o{ CONFIG_ACTIVE : activated_by\n    CONFIG_BLOBS ||--o{ CONFIG_USAGE : referenced_by\n    JOBS ||--o{ CONFIG_USAGE : uses\n\n    CONFIG_BLOBS {\n        text hash_PK\n        text config_type\n        json content\n        text changed_by\n        text change_reason\n        datetime created_at\n    }\n\n    CONFIG_ACTIVE {\n        text config_type_PK\n        text blob_hash_FK\n        datetime activated_at\n        text activated_by\n    }\n\n    CONFIG_USAGE {\n        int id_PK\n        int job_id_FK\n        text config_type\n        text blob_hash_FK\n        datetime snapshot_at\n    }\n\n    JOBS {\n        int JobId_PK\n        string Status\n        datetime CreatedAt\n    }\n\n{{/mermaid}}\n\n**UCM audit capabilities:**\n* Every config change stored as immutable blob (content-addressed by hash)\n* Activation history: which config was active when, changed by whom\n* Per-job config snapshots: every analysis references the exact config used\n* Full reproducibility: re-run any analysis with its original config\n\n= Design Principles =\n\n* **Analysis data is immutable**  never edited after creation\n* **Improve the system, not the data**  quality improvements flow through UCM config changes\n* **Every report references its config**  via ##config_usage## linking job to config snapshot\n* **Config blobs are never deleted**  complete audit trail preserved\n\nSee [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] for complete architecture.\n", "Product Development.Diagrams.Automation Level.WebHome": "{{info}}\n**Current Status: Level 0 (POC/Demo)** - v2.10.2. FactHarbor is currently at POC level with full AKEL automation but limited production features.\n{{/info}}\n\n= Automation Maturity Progression =\n\n{{mermaid}}\n\ngraph TD\n    POC[Level 0 POC Demo CURRENT]\n    R05[Level 0.5 Limited Production]\n    R10[Level 1.0 Full Production]\n    R20[Level 2.0+ Distributed Intelligence]\n\n    POC --> R05\n    R05 --> R10\n    R10 --> R20\n\n{{/mermaid}}\n\n== Level Descriptions ==\n\n|= Level |= Name |= Key Features\n| **Level 0** | POC/Demo (CURRENT) | All content auto-analyzed, AKEL generates verdicts, no risk tier filtering, single-user demo mode\n| **Level 0.5** | Limited Production | Multi-user support, risk tier classification, basic sampling audit, algorithm improvement focus\n| **Level 1.0** | Full Production | All tiers auto-published, clear risk labels, reduced sampling, mature algorithms\n| **Level 2.0+** | Distributed | Federated multi-node, cross-node audits, advanced patterns, strategic sampling only\n\n= Current Implementation (v2.6.33) =\n\n|= Feature |= POC Target |= Actual Status\n| AKEL auto-analysis | Yes | Implemented\n| Verdict generation | Yes | Implemented (7-point scale)\n| Quality Gates | Basic | Gates 1 and 4 implemented\n| Risk tiers | Yes | Not implemented\n| Sampling audits | High sampling | Not implemented\n| User system | Demo only | Anonymous only\n\n= Key Principles =\n\n**Across All Levels:**\n* AKEL makes all publication decisions\n* No human approval gates\n* Humans monitor metrics and improve algorithms\n* Risk tiers guide audit priorities, not publication\n* Sampling audits inform improvements", "Product Development.Diagrams.Automation Roadmap.WebHome": "{{info}}\n**Current Status: POC** (v2.10.2) - FactHarbor is at Proof of Concept stage. No risk tiers, no sampling audits yet.\n{{/info}}\n\n= Automation Roadmap =\n\n{{mermaid}}\n\ngraph LR\n    subgraph QA[Quality Assurance Evolution]\n        QA1[Initial High Sampling]\n        QA2[Intermediate Strategic]\n        QA3[Mature Anomaly-Triggered]\n\n        QA1 --> QA2\n        QA2 --> QA3\n    end\n\n    subgraph POC[POC CURRENT]\n        POC_F[POC Features]\n    end\n\n    subgraph R05[Release 0.5]\n        R05_F[Limited Production]\n    end\n\n    subgraph R10[Release 1.0]\n        R10_F[Full Production]\n    end\n\n    subgraph Future[Future]\n        Future_F[Distributed Intelligence]\n    end\n\n    POC_F --> R05_F\n    R05_F --> R10_F\n    R10_F --> Future_F\n\n{{/mermaid}}\n\n= Phase Details =\n\n== POC (Current v2.6.33) ==\n\n* All content analyzed\n* Basic AKEL Processing\n* No risk tiers yet\n* No sampling audits\n\n== Release 0.5 (Planned) ==\n\n* Tier A/B/C Published\n* All auto-publication\n* Risk Labels Active\n* Contradiction Detection\n* Sampling-Based QA\n\n== Release 1.0 (Planned) ==\n\n* Comprehensive AI Publication\n* Strategic Audits Only\n* Federated Nodes Beta\n* Cross-Node Data Sharing\n* Mature Algorithm Performance\n\n== Future (V2.0+) ==\n\n* Advanced Pattern Detection\n* Global Contradiction Network\n* Minimal Human QA\n* Full Federation\n\n= Philosophy =\n\n**Automation Philosophy:** At all stages, AKEL publishes automatically. Humans improve algorithms, not review content.\n\n**Sampling Rates:** Start higher for learning, reduce as confidence grows.", "Product Development.Diagrams.Circuit Breaker States.WebHome": "= Circuit Breaker States =\n\n{{mermaid}}\nstateDiagram-v2\n    [*] --> CLOSED: System starts healthy\n    CLOSED --> OPEN: Consecutive failures >= threshold\n    OPEN --> HALF_OPEN: Admin resumes system\n    HALF_OPEN --> CLOSED: Next call succeeds\n    HALF_OPEN --> OPEN: Next call fails\n\n    note right of CLOSED\n        Normal operation.\n        Failures counted.\n    end note\n\n    note right of OPEN\n        Circuit tripped.\n        System auto-paused.\n        New jobs stay QUEUED.\n        Webhook fires to admin.\n    end note\n\n    note right of HALF_OPEN\n        Admin resumed system.\n        Testing if provider recovered.\n    end note\n{{/mermaid}}\n", "Product Development.Diagrams.Claim and Scenario Lifecycle (Overview).WebHome": "= Claim and Scenario Lifecycle (Overview) =\n\n{{warning}}\n**Partially Outdated  Orchestrated Pipeline terminology (v2.10.x)**\n\nThis diagram uses Orchestrated pipeline terms (Context Refinement, Per-Context Verdicts, KeyFactor Analysis, Cross-Context Aggregation). In the current ClaimAssessmentBoundary pipeline, Stage 3 clusters evidence into ClaimAssessmentBoundaries, and Stage 4 generates verdicts per boundary via the AdvocateChallengerReconciliation debate pattern. A full diagram refresh is needed to reflect CB pipeline stages accurately.\n{{/warning}}\n\n{{mermaid}}\nflowchart TD\n    classDef ai fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,stroke-dasharray: 5 5;\n    classDef gate fill:#e3f2fd,stroke:#1565c0,stroke-width:2px;\n    classDef ucm fill:#fff3e0,stroke:#e65100,stroke-width:2px;\n    %% 1. Understanding\n    subgraph Understanding [\"1. Understanding\"]\n        direction TB\n        Input[User Submits URL/Text] --> Understand[Understand Claim]\n        Understand:::ai --> Discovered[\"Discovers: AtomicClaims,\\nResearch Queries\"]\n        Discovered:::ai --> Gate1[Gate 1: Claim Validation]\n        Gate1:::gate\n    end\n    %% 2. Evidence Retrieval\n    subgraph Evidence [\"2. Evidence Retrieval\"]\n        direction TB\n        Search[AI Web Search]:::ai --> Fetch[Source Fetching]\n        Fetch:::ai --> Extract[Evidence Extraction]\n        Extract:::ai --> Quality[Quality Filtering]\n        Quality:::ai --> Refine[Context Refinement]\n        Refine:::ai\n    end\n    %% 3. Verdict Generation\n    subgraph Verdicts [\"3. Verdict Generation\"]\n        direction TB\n        PerContext[Per-Context Verdicts]:::ai --> KeyFactors[KeyFactor Analysis]\n        KeyFactors:::ai --> Aggregate[Cross-Context Aggregation]\n        Aggregate:::ai --> Gate4[Gate 4: Confidence Check]\n        Gate4:::gate\n    end\n    %% 4. Presentation\n    subgraph Public [\"4. Public Presentation\"]\n        direction TB\n        Summary[Analysis Summary]\n        VerdictScale[7-Point Verdict Scale]\n        EvidenceView[Evidence & Sources]\n    end\n    %% 5. UCM Feedback Loop\n    subgraph UCMLoop [\"5. System Improvement\"]\n        direction TB\n        Metrics[Monitor Quality Metrics]\n        UCMConfig[UCM Config Update]:::ucm\n    end\n    %% Flow\n    Understanding --> Evidence\n    Evidence --> Verdicts\n    Verdicts --> Public\n    Public -.-> Metrics\n    UCMConfig -.->|improved config| Understanding\n{{/mermaid}}\n\nFully automated pipeline. No human editing of analysis data. Quality gates (blue) are mandatory: Gate 1 filters non-factual claims before research; Gate 4 validates verdict confidence before publication. System improvements flow through UCM configuration changes (dashed orange). ClaimAssessmentBoundaries emerge from evidence clustering (Stage 3)  not pre-detected during claim extraction.\n", "Product Development.Diagrams.Claim and Scenario Workflow.WebHome": "{{warning}}\n**STALE  Orchestrated Pipeline Era (v2.10.x)**\n\nThis workflow diagram was created for the Orchestrated pipeline. It shows ##DetectContexts## and ##KeyFactors## steps in Step 1  these were removed in v2.11.0. The ClaimAssessmentBoundary pipeline replaced them: AtomicClaims are extracted in Stage 1; ClaimAssessmentBoundaries emerge from evidence clustering in Stage 3. The \"KeyFactors (Replaces Scenarios)\" section below is historical. See [[ClaimAssessmentBoundary Pipeline Detail>>FactHarbor.Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail.WebHome]] for current architecture.\n{{/warning}}\n\n{{info}}\n**Historical Reference (v2.10.x)**  This diagram shows the Orchestrated-era pipeline. Step 1 used **AnalysisContexts** (bounded analytical frames) and **KeyFactors** (decomposition questions). Both were removed in v2.11.0.\n{{/info}}\n\n= Claim Analysis Workflow =\n\n{{mermaid}}\n\ngraph TB\n    Start[User Submission]\n\n    subgraph Step1[Step 1 Understand]\n        Extract{understandClaim LLM Analysis}\n        Gate1{Gate 1 Claim Validation}\n        DetectType[Detect Input Type]\n        DetectContexts[Detect Contexts]\n        KeyFactors[Discover KeyFactors]\n    end\n\n    subgraph Step2[Step 2 Research]\n        Decide[decideNextResearch]\n        Search[Web Search]\n        Fetch[Fetch Sources]\n        Facts[extractEvidence]\n    end\n\n    subgraph Step3[Step 3 Verdict]\n        Verdict[generateVerdicts]\n        Gate4{Gate 4 Confidence Check}\n    end\n\n    subgraph Output[Output]\n        Publish[Publish Result]\n        LowConf[Low Confidence Flag]\n    end\n\n    Start --> Extract\n    Extract --> Gate1\n    Gate1 -->|Pass Factual| DetectType\n    Gate1 -->|Fail Opinion| Exclude[Exclude from analysis]\n    DetectType --> DetectContexts\n    DetectContexts --> KeyFactors\n    KeyFactors --> Decide\n    Decide --> Search\n    Search --> Fetch\n    Fetch --> Facts\n    Facts -->|More research needed| Decide\n    Facts -->|Sufficient evidence| Verdict\n    Verdict --> Gate4\n    Gate4 -->|High or Medium confidence| Publish\n    Gate4 -->|Low or Insufficient| LowConf\n\n{{/mermaid}}\n\n== Quality Gates (Implemented) ==\n\n|= Gate |= Name |= Purpose |= Pass Criteria\n| **Gate 1** | Claim Validation | Filter non-factual claims | Factual, opinion score 0.3 or less, specificity 0.3 or more\n| **Gate 4** | Verdict Confidence | Ensure sufficient evidence | 2 or more sources, avg quality 0.6 or more, agreement 60% or more\n\n//Gates 2 (Contradiction Search) and 3 (Uncertainty Quantification) are not yet implemented.//\n\n== KeyFactors (Replaces Scenarios) ==\n\n**KeyFactors** are optional decomposition questions discovered during the understanding phase:\n* Not stored as separate entities\n* Help break down complex claims into checkable sub-questions\n* See [[KeyFactors Design>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome]] for design rationale\n\n== 7-Point Verdict Scale ==\n\n* **TRUE** (86-100%) - Claim is well-supported by evidence\n* **MOSTLY-TRUE** (72-85%) - Largely accurate with minor caveats\n* **LEANING-TRUE** (58-71%) - More evidence supports than contradicts\n* **MIXED** (43-57%, high confidence) - Roughly equal evidence both ways\n* **UNVERIFIED** (43-57%, low confidence) - Insufficient evidence to determine\n* **LEANING-FALSE** (29-42%) - More evidence contradicts than supports\n* **MOSTLY-FALSE** (15-28%) - Largely inaccurate\n* **FALSE** (0-14%) - Claim is refuted by evidence", "Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail.WebHome": "= ClaimAssessmentBoundary Pipeline Detail =\n\n{{info}}\n**Current Architecture (v2.11.0+):** The ClaimAssessmentBoundary pipeline is the production default pipeline. Source: ##claimboundary-pipeline.ts## and ##verdict-stage.ts##. For architecture reference, see [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]].\n\nUpdated 2026-02-22  B-sequence quality features (B-5a structured challenger, B-4 pro/con queries, B-6 verifiability annotation, B-7 misleadingness, B-8 explanation quality) documented in flowchart, sequence diagram, and LLM Call Budget.\n{{/info}}\n\n{{mermaid}}\nflowchart TB\n    subgraph Entry[\"Entry Point\"]\n        START[\"runClaimBoundaryAnalysis()\"]\n        INIT[\"Initialize State<br/>Budget tracker, timeline, config\"]\n    end\n\n    subgraph Stage1[\"Stage 1: EXTRACT CLAIMS\"]\n        CLASSIFY[\"[LLM] Input Classification<br/>article vs claim, topic identification\"]\n        DECOMPOSE[\"[LLM] Claim Decomposition<br/>Extract atomic claims (maxAtomicClaims)\"]\n        PRELIM_SEARCH[\"[WEB] Preliminary Search<br/>Grounding pass (2 queries x 5 sources per claim)\"]\n        PRELIM_EXTRACT[\"[LLM] Preliminary Evidence<br/>Extract evidence from prelim sources\"]\n        GATE1[\"Gate 1: Claim Validation<br/>Filter opinion/prediction/vague<br/>Central claims auto-pass\"]\n        ANNOTATE[\"Verifiability Annotation (B-6)<br/>claimAnnotationMode: off|verifiability|...<br/>Assigns high/medium/low/none per claim\"]\n        CLAIMS_OUT[\"AtomicClaim[] (5-15 claims)\"]\n    end\n\n    subgraph Stage2[\"Stage 2: RESEARCH EVIDENCE\"]\n        RESEARCH_LOOP[\"Research Loop (maxTotalIterations)\"]\n        SUFFICIENCY[\"Claim Sufficiency Check<br/>claimSufficiencyThreshold (default: 3)\"]\n        CONTRADICTION[\"Contradiction Search<br/>Reserved iterations (default: 2)\"]\n        DECIDE_NEXT[\"decideNextResearch()<br/>Priority: unsufficient > central > high-harm\"]\n        QUERY_GEN[\"[LLM] generateResearchQueries() (B-4)<br/>queryStrategyMode: legacy|pro_con<br/>pro_con: interleaved supporting/refuting<br/>Shared budget (max 3/call)\"]\n        SEARCH[\"[WEB] searchWebWithProvider()\"]\n        RELEVANCE[\"[LLM] classifyRelevance()<br/>Primary / secondary / unrelated\"]\n        FETCH[\"[WEB] fetchSourceContent()\"]\n        SR_PREFETCH[\"[EXT] prefetchSourceReliability()<br/>Batch lookup (async)\"]\n        EXTRACT[\"[LLM] extractEvidence()<br/>Per source > EvidenceItem[]\"]\n        EVIDENCE_OUT[\"EvidenceItem[] (boundary-unassigned)\"]\n    end\n\n    subgraph Stage3[\"Stage 3: CLUSTER BOUNDARIES\"]\n        PROPOSE[\"[LLM] Propose ClaimAssessmentBoundaries<br/>Evidence-emergent clustering\"]\n        VALIDATE[\"Structural Validation<br/>Coverage, coherence, overlap checks\"]\n        MERGE{\"Merge needed?<br/>(>maxClaimBoundaries)\"}\n        MERGE_BOUNDARIES[\"[LLM] Merge similar boundaries<br/>Reduce count, preserve coverage\"]\n        ASSIGN[\"Assign evidence to boundaries<br/>+ build coverage matrix\"]\n        BOUNDARIES_OUT[\"ClaimAssessmentBoundary[]<br/>CoverageMatrix\"]\n    end\n\n    subgraph Stage4[\"Stage 4: VERDICT (verdict-stage.ts)\"]\n        ADVOCATE[\"[LLM] Step 1: Advocate Verdict (Sonnet)<br/>Initial verdicts for all claims<br/>with per-boundary signals\"]\n        SELF_CONSIST{\"selfConsistencyMode?\"}\n        SC_CHECK[\"[LLM] Step 2: Self-Consistency (Sonnet x2)<br/>Re-run at temp=0.3, measure spread\"]\n        CHALLENGE[\"[LLM] Step 3: Adversarial Challenge (B-5a)<br/>Structured 5-point analysis (Sonnet):<br/>provenance, bidirectional challenge,<br/>coverage gaps, boundary agreement,<br/>quality asymmetry  evidence IDs mandatory\"]\n        RECONCILE[\"[LLM] Step 4: Reconciliation (Sonnet)<br/>Final verdicts incorporating challenges<br/>+ misleadingness assessment (B-7)<br/>when claimAnnotationMode includes it\"]\n        VERDICT_VAL[\"[LLM] Step 5: Verdict Validation (Haiku x2)<br/>Grounding check + direction check\"]\n        STRUCT_CHECK[\"Structural Consistency Check<br/>(deterministic)\"]\n        VERDICTS_OUT[\"CBClaimVerdict[]\"]\n    end\n\n    subgraph Stage5[\"Stage 5: AGGREGATE (aggregateAssessment)\"]\n        TRIANG_STEP[\"computeTriangulationScore()<br/>Cross-boundary agreement per claim\"]\n        WEIGHTED_AGG[\"Weighted Aggregation<br/>Centrality x harm x confidence x<br/>triangulation x derivative weights\"]\n        NARRATIVE[\"[LLM] generateVerdictNarrative() (Sonnet)<br/>Headline, key finding, limitations\"]\n        EXPLAIN_QC[\"Explanation Quality Check (B-8)<br/>explanationQualityMode: off|structural|rubric<br/>Tier 1: structural (deterministic)<br/>Tier 2: rubric (LLM  Haiku, 5 dimensions)\"]\n        GATE4[\"Gate 4: buildQualityGates()<br/>Classify confidence tiers\"]\n        ENVELOPE[\"Result Envelope<br/>schemaVersion: 3.0.0-cb\"]\n    end\n\n    START --> INIT\n    INIT --> CLASSIFY\n    CLASSIFY --> DECOMPOSE\n    DECOMPOSE --> PRELIM_SEARCH\n    PRELIM_SEARCH --> PRELIM_EXTRACT\n    PRELIM_EXTRACT --> GATE1\n    GATE1 --> ANNOTATE\n    ANNOTATE --> CLAIMS_OUT\n\n    CLAIMS_OUT --> RESEARCH_LOOP\n    RESEARCH_LOOP --> SUFFICIENCY\n    SUFFICIENCY -->|\"insufficient\"| DECIDE_NEXT\n    SUFFICIENCY -->|\"sufficient\"| CONTRADICTION\n    CONTRADICTION --> DECIDE_NEXT\n    DECIDE_NEXT --> QUERY_GEN\n    QUERY_GEN --> SEARCH\n    SEARCH --> RELEVANCE\n    RELEVANCE --> FETCH\n    FETCH --> SR_PREFETCH\n    SR_PREFETCH --> EXTRACT\n    EXTRACT --> EVIDENCE_OUT\n    EVIDENCE_OUT -->|\"loop continues\"| RESEARCH_LOOP\n\n    EVIDENCE_OUT --> PROPOSE\n    PROPOSE --> VALIDATE\n    VALIDATE --> MERGE\n    MERGE -->|\"yes\"| MERGE_BOUNDARIES\n    MERGE_BOUNDARIES --> ASSIGN\n    MERGE -->|\"no\"| ASSIGN\n    ASSIGN --> BOUNDARIES_OUT\n\n    BOUNDARIES_OUT --> ADVOCATE\n    ADVOCATE --> SELF_CONSIST\n    SELF_CONSIST -->|\"enabled\"| SC_CHECK\n    SELF_CONSIST -->|\"disabled\"| CHALLENGE\n    SC_CHECK --> CHALLENGE\n    CHALLENGE --> RECONCILE\n    RECONCILE --> VERDICT_VAL\n    VERDICT_VAL --> STRUCT_CHECK\n    STRUCT_CHECK --> VERDICTS_OUT\n\n    VERDICTS_OUT --> TRIANG_STEP\n    TRIANG_STEP --> WEIGHTED_AGG\n    WEIGHTED_AGG --> NARRATIVE\n    NARRATIVE --> EXPLAIN_QC\n    EXPLAIN_QC --> GATE4\n    GATE4 --> ENVELOPE\n\n    style Stage1 fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Stage2 fill:#fff9c4,stroke:#f57f17,color:#000\n    style Stage3 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Stage4 fill:#f3e5f5,stroke:#7b1fa2,color:#000\n    style Stage5 fill:#fce4ec,stroke:#c2185b,color:#000\n{{/mermaid}}\n\n== Key Features ==\n\n* **Evidence-emergent boundaries:** ClaimAssessmentBoundaries emerge from evidence clustering (Stage 3), not pre-detected\n* **5-step LLM debate:** Each verdict goes through structured debate (advocate > self-consistency > adversarial challenge > reconciliation > validation). Steps 2+3 run in parallel.\n* **Structured adversarial analysis (B-5a):** Step 3 challenger performs 5-point analysis  evidence provenance, bidirectional challenge, coverage gaps, boundary agreement scrutiny, quality asymmetry. Every challenge point must cite specific evidence IDs; baseless challenges are discounted by the reconciler.\n* **Pro/con query separation (B-4):** UCM ##queryStrategyMode## (\"legacy\" or \"pro_con\"). In pro_con mode, research queries are split into supporting and refuting variants, interleaved within a shared budget (max 3 per call).\n* **Verifiability annotation (B-6):** UCM ##claimAnnotationMode## controls optional per-claim verifiability assessment (high/medium/low/none) at Gate 1. Independent of claim category  evaluates fact-checkability.\n* **Misleadingness detection (B-7):** When ##claimAnnotationMode## = \"verifiability_and_misleadingness\", the reconciler (Step 4) assesses misleadingness independently of truthPercentage. A claim can be true yet misleading (cherry-picked, missing context, false causation).\n* **Explanation quality checks (B-8):** UCM ##explanationQualityMode## (\"off\", \"structural\", \"rubric\"). Tier 1: deterministic structural checks (evidence cited, verdict stated, confidence, limitations). Tier 2: LLM rubric evaluation (clarity, completeness, neutrality, evidence support, hedging  scored 1-5 each).\n* **Self-consistency checking:** Optional re-run with temperature variance to detect unstable verdicts (stable/moderate/unstable spread thresholds)\n* **Triangulation scoring:** Cross-boundary agreement assessment (strong/moderate/weak/conflicted levels)\n* **Quality gates:** Gate 1 (claim validation) and Gate 4 (verdict confidence distribution) are mandatory checkpoints\n* **UCM-configurable:** Pipeline parameters organized by stage, editable in Admin UI\n\n== Comparison to Legacy Pipelines ==\n\n| Aspect | Orchestrated (removed v2.11.0) | ClaimAssessmentBoundary (v2.11.0+) |\n|--------|-------------------------------|-----------------------------------|\n| **Context detection** | Pre-detected AnalysisContexts | Evidence-emergent ClaimAssessmentBoundaries |\n| **Verdict generation** | Single LLM call per claim | 5-step debate pattern |\n| **Cross-boundary analysis** | None | Triangulation scoring |\n| **Self-consistency** | None | Optional spread checking |\n| **Budget control** | Per-context iteration limits | Claim sufficiency + reserved contradiction iterations |\n| **Schema version** | 2.x or 3.0.0 | 3.0.0-cb |\n\n== Sequence Diagram ==\n\nComponent interaction during a full ClaimAssessmentBoundary pipeline analysis, including UCM configuration loading, contradiction search, and evidence quality filtering.\n\n{{mermaid}}\nsequenceDiagram\n    participant UI as Client/UI\n    participant API as API (ASP.NET)\n    participant Runner as Runner (Next.js)\n    participant Pipeline as ClaimAssessmentBoundary Pipeline\n    participant LLM as LLM Service\n    participant Search as Web Search\n    participant SR as Source Reliability\n    participant UCM as Config (UCM)\n\n    UI->>API: POST /api/jobs (create job)\n    API->>Runner: POST /api/internal/run-job\n    Runner->>Pipeline: runClaimBoundaryAnalysis(jobId)\n\n    Note over Pipeline: Stage 1: EXTRACT CLAIMS\n    Pipeline->>UCM: getConfig(\"pipeline\")\n    Pipeline->>LLM: [LLM] Pass 1: extractAtomicClaims(inputText)  Haiku\n    LLM-->>Pipeline: AtomicClaim[] (with centrality)\n    Pipeline->>Search: [Search] preliminarySearch(thesis + top claims)\n    Search-->>Pipeline: SearchResult[]\n    Pipeline->>LLM: [LLM] Pass 1: extractPreliminaryEvidence(sources)  Haiku\n    LLM-->>Pipeline: EvidenceItem[] + EvidenceScope[]\n    Pipeline->>LLM: [LLM] Pass 2: groundedClaimExtraction(input + prelim evidence)  Sonnet\n    LLM-->>Pipeline: AtomicClaim[] (refined, with groundingQuality)\n    Pipeline->>Pipeline: filterCentralClaims()\n    Pipeline->>LLM: [LLM] Gate 1: validateClaims(centralClaims)  Haiku\n    LLM-->>Pipeline: Gate1Result\n    Note right of Pipeline: B-6: If claimAnnotationMode != \"off\",<br/>verifiability (high/medium/low/none)<br/>retained on each AtomicClaim\n\n    Note over Pipeline: Stage 2: RESEARCH\n    loop For each research iteration\n        Pipeline->>LLM: [LLM] generateSearchQueries(claim, queryStrategyMode)  Haiku\n        Note right of Pipeline: B-4: queryStrategyMode=\"pro_con\"<br/>splits into supporting + refuting variants,<br/>interleaved, shared budget (max 3/call)\n        LLM-->>Pipeline: SearchQuery[] (with optional variantType)\n        Pipeline->>Search: [Search] execute(queries)\n        Search-->>Pipeline: SearchResult[]\n        Pipeline->>LLM: [LLM] classifyRelevance(results)  Haiku\n        Pipeline->>Pipeline: [Ext] fetchSources(relevantUrls)\n        Pipeline->>SR: [Ext] prefetchSourceReliability(urls)\n        SR-->>Pipeline: reliabilityScores\n        Pipeline->>LLM: [LLM] extractEvidence(sourceText)  Haiku\n        LLM-->>Pipeline: EvidenceItem[] + EvidenceScope[] (incl. additionalDimensions)\n        Pipeline->>LLM: [LLM] evidenceQualityFilter(items)  Haiku\n        Pipeline->>API: [Ext] updateProgress(jobId, stage, %)\n    end\n\n    Note over Pipeline: Contradiction Search (reserved)\n    Pipeline->>LLM: [LLM] generateContradictionQueries(claims, evidence)  Haiku\n    Pipeline->>Search: [Search] execute(contraQueries)\n    Pipeline->>LLM: [LLM] extractContraryEvidence(sources)  Haiku\n\n    Note over Pipeline: Stage 3: CLUSTER BOUNDARIES\n    Pipeline->>LLM: [LLM] clusterEvidenceScopes(allScopes, allEvidence)  Sonnet\n    LLM-->>Pipeline: ClaimAssessmentBoundary[] + assignments\n    Pipeline->>Pipeline: assignEvidenceToBoundaries()\n\n    Note over Pipeline: Stage 4: VERDICT (LLM Debate  verdict-stage.ts)\n    Pipeline->>LLM: [LLM] Step 1: advocateVerdict(claims, evidence, boundaries)  Sonnet\n    LLM-->>Pipeline: ClaimVerdict[] (with per-boundary quantitative signals)\n\n    par Self-Consistency + Adversarial Challenge (parallel)\n        Pipeline->>LLM: [LLM] Step 2: selfConsistencyCheck(advocatePrompt x 2, temp=0.3)  Sonnet\n        LLM-->>Pipeline: ConsistencyResult[] (spread per claim)\n    and\n        Pipeline->>LLM: [LLM] Step 3: adversarialChallenge(advocateVerdicts)  Sonnet\n        Note right of Pipeline: B-5a: Structured 5-point analysis <br/>provenance, bidirectional challenge,<br/>coverage gaps, boundary agreement,<br/>quality asymmetry. Evidence IDs mandatory.\n        LLM-->>Pipeline: ChallengeDocument (per-claim challenges with evidence IDs)\n    end\n\n    Pipeline->>LLM: [LLM] Step 4: reconciliation(verdicts + challenges + consistency)  Sonnet\n    Note right of Pipeline: B-7: If claimAnnotationMode includes<br/>misleadingness, reconciler assesses<br/>misleadingness independently of truth%\n    LLM-->>Pipeline: Final ClaimVerdict[] (with challenge responses + optional misleadingness)\n    Pipeline->>LLM: [LLM] Step 5a: validateVerdictGrounding(verdicts, evidence)  Haiku\n    Pipeline->>LLM: [LLM] Step 5b: validateVerdictDirection(verdicts, evidence)  Haiku\n    Pipeline->>Pipeline: structuralConsistencyCheck()\n    Pipeline->>Pipeline: gate4ConfidenceCheck()\n\n    Note over Pipeline: Stage 5: AGGREGATE\n    Pipeline->>Pipeline: computeCoverageMatrix(claims, boundaries, evidence)\n    Pipeline->>Pipeline: computeTriangulation(coverageMatrix, verdicts)\n    Pipeline->>Pipeline: weightedAggregation(verdicts, triangulation, derivatives)\n    Pipeline->>LLM: [LLM] generateVerdictNarrative(verdicts, boundaries, matrix)  Sonnet\n    LLM-->>Pipeline: VerdictNarrative\n    Note right of Pipeline: B-8: If explanationQualityMode != \"off\",<br/>run structural check (deterministic).<br/>If \"rubric\", also LLM evaluation (Haiku).\n    Pipeline->>Pipeline: checkExplanationStructure(narrative)\n    opt explanationQualityMode = \"rubric\"\n        Pipeline->>LLM: [LLM] evaluateExplanationRubric(narrative)  Haiku\n        LLM-->>Pipeline: ExplanationRubricScores (5 dimensions, 1-5 each)\n    end\n    Pipeline->>Pipeline: assembleReport(claims, evidence, boundaries, verdicts, narrative)\n    Pipeline->>API: [Ext] submitResults(jobId, report)\n    API-->>UI: Job complete (poll/webhook)\n{{/mermaid}}\n\n== LLM Call Budget ==\n\n|= Stage |= LLM Calls |= Model Tier |= Purpose\n| Pass 1 Extraction | 1 | Haiku | Quick rough claim scan\n| Pass 1 Evidence | 1-3 | Haiku | Extract evidence from preliminary sources\n| Pass 2 Extraction | 1 | Sonnet | Evidence-grounded claim extraction (incl. groundingQuality)\n| Gate 1 Validation | 1 | Haiku | Factual + specificity + grounding check\n| Decomposition Retry | 0-1 | Haiku | Split vague claims (if needed)\n| Gate 1 Retry | 0-4 | Haiku+Sonnet | If >50% fail, re-search + re-extract (max 1 retry)\n| Research Queries | N (per claim per iteration) | Haiku | Search query generation\n| Relevance Classification | N (per search result batch) | Haiku | Accept/reject search results\n| Evidence Extraction | N (per relevant source) | Haiku | Extract evidence + mandatory EvidenceScope + isDerivative\n| Scope Validation Retry | 0-N | Haiku | Re-extract items with missing scope (if needed)\n| Evidence Quality | N (per evidence batch) | Haiku | Quality assessment\n| Contradiction Queries | 1 | Haiku | Generate opposing search terms\n| Boundary Clustering | 1 | Sonnet | Congruence-based scope clustering\n| Step 1: Advocate | 1 | Sonnet | Advocate verdicts with per-boundary signals\n| Step 2: Self-Consistency | 0 or 2 | Sonnet | Re-run advocate at elevated temp (parallel with Step 3)\n| Step 3: Challenge | 1 | Sonnet | Devil's advocate challenges (parallel with Step 2)\n| Step 4: Reconciliation | 1 | Sonnet | Final verdicts incorporating challenges + consistency\n| Step 5: Validation | 2 (grounding + direction) | Haiku | Validate verdict quality\n| Verdict Narrative | 1 | Sonnet | Generate structured VerdictNarrative for overall assessment\n| Explanation Quality Rubric (B-8) | 0 or 1 | Haiku | LLM rubric evaluation (5 dimensions)  only when ##explanationQualityMode## = \"rubric\"\n\n//Expected total: 18-38 LLM calls per analysis. The 5-7 added calls for the debate pattern + narrative (advocate + 0-2 consistency + challenger + reconciliation + narrative) are offset by eliminating context detection, canonicalization, refinement, and supplemental context calls. B-8 rubric adds 0-1 Haiku call.//\n\n== Related Documentation ==\n\n* [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]]  Architecture reference (pipeline variants, shared modules, budget controls)\n* [[Pipeline Variant Dispatch>>FactHarbor.Product Development.Diagrams.Pipeline Variant Dispatch.WebHome]]  How pipelines are selected\n* [[Quality Gates Flow>>FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome]]  Gate 1 and Gate 4 details\n* [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]]  Entity relationships\n* [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]]  Multi-view entity reference\n", "Product Development.Diagrams.Context Detection Decision Tree.WebHome": "= Boundary Clustering Decision Flow =\n\n{{mermaid}}\nflowchart TD\n    START{\"Processing Evidence Item\"}\n\n    START -->|\"Stage 2: RESEARCH\"| ES[\"Extract EvidenceScope<br/>(MANDATORY)\"]\n\n    ES --> ESF[\"EvidenceScope Fields:<br/>- methodology (primary, optional)<br/>- temporal (primary, optional)<br/>- boundaries (optional)<br/>- geographic (optional)\"]\n\n    ESF --> STORE[\"Store on EvidenceItem.<br/>evidenceScope\"]\n\n    STORE --> NEXT{\"All evidence<br/>extracted?\"}\n\n    NEXT -->|\"No\"| START\n    NEXT -->|\"Yes\"| STAGE3[\"Stage 3:<br/>CLUSTER BOUNDARIES\"]\n\n    STAGE3 --> COLLECT[\"Collect all unique<br/>EvidenceScopes\"]\n\n    COLLECT --> LLM[\"LLM Congruence Assessment:<br/>Are scopes COMPATIBLE?\"]\n\n    LLM -->|\"Compatible<br/>(same/similar methodology,<br/>temporal, boundaries)\"| MERGE[\"Cluster into<br/>SINGLE ClaimBoundary\"]\n\n    LLM -->|\"Incompatible<br/>(different methodology,<br/>boundaries, jurisdiction)\"| SPLIT[\"Create<br/>SEPARATE ClaimBoundaries\"]\n\n    MERGE --> EX1[\"Example:<br/>All evidence uses<br/>'Standard S, Period P'<br/> 1 boundary (CB_01)\"]\n\n    SPLIT --> EX2[\"Example:<br/>'Method A (full)' vs<br/>'Method B (subsystem)'<br/> 2 boundaries (CB_01, CB_02)\"]\n\n    EX1 --> ASSIGN[\"Assign claimBoundaryId<br/>to each EvidenceItem\"]\n    EX2 --> ASSIGN\n\n    ASSIGN --> VERDICT[\"Stage 4: VERDICT<br/>(use boundaries for<br/>per-boundary findings)\"]\n\n    style ES fill:#fff9c4,color:#000\n    style MERGE fill:#c8e6c9,color:#000\n    style SPLIT fill:#ffccbc,color:#000\n    style VERDICT fill:#e3f2fd,color:#000\n{{/mermaid}}\n\n//Yellow = EvidenceScope extraction. Green = single boundary (compatible). Orange = multiple boundaries (incompatible). Blue = verdict stage.//\n", "Product Development.Diagrams.Context Detection Phases.WebHome": "= ClaimBoundary Pipeline Stages =\n\n{{mermaid}}\nflowchart TB\n    subgraph EXTRACT[\"Stage 1: EXTRACT CLAIMS\"]\n        Input[User Input] --> Pass1[\"Pass 1: Quick Claim Scan<br/>(Haiku)\"]\n        Pass1 --> PrelimSearch[\"Preliminary Search<br/>(2-3 queries)\"]\n        PrelimSearch --> PrelimEvidence[\"Prelim Evidence Extract<br/>with EvidenceScopes\"]\n        PrelimEvidence --> Pass2[\"Pass 2: Evidence-Grounded<br/>Claim Extraction (Sonnet)\"]\n        Pass2 --> CentralFilter[\"Centrality Filter<br/>(keep only central claims)\"]\n        CentralFilter --> Gate1[\"Gate 1: Claim Validation<br/>(factual + specificity)\"]\n        Gate1 --> Claims[\"AtomicClaim[]\"]\n    end\n\n    subgraph RESEARCH[\"Stage 2: RESEARCH\"]\n        Claims --> QueryGen[\"Generate Search Queries<br/>(per claim, Haiku)\"]\n        QueryGen --> WebSearch[\"Web Search<br/>(Tavily/Brave)\"]\n        WebSearch --> Relevance[\"Relevance Classification<br/>(Haiku)\"]\n        Relevance --> FetchSources[\"Fetch Sources\"]\n        FetchSources --> ExtractEv[\"Extract Evidence +<br/>EvidenceScope (MANDATORY)<br/>(Haiku)\"]\n        ExtractEv --> EvidenceFilter[\"Evidence Quality Filter<br/>(Haiku)\"]\n        EvidenceFilter --> Evidence[\"EvidenceItem[]<br/>(each with EvidenceScope)\"]\n    end\n\n    subgraph CLUSTER[\"Stage 3: CLUSTER BOUNDARIES\"]\n        Evidence --> CollectScopes[\"Collect Unique<br/>EvidenceScopes\"]\n        CollectScopes --> CongruenceAssessment[\"Congruence Assessment<br/>(LLM clustering, Sonnet)\"]\n        CongruenceAssessment --> AssignBoundaries[\"Assign claimBoundaryId<br/>to each EvidenceItem\"]\n        AssignBoundaries --> Boundaries[\"ClaimBoundary[]<br/>(1-5 boundaries)\"]\n    end\n\n    subgraph VERDICT[\"Stage 4: VERDICT\"]\n        Boundaries --> Advocate[\"Step 1: Advocate Verdict<br/>(Sonnet)\"]\n        Advocate --> SelfConsistency[\"Step 2: Self-Consistency<br/>(Sonnet  2, parallel)\"]\n        Advocate --> Challenge[\"Step 3: Adversarial Challenge<br/>(Sonnet, parallel)\"]\n        SelfConsistency --> Reconcile[\"Step 4: Reconciliation<br/>(Sonnet)\"]\n        Challenge --> Reconcile\n        Reconcile --> Validation[\"Step 5: Verdict Validation<br/>(Haiku  2)\"]\n        Validation --> Gate4[\"Gate 4: Confidence Check\"]\n        Gate4 --> Verdicts[\"ClaimVerdict[]<br/>(with boundaryFindings[])\"]\n    end\n\n    subgraph AGGREGATE[\"Stage 5: AGGREGATE\"]\n        Verdicts --> CoverageMatrix[\"Compute Coverage Matrix<br/>(claims  boundaries)\"]\n        CoverageMatrix --> Triangulation[\"Triangulation Scoring<br/>(boundary agreement)\"]\n        Triangulation --> WeightedAgg[\"Weighted Aggregation<br/>(centrality  harm  confidence<br/> triangulation  derivative)\"]\n        WeightedAgg --> Narrative[\"Generate VerdictNarrative<br/>(Sonnet)\"]\n        Narrative --> Report[\"Assemble Final Report\"]\n    end\n\n    style Pass1 fill:#fff3e0,color:#000\n    style Pass2 fill:#fff3e0,color:#000\n    style ExtractEv fill:#fff9c4,color:#000\n    style CongruenceAssessment fill:#c8e6c9,color:#000\n    style Boundaries fill:#e3f2fd,color:#000\n    style Verdicts fill:#e3f2fd,color:#000\n    style Report fill:#e1bee7,color:#000\n{{/mermaid}}\n\n//Orange = claim extraction (two-pass). Yellow = EvidenceScope extraction (mandatory). Green = boundary clustering (congruence). Blue = boundaries + verdicts. Purple = final report.//\n", "Product Development.Diagrams.Core Data Model ERD.WebHome": "{{info}}\n**Current Implementation (CB Pipeline v2.11.0+)**  This ERD shows the ClaimAssessmentBoundary pipeline data model as implemented in ##types.ts## and stored as JSON blobs in SQLite ##ResultJson## field.\n\nUpdated 2026-02-22 per source code audit against ##apps/web/src/lib/analyzer/types.ts## (CB pipeline interfaces: ##AtomicClaim##, ##ClaimAssessmentBoundary##, ##CBClaimVerdict##, ##BoundaryFinding##, ##OverallAssessment##, ##VerdictNarrative##, ##CBClaimUnderstanding##, etc.).\n{{/info}}\n\n= ClaimAssessmentBoundary Data Model (v2.11.0+) =\n\n{{mermaid}}\n\nerDiagram\n    INPUT_ARTICLE ||--o{ ATOMIC_CLAIM : \"extracts\"\n    ATOMIC_CLAIM ||--o{ SEARCH_QUERY : \"generates\"\n    SEARCH_QUERY }o--o{ SOURCE : \"discovers\"\n    SOURCE ||--o{ EVIDENCE_ITEM : \"yields\"\n    EVIDENCE_ITEM |o--o| EVIDENCE_SCOPE : \"has\"\n    EVIDENCE_SCOPE }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"clusters into\"\n    EVIDENCE_ITEM }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"assigned to\"\n    ATOMIC_CLAIM ||--o{ CB_CLAIM_VERDICT : \"receives\"\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"cites\"\n    CB_CLAIM_VERDICT }o--o| CLAIM_ASSESSMENT_BOUNDARY : \"scoped by\"\n    CB_CLAIM_VERDICT }o--|| OVERALL_ASSESSMENT : \"aggregates into\"\n    OVERALL_ASSESSMENT ||--o{ CLAIM_ASSESSMENT_BOUNDARY : \"presents\"\n    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : \"contains\"\n    BOUNDARY_FINDING }o--|| CLAIM_ASSESSMENT_BOUNDARY : \"per boundary\"\n\n    INPUT_ARTICLE {\n        string id_PK\n        string inputType \"text or url\"\n        string detectedInputType \"claim or article\"\n        string impliedClaim \"LLM-extracted thesis\"\n        string articleThesis\n        string backgroundDetails\n        string riskTier \"A B or C\"\n    }\n\n    ATOMIC_CLAIM {\n        string id_PK \"AC_01 AC_02\"\n        string statement \"The verifiable assertion\"\n        string category \"factual evaluative procedural\"\n        string centrality \"high medium\"\n        string harmPotential \"critical high medium low\"\n        boolean isCentral \"Always true (filtered)\"\n        string claimDirection \"supports_thesis contradicts_thesis contextual\"\n        string verifiability \"high medium low none (optional)\"\n        string checkWorthiness \"high medium\"\n        float specificityScore \"0-1 Gate1 min 0.6\"\n        string groundingQuality \"strong moderate weak none\"\n        json keyEntities \"Named entities referenced\"\n        json expectedEvidenceProfile \"methodologies metrics sourceTypes\"\n    }\n\n    SEARCH_QUERY {\n        string query\n        int iteration\n        string focus \"Evidence type sought\"\n        int resultsCount\n        string timestamp\n        string searchProvider\n    }\n\n    SOURCE {\n        string id_PK\n        string url\n        string title\n        string fullText \"Retrieved page content\"\n        float trackRecordScore \"0.0-1.0\"\n        float trackRecordConfidence \"0.0-1.0\"\n        boolean trackRecordConsensus\n        string category\n        boolean fetchSuccess\n        string fetchedAt\n    }\n\n    EVIDENCE_ITEM {\n        string id_PK \"EV_001 EV_002\"\n        string statement \"Extracted evidence text\"\n        string category \"statistic expert_quote event legal_provision etc\"\n        string sourceId_FK\n        string sourceUrl\n        string sourceTitle\n        string sourceExcerpt\n        string claimDirection \"supports contradicts neutral\"\n        string probativeValue \"high medium low\"\n        float extractionConfidence \"0-100\"\n        string claimBoundaryId_FK \"Assigned in Stage 3\"\n        json relevantClaimIds \"Which claims this relates to\"\n        string scopeQuality \"complete partial incomplete\"\n        string sourceType \"peer_reviewed_study news_primary etc\"\n        boolean isDerivative \"Cites another source study\"\n        string derivedFromSourceUrl \"Original source URL\"\n        boolean derivativeClaimUnverified \"Original source not fetched\"\n    }\n\n    EVIDENCE_SCOPE {\n        string name \"Short label: WTW TTW EU-LCA\"\n        string methodology \"ISO 14040 EU RED II etc\"\n        string temporal \"Source data time period\"\n        string boundaries \"What is included or excluded\"\n        string geographic \"Source data geography\"\n        string sourceType \"peer_reviewed_study government_report etc\"\n        map additionalDimensions \"Domain-specific scope data\"\n    }\n\n    CLAIM_ASSESSMENT_BOUNDARY {\n        string id_PK \"CB_01 CB_02\"\n        string name \"Human-readable label\"\n        string shortName \"Short label for UI tabs\"\n        string description \"What this boundary represents\"\n        string methodology \"Dominant methodology\"\n        string boundaries \"Scope boundaries\"\n        string geographic \"Geographic scope\"\n        string temporal \"Temporal scope\"\n        json constituentScopes \"EvidenceScopes composing this boundary\"\n        float internalCoherence \"0-1 consistency\"\n        int evidenceCount\n    }\n\n    CB_CLAIM_VERDICT {\n        string id_PK\n        string claimId_FK\n        float truthPercentage \"0-100\"\n        string verdict \"7-point scale label\"\n        float confidence \"0-100\"\n        string reasoning \"LLM-generated explanation\"\n        string harmPotential \"critical high medium low\"\n        boolean isContested\n        json supportingEvidenceIds_FK\n        json contradictingEvidenceIds_FK\n        json boundaryFindings \"Per-boundary signals\"\n        json consistencyResult \"Self-consistency check\"\n        json challengeResponses \"Adversarial challenge responses\"\n        json triangulationScore \"Cross-boundary agreement\"\n        json truthPercentageRange \"min max plausible range\"\n        string misleadingness \"not_misleading potentially highly (optional)\"\n        string misleadingnessReason \"Reason (optional)\"\n    }\n\n    BOUNDARY_FINDING {\n        string boundaryId_FK\n        string boundaryName\n        float truthPercentage \"Per-boundary 0-100\"\n        float confidence \"Per-boundary 0-100\"\n        string evidenceDirection \"supports contradicts mixed neutral\"\n        int evidenceCount\n    }\n\n    OVERALL_ASSESSMENT {\n        float truthPercentage \"0-100 weighted\"\n        string verdict \"7-point scale label\"\n        float confidence \"0-100 weighted\"\n        boolean hasMultipleBoundaries\n        json claimBoundaries \"All ClaimAssessmentBoundary objects\"\n        json claimVerdicts \"All CBClaimVerdict objects\"\n        json verdictNarrative \"headline keyFinding limitations\"\n        json coverageMatrix \"claims x boundaries\"\n        json qualityGates \"gate1Stats gate4Stats\"\n        json truthPercentageRange \"min max plausible range\"\n    }\n\n{{/mermaid}}\n\n== Key Implementation Notes ==\n\n**7-Point Verdict Scale:**\n* TRUE (86-100%) / MOSTLY-TRUE (72-85%) / LEANING-TRUE (58-71%)\n* MIXED (43-57%, confidence >= 40%) / UNVERIFIED (43-57%, confidence < 40%)\n* LEANING-FALSE (29-42%) / MOSTLY-FALSE (15-28%) / FALSE (0-14%)\n\n**EvidenceScope (mandatory core fields):** Per-evidence metadata describing the methodology and boundaries of the source data. ##methodology## and ##temporal## are the primary scope dimensions populated when available from the source. All fields except ##name## are optional in the TypeScript interface, but the extraction prompt targets methodology and temporal as mandatory when source data permits. Embedded in EvidenceItem, not a separate stored entity. Extensible via ##additionalDimensions## (Decision D4).\n\n**harmPotential (4-level, Decision D9):** ##critical## (1.5x weight) = death/injury allegations, ##high## (1.2x) = serious but not life-threatening, ##medium## (1.0x) = moderate, ##low## (1.0x) = minimal. Applied to both AtomicClaim and CBClaimVerdict.\n\n**claimDirection \"contextual\" (Decision D6):** Evidence providing relevant background without directional stance. Renamed from \"neutral\" to clarify semantics. Used in AtomicClaim. Note: EvidenceItem still uses \"supports\" / \"contradicts\" / \"neutral\" for backward compatibility.\n\n**Derivative evidence (CB pipeline):** Evidence items that cite another source's underlying study are flagged with ##isDerivative## and ##derivedFromSourceUrl##. If the original source was not fetched, ##derivativeClaimUnverified## = true. Derivative evidence receives reduced weight in aggregation.\n\n**VerdictNarrative (Decision D7):** Structured type with ##headline##, ##evidenceBaseSummary##, ##keyFinding##, ##boundaryDisagreements[]##, and ##limitations##. LLM-generated (Sonnet, 1 call) after weighted aggregation. Stored within OverallAssessment.\n\n**BoundaryFinding:** Per-boundary quantitative signals within a CBClaimVerdict. Provides nuance when different methodological boundaries yield different conclusions about the same claim. Each BoundaryFinding records truth percentage, confidence, evidence direction, and evidence count for one ClaimAssessmentBoundary.\n\n**Self-Consistency & Triangulation:** CBClaimVerdict includes ##consistencyResult## (spread of truth percentages across multiple LLM runs) and ##triangulationScore## (cross-boundary agreement: strong/moderate/weak/conflicted). Both influence final confidence.\n\n**Adversarial Challenge:** CBClaimVerdict includes ##challengeResponses## recording how each adversarial challenge point was addressed in reconciliation. Challenges must be evidence-backed to adjust verdicts; unsubstantiated objections do not reduce truth percentage.\n\n**Misleadingness (B-7):** Optional independent assessment on CBClaimVerdict. Values: ##not_misleading##, ##potentially_misleading##, ##highly_misleading##. Output-only; not fed back into the debate.\n\n**Storage:** All data stored as JSON blob in SQLite ##ResultJson## field. Schema version: ##3.0.0-cb##.\n\n**See Also:** [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]] for multi-view field-level detail. [[Quality Gates Flow>>FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome]] for Gate 1 and Gate 4 detail.\n", "Product Development.Diagrams.Deployment Topology.WebHome": "= Deployment Topology =\n\n{{mermaid}}\nflowchart TB\n    subgraph Current[\"Current (POC / Development)\"]\n        direction TB\n        HOST[\"Single Host\"]\n        NEXT[\"Next.js\\nport 3000\"]\n        DOTNET[\".NET API\\nport 5000\"]\n        SQLITE[\"factharbor.db\\nconfig.db\\nsource-reliability.db\"]\n\n        HOST --> NEXT\n        HOST --> DOTNET\n        HOST --> SQLITE\n    end\n\n    subgraph Production[\"Target Production\"]\n        direction TB\n        LB[\"Load Balancer\"]\n        API1[\"API Server 1\"]\n        API2[\"API Server N\"]\n        WORKERS[\"AKEL Worker Pool\\n(auto-scaling)\"]\n        PG_PRIMARY[\"PostgreSQL\\nPrimary\"]\n        PG_REPLICA[\"PostgreSQL\\nReplica(s)\"]\n        REDIS_OPT[\"Redis\\n(if multi-instance)\"]\n        MONITOR[\"Prometheus\\n+ Grafana\"]\n\n        LB --> API1\n        LB --> API2\n        API1 --> WORKERS\n        API2 --> WORKERS\n        API1 --> PG_PRIMARY\n        API2 --> PG_REPLICA\n        PG_PRIMARY --> PG_REPLICA\n        WORKERS --> PG_PRIMARY\n        MONITOR -.-> API1\n        MONITOR -.-> WORKERS\n    end\n\n    style Current fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style Production fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Development Roadmap.WebHome": "= Development Roadmap =\n\n{{mermaid}}\n\ngraph LR\n    subgraph Phase1[POC Complete]\n        P1A[Run baseline test]\n        P1B[Integrate metrics]\n    end\n\n    subgraph Phase2[Alpha]\n        P2A[Alpha 0.1: Baseline + metrics]\n        P2B[Alpha 0.2: Caching]\n        P2C[Alpha 0.3: Quality improvements]\n        P2D[Alpha 0.4: Performance]\n        P2E[Alpha 0.5: Security review]\n    end\n\n    subgraph Phase3[Beta]\n        P3A[PostgreSQL migration]\n        P3B[User accounts]\n        P3C[Browse/search]\n        P3D[Evidence dedup FR54]\n    end\n\n    subgraph Phase4[V1.0]\n        P4A[Public launch]\n    end\n\n    Phase1 --> Phase2\n    Phase2 --> Phase3\n    Phase3 --> Phase4\n\n{{/mermaid}}\n\n//The agreed development roadmap: POC -> Alpha -> Beta -> V1.0, with key milestones per phase.//\n", "Product Development.Diagrams.Doubted vs Contested Flow.WebHome": "= Doubted vs Contested Flow =\n\n{{mermaid}}\nflowchart TD\n    subgraph Input[\"Opposition/Criticism\"]\n        OPP[Someone opposes or criticizes the claim]\n    end\n\n    OPP --> CHECK{Has documented<br/>counter-evidence?}\n\n    CHECK -->|No evidence| DOUBTED[\"DOUBTED<br/>factualBasis: opinion/alleged\"]\n    CHECK -->|Some evidence| DISPUTED[\"CONTESTED<br/>factualBasis: disputed\"]\n    CHECK -->|Strong evidence| ESTABLISHED[\"CONTESTED<br/>factualBasis: established\"]\n\n    DOUBTED --> W1[\"Weight: 1.0x<br/>Full weight\"]\n    DISPUTED --> W2[\"Weight: 0.5x<br/>Reduced\"]\n    ESTABLISHED --> W3[\"Weight: 0.3x<br/>Heavily reduced\"]\n\n    style DOUBTED fill:#fff3e0,color:#000\n    style DISPUTED fill:#ffecb3,color:#000\n    style ESTABLISHED fill:#ffcdd2,color:#000\n    style W1 fill:#c8e6c9,color:#000\n    style W2 fill:#fff9c4,color:#000\n    style W3 fill:#ffcdd2,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Entity Views.WebHome": "{{info}}\n**Multi-View Entity Reference (CB Pipeline v2.11.0+)** -- Five complementary ERD views of the FactHarbor entity landscape. Each view highlights a different aspect: overview, analysis result, target database, runtime processing, and UI visibility.\n\n**Source of truth**: ##apps/web/src/lib/analyzer/types.ts##, ##apps/api/Data/Entities.cs##, ##apps/web/src/lib/config-storage.ts##\n\nUpdated 2026-02-22 per CB pipeline interfaces in ##types.ts##.\n{{/info}}\n\n= Entity Views =\n\nFive views of the same entity landscape, each serving a different audience and purpose.\n\n|= View |= Purpose |= Audience\n| **[[Overview>>||anchor=\"HOverviewERD\"]]** | Bird's-eye view of all entity groups | Everyone (entry point)\n| **[[Analysis Result>>||anchor=\"HAnalysisResultEntities\"]]** | Everything persisted in ##resultJson## | Developers, data architects\n| **[[Target Database>>||anchor=\"HTargetDatabaseEntities\"]]** | Future PostgreSQL table design | Database architects, backend developers\n| **[[Runtime Process>>||anchor=\"HRuntimeProcessEntities\"]]** | Transient entities during pipeline execution | Pipeline developers\n| **[[UI Visible>>||anchor=\"HUI-VisibleEntities\"]]** | What users see in the browser | Frontend developers, UX designers\n\n=== Color Legend ===\n\nAll views use a consistent color scheme:\n\n|= Color |= Meaning\n| (% style=\"background-color:#c8e6c9; padding:4px;\" %)Green | Core analysis (verdicts, claims)\n| (% style=\"background-color:#e3f2fd; padding:4px;\" %)Blue | Infrastructure (jobs, config, metrics)\n| (% style=\"background-color:#fff3e0; padding:4px;\" %)Orange | Evidence and sources\n| (% style=\"background-color:#e1bee7; padding:4px;\" %)Purple | Understanding and decomposition\n| (% style=\"background-color:#fff9c4; padding:4px;\" %)Yellow | Quality and validation\n| (% style=\"background-color:#f5f5f5; padding:4px;\" %)Grey | Runtime-only / transient\n| (% style=\"background-color:#ffcdd2; padding:4px;\" %)Red | Planned / not yet implemented\n\n----\n\n== Overview ERD ==\n\nBird's-eye view showing all major entity groups and their primary relationships. Minimal field detail -- use the other views for field-level information.\n\n{{mermaid}}\n\nerDiagram\n    JOB ||--|| ANALYSIS_RESULT : produces\n    JOB ||--o{ JOB_EVENT : logs\n    JOB }o--|| CONFIG_SNAPSHOT : uses\n\n    ANALYSIS_RESULT ||--|| CB_CLAIM_UNDERSTANDING : contains\n    ANALYSIS_RESULT ||--o{ ATOMIC_CLAIM : contains\n    ANALYSIS_RESULT ||--o{ EVIDENCE_ITEM : contains\n    ANALYSIS_RESULT ||--o{ FETCHED_SOURCE : contains\n    ANALYSIS_RESULT ||--o{ CLAIM_BOUNDARY : contains\n    ANALYSIS_RESULT ||--o{ CB_CLAIM_VERDICT : contains\n    ANALYSIS_RESULT ||--|| OVERALL_ASSESSMENT : aggregates\n    ANALYSIS_RESULT ||--|| QUALITY_GATES : validated_by\n    ANALYSIS_RESULT ||--|| TWO_PANEL_SUMMARY : summarized_as\n\n    CB_CLAIM_UNDERSTANDING ||--o{ ATOMIC_CLAIM : extracts\n\n    ATOMIC_CLAIM ||--o{ CB_CLAIM_VERDICT : receives\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : cites\n    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : contains\n    BOUNDARY_FINDING }o--|| CLAIM_BOUNDARY : per_boundary\n    EVIDENCE_ITEM }o--|| FETCHED_SOURCE : extracted_from\n    EVIDENCE_ITEM }o--o| CLAIM_BOUNDARY : assigned_to\n\n    OVERALL_ASSESSMENT ||--|| VERDICT_NARRATIVE : has\n    OVERALL_ASSESSMENT ||--|| COVERAGE_MATRIX : has\n\n    SOURCE_RELIABILITY ||--o{ FETCHED_SOURCE : scores\n\n    JOB {\n        string JobId_PK\n        string Status\n        string PipelineVariant\n    }\n\n    ANALYSIS_RESULT {\n        string schemaVersion \"3.0.0-cb\"\n        json ResultJson\n    }\n\n    CB_CLAIM_UNDERSTANDING {\n        string detectedInputType\n        string articleThesis\n        string riskTier\n    }\n\n    ATOMIC_CLAIM {\n        string id_PK\n        string statement\n        string centrality\n        string harmPotential\n    }\n\n    CLAIM_BOUNDARY {\n        string id_PK\n        string name\n        string shortName\n    }\n\n    CB_CLAIM_VERDICT {\n        string id_PK\n        string claimId_FK\n        int truthPercentage\n        string verdict\n        int confidence\n    }\n\n    BOUNDARY_FINDING {\n        string boundaryId_FK\n        int truthPercentage\n        string evidenceDirection\n    }\n\n    EVIDENCE_ITEM {\n        string id_PK\n        string statement\n        string probativeValue\n    }\n\n    FETCHED_SOURCE {\n        string id_PK\n        string url\n        float trackRecordScore\n    }\n\n    OVERALL_ASSESSMENT {\n        int truthPercentage\n        string verdict\n        int confidence\n    }\n\n    VERDICT_NARRATIVE {\n        string headline\n        string keyFinding\n    }\n\n    COVERAGE_MATRIX {\n        json claims_x_boundaries\n    }\n\n    QUALITY_GATES {\n        boolean passed\n        json gate1Stats\n        json gate4Stats\n    }\n\n    TWO_PANEL_SUMMARY {\n        json articleSummary\n        json factharborAnalysis\n    }\n\n    CONFIG_SNAPSHOT {\n        text hash_PK\n        text config_type\n    }\n\n    JOB_EVENT {\n        long Id_PK\n        string JobId_FK\n        string Level\n        string Message\n    }\n\n    SOURCE_RELIABILITY {\n        text domain_PK\n        float score\n        float confidence\n    }\n\n{{/mermaid}}\n\n//Overview: JOB produces an ANALYSIS_RESULT containing decomposed claims (CB_CLAIM_UNDERSTANDING with ATOMIC_CLAIMs), per-claim verdicts (CB_CLAIM_VERDICT with per-boundary BOUNDARY_FINDINGs) supported by evidence (EVIDENCE_ITEM) from web sources (FETCHED_SOURCE), grouped into evidence-emergent CLAIM_BOUNDARYs, and aggregated into an OVERALL_ASSESSMENT with VERDICT_NARRATIVE and COVERAGE_MATRIX. Quality validation (QUALITY_GATES) and a user-facing summary (TWO_PANEL_SUMMARY) complete the result. SOURCE_RELIABILITY provides cached domain-level trust scores for FETCHED_SOURCEs.//\n\n----\n\n== Analysis Result Entities ==\n\nEverything persisted in the ##resultJson## blob at the end of an analysis. This is the complete entity model inside the JSON. Grouped by pipeline stage.\n\n=== Stage 1: Extract Claims ===\n\n{{mermaid}}\n\nerDiagram\n    CB_CLAIM_UNDERSTANDING ||--o{ ATOMIC_CLAIM : extracts\n    CB_CLAIM_UNDERSTANDING ||--|| GATE_1_STATS : produces\n\n    CB_CLAIM_UNDERSTANDING {\n        string detectedInputType \"claim_or_article\"\n        string impliedClaim\n        string articleThesis\n        string backgroundDetails\n        string riskTier \"A_B_or_C\"\n        json distinctEvents\n        json preliminaryEvidence\n    }\n\n    ATOMIC_CLAIM {\n        string id_PK \"AC_01_AC_02\"\n        string statement\n        string category \"factual_evaluative_procedural\"\n        string centrality \"high_medium\"\n        string harmPotential \"critical_high_medium_low\"\n        boolean isCentral \"Always_true_(filtered)\"\n        string claimDirection \"supports_thesis_contradicts_thesis_contextual\"\n        string verifiability \"high_medium_low_none_(optional)\"\n        string checkWorthiness \"high_medium\"\n        float specificityScore \"0-1\"\n        string groundingQuality \"strong_moderate_weak_none\"\n        json keyEntities\n        json expectedEvidenceProfile\n    }\n\n    GATE_1_STATS {\n        int totalClaims\n        int passedOpinion\n        int passedSpecificity\n        int passedFidelity\n        int filteredCount\n        boolean overallPass\n    }\n\n{{/mermaid}}\n\n//Stage 1 output: CB_CLAIM_UNDERSTANDING with ATOMIC_CLAIMs (the analytical units). Each claim has centrality, harmPotential (4-level: critical/high/medium/low), groundingQuality (strong/moderate/weak/none), specificityScore (0-1, Gate 1 minimum 0.6), and an expectedEvidenceProfile describing what evidence would verify or refute the claim. Gate 1 stats include passedFidelity (claim-to-input fidelity check). The optional verifiability field (B-6) independently assesses fact-checkability.//\n\n=== Stage 2: Research ===\n\n{{mermaid}}\n\nerDiagram\n    EVIDENCE_ITEM }o--|| FETCHED_SOURCE : extracted_from\n    EVIDENCE_ITEM ||--o| EVIDENCE_SCOPE : has_scope\n\n    SEARCH_QUERY ||--o{ FETCHED_SOURCE : found\n\n    EVIDENCE_ITEM {\n        string id_PK \"EV_001_EV_002\"\n        string statement\n        string category \"statistic_expert_quote_event_legal_provision_etc\"\n        string sourceId_FK\n        string sourceUrl\n        string sourceTitle\n        string sourceExcerpt\n        string claimDirection \"supports_contradicts_neutral\"\n        string sourceAuthority \"primary_secondary_opinion\"\n        string evidenceBasis \"scientific_documented_anecdotal_theoretical_pseudoscientific\"\n        string probativeValue \"high_medium_low\"\n        float extractionConfidence \"0-100\"\n        string sourceType \"peer_reviewed_study_etc\"\n        json relevantClaimIds\n        string claimBoundaryId_FK \"Assigned_in_Stage_3\"\n        boolean isDerivative\n        string derivedFromSourceUrl\n        boolean derivativeClaimUnverified\n        string scopeQuality \"complete_partial_incomplete\"\n        boolean isContestedClaim\n        boolean fromOppositeClaimSearch\n    }\n\n    EVIDENCE_SCOPE {\n        string name\n        string methodology\n        string temporal\n        string boundaries\n        string geographic\n        string sourceType\n        map additionalDimensions\n    }\n\n    FETCHED_SOURCE {\n        string id_PK\n        string url\n        string title\n        string fullText\n        float trackRecordScore \"0.0-1.0\"\n        float trackRecordConfidence \"0.0-1.0\"\n        boolean trackRecordConsensus\n        string category\n        boolean fetchSuccess\n        string fetchedAt\n        string searchQuery\n    }\n\n    SEARCH_QUERY {\n        string query\n        int iteration\n        string focus\n        int resultsCount\n        string searchProvider\n        string timestamp\n        string error\n    }\n\n{{/mermaid}}\n\n//Stage 2 output: EVIDENCE_ITEMs with EVIDENCE_SCOPEs from FETCHED_SOURCEs. CB pipeline additions: relevantClaimIds (which atomic claims evidence relates to), claimBoundaryId (assigned in Stage 3), isDerivative/derivedFromSourceUrl/derivativeClaimUnverified (derivative evidence tracking), scopeQuality assessment (complete/partial/incomplete), fromOppositeClaimSearch (contradiction research flag). Each SEARCH_QUERY records provider, results, and any error. FETCHED_SOURCE includes trackRecordScore, trackRecordConfidence, and trackRecordConsensus from Source Reliability evaluation.//\n\n=== Stage 3: Cluster Boundaries ===\n\n{{mermaid}}\n\nerDiagram\n    CLAIM_BOUNDARY ||--o{ EVIDENCE_SCOPE : \"composed_of\"\n    EVIDENCE_ITEM }o--o| CLAIM_BOUNDARY : \"assigned_to\"\n\n    CLAIM_BOUNDARY {\n        string id_PK \"CB_01_CB_02\"\n        string name \"Human-readable_label\"\n        string shortName \"Short_label_for_UI_tabs\"\n        string description\n        string methodology \"Dominant_methodology\"\n        string boundaries \"Scope_boundaries\"\n        string geographic \"Geographic_scope\"\n        string temporal \"Temporal_scope\"\n        json constituentScopes \"EvidenceScope[]\"\n        float internalCoherence \"0-1\"\n        int evidenceCount\n    }\n\n    EVIDENCE_SCOPE {\n        string name\n        string methodology\n        string temporal\n    }\n\n    EVIDENCE_ITEM {\n        string claimBoundaryId_FK\n    }\n\n{{/mermaid}}\n\n//Stage 3 output: CLAIM_BOUNDARYs (ClaimAssessmentBoundary) emerge from clustering compatible EVIDENCE_SCOPEs. Each boundary has a name, shortName (for UI tabs), derived methodology/boundaries/geographic/temporal from constituent scopes, internalCoherence (0-1), and evidenceCount. Evidence items receive their claimBoundaryId assignment in this stage.//\n\n=== Stage 4: Verdict (LLM Debate) ===\n\n{{mermaid}}\n\nerDiagram\n    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : contains\n    CB_CLAIM_VERDICT ||--|| CONSISTENCY_RESULT : has\n    CB_CLAIM_VERDICT ||--o{ CHALLENGE_RESPONSE : has\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"cites_supporting\"\n    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"cites_contradicting\"\n    CB_CLAIM_VERDICT ||--|| TRIANGULATION_SCORE : has\n    CB_CLAIM_VERDICT ||--o| TRUTH_PERCENTAGE_RANGE : has\n\n    CB_CLAIM_VERDICT {\n        string id_PK\n        string claimId_FK\n        float truthPercentage \"0-100\"\n        string verdict \"7-point_label\"\n        float confidence \"0-100\"\n        string reasoning\n        string harmPotential \"critical_high_medium_low\"\n        boolean isContested\n        json supportingEvidenceIds\n        json contradictingEvidenceIds\n        json boundaryFindings\n        json consistencyResult\n        json challengeResponses\n        json triangulationScore\n        json truthPercentageRange\n        string misleadingness \"not_potentially_highly_(optional)\"\n        string misleadingnessReason\n    }\n\n    BOUNDARY_FINDING {\n        string boundaryId_FK\n        string boundaryName\n        float truthPercentage \"Per-boundary_0-100\"\n        float confidence \"Per-boundary_0-100\"\n        string evidenceDirection \"supports_contradicts_mixed_neutral\"\n        int evidenceCount\n    }\n\n    CONSISTENCY_RESULT {\n        string claimId_FK\n        json percentages \"Truth_%_from_each_run\"\n        float average\n        float spread \"max_minus_min\"\n        boolean stable \"spread_within_threshold\"\n        boolean assessed \"false_if_disabled\"\n    }\n\n    CHALLENGE_RESPONSE {\n        string challengeType \"assumption_missing_evidence_methodology_weakness_independence_concern\"\n        string response\n        boolean verdictAdjusted\n        json adjustmentBasedOnChallengeIds\n    }\n\n    TRIANGULATION_SCORE {\n        int boundaryCount\n        int supporting\n        int contradicting\n        string level \"strong_moderate_weak_conflicted\"\n        float factor \"Weight_adjustment\"\n    }\n\n    TRUTH_PERCENTAGE_RANGE {\n        float min \"0-100\"\n        float max \"0-100\"\n    }\n\n{{/mermaid}}\n\n//Stage 4 output: CB_CLAIM_VERDICTs via 5-step LLM debate (Advocate -> Self-Consistency -> Adversarial Challenge -> Reconciliation -> Validation). Each verdict includes per-boundary BOUNDARY_FINDINGs, CONSISTENCY_RESULT (self-consistency spread across multiple LLM runs), CHALLENGE_RESPONSEs (how challenges were addressed, with adjustmentBasedOnChallengeIds for provenance), TRIANGULATION_SCORE (cross-boundary agreement), TRUTH_PERCENTAGE_RANGE (plausible range from consistency spread and boundary variance), and optional misleadingness assessment (B-7, output-only).//\n\n=== Stage 5: Aggregate + Quality ===\n\n{{mermaid}}\n\nerDiagram\n    OVERALL_ASSESSMENT ||--o{ CB_CLAIM_VERDICT : aggregates\n    OVERALL_ASSESSMENT ||--o{ CLAIM_BOUNDARY : presents\n    OVERALL_ASSESSMENT ||--|| VERDICT_NARRATIVE : has\n    OVERALL_ASSESSMENT ||--|| COVERAGE_MATRIX : has\n    OVERALL_ASSESSMENT ||--|| QUALITY_GATES : has\n    OVERALL_ASSESSMENT ||--o| EXPLANATION_QUALITY_CHECK : has\n    OVERALL_ASSESSMENT ||--o| TRUTH_PERCENTAGE_RANGE : has\n\n    QUALITY_GATES ||--o| GATE1_STATS : claim_validation\n    QUALITY_GATES ||--o| GATE4_STATS : verdict_confidence\n    QUALITY_GATES ||--o| QUALITY_GATES_SUMMARY : high_level\n\n    EXPLANATION_QUALITY_CHECK ||--|| EXPLANATION_STRUCTURAL_FINDINGS : has\n    EXPLANATION_QUALITY_CHECK ||--o| EXPLANATION_RUBRIC_SCORES : has\n\n    OVERALL_ASSESSMENT {\n        float truthPercentage \"0-100_weighted\"\n        string verdict \"7-point_label\"\n        float confidence \"0-100_weighted\"\n        boolean hasMultipleBoundaries\n        json verdictNarrative\n        json claimBoundaries\n        json claimVerdicts\n        json coverageMatrix\n        json qualityGates\n        json truthPercentageRange\n        json explanationQualityCheck\n    }\n\n    VERDICT_NARRATIVE {\n        string headline\n        string evidenceBaseSummary\n        string keyFinding\n        json boundaryDisagreements\n        string limitations\n    }\n\n    COVERAGE_MATRIX {\n        json claims \"Rows_(claim_IDs)\"\n        json boundaries \"Columns_(boundary_IDs)\"\n        json counts \"Evidence_per_cell\"\n    }\n\n    QUALITY_GATES {\n        boolean passed\n    }\n\n    QUALITY_GATES_SUMMARY {\n        int totalEvidenceItems\n        int totalSources\n        int searchesPerformed\n        boolean contradictionSearchPerformed\n    }\n\n    GATE1_STATS {\n        int total\n        int passed\n        int filtered\n        int centralKept\n    }\n\n    GATE4_STATS {\n        int total\n        int publishable\n        int highConfidence\n        int mediumConfidence\n        int lowConfidence\n        int insufficient\n        int centralKept\n    }\n\n    TRUTH_PERCENTAGE_RANGE {\n        float min \"0-100\"\n        float max \"0-100\"\n    }\n\n    EXPLANATION_QUALITY_CHECK {\n        string mode \"structural_or_rubric\"\n    }\n\n    EXPLANATION_STRUCTURAL_FINDINGS {\n        boolean hasCitedEvidence\n        boolean hasVerdictCategory\n        boolean hasConfidenceStatement\n        boolean hasLimitations\n    }\n\n    EXPLANATION_RUBRIC_SCORES {\n        int clarity \"1-5\"\n        int completeness \"1-5\"\n        int neutrality \"1-5\"\n        int evidenceSupport \"1-5\"\n        int appropriateHedging \"1-5\"\n        float overallScore \"weighted_average\"\n        json flags\n    }\n\n    ANALYSIS_WARNING {\n        string type\n        string severity \"error_warning_info\"\n        string message\n        json details\n    }\n\n    TWO_PANEL_SUMMARY {\n        json articleSummary\n        json factharborAnalysis\n    }\n\n    PSEUDOSCIENCE_ANALYSIS {\n        boolean isPseudoscience\n        int confidence\n        json categories\n        json matchedPatterns\n    }\n\n{{/mermaid}}\n\n//Stage 5 output: OVERALL_ASSESSMENT aggregates CB_CLAIM_VERDICTs with weighted averaging (centrality, harm, confidence, triangulation, derivative). VERDICT_NARRATIVE provides structured summary (headline, evidenceBaseSummary, keyFinding, boundaryDisagreements, limitations). COVERAGE_MATRIX maps claims to boundaries. QUALITY_GATES summarize Gate 1 (claim validation with total/passed/filtered/centralKept) and Gate 4 (verdict confidence with publishable/high/medium/low/insufficient/centralKept), plus QUALITY_GATES_SUMMARY (evidence and search counts). EXPLANATION_QUALITY_CHECK (B-8) provides Tier 1 structural findings and optional Tier 2 rubric scores. TRUTH_PERCENTAGE_RANGE gives the plausible overall range. TWO_PANEL_SUMMARY, PSEUDOSCIENCE_ANALYSIS, and ANALYSIS_WARNINGs complete the result.//\n\n----\n\n== Target Database Entities ==\n\nEntities that should become PostgreSQL tables in the target architecture. Color indicates implementation status: green = exists as table, blue = target (currently in JSON blob), red = planned but not implemented.\n\n{{mermaid}}\n\nerDiagram\n    JOBS ||--o{ JOB_EVENTS : logs\n    JOBS ||--o| ANALYSIS_METRICS : tracked_by\n    JOBS ||--o{ CONFIG_USAGE : snapshot\n    CONFIG_USAGE }o--|| CONFIG_BLOBS : references\n    CONFIG_ACTIVE }o--|| CONFIG_BLOBS : points_to\n\n    JOBS ||--o{ ATOMIC_CLAIMS : produces\n    JOBS ||--o{ CB_CLAIM_VERDICTS : produces\n    JOBS ||--o{ EVIDENCE_ITEMS : produces\n    JOBS ||--o{ FETCHED_SOURCES : produces\n    JOBS ||--o{ CLAIM_BOUNDARIES : produces\n\n    CB_CLAIM_VERDICTS }o--o{ EVIDENCE_ITEMS : supported_by\n    CB_CLAIM_VERDICTS }o--|| ATOMIC_CLAIMS : for_claim\n    EVIDENCE_ITEMS }o--|| FETCHED_SOURCES : from\n    EVIDENCE_ITEMS }o--o| CLAIM_BOUNDARIES : assigned_to\n    FETCHED_SOURCES }o--o| SOURCE_RELIABILITY : cached_score\n\n    USERS ||--o{ FLAGS : reports\n    FLAGS }o--|| JOBS : targets\n\n    JOBS {\n        string JobId_PK\n        string Status\n        int Progress\n        string InputType\n        string InputValue\n        string InputPreview\n        string PipelineVariant\n        string ParentJobId_FK\n        int RetryCount\n        datetime RetriedFromUtc\n        string RetryReason\n        string PromptContentHash\n        datetime PromptLoadedUtc\n        datetime CreatedUtc\n        datetime UpdatedUtc\n        json ResultJson\n        text ReportMarkdown\n    }\n\n    JOB_EVENTS {\n        long Id_PK\n        string JobId_FK\n        datetime TsUtc\n        string Level\n        string Message\n    }\n\n    ANALYSIS_METRICS {\n        guid Id_PK\n        string JobId_FK\n        json MetricsJson\n        datetime CreatedUtc\n    }\n\n    CONFIG_BLOBS {\n        text hash_PK\n        text config_type\n        json content\n        text changed_by\n        text change_reason\n        datetime created_at\n    }\n\n    CONFIG_ACTIVE {\n        text config_type_PK\n        text blob_hash_FK\n        datetime activated_at\n    }\n\n    CONFIG_USAGE {\n        string job_id_FK\n        text config_type\n        text blob_hash_FK\n        datetime snapshot_at\n    }\n\n    SOURCE_RELIABILITY {\n        text domain_PK\n        float score\n        float confidence\n        boolean consensus\n        datetime evaluated_at\n        int ttl_days\n    }\n\n    ATOMIC_CLAIMS {\n        string id_PK \"AC_01\"\n        string jobId_FK\n        string statement\n        string category\n        string centrality\n        string harmPotential \"critical_high_medium_low\"\n        float specificityScore\n        string groundingQuality\n        string claimDirection\n        json keyEntities\n        json expectedEvidenceProfile\n    }\n\n    CB_CLAIM_VERDICTS {\n        string id_PK\n        string jobId_FK\n        string claimId_FK\n        int truthPercentage\n        int confidence\n        string verdict\n        string reasoning\n        string harmPotential\n        boolean isContested\n        json supportingEvidenceIds\n        json contradictingEvidenceIds\n        json boundaryFindings\n        json consistencyResult\n        json challengeResponses\n        json triangulationScore\n        json truthPercentageRange\n        string misleadingness\n    }\n\n    EVIDENCE_ITEMS {\n        string id_PK\n        string jobId_FK\n        string statement\n        string category\n        string sourceId_FK\n        string probativeValue\n        string claimDirection\n        string claimBoundaryId_FK\n        json relevantClaimIds\n        boolean isDerivative\n        string derivedFromSourceUrl\n        boolean derivativeClaimUnverified\n        string scopeQuality\n        string sourceType\n        float extractionConfidence\n    }\n\n    FETCHED_SOURCES {\n        string id_PK\n        string jobId_FK\n        string url\n        string domain\n        string title\n        float trackRecordScore\n        float trackRecordConfidence\n        boolean trackRecordConsensus\n        boolean fetchSuccess\n        datetime fetchedAt\n    }\n\n    CLAIM_BOUNDARIES {\n        string id_PK \"CB_01\"\n        string jobId_FK\n        string name\n        string shortName\n        string description\n        string methodology\n        string geographic\n        string temporal\n        float internalCoherence\n        int evidenceCount\n    }\n\n    USERS {\n        uuid id_PK\n        string username\n        string email\n        string role\n        datetime created_at\n    }\n\n    FLAGS {\n        uuid id_PK\n        string entity_type\n        string entity_id_FK\n        uuid reported_by_FK\n        string issue_type\n        string status\n    }\n\n    QUALITY_METRICS {\n        uuid id_PK\n        string metric_type\n        string category\n        float value\n        float target\n        datetime timestamp\n    }\n\n{{/mermaid}}\n\n=== Implementation Status ===\n\n|= Table |= Status |= Technology |= Notes\n| ##jobs## | Exists | .NET EF Core (SQLite) | Analysis results stored as JSON blob in ##ResultJson##. Includes retry tracking (ParentJobId, RetryCount) and prompt tracking (PromptContentHash).\n| ##job_events## | Exists | .NET EF Core (SQLite) | Event log with SSE streaming\n| ##analysis_metrics## | Exists | .NET EF Core (SQLite) | Metrics as JSON blob\n| ##config_blobs## | Exists | Next.js better-sqlite3 | Immutable, content-addressed\n| ##config_active## | Exists | Next.js better-sqlite3 | Activation pointers\n| ##config_usage## | Exists | Next.js better-sqlite3 | Per-job config snapshots\n| ##source_reliability## | Exists | Next.js better-sqlite3 | LLM-evaluated cache (90-day TTL)\n| ##atomic_claims## | **Target** | PostgreSQL | Normalised from JSON blob (includes claimDirection, keyEntities, expectedEvidenceProfile)\n| ##cb_claim_verdicts## | **Target** | PostgreSQL | Normalised from JSON blob (includes boundaryFindings, consistencyResult, challengeResponses, triangulationScore, truthPercentageRange, misleadingness)\n| ##evidence_items## | **Target** | PostgreSQL | Normalised from JSON blob (includes claimBoundaryId, derivative flags, scopeQuality, sourceType)\n| ##fetched_sources## | **Target** | PostgreSQL | Normalised from JSON blob (includes trackRecordConfidence, trackRecordConsensus)\n| ##claim_boundaries## | **Target** | PostgreSQL | Normalised from JSON blob (includes geographic, temporal, internalCoherence)\n| ##users## | **Planned** | PostgreSQL | Not yet implemented (Alpha phase)\n| ##flags## | **Planned** | PostgreSQL | Not yet implemented (Alpha phase)\n| ##quality_metrics## | **Planned** | PostgreSQL | Not yet implemented (time-series)\n\nFor detailed field descriptions, denormalisation strategy, and cost projections, see [[Target Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]].\n\n----\n\n== Runtime Process Entities ==\n\nEntities that exist only during pipeline execution. These are transient -- they facilitate processing but are not directly stored in the result JSON. Some contribute data that flows into stored entities.\n\n{{mermaid}}\n\nflowchart TD\n    subgraph Extract[\"EXTRACT CLAIMS (Stage 1)\"]\n        ICR[\"InputClassificationResult\\nisComparative, isCompound\\nclaimType, complexity\"]\n        G1M[\"Gate 1 Validation\\nclaims filtered, fidelity check\"]\n    end\n\n    subgraph Research[\"RESEARCH (Stage 2)\"]\n        RS[\"CBResearchState\\n(main mutable container)\\nunderstanding, evidenceItems\\nsources, searchQueries, llmCalls\\nmainIterationsUsed\\ncontradictionIterations\\nqueryBudgetUsageByClaim\\nwarnings\"]\n        RD[\"ResearchDecision\\ncomplete, focus, queries\"]\n        EQR[\"EvidenceQualityResult\\nqualityAssessment, issues\\nreasoning (per evidence)\"]\n        PV[\"ProvenanceValidation\\nURL validation, excerpt check\"]\n        BT[\"BudgetTracker\\ntokens, iterations\\nllmCalls, budgetExceeded\"]\n    end\n\n    subgraph Cluster[\"CLUSTER BOUNDARIES (Stage 3)\"]\n        SCOPE_CLUSTER[\"Scope Clustering\\nEvidenceScope compatibility\\nmerge vs separate decision\"]\n    end\n\n    subgraph Verdict[\"VERDICT (Stage 4)\"]\n        ADV[\"Advocate Verdict\\ninitial per-claim verdicts\"]\n        SC[\"Self-Consistency Check\\ntemp=0.3, spread measurement\"]\n        CHAL[\"ChallengeDocument\\nchallenge points per claim\\nwith ChallengeValidation\"]\n        REC[\"Reconciliation\\nfinal verdicts\"]\n        VV[\"Verdict Validation\\ngrounding + direction checks\"]\n        G4M[\"Gate 4 Assessment\\nconfidence distribution\"]\n    end\n\n    subgraph Metrics[\"METRICS (Full Pipeline)\"]\n        MC[\"MetricsCollector\\naccumulates all telemetry\"]\n        LCM[\"LLMCallMetric\\ntokens, duration, success\"]\n        SQM[\"SearchQueryMetric\\nquery, provider, duration\"]\n    end\n\n    ICR -->|\"data flows into\"| CU[\"CBClaimUnderstanding\\n(STORED in result)\"]\n    G1M -->|\"summary\"| QG[\"gate1Stats\\n(STORED in result)\"]\n    EQR -->|\"filters\"| EI[\"EvidenceItems\\n(STORED in result)\"]\n    PV -->|\"filters\"| EI\n    SCOPE_CLUSTER -->|\"creates\"| CB[\"ClaimBoundaries\\n(STORED in result)\"]\n    ADV -->|\"initial\"| VERD[\"CBClaimVerdicts\\n(STORED in result)\"]\n    SC -->|\"consistency\"| VERD\n    CHAL -->|\"challenges\"| VERD\n    REC -->|\"final\"| VERD\n    VV -->|\"validates\"| VERD\n    G4M -->|\"summary\"| QG4[\"gate4Stats\\n(STORED in result)\"]\n    BT -->|\"summary\"| BS[\"meta.budgetStats\\n(STORED in result)\"]\n    RD -->|\"discarded\"| DISC[\"Discarded\"]\n    MC -->|\"sent to API\"| AM[\"AnalysisMetrics\\n(STORED in separate DB)\"]\n    LCM --> MC\n    SQM --> MC\n    RS -->|\"fields flow into\"| RES[\"resultJson fields\\n(STORED in result)\"]\n\n    style CU fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style QG fill:#fff9c4,stroke:#f9a825,color:#000\n    style EI fill:#fff3e0,stroke:#e65100,color:#000\n    style CB fill:#e1bee7,stroke:#7b1fa2,color:#000\n    style VERD fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style QG4 fill:#fff9c4,stroke:#f9a825,color:#000\n    style BS fill:#e3f2fd,stroke:#1565c0,color:#000\n    style AM fill:#e3f2fd,stroke:#1565c0,color:#000\n    style RES fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style DISC fill:#f5f5f5,stroke:#9e9e9e,color:#666\n\n    style ICR fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style G1M fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style RS fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style RD fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style EQR fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style PV fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style BT fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style SCOPE_CLUSTER fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style ADV fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style SC fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style CHAL fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style REC fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style VV fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style G4M fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style MC fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style LCM fill:#f5f5f5,stroke:#9e9e9e,color:#000\n    style SQM fill:#f5f5f5,stroke:#9e9e9e,color:#000\n\n{{/mermaid}}\n\n//Grey boxes = transient runtime entities. Colored boxes = stored destinations. Arrows show how runtime data flows into (or is discarded from) the final result. CBResearchState is the main mutable container through the pipeline, accumulating evidenceItems, sources, searchQueries, claimBoundaries, and warnings. ChallengeDocument contains ChallengePoints with ChallengeValidation (structural validation of evidence references). Source files: ##types.ts##, ##metrics.ts##, ##budgets.ts##, ##claimboundary-pipeline.ts##, ##verdict-stage.ts##.//\n\n=== Runtime Entity Reference ===\n\n|= Entity |= Source File |= Lifecycle |= Destination\n| ##CBResearchState## | ##types.ts:971## | Full pipeline | Container -- fields distributed into resultJson. Includes queryBudgetUsageByClaim, iteration counters, and accumulated warnings.\n| ##BudgetTracker## | ##budgets.ts## | Full pipeline | ##meta.budgetStats##\n| ##ResearchDecision## | ##types.ts:599## | Per iteration | Discarded after use\n| ##InputClassificationResult## | ##text-analysis-types.ts## | Extract phase | Data flows into ##CBClaimUnderstanding##\n| ##EvidenceQualityResult## | ##text-analysis-types.ts## | Research phase | Filters ##EvidenceItems## (pass/fail)\n| ##VerdictValidationResult## | ##types.ts:121## | Verdict phase | Advisory checks on ##CBClaimVerdicts##\n| ##MetricsCollector## | ##metrics.ts## | Full pipeline | ##AnalysisMetrics## (separate DB)\n| ##LLMCallMetric## | ##metrics.ts## | Per LLM call | Aggregated in ##MetricsCollector##\n| ##SearchQueryMetric## | ##metrics.ts## | Per search call | Aggregated in ##MetricsCollector##\n| ##ProvenanceValidation## | ##evidence-filter.ts## | Research phase | Filters evidence (pass/fail)\n| ##ChallengeDocument## | ##types.ts:845## | Verdict Step 3 | Challenge points consumed by Reconciliation (Step 4)\n| ##ChallengePoint## | ##types.ts:857## | Verdict Step 3 | Individual challenges with ChallengeValidation, consumed by Reconciliation\n| ##ChallengeValidation## | ##types.ts:874## | Verdict Step 3 | Structural validation of evidence references (validIds/invalidIds)\n| ##ConsistencyResult## | ##types.ts:820## | Verdict Step 2 | Stored in ##CBClaimVerdict.consistencyResult##\n\n----\n\n== UI-Visible Entities ==\n\nEntities and fields that surface to users in the browser. Grouped by UI page.\n\n{{mermaid}}\n\nflowchart TD\n    subgraph JobsList[\"/jobs - Jobs List Page\"]\n        JL_JOB[\"JOB\\nstatus, progress\\ninputPreview\\npipelineVariant\\ncreatedUtc\"]\n    end\n\n    subgraph JobDetail[\"/jobs/id - Job Detail Page\"]\n\n        subgraph VerdictBanner[\"Verdict Banner\"]\n            JD_OA[\"OVERALL_ASSESSMENT\\ntruthPercentage, verdict\\nconfidence\\ntruthPercentageRange\"]\n            JD_VN[\"VERDICT_NARRATIVE\\nheadline, keyFinding\\nlimitations\"]\n            JD_TPS[\"TWO_PANEL_SUMMARY\\narticleSummary\\nfactharborAnalysis\"]\n        end\n\n        subgraph BoundarySection[\"ClaimAssessmentBoundary Tabs\"]\n            JD_CB[\"CLAIM_BOUNDARY\\nname, shortName\\nmethodology, temporal\\ngeographic\"]\n            JD_BF[\"BOUNDARY_FINDING\\ntruthPercentage, confidence\\nevidenceDirection, evidenceCount\"]\n        end\n\n        subgraph ClaimsSection[\"Claims Analyzed\"]\n            JD_AC[\"ATOMIC_CLAIM\\nstatement, category\\ncentrality, harmPotential\"]\n            JD_CV[\"CB_CLAIM_VERDICT\\nverdict, truthPercentage\\nconfidence, reasoning\\nharmPotential, isContested\\nmisleadingness\"]\n        end\n\n        subgraph EvidenceSection[\"Evidence Panel\"]\n            JD_EI[\"EVIDENCE_ITEM\\nstatement, category\\nsourceTitle, claimDirection\\nprobativeValue\"]\n            JD_ES[\"EVIDENCE_SCOPE\\n(tooltip: methodology\\nboundaries, geographic)\"]\n        end\n\n        subgraph SourcesSection[\"Sources Tab\"]\n            JD_FS[\"FETCHED_SOURCE\\nurl, title\\ntrackRecordScore\\nfetchSuccess\"]\n        end\n\n        subgraph QualitySection[\"Quality Panel\"]\n            JD_QG[\"QUALITY_GATES\\npassed, gate1Stats\\ngate4Stats\"]\n            JD_AW[\"ANALYSIS_WARNING\\ntype, severity\\nmessage\"]\n            JD_EQC[\"EXPLANATION_QUALITY_CHECK\\nmode, structuralFindings\\nrubricScores (when rubric)\"]\n        end\n\n        subgraph SearchSection[\"Search Queries\"]\n            JD_SQ[\"SEARCH_QUERY\\nquery, resultsCount\\nsearchProvider\"]\n        end\n    end\n\n    subgraph Admin[\"/admin - Admin Pages\"]\n        AD_HEALTH[\"Provider Health\\nstate, consecutiveFailures\"]\n        AD_METRICS[\"ANALYSIS_METRICS\\navgDuration, avgCost\\ngate1PassRate\\ngate4HighConfidenceRate\"]\n        AD_CONFIG[\"CONFIG_SNAPSHOT\\npipelineConfig\\nsearchConfig\"]\n    end\n\n    JL_JOB -->|\"click job\"| JobDetail\n    JD_AC -->|\"has verdict\"| JD_CV\n    JD_CV -->|\"references\"| JD_EI\n    JD_EI -->|\"from\"| JD_FS\n    JD_CB -->|\"findings\"| JD_BF\n\n    style JL_JOB fill:#e3f2fd,stroke:#1565c0,color:#000\n    style JD_OA fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style JD_VN fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style JD_TPS fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style JD_CB fill:#e1bee7,stroke:#7b1fa2,color:#000\n    style JD_BF fill:#e1bee7,stroke:#7b1fa2,color:#000\n    style JD_AC fill:#e1bee7,stroke:#7b1fa2,color:#000\n    style JD_CV fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style JD_EI fill:#fff3e0,stroke:#e65100,color:#000\n    style JD_ES fill:#fff3e0,stroke:#e65100,color:#000\n    style JD_FS fill:#fff3e0,stroke:#e65100,color:#000\n    style JD_QG fill:#fff9c4,stroke:#f9a825,color:#000\n    style JD_AW fill:#fff9c4,stroke:#f9a825,color:#000\n    style JD_EQC fill:#fff9c4,stroke:#f9a825,color:#000\n    style JD_SQ fill:#fff3e0,stroke:#e65100,color:#000\n    style AD_HEALTH fill:#e3f2fd,stroke:#1565c0,color:#000\n    style AD_METRICS fill:#e3f2fd,stroke:#1565c0,color:#000\n    style AD_CONFIG fill:#e3f2fd,stroke:#1565c0,color:#000\n\n{{/mermaid}}\n\n//UI visibility: The Jobs list shows minimal JOB metadata. The Job detail page renders the full analysis result across multiple sections: verdict banner (OVERALL_ASSESSMENT with truthPercentageRange + VERDICT_NARRATIVE), ClaimAssessmentBoundary tabs with BOUNDARY_FINDINGs (including evidenceCount), claims section showing ATOMIC_CLAIMs with their CB_CLAIM_VERDICTs (including misleadingness when present), evidence panel, sources, quality gates (including EXPLANATION_QUALITY_CHECK from B-8), and search queries. Admin pages show operational data (provider health, aggregated metrics, config snapshots).//\n\n=== What Users See vs. What's Internal ===\n\n|= Entity |= User-Visible Fields |= Internal-Only Fields\n| **AtomicClaim** | statement, category, centrality, harmPotential, claimDirection | specificityScore, groundingQuality, checkWorthiness, keyEntities, expectedEvidenceProfile, verifiability\n| **CBClaimVerdict** | truthPercentage, verdict, confidence, reasoning, isContested, harmPotential, misleadingness | triangulationScore, consistencyResult details, challengeResponses, supportingEvidenceIds, contradictingEvidenceIds, truthPercentageRange details\n| **EvidenceItem** | statement, category, sourceTitle, claimDirection, probativeValue | sourceId, extractionConfidence, scopeQuality, sourceAuthority, evidenceBasis, isDerivative, derivedFromSourceUrl, derivativeClaimUnverified, fromOppositeClaimSearch, isContestedClaim, relevantClaimIds\n| **FetchedSource** | url, title, trackRecordScore, fetchSuccess | fullText, fetchedAt, searchQuery, trackRecordConfidence, trackRecordConsensus, category\n| **ClaimBoundary** | name, shortName, description, methodology, geographic, temporal | internalCoherence, constituentScopes, evidenceCount\n| **BoundaryFinding** | truthPercentage, confidence, evidenceDirection, evidenceCount | boundaryId (used for cross-referencing)\n| **OverallAssessment** | truthPercentage, verdict, confidence, truthPercentageRange | hasMultipleBoundaries, coverageMatrix details\n| **VerdictNarrative** | headline, keyFinding, limitations, boundaryDisagreements | evidenceBaseSummary\n| **QualityGates** | passed, gate1Stats (counts), gate4Stats (counts) | Individual ClaimValidationResult, VerdictValidationResult per claim\n| **ExplanationQualityCheck** | mode, structuralFindings, rubricScores (when rubric mode) | Internal scoring details\n\n----\n\n**Navigation:** [[Diagrams Index>>FactHarbor.Product Development.Diagrams.WebHome]] | [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] | [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] | [[Target Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]\n", "Product Development.Diagrams.Evidence and Verdict Workflow.WebHome": "{{info}}\n**Current Implementation (v2.10.2)** - Simplified model without versioning. Uses 7-point symmetric verdict scale.\n{{/info}}\n\n= Evidence and Verdict Data Model =\n\n{{mermaid}}\n\nerDiagram\n    CLAIM ||--|| CLAIM_VERDICT : has\n    CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : supported_by\n    EVIDENCE_ITEM }o--|| SOURCE : from\n\n    CLAIM {\n        string id_PK\n        string text\n        string type\n        string claimRole\n        boolean isCentral\n        string_array dependsOn\n    }\n\n    CLAIM_VERDICT {\n        string id_PK\n        string claimId_FK\n        string verdict\n        int truthPercentage\n        int confidence\n        string explanation\n        string_array supportingEvidenceIds\n        string_array opposingEvidenceIds\n        string contestationStatus\n        float harmPotential\n    }\n\n    EVIDENCE_ITEM {\n        string id_PK\n        string sourceId_FK\n        string statement\n        string sourceExcerpt\n        string category\n        string claimDirection\n        string contextId\n    }\n\n    SOURCE {\n        string id_PK\n        string title\n        string domain\n        string url\n        float trackRecordScore\n        string bias\n        string factualReporting\n    }\n\n{{/mermaid}}\n\n= Verdict Generation Flow =\n\n{{mermaid}}\n\nflowchart TB\n    subgraph Research[Research Phase]\n        EVIDENCE[Collected Evidence]\n        SOURCES[Source Metadata]\n    end\n\n    subgraph Analysis[Analysis]\n        WEIGHT[Weight Evidence by source reliability]\n        CONTEST[Check Contestation doubted vs contested]\n        HARM[Assess Harm Potential]\n    end\n\n    subgraph Verdict[Verdict Generation]\n        CALC[Calculate Truth Percentage]\n        MAP[Map to 7-point Scale]\n        CONF[Assign Confidence]\n    end\n\n    subgraph Output[Result]\n        CLAIM_V[Claim Verdict]\n        ARTICLE_V[Article Verdict]\n    end\n\n    EVIDENCE --> WEIGHT\n    SOURCES --> WEIGHT\n    WEIGHT --> CONTEST\n    CONTEST --> HARM\n    HARM --> CALC\n    CALC --> MAP\n    MAP --> CONF\n    CONF --> CLAIM_V\n    CLAIM_V --> ARTICLE_V\n\n{{/mermaid}}\n\n== 7-Point Verdict Scale ==\n\n|= Verdict |= Truth % Range |= Description\n| **TRUE** | 86-100% | Claim is well-supported by evidence\n| **MOSTLY-TRUE** | 72-85% | Largely accurate with minor caveats\n| **LEANING-TRUE** | 58-71% | More evidence supports than contradicts\n| **MIXED** | 43-57% (high conf) | Roughly equal evidence both ways\n| **UNVERIFIED** | 43-57% (low conf) | Insufficient evidence to determine\n| **LEANING-FALSE** | 29-42% | More evidence contradicts than supports\n| **MOSTLY-FALSE** | 15-28% | Largely inaccurate\n| **FALSE** | 0-14% | Claim is refuted by evidence\n\n== Contestation Status ==\n\n* **Doubted**: Evidence is weak, uncertain, or ambiguous\n* **Contested**: Strong evidence exists on both sides\n\n== Source Reliability ==\n\nSource reliability scores use **LLM + Cache architecture** (v2.2):\n* LLM-based assessment with in-memory caching\n* Batch prefetch  in-memory map  sync lookup\n* Configurable via UCM SR config (##source-reliability.ts##)", "Product Development.Diagrams.Evidence Defence in Depth.WebHome": "= Evidence Defence in Depth =\n\n{{mermaid}}\nflowchart TB\n    RAW[\"Raw Evidence\\n(from LLM extraction)\"]\n\n    subgraph P1[\"Phase 1: Evidence Quality (Pre-Verdict)\"]\n        L1[\"LLM Prompt Instructions\\n(guide extraction quality)\"]\n        L2[\"Deterministic Filter\\n(min length, vague phrases, dedup,\\ncategory rules, probative scoring)\"]\n        L3[\"Provenance Validation\\n(reject synthetic/LLM-generated content)\"]\n    end\n\n    subgraph P2[\"Phase 2: Verdict Quality (Post-Verdict)\"]\n        L4[\"Source Reliability Weighting\\n(domain credibility 0.0-1.0)\"]\n        L5[\"Aggregation Pruning\\n(tangential, opinion-only,\\ncontestation validation)\"]\n        L6[\"Verdict Corrections\\n(inversion detection,\\ndirection mismatch fix)\"]\n    end\n\n    CLEAN[\"Validated Verdict\\n(published to report)\"]\n\n    RAW --> L1 --> L2 --> L3 --> L4 --> L5 --> L6 --> CLEAN\n\n    style RAW fill:#ffcdd2,stroke:#b71c1c,color:#000\n    style CLEAN fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style P1 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style P2 fill:#fff9c4,stroke:#f9a825,color:#000\n    style L1 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style L2 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style L3 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style L4 fill:#fff9c4,stroke:#f9a825,color:#000\n    style L5 fill:#fff9c4,stroke:#f9a825,color:#000\n    style L6 fill:#fff9c4,stroke:#f9a825,color:#000\n{{/mermaid}}\n\n//Two-phase defence system: Phase 1 (blue) filters evidence before verdict generation, Phase 2 (yellow) protects verdict quality through source reliability weighting, aggregation pruning, and verdict corrections.//\n", "Product Development.Diagrams.Evidence Quality Filtering Pipeline.WebHome": "= Evidence Quality Filtering Pipeline =\n\nThis diagram shows the evidence quality filtering layers across both pipeline variants. The **ClaimAssessmentBoundary pipeline** (default, production) uses Layers 1, 6, and 7. Layers 2-5 are defined in shared modules but currently only active in the **Monolithic Dynamic pipeline** (alternative).\n\n{{mermaid}}\nflowchart TB\n    subgraph CB[\"ClaimAssessmentBoundary Pipeline (default)\"]\n        L1[\"Layer 1: Evidence Quality Filtering<br/>evidence-filter.ts<br/>Structural safety net: opinion sources,<br/>low probativeValue, short statements,<br/>missing excerpts/URLs, statistics without numbers\"]\n        L6[\"Layer 6: Baseless Challenge Enforcement<br/>verdict-stage.ts<br/>Revert verdict adjustments from challenges<br/>lacking documented counter-evidence\"]\n        L7[\"Layer 7: Boundary Clustering<br/>claimboundary-pipeline.ts<br/>Stage 3: clusterBoundaries<br/>Group evidence by EvidenceScope congruence\"]\n    end\n\n    subgraph MONO[\"Monolithic Dynamic Pipeline only\"]\n        L2[\"Layer 2: Provenance Validation<br/>provenance-validation.ts<br/>Reject evidence items missing<br/>valid source URL or excerpt\"]\n        L3[\"Layer 3: Tangential Baseless Pruning<br/>aggregation.ts<br/>Remove tangential/irrelevant claims<br/>with insufficient supporting evidence\"]\n        L4[\"Layer 4: Thesis Relevance Filtering<br/>aggregation.ts<br/>Non-direct claims get weight=0<br/>via getClaimWeight\"]\n        L5[\"Layer 5: Opinion-Only KeyFactor Pruning<br/>aggregation.ts<br/>Drop KeyFactors with no documented<br/>evidence basis (opinion/unknown)\"]\n    end\n\n    L1 --> L6 --> L7\n    L2 --> L3 --> L4 --> L5\n\n    style L1 fill:#c8e6c9,color:#000\n    style L6 fill:#ffccbc,color:#000\n    style L7 fill:#e3f2fd,color:#000\n    style L2 fill:#fff9c4,color:#000\n    style L3 fill:#fff9c4,color:#000\n    style L4 fill:#fff9c4,color:#000\n    style L5 fill:#fff9c4,color:#000\n\n{{/mermaid}}\n\n//Green = structural evidence filter (deterministic field checks). Orange = baseless challenge enforcement (hybrid: deterministic revert of LLM-assessed challenges). Blue = boundary clustering (LLM-powered Stage 3). Yellow = deterministic enforcement layers (Monolithic Dynamic pipeline only).//\n\n== Layer Details ==\n\n=== Layer 1: Evidence Quality Filtering (both pipelines) ===\n\n**File:** ##evidence-filter.ts## | **Function:** ##filterByProbativeValue##\n\nDeterministic structural safety net. Semantic quality assessment (vague phrases, attribution, deduplication) is handled by the LLM evidence quality service (##assessEvidenceQuality##) which runs before this filter.\n\nStructural checks:\n* Opinion sources (##sourceAuthority === \"opinion\"##)\n* Low probativeValue (LLM-assigned field, ##probativeValue === \"low\"##)\n* Statement too short (< 20 characters)\n* Missing or short source excerpt (< 30 characters)\n* Missing source URL\n* Statistics without numbers (digit presence check)\n\n=== Layer 6: Baseless Challenge Enforcement (CB pipeline) ===\n\n**File:** ##verdict-stage.ts## | **Function:** ##enforceBaselessChallengePolicy##\n\nAfter the 5-step LLM debate pattern (advocate, consistency, challenge, reconcile, validate), this layer reverts any verdict adjustments caused by challenges that lack documented counter-evidence. Satisfies the AGENTS.md rule: //evidence-weighted contestation -- baseless challenges MUST NOT reduce truth% or confidence.//\n\n=== Layer 7: Boundary Clustering (CB pipeline) ===\n\n**File:** ##claimboundary-pipeline.ts## | **Function:** ##clusterBoundaries##\n\nStage 3 of the ClaimAssessmentBoundary pipeline. A single Sonnet-tier LLM call groups EvidenceScopes with compatible methodology, geography, temporal period, and analytical boundaries into ClaimAssessmentBoundaries.\n\n=== Layers 2-5: Monolithic Dynamic Pipeline Only ===\n\nThese layers are defined in shared modules (##provenance-validation.ts##, ##aggregation.ts##) but are not imported or called by the ClaimAssessmentBoundary pipeline. They remain active in the Monolithic Dynamic pipeline variant.\n\n* **Layer 2** (##filterEvidenceByProvenance##): Validates source URL format (HTTP/S) and excerpt presence. Evidence items without valid provenance are rejected before verdict processing.\n* **Layer 3** (##pruneTangentialBaselessClaims##): Removes tangential or irrelevant claims that have fewer than 2 supporting evidence items, or whose evidence lacks medium/high probativeValue.\n* **Layer 4** (##getClaimWeight## / ##calculateWeightedVerdictAverage##): Non-direct claims (##thesisRelevance !== \"direct\"##) receive weight=0, excluding them from the weighted verdict average.\n* **Layer 5** (##pruneOpinionOnlyFactors##): Drops KeyFactors where ##factualBasis## is \"opinion\" or \"unknown\". Only factors with documented evidence (\"established\" or \"disputed\") are kept.\n", "Product Development.Diagrams.External Dependencies Map.WebHome": "= External Dependencies Map =\n\n{{mermaid}}\nflowchart TB\n    subgraph FH[\"FactHarbor\"]\n        AKEL[\"AKEL Pipeline\"]\n        TIERING[\"Model Tiering\\n(model-tiering.ts)\"]\n        SEARCH_ABS[\"Search Abstraction\\n(web-search.ts)\"]\n        RETRIEVAL[\"Content Retrieval\\n(retrieval.ts)\"]\n        HEALTH[\"Provider Health\\n(provider-health.ts)\"]\n    end\n\n    subgraph LLM[\"LLM Providers (via Vercel AI SDK)\"]\n        ANTHROPIC[\"Anthropic\\nClaude Haiku 4.5\\nClaude Opus 4.6\"]\n        OPENAI[\"OpenAI\\nGPT-4.1-mini\\nGPT-4.1\"]\n        GOOGLE[\"Google\\nGemini 2.5-flash\\nGemini 2.5-pro\"]\n        MISTRAL[\"Mistral\\nLarge / Small\"]\n    end\n\n    subgraph SEARCH[\"Search Providers\"]\n        CSE[\"Google Custom\\nSearch Engine\"]\n        SERP[\"SerpAPI\"]\n    end\n\n    subgraph CONTENT[\"Content Sources\"]\n        HTML[\"HTML Pages\\n(via cheerio)\"]\n        PDF[\"PDF Documents\\n(via pdf2json)\"]\n    end\n\n    AKEL --> TIERING\n    TIERING --> ANTHROPIC\n    TIERING --> OPENAI\n    TIERING --> GOOGLE\n    TIERING --> MISTRAL\n    AKEL --> SEARCH_ABS\n    SEARCH_ABS -->|\"Primary\"| CSE\n    SEARCH_ABS -->|\"Fallback\"| SERP\n    AKEL --> RETRIEVAL\n    RETRIEVAL --> HTML\n    RETRIEVAL --> PDF\n    HEALTH -.->|\"monitors\"| LLM\n    HEALTH -.->|\"monitors\"| SEARCH\n\n    style FH fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style LLM fill:#e3f2fd,stroke:#1565c0,color:#000\n    style SEARCH fill:#fff3e0,stroke:#e65100,color:#000\n    style CONTENT fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n\n//FactHarbor's external dependency map showing integrations with 4 LLM providers, 2 search providers, and web content sources, all routed through abstraction layers and monitored by the provider health system.//\n", "Product Development.Diagrams.Federation Architecture.WebHome": "{{warning}}\n**Not Implemented (v2.10.2)**  Federation is planned for V2.0+. Current implementation is single-instance only.\n{{/warning}}\n\n= Federation Architecture (Future) =\n\n{{mermaid}}\n\ngraph LR\n    FH1[FactHarbor Instance 1]\n    FH2[FactHarbor Instance 2]\n    FH3[FactHarbor Instance 3]\n    FH1 -.->|V1.0+ Sync claims| FH2\n    FH2 -.->|V1.0+ Sync claims| FH3\n    FH3 -.->|V1.0+ Sync claims| FH1\n    U1[Users] --> FH1\n    U2[Users] --> FH2\n    U3[Users] --> FH3\n\n{{/mermaid}}\n\n**Federation Architecture** - Future (V1.0+): Independent FactHarbor instances can sync claims for broader reach while maintaining local control.\n\n== Target Features ==\n\n|= Feature |= Purpose |= Status\n| **Claim synchronization** | Share verified claims across instances | Not implemented\n| **Cross-node audits** | Distributed quality assurance | Not implemented\n| **Local control** | Each instance maintains autonomy | N/A\n| **Contradiction detection** | Cross-instance contradiction checking | Not implemented\n\n== Current Implementation ==\n\n* Single-instance deployment only\n* No inter-instance communication\n* All data stored locally in SQLite", "Product Development.Diagrams.Human User Roles.WebHome": "= User Role Structure =\n\n{{mermaid}}\n\ngraph TD\n    READER[Reader - Guest/Anonymous] --> |Can| R1[Browse Published Analyses]\n    READER --> |Can| R2[Search Content]\n    READER --> |Can| R3[View Analysis Results]\n    USER[User - Registered] --> |Can| RU1[Everything Reader Can]\n    USER --> |Can| RU2[Submit URLs/Text - Rate-Limited]\n    USER --> |Can| RU3[Flag Issues]\n    UCM_ADMIN[UCM Administrator - Appointed] --> |Can| U1[Manage UCM Config]\n    UCM_ADMIN --> |Can| U2[View Config Audit Trail]\n    UCM_ADMIN --> |Can| U3[Trigger Re-Analysis]\n    UCM_ADMIN --> |Can| U4[View System Metrics]\n    MODERATOR[Moderator - Appointed] --> |Can| M1[Review Flags]\n    MODERATOR --> |Can| M2[Hide Harmful Content]\n    MODERATOR --> |Can| M3[Ban Abusive Users]\n\n{{/mermaid}}\n\n= Role Descriptions =\n\n|= Role |= Purpose |= Current Status\n| **Reader (Guest)** | Anonymous browsing, searching, and viewing | Implemented (all users)\n| **User (Registered)** | Submit URLs/text for analysis (rate-limited) | Not yet implemented (no auth)\n| **UCM Administrator** | Manage UCM configuration, view audit trail | Partially implemented (CLI/direct DB)\n| **Moderator** | Handle abuse, enforce community guidelines | Not yet implemented\n\n= Current Implementation =\n\nAll users are anonymous **Readers**:\n* Can view analysis results\n* Can browse and search published analyses\n* No persistent accounts (no authentication system yet)\n* No submission rate limiting (single-user development mode)\n\n= Design Principles =\n\n* **No data editing**  analysis outputs are immutable\n* **Improve the system, not the data**  UCM Administrators tune configuration to improve quality\n* **Moderators handle abuse only**  not content quality (that is automated)\n* **Low barrier to entry**  anyone can browse and search without registration; submission requires a free account\n* **Rate-limited submissions**  LLM inference and web search are not free; registered users have configurable quotas\n", "Product Development.Diagrams.Job Lifecycle ERD.WebHome": "= Job Lifecycle ERD =\n\n{{mermaid}}\nerDiagram\n    JOB ||--o{ JOB_EVENT : \"emits\"\n    JOB ||--o| ANALYSIS_RESULT : \"produces\"\n\n    JOB {\n        string JobId PK \"GUID\"\n        string Status \"QUEUED RUNNING SUCCEEDED FAILED PAUSED\"\n        int Progress \"0-100\"\n        string InputType \"text or url\"\n        string InputValue \"The submitted content\"\n        string InputPreview \"First N chars for display\"\n        string PipelineVariant \"orchestrated or monolithic_dynamic\"\n        string PromptContentHash \"SHA-256 of prompt used\"\n        string ResultJson \"Analysis result JSON blob\"\n        string ReportMarkdown \"Generated report\"\n        datetime CreatedUtc \"When submitted\"\n        datetime UpdatedUtc \"Last status change\"\n        datetime PromptLoadedUtc \"When prompt was loaded\"\n    }\n\n    JOB_EVENT {\n        long Id PK \"Auto-increment\"\n        string JobId FK \"Parent job\"\n        string Level \"info, warn, error\"\n        string Message \"Event description\"\n        datetime TsUtc \"Event timestamp\"\n    }\n\n    ANALYSIS_RESULT {\n        string schemaVersion \"2.7.0\"\n        json meta \"Providers, timing, gate stats\"\n        string articleVerdict \"7-point verdict\"\n        int articleTruthPercentage \"0-100\"\n        json claimPattern \"supported/uncertain/refuted counts\"\n        int llmCalls \"Total LLM invocations\"\n    }\n{{/mermaid}}\n\n//Jobs progress through a lifecycle: QUEUED -> RUNNING -> SUCCEEDED or FAILED. If the system auto-pauses due to provider outage, jobs remain QUEUED until processing resumes. Events are logged at each stage and streamed to the client via SSE.//\n", "Product Development.Diagrams.KeyFactor Claim Mapping.WebHome": "= KeyFactor Claim Mapping =\n\n{{mermaid}}\ngraph TD\n    KF1[KeyFactor: Due Process] --> C1[Claim: Court followed procedures]\n    KF1 --> C2[Claim: Defendant had legal representation]\n    KF1 --> C3[Claim: Appeals process was available]\n\n    KF2[KeyFactor: Evidence Basis] --> C4[Claim: Ruling cited documented evidence]\n    KF2 --> C5[Claim: Video evidence was authenticated]\n\n    KF3[KeyFactor: Impartiality] --> C6[Claim: Judge had no conflicts of interest]\n    KF3 --> C7[Claim: Judge served on both cases]\n\n    style KF1 fill:#e1f5fe,color:#000\n    style KF2 fill:#e1f5fe,color:#000\n    style KF3 fill:#e1f5fe,color:#000\n    style C1 fill:#fff3e0,color:#000\n    style C2 fill:#fff3e0,color:#000\n    style C3 fill:#fff3e0,color:#000\n    style C4 fill:#fff3e0,color:#000\n    style C5 fill:#fff3e0,color:#000\n    style C6 fill:#fff3e0,color:#000\n    style C7 fill:#fff3e0,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.KeyFactor Data Flow.WebHome": "= KeyFactor Data Flow =\n\n{{mermaid}}\nflowchart TD\n    subgraph \"Step 1: Understand\"\n        INPUT[User Input] --> DETECT[Detect Input Type]\n        DETECT --> THESIS_EXT[Extract Thesis]\n        THESIS_EXT --> DECOMPOSE[Decompose into KeyFactors]\n        DECOMPOSE --> CLAIMS_EXT[Extract Claims per Factor]\n    end\n\n    subgraph \"Step 2: Research\"\n        CLAIMS_EXT --> SEARCH[Search for Evidence]\n        SEARCH --> FETCH[Fetch Sources]\n        FETCH --> EXTRACT[Extract Evidence]\n    end\n\n    subgraph \"Step 3: Verdict Generation\"\n        EXTRACT --> CLAIM_EVAL[Evaluate Each Claim]\n        CLAIM_EVAL --> FACTOR_AGG[Aggregate by KeyFactor]\n        FACTOR_AGG --> CONTEST[Identify Contestations]\n        CONTEST --> OVERALL[Generate Overall Verdict]\n    end\n\n    subgraph \"Output\"\n        OVERALL --> RESULT[Analysis Result]\n        RESULT --> KF_OUT[KeyFactors with verdicts]\n        RESULT --> CV_OUT[Claim Verdicts]\n        RESULT --> OV_OUT[Overall Verdict]\n    end\n{{/mermaid}}\n\n//KeyFactors are discovered in the Understand step and guide research. Verdicts are aggregated per factor in the Verdict step.//\n", "Product Development.Diagrams.KeyFactor Entity Model.WebHome": "= KeyFactor Entity Model =\n\n{{warning}}\n**HISTORICAL  Orchestrated Pipeline (removed v2.11.0)**\n\n##CLAIM_UNDERSTANDING##, ##SUB_CLAIM##, ##KEY_FACTOR_DEF##, ##KEY_FACTOR_VERDICT##, and ##CONTEXT_ANSWER## are Orchestrated pipeline entities that no longer exist in the codebase. The ClaimAssessmentBoundary pipeline replaced them with ##AtomicClaim##, ##EvidenceScope##, and ##ClaimAssessmentBoundary##. This diagram is retained for historical reference only.\n{{/warning}}\n\n{{info}}\n**Historical reference**  KeyFactors were decomposition questions discovered during the Understand phase (Orchestrated pipeline, removed v2.11.0). Updated 2026-02-11 per ##types.ts## interfaces: ##ClaimUnderstanding.keyFactors##, ##KeyFactor##, ##FactorAnalysis##, ##AnalysisContextAnswer##.\n{{/info}}\n\n{{mermaid}}\n\nerDiagram\n    CLAIM_UNDERSTANDING ||--o{ KEY_FACTOR_DEF : \"discovers\"\n    CLAIM_UNDERSTANDING ||--o{ SUB_CLAIM : \"decomposes into\"\n\n    SUB_CLAIM }o--o| KEY_FACTOR_DEF : \"addresses\"\n    SUB_CLAIM ||--|| CLAIM_VERDICT : \"has\"\n    CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : \"supported by\"\n\n    CONTEXT_ANSWER ||--o{ KEY_FACTOR_VERDICT : \"contains\"\n    CONTEXT_ANSWER ||--o| FACTOR_ANALYSIS : \"summarises\"\n    KEY_FACTOR_VERDICT }o--o| KEY_FACTOR_DEF : \"assesses\"\n\n    ARTICLE_ANALYSIS ||--o{ CONTEXT_ANSWER : \"per context\"\n\n    CLAIM_UNDERSTANDING {\n        string impliedClaim \"Main assertion\"\n        string articleThesis \"Article thesis\"\n        string riskTier \"A B or C\"\n    }\n\n    KEY_FACTOR_DEF {\n        string id PK \"KF_xxx\"\n        string question \"Decomposition question\"\n        string factor \"Short label\"\n        string category \"procedural evidential methodological factual evaluative\"\n    }\n\n    KEY_FACTOR_VERDICT {\n        string factor \"Short label\"\n        string supports \"yes no neutral\"\n        string explanation \"Reasoning\"\n        boolean isContested\n        string contestedBy \"Who or what contests\"\n        string contestationReason \"Why contested\"\n        string factualBasis \"established disputed opinion unknown\"\n    }\n\n    FACTOR_ANALYSIS {\n        number positiveFactors\n        number negativeFactors\n        number neutralFactors\n        number contestedNegatives\n        string verdictExplanation\n    }\n\n    SUB_CLAIM {\n        string id PK \"SC1 C1 etc\"\n        string text \"Claim statement\"\n        string type \"legal procedural factual evaluative\"\n        string claimRole \"core attribution source timing\"\n        boolean isCentral\n        string keyFactorId FK \"Linked KeyFactor\"\n        string contextId FK \"Linked AnalysisContext\"\n    }\n\n    CLAIM_VERDICT {\n        string claimId PK\n        int truthPercentage \"0-100 calibrated\"\n        int confidence \"0-100\"\n        string verdict \"7-point scale label\"\n        string reasoning\n        boolean isContested\n        string factualBasis \"established disputed opinion unknown\"\n    }\n\n    EVIDENCE_ITEM {\n        string id PK \"S1-E1 format\"\n        string statement \"Extracted evidence text\"\n        string category \"legal_provision statistic event etc\"\n        string claimDirection \"supports contradicts neutral\"\n        string probativeValue \"high medium low\"\n    }\n\n    CONTEXT_ANSWER {\n        string contextId FK \"CTX_xxx\"\n        string contextName\n        number truthPercentage \"0-100\"\n        number confidence \"0-100\"\n        string shortAnswer\n    }\n\n    ARTICLE_ANALYSIS {\n        number articleTruthPercentage \"0-100 calibrated\"\n        number articleVerdict \"7-point scale number\"\n        string articleVerdictReason\n    }\n\n{{/mermaid}}\n\n== KeyFactor Lifecycle ==\n\n1. **Understand phase**  CLAIM_UNDERSTANDING discovers KEY_FACTOR_DEFs (decomposition questions) and SUB_CLAIMs. Each SUB_CLAIM may be mapped to a KEY_FACTOR_DEF via ##keyFactorId##.\n1. **Research phase**  EVIDENCE_ITEMs are gathered for each SUB_CLAIM.\n1. **Verdict phase**  Each SUB_CLAIM receives a CLAIM_VERDICT. Per-context CONTEXT_ANSWERs contain KEY_FACTOR_VERDICTs (one per factor) and an optional FACTOR_ANALYSIS summarising the factor verdict counts.\n1. **Aggregation**  ARTICLE_ANALYSIS aggregates across all CONTEXT_ANSWERs.\n\n== Contestation ==\n\nContestation is **embedded in KEY_FACTOR_VERDICT**, not a separate entity. When ##isContested## is true, ##contestedBy## and ##contestationReason## describe the dissent. The ##factualBasis## field (established / disputed / opinion / unknown) indicates the epistemological status of the factor's conclusion.\n\n//For full field-level detail see [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]]. For the complete analysis entity chain see [[Analysis Entity Model ERD>>FactHarbor.Product Development.Diagrams.Analysis Entity Model ERD.WebHome]].//\n", "Product Development.Diagrams.KeyFactor Hierarchy.WebHome": "= KeyFactor Hierarchy =\n\n{{mermaid}}\nflowchart TB\n    THESIS[\"THESIS\\n'The Bolsonaro trial was fair'\"]\n\n    KF1[\"KEY FACTOR\\n'Due process\\nfollowed?'\"]\n    KF2[\"KEY FACTOR\\n'Evidence basis'\"]\n    KF3[\"KEY FACTOR\\n'Impartiality'\"]\n\n    THESIS --> KF1\n    THESIS --> KF2\n    THESIS --> KF3\n\n    C1[\"Claim\\nSC1\"]\n    C2[\"Claim\\nSC2\"]\n    C3[\"Claim\\nSC3\"]\n    C4[\"Claim\\nSC4\"]\n    C5[\"Claim\\nSC5\"]\n    C6[\"Claim\\nSC6\"]\n\n    KF1 --> C1\n    KF1 --> C2\n    KF2 --> C3\n    KF2 --> C4\n    KF3 --> C5\n    KF3 --> C6\n\n    style THESIS fill:#f3e5f5,stroke:#7b1fa2,color:#000\n    style KF1 fill:#e1f5fe,stroke:#0288d1,color:#000\n    style KF2 fill:#e1f5fe,stroke:#0288d1,color:#000\n    style KF3 fill:#e1f5fe,stroke:#0288d1,color:#000\n    style C1 fill:#fff3e0,stroke:#ef6c00,color:#000\n    style C2 fill:#fff3e0,stroke:#ef6c00,color:#000\n    style C3 fill:#fff3e0,stroke:#ef6c00,color:#000\n    style C4 fill:#fff3e0,stroke:#ef6c00,color:#000\n    style C5 fill:#fff3e0,stroke:#ef6c00,color:#000\n    style C6 fill:#fff3e0,stroke:#ef6c00,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.LLM Abstraction Architecture.WebHome": "{{info}}\n**Current Implementation**  Uses Vercel AI SDK for multi-provider abstraction. Provider selected via ##LLM_PROVIDER## environment variable (default: anthropic). Tiered model routing uses budget models for extraction, premium for verdict reasoning. Per-task model overrides available via UCM.\n\nUpdated 2026-02-12  Restored from Outdated after source-code verification.\n{{/info}}\n\n= LLM Abstraction Architecture =\n\n{{mermaid}}\n\ngraph TB\n    subgraph Pipelines[\"Pipeline Implementations\"]\n        CB[\"ClaimAssessmentBoundary<br/>claimboundary-pipeline.ts\"]\n        DYN[\"Monolithic Dynamic<br/>monolithic-dynamic.ts\"]\n    end\n\n    subgraph Tiering[\"Model Tiering\"]\n        ROUTE[\"model-tiering.ts<br/>Task  Tier Router\"]\n    end\n\n    subgraph AISDK[\"Vercel AI SDK\"]\n        SDK[\"generateText()<br/>generateObject()\"]\n    end\n\n    subgraph Providers[\"LLM Providers\"]\n        ANT[\"Anthropic<br/>Haiku 4.5 / Sonnet 4.5\"]\n        OAI[\"OpenAI<br/>GPT-4.1-mini / GPT-4.1\"]\n        GOO[\"Google<br/>Gemini 2.5-flash / 2.5-pro\"]\n        MIS[\"Mistral<br/>mistral-small / mistral-large\"]\n    end\n\n    subgraph Config[\"Configuration\"]\n        ENV[\"LLM_PROVIDER<br/>FH_DETERMINISTIC\"]\n        UCM[\"UCM Overrides<br/>modelUnderstand, modelVerdict, ...\"]\n    end\n\n    CB --> ROUTE\n    DYN --> ROUTE\n    ROUTE --> SDK\n    SDK --> ANT\n    SDK --> OAI\n    SDK --> GOO\n    SDK --> MIS\n    ENV --> ROUTE\n    UCM --> ROUTE\n\n    style Pipelines fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Tiering fill:#e3f2fd,stroke:#1565c0,color:#000\n    style AISDK fill:#fff3e0,stroke:#e65100,color:#000\n    style Providers fill:#fff9c4,stroke:#f9a825,color:#000\n    style Config fill:#f3e5f5,stroke:#6a1b9a,color:#000\n\n{{/mermaid}}\n\n== Tiered Model Routing ==\n\nTasks are routed to appropriate model tiers for cost optimization:\n\n|= Task Type |= Tier |= Purpose\n| ##understand## | Budget | Claim extraction and classification\n| ##extract_evidence## | Budget | Evidence extraction from sources\n| ##context_refinement## | Standard | EvidenceScope clustering into ClaimAssessmentBoundaries (Stage 3)\n| ##verdict## | Premium | Verdict reasoning (critical quality)\n| ##supplemental## | Standard | Supplemental generation\n| ##summary## | Standard | Summary generation\n\n== Provider Model Mapping ==\n\n|= Provider |= Budget |= Standard |= Premium\n| **Anthropic** | claude-haiku-4-5 | claude-haiku-4-5 | claude-sonnet-4-5\n| **OpenAI** | gpt-4.1-mini | gpt-4.1 | gpt-4.1\n| **Google** | gemini-2.5-flash | gemini-2.5-pro | gemini-2.5-pro\n| **Mistral** | mistral-small-latest | mistral-large-latest | mistral-large-latest\n\n{{warning}}\n**Mistral dual-path note:** When tiered model routing is enabled (##model-tiering.ts##), Mistral falls back to Anthropic models for all tiers. When tiering is disabled (##llm.ts## default path), Mistral uses its own models as shown above.\n{{/warning}}\n\n== Configuration ==\n\n|= Variable |= Default |= Options\n| ##LLM_PROVIDER## | anthropic | anthropic, openai, google, mistral\n| ##FH_DETERMINISTIC## | true | true = temperature 0, false = default\n| ##modelUnderstand## | claude-haiku-4-5-20251001 | Any model ID (UCM override)\n| ##modelExtractEvidence## | claude-haiku-4-5-20251001 | Any model ID (UCM override)\n| ##modelVerdict## | claude-opus-4-6 | Any model ID (UCM override)\n\n== Implementation Status ==\n\n|= Feature |= Status |= Notes\n| **Multi-provider support** | Implemented | Anthropic, OpenAI, Google, Mistral\n| **Provider selection** | Implemented | Via ##LLM_PROVIDER## env var\n| **Deterministic mode** | Implemented | ##FH_DETERMINISTIC=true## sets temperature 0\n| **Tiered model routing** | Implemented | ##model-tiering.ts## routes tasks to budget/standard/premium\n| **Per-task model overrides** | Implemented | Via UCM: ##modelUnderstand##, ##modelExtractEvidence##, ##modelVerdict##\n| **Structured output** | Implemented | Zod schemas with ##generateObject()##, provider-specific adaptations\n| **Automatic failover** | Not implemented | Manual provider switch only\n| **Per-stage provider** | Not implemented | Single provider for all stages (different models per tier)\n\n== Key Files ==\n\n|= File |= Purpose\n| ##llm.ts## | Provider selection, model info, structured output helpers\n| ##model-tiering.ts## | Task-to-tier routing, model definitions, cost calculation\n| ##schema-retry.ts## | Structured output retry with provider-specific fallbacks\n| ##prompts/prompt-builder.ts## | Provider-adapted prompt construction\n| ##prompts/config-adaptations/structured-output.ts## | Per-provider structured output guidance\n", "Product Development.Diagrams.LLM Model Tiering.WebHome": "= LLM Model Tiering =\n\n{{mermaid}}\nflowchart LR\n    subgraph Tasks[\"Pipeline Tasks\"]\n        UNDERSTAND[\"Understand\\n(budget)\"]\n        EXTRACT[\"Extract Evidence\\n(budget)\"]\n        REFINE[\"Context Refinement\\n(standard)\"]\n        VERDICT[\"Verdict Generation\\n(premium)\"]\n    end\n\n    subgraph Tiering[\"Model Tiering\"]\n        ROUTER[\"model-tiering.ts\"]\n    end\n\n    subgraph SDK[\"Vercel AI SDK\"]\n        AISDK[\"generateText()\\ngenerateObject()\"]\n    end\n\n    subgraph Providers[\"Active Provider\"]\n        BUDGET[\"Budget Model\"]\n        STANDARD[\"Standard Model\"]\n        PREMIUM[\"Premium Model\"]\n    end\n\n    UNDERSTAND --> ROUTER\n    EXTRACT --> ROUTER\n    REFINE --> ROUTER\n    VERDICT --> ROUTER\n    ROUTER --> AISDK\n    AISDK --> BUDGET\n    AISDK --> STANDARD\n    AISDK --> PREMIUM\n\n    style Tasks fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Tiering fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Providers fill:#fff9c4,stroke:#f9a825,color:#000\n{{/mermaid}}\n\n//Per-task model tiering: lightweight tasks (extraction, understanding) use budget models, while critical tasks (verdict reasoning) use premium models. All calls routed through the Vercel AI SDK.//\n", "Product Development.Diagrams.Manual vs Automated matrix.WebHome": "{{info}}\n**Design Philosophy** - This matrix shows the intended division of responsibilities between AKEL and humans. v2.6.33 implements the automated claim evaluation; human responsibilities require the user system (not yet implemented).\n{{/info}}\n\n= Manual vs Automated Matrix =\n\n{{mermaid}}\n\ngraph TD\n    subgraph Automated[Automated by AKEL]\n        A1[Claim Evaluation]\n        A2[Quality Assessment]\n        A3[Content Management]\n    end\n    subgraph Human[Human Responsibilities]\n        H1[Algorithm Improvement]\n        H2[Policy Governance]\n        H3[Exception Handling]\n        H4[Strategic Decisions]\n    end\n\n{{/mermaid}}\n\n= Automated by AKEL =\n\n|= Function |= Details |= Status\n| **Claim Evaluation** | Evidence extraction, source scoring, verdict generation, risk classification, publication | Implemented\n| **Quality Assessment** | Contradiction detection, confidence scoring, pattern recognition, anomaly flagging | Partial (Gates 1 and 4)\n| **Content Management** | KeyFactor generation, evidence linking, source tracking | Implemented\n\n= Human Responsibilities =\n\n|= Function |= Details |= Status\n| **Algorithm Improvement** | Monitor metrics, identify issues, propose fixes, test, deploy | Via code changes\n| **Policy Governance** | Set criteria, define risk tiers, establish thresholds, update guidelines | Not implemented (env vars only)\n| **Exception Handling** | Review flagged items, handle abuse, address safety, manage legal | Not implemented\n| **Strategic Decisions** | Budget, hiring, major policy, partnerships | N/A\n\n= Key Principles =\n\n**Never Manual:**\n* Individual claim approval\n* Routine content review\n* Verdict overrides (fix algorithm instead)\n* Publication gates\n\n**Key Principle:** AKEL handles all content decisions. Humans improve the system, not the data.", "Product Development.Diagrams.Monolithic Dynamic Pipeline Internal.WebHome": "{{info}}\n**Fast Alternative Pipeline**  Flexible output structure not bound to canonical schema. File: ##apps/web/src/lib/analyzer/monolithic-dynamic.ts##. Streamlined analysis at lower cost, complementing the ClaimAssessmentBoundary pipeline. Updated 2026-02-19.\n{{/info}}\n\n= Monolithic Dynamic Pipeline Internal Flow =\n\n{{mermaid}}\n\nflowchart TB\n    subgraph Entry[Entry Point]\n        INPUT[runMonolithicDynamic]\n        BUDGET[Initialize Budget maxIterations 4 maxSearches 6]\n        TIMEOUT[Start timeout 2.5 minutes]\n        SR_CLEAR[Clear SR cache]\n    end\n\n    subgraph Plan[Step 1: Plan Research]\n        PLAN[\"[LLM] Initial analysis<br/>DynamicPlanSchema\"]\n    end\n\n    subgraph Research[Step 2: Research Loop]\n        SEARCH[\"[WEB] searchWebWithProvider()<br/>Execute search queries\"]\n        FETCH[\"[WEB] fetchSourceContent()<br/>Parallel fetch\"]\n        SR_PREFETCH[\"[EXT] prefetchSourceReliability()<br/>Batch reliability lookup\"]\n        BUDGET_CHK{Budget exceeded?<br/>maxSearches / maxFetches}\n    end\n\n    subgraph Analysis[Step 3: Analysis]\n        PROMPT[Build dynamic analysis prompt<br/>+ source texts + reliability scores]\n        LLM[\"[LLM] Final analysis<br/>DynamicAnalysisSchema\"]\n    end\n\n    subgraph Safety[Safety Contract Enforcement]\n        CITATIONS[Extract citations array]\n        RAW[Store rawJson]\n        VALIDATE[Validate minimum fields]\n        SR_ATTACH[\"[EXT] Attach trackRecordScore<br/>to citations\"]\n    end\n\n    subgraph Transform[Output Transform]\n        DYNAMIC[Build DynamicAnalysisResult]\n        REPORT[Generate experimental report]\n    end\n\n    subgraph Output[Output]\n        RESULT[resultJson dynamic structure]\n        MARKDOWN[reportMarkdown experimental label]\n    end\n\n    INPUT --> BUDGET --> TIMEOUT --> SR_CLEAR --> PLAN\n\n    PLAN --> SEARCH\n    SEARCH --> FETCH --> SR_PREFETCH --> BUDGET_CHK\n    BUDGET_CHK -->|No  next iteration| SEARCH\n    BUDGET_CHK -->|Yes| PROMPT\n\n    PROMPT --> LLM --> CITATIONS\n\n    CITATIONS --> RAW --> VALIDATE --> SR_ATTACH\n\n    SR_ATTACH --> DYNAMIC --> REPORT\n    REPORT --> RESULT\n    REPORT --> MARKDOWN\n\n    style PLAN fill:#e3f2fd,stroke:#1565c0,color:#000\n    style LLM fill:#e3f2fd,stroke:#1565c0,color:#000\n    style SEARCH fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style FETCH fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style SR_PREFETCH fill:#fff3e0,stroke:#e65100,color:#000\n    style SR_CLEAR fill:#fff3e0,stroke:#e65100,color:#000\n    style SR_ATTACH fill:#fff3e0,stroke:#e65100,color:#000\n\n{{/mermaid}}\n\n**Legend:** ##[LLM]## = LLM call ~ ##[WEB]## = Web/Search/Fetch call ~ ##[EXT]## = External service/cache boundary. (Colors remain as secondary visual cue.)\n\n= Key Characteristics =\n\n|= Feature |= Description\n| **Flexible Output** | LLM can structure analysis freely\n| **Experimental** | Labeled as experimental in UI\n| **Safety Contract** | Must always include citations[] and rawJson\n| **Shorter Budget** | More restrictive limits than canonical\n| **No Fallback** | Does not fall back to other pipelines\n\n= Budget Configuration =\n\n|= Parameter |= Value |= Comparison to Canonical\n| maxIterations | 4 | vs 5 (20% less)\n| maxSearches | 6 | vs 8 (25% less)\n| maxFetches | 8 | vs 10 (20% less)\n| timeoutMs | 150000 | vs 180000 (2.5 min vs 3 min)\n\n= Minimum Safety Contract =\n\n**Required fields (always present):**\n\n|= Field |= Type |= Purpose\n| ##citations## | Array | Source URLs with excerpts (provenance)\n| ##rawJson## | Any | Full LLM output for auditing\n\n**Optional fields (LLM decides):**\n\n|= Field |= Type |= Purpose\n| ##summary## | String | Analysis summary\n| ##verdict## | Object | label, score, confidence, reasoning\n| ##findings## | Array | Key findings with support levels\n| ##methodology## | String | Analysis approach description\n| ##limitations## | Array | Known analysis limitations\n\n= Output Schema =\n\n{{mermaid}}\n\nerDiagram\n    DYNAMIC_RESULT ||--o{ CITATION : has\n    DYNAMIC_RESULT ||--o| VERDICT : may_have\n    DYNAMIC_RESULT ||--o{ FINDING : may_have\n\n    DYNAMIC_RESULT {\n        array citations_required\n        any rawJson_required\n        string summary\n        string methodology\n        array limitations\n    }\n\n    CITATION {\n        string url_required\n        string title\n        string excerpt_required\n        string accessedAt\n    }\n\n    VERDICT {\n        string label\n        number score_0_to_100\n        number confidence_0_to_100\n        string reasoning\n    }\n\n    FINDING {\n        string point\n        string support_strong_moderate_weak_none\n        array sources\n        string notes\n    }\n\n{{/mermaid}}\n\n= Differences from Other Pipelines =\n\n|= Aspect |= Orchestrated |= Mono Dynamic\n| Schema | Canonical fixed | Flexible\n| UI | Jobs page | Dynamic viewer\n| Fallback | Default | None\n| Budget | Generous | Restrictive\n| Label | Comprehensive | Streamlined\n\n= Use Cases =\n\n**When to use Dynamic:**\n* Exploratory analysis of novel claim types\n* Research where canonical structure is limiting\n* Comparing LLM reasoning approaches\n* Debugging LLM behavior\n\n**When NOT to use:**\n* Production fact-checking\n* Results that need UI rendering\n* Comparing results across pipelines", "Product Development.Diagrams.Outdated.WebHome": "= Outdated Diagrams =\n\n//No outdated diagrams at this time. Previously archived diagrams were reviewed on 2026-02-12: four were deleted (replaced by current diagrams), one (LLM Abstraction Architecture) was updated and restored to the active Diagrams folder.//\n", "Product Development.Diagrams.Pipeline Shared Primitives.WebHome": "= Pipeline Shared Primitives =\n\n{{mermaid}}\nflowchart TD\n    subgraph Shared[\"Shared Primitives\"]\n        NORM[\"Input Intake (trim only)\"]\n        BUDGETS[\"Budget Framework\"]\n        SEARCH[\"Search + Fetch Adapters\"]\n        PROV[\"Provenance Validation\"]\n        META[\"Result Envelope Metadata\"]\n    end\n\n    subgraph Modules[\"Shared Analyzer Modules\"]\n        AGG[\"aggregation.ts\"]\n        CLAIM[\"claim-decomposition.ts\"]\n        SR[\"source-reliability.ts\"]\n        EF[\"evidence-filter.ts\"]\n        QG[\"quality-gates.ts\"]\n    end\n\n    subgraph Pipelines[\"Pipeline Implementations (Isolated)\"]\n        CB[\"claimboundary-pipeline.ts\"]\n        DYN[\"monolithic-dynamic.ts\"]\n    end\n\n    Shared --> Pipelines\n    Modules --> CB\n    SR --> DYN\n    PROV --> DYN\n\n    style Shared fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Modules fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style Pipelines fill:#fff3e0,stroke:#e65100,color:#000\n{{/mermaid}}\n\n//Shared primitives and analyzer modules are used by both pipelines. Pipeline logic is isolated per variant  no cross-calls between implementations.//\n", "Product Development.Diagrams.Pipeline Variant Dispatch.WebHome": "= Pipeline Variant Dispatch =\n\n{{mermaid}}\nflowchart TD\n    UI[\"Analyze UI\"] --> API[\"API: Create Job\"]\n    API --> STORE[\"Job Store\"]\n    STORE --> RUNNER[\"Runner\"]\n    RUNNER --> DISPATCH{\"Dispatch by<br/>pipelineVariant\"}\n\n    DISPATCH -->|\"claimboundary<br/>(default)\"| CB[\"ClaimAssessmentBoundary Pipeline<br/>claimboundary-pipeline.ts<br/>5-stage pipeline + LLM debate<br/>Production-ready (v2.11.0)\"]\n    DISPATCH -->|\"monolithic_dynamic\"| DYN[\"Monolithic Dynamic<br/>LLM tool-loop<br/>Flexible schema<br/>Fast alternative\"]\n\n    CB --> CB_PAYLOAD[\"CB Payload (3.0.0-cb)\"]\n    DYN --> DYNAMIC_PAYLOAD[\"Dynamic Payload\"]\n\n    CB_PAYLOAD --> ENVELOPE[\"Result Envelope\"]\n    DYNAMIC_PAYLOAD --> ENVELOPE\n\n    ENVELOPE --> RESULT[\"Store + Display\"]\n\n    style CB fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style DYN fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Promptfoo Test Coverage.WebHome": "= Promptfoo Test Coverage =\n\n{{mermaid}}\nflowchart TB\n    subgraph Configs[\" Test Configurations\"]\n        SR[promptfooconfig.source-reliability.yaml<br/>7 test cases]\n        VD[promptfooconfig.yaml<br/>5 test cases]\n        TA[promptfooconfig.text-analysis.yaml<br/>26 test cases]\n    end\n\n    subgraph TextAnalysis[\" Text Analysis Tests (26 cases)\"]\n        INPUT[Input Classification<br/>8 tests]\n        EVIDENCE[Evidence Quality<br/>5 tests]\n        SCOPE[Context Similarity<br/>5 tests]\n        VERDICT[Verdict Validation<br/>8 tests]\n    end\n\n    subgraph Providers[\" Test Providers\"]\n        HAIKU[Claude Haiku]\n        GPT4MINI[GPT-4o Mini]\n    end\n\n    TA --> INPUT\n    TA --> EVIDENCE\n    TA --> SCOPE\n    TA --> VERDICT\n\n    INPUT --> HAIKU\n    INPUT --> GPT4MINI\n    EVIDENCE --> HAIKU\n    EVIDENCE --> GPT4MINI\n    SCOPE --> HAIKU\n    SCOPE --> GPT4MINI\n    VERDICT --> HAIKU\n    VERDICT --> GPT4MINI\n{{/mermaid}}\n", "Product Development.Diagrams.Quality and Audit Workflow.WebHome": "{{info}}\n**Current Implementation (v2.10.2)** - Only Gate 1 (Claim Validation) and Gate 4 (Verdict Confidence) are implemented. Gates 2-3 are planned for future.\n{{/info}}\n\n= Quality Gates Flow =\n\n{{mermaid}}\n\nflowchart TB\n    subgraph Input[Input]\n        CLAIM[Extracted Claim]\n    end\n\n    subgraph Gate1[Gate 1 Claim Validation]\n        G1_CHECK{Is claim factual}\n        G1_OPINION[Opinion Detection]\n        G1_SPECIFIC[Specificity Check]\n        G1_FUTURE[Future Prediction]\n    end\n\n    subgraph Research[Research]\n        EVIDENCE[Gather Evidence]\n    end\n\n    subgraph Gate4[Gate 4 Verdict Confidence]\n        G4_COUNT{Evidence Count}\n        G4_QUALITY{Source Quality}\n        G4_AGREE{Evidence Agreement}\n        G4_TIER[Assign Confidence Tier]\n    end\n\n    subgraph Output[Output]\n        PUBLISH[Publish Verdict]\n        EXCLUDE[Exclude]\n        LOWCONF[Flag for Review]\n    end\n\n    CLAIM --> G1_CHECK\n    G1_CHECK --> G1_OPINION\n    G1_OPINION --> G1_SPECIFIC\n    G1_SPECIFIC --> G1_FUTURE\n    G1_FUTURE -->|Pass| EVIDENCE\n    G1_FUTURE -->|Fail| EXCLUDE\n    EVIDENCE --> G4_COUNT\n    G4_COUNT -->|2 or more| G4_QUALITY\n    G4_COUNT -->|less than 2| LOWCONF\n    G4_QUALITY -->|0.6 or more| G4_AGREE\n    G4_QUALITY -->|less than 0.6| LOWCONF\n    G4_AGREE -->|60 percent or more| G4_TIER\n    G4_AGREE -->|less than 60 percent| LOWCONF\n    G4_TIER -->|HIGH or MEDIUM| PUBLISH\n    G4_TIER -->|LOW| LOWCONF\n\n{{/mermaid}}\n\n= Gate Details =\n\n== Gate 1: Claim Validation ==\n\n**Purpose:** Ensure extracted claims are factual assertions that can be verified.\n\n|= Check |= Purpose |= Pass Criteria\n| Factuality Test | Can this claim be proven true/false? | Must be verifiable\n| Opinion Detection | Contains subjective language? | Opinion score 0.3 or less\n| Specificity Check | Contains concrete details? | Specificity score 0.3 or more\n| Future Prediction | About future events? | Must be about past/present\n\n== Gate 4: Verdict Confidence Assessment ==\n\n**Purpose:** Only display verdicts with sufficient evidence and confidence.\n\n|= Tier |= Evidence |= Avg Quality |= Agreement |= Publishable?\n| **HIGH** | 3+ sources | 0.7 or more | 80% or more | Yes\n| **MEDIUM** | 2+ sources | 0.6 or more | 60% or more | Yes\n| **LOW** | 2+ sources | 0.5 or more | 40% or more | Needs review\n| **INSUFFICIENT** | Less than 2 sources | Any | Any | More research needed\n\n= Not Yet Implemented =\n\n**Gate 2: Contradiction Search** (planned) - Counter-evidence actively searched\n\n**Gate 3: Uncertainty Quantification** (planned) - Data gaps identified and disclosed", "Product Development.Diagrams.Quality Gates Flow.WebHome": "{{info}}\n**Current Implementation (CB Pipeline v2.11.0+)**  Quality gates in the ClaimAssessmentBoundary pipeline. Gate 1 validates claims after extraction (Stage 1), Structural Consistency Check runs after verdict (Stage 4), and Gate 4 assesses verdict confidence before aggregation (Stage 5).\n\nUpdated 2026-02-22 per ##claimboundary-pipeline.ts##, ##verdict-stage.ts##, and ##quality-gates.ts##. Added B-6 verifiability annotation (Gate 1) and B-8 explanation quality check (Stage 5).\n{{/info}}\n\n= Quality Gates Flow =\n\n== Overview ==\n\nThe ClaimAssessmentBoundary pipeline enforces three quality checkpoints plus two optional quality features. **Gate 1** filters non-verifiable or unfaithful claims before research begins, with optional verifiability annotation (B-6). The **Structural Consistency Check** validates deterministic invariants after the verdict debate. **Gate 4** classifies verdict confidence into tiers for publication decisions. The **Explanation Quality Check** (B-8) optionally validates narrative quality after aggregation.\n\n{{mermaid}}\nflowchart TB\n    subgraph Stage1[\"Stage 1: EXTRACT CLAIMS\"]\n        CLAIMS[\"AtomicClaims extracted\\n(Pass 1 + Pass 2)\"]\n        GATE1{{\"Gate 1\\nClaim Validation\\n(LLM  Haiku, batched)\"}}\n        PASS1[\"Validated claims\\nproceed to research\"]\n        FAIL1[\"Filtered claims\\n(infidelity, opinion,\\nprediction, low specificity)\"]\n        RESCUE{{\"Safety Net\\nAll filtered?\"}}\n        RESCUED[\"Rescue highest-centrality\\nclaim (prevent empty pipeline)\"]\n        VERIF[\"B-6: Verifiability Annotation\\n(claimAnnotationMode)\\nhigh|medium|low|none per claim\"]\n    end\n\n    subgraph Stage23[\"Stage 23: RESEARCH + CLUSTER\"]\n        EVIDENCE[\"Evidence gathered\\nand clustered into\\nClaimAssessmentBoundaries\"]\n    end\n\n    subgraph Stage4[\"Stage 4: VERDICT (5-step debate)\"]\n        VERDICTS[\"CBClaimVerdicts\\ngenerated via LLM debate\\n(Steps 14)\"]\n        STEP5[\"Step 5: Verdict Validation\\n(Haiku x2  grounding + direction)\"]\n        STRUCT{{\"Structural\\nConsistency Check\\n(deterministic)\"}}\n        STRUCT_WARN[\"Warnings logged\\n(non-blocking)\"]\n        HARM[\"High-harm confidence floor\\n(C8  Stammbach/Ash)\"]\n        GATE4{{\"Gate 4\\nConfidence Classification\"}}\n        HIGH[\"HIGH confidence\\n(>= 70)\"]\n        MEDIUM[\"MEDIUM confidence\\n(>= 40, < 70)\"]\n        LOW[\"LOW confidence\\n(> 0, < 40)\\nflagged for review\"]\n        INSUF[\"INSUFFICIENT\\n(confidence = 0)\\nmarked UNVERIFIED\"]\n    end\n\n    subgraph Stage5[\"Stage 5: AGGREGATE\"]\n        AGG[\"Aggregation +\\nVerdict Narrative\"]\n        EQC{{\"B-8: Explanation\\nQuality Check\\n(explanationQualityMode)\"}}\n        EQC_STRUCT[\"Tier 1: Structural\\n(deterministic  4 checks)\"]\n        EQC_RUBRIC[\"Tier 2: Rubric\\n(LLM  Haiku, 5 dimensions)\"]\n    end\n\n    CLAIMS --> GATE1\n    GATE1 -->|\"Pass\"| PASS1\n    GATE1 -->|\"Fail\"| FAIL1\n    GATE1 --> RESCUE\n    RESCUE -->|\"Yes: all filtered\"| RESCUED\n    RESCUED --> PASS1\n    RESCUE -->|\"No: some passed\"| PASS1\n    PASS1 --> VERIF\n    VERIF --> EVIDENCE\n    EVIDENCE --> VERDICTS\n    VERDICTS --> STEP5\n    STEP5 --> STRUCT\n    STRUCT -->|\"Warnings\"| STRUCT_WARN\n    STRUCT --> HARM\n    HARM --> GATE4\n    GATE4 -->|\"HIGH\"| HIGH\n    GATE4 -->|\"MEDIUM\"| MEDIUM\n    GATE4 -->|\"LOW\"| LOW\n    GATE4 -->|\"INSUFFICIENT\"| INSUF\n    HIGH --> AGG\n    MEDIUM --> AGG\n    LOW --> AGG\n    INSUF --> AGG\n    AGG --> EQC\n    EQC -->|\"structural\"| EQC_STRUCT\n    EQC -->|\"rubric\"| EQC_STRUCT\n    EQC_STRUCT -->|\"rubric mode\"| EQC_RUBRIC\n\n    style Stage1 fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style Stage23 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Stage4 fill:#fff9c4,stroke:#f9a825,color:#000\n    style Stage5 fill:#f3e5f5,stroke:#7b1fa2,color:#000\n{{/mermaid}}\n\n//Quality gates in the ClaimAssessmentBoundary pipeline: Gate 1 filters non-verifiable and evidence-contaminated claims before research (with optional B-6 verifiability annotation), Structural Consistency Check validates verdict integrity (non-blocking), Gate 4 classifies verdict confidence based on evidence count, source quality, agreement, and self-consistency spread. B-8 Explanation Quality Check optionally validates narrative quality after aggregation.//\n\n----\n\n== Gate 1: Claim Validation (Detail) ==\n\nGate 1 runs after Stage 1 (Extract Claims  Pass 1 + Pass 2) to filter non-verifiable or unfaithful claims before research. It uses a **batched LLM call** (Haiku tier) that validates all claims in a single request for efficiency.\n\n=== Gate 1 Decision Flow ===\n\n{{mermaid}}\nflowchart TD\n    INPUT[\"AtomicClaim\\nfrom Pass 2\"] --> FIDELITY{{\"passedFidelity?\\n(Claim faithful to\\noriginal input meaning)\"}}\n\n    FIDELITY -->|\"No\"| FILTERED[\"FILTERED\\n(evidence-contaminated\\nor hallucinated)\"]\n    FIDELITY -->|\"Yes\"| OPINION{{\"passedOpinion?\\n(Factual, not pure\\nopinion or prediction)\"}}\n\n    OPINION -->|\"No\"| SPEC_CHECK{{\"passedSpecificity?\"}}\n    OPINION -->|\"Yes\"| SPEC{{\"specificityScore\\n>= UCM minimum\\n(default 0.6)\"}}\n\n    SPEC_CHECK -->|\"No (fails both)\"| FILTERED2[\"FILTERED\\n(opinion + not specific)\"]\n    SPEC_CHECK -->|\"Yes\"| SPEC\n\n    SPEC -->|\"Yes\"| GROUNDING[\"Check groundingQuality\"]\n    SPEC -->|\"No, grounded\\n(moderate/strong/weak)\"| FILTERED3[\"FILTERED\\n(too vague, grounding\\navailable but insufficient)\"]\n    SPEC -->|\"No, ungrounded\\n(groundingQuality=none)\"| EXEMPT[\"EXEMPT from\\nspecificity filter\\n(cold extraction)\"]\n\n    EXEMPT --> GROUNDING\n\n    GROUNDING --> GQ_STRONG[\"strong:\\nFully grounded in\\npreliminary evidence\"]\n    GROUNDING --> GQ_MODERATE[\"moderate:\\nEvidence themes referenced\\nbut lacks specifics\"]\n    GROUNDING --> GQ_WEAK[\"weak:\\nAcceptable for claims\\ninput article states explicitly\"]\n    GROUNDING --> GQ_NONE[\"none:\\nCold extraction  may indicate\\npoor preliminary search\"]\n\n    GQ_STRONG --> PASSED[\"PASSED\\n(proceed to research)\"]\n    GQ_MODERATE --> PASSED\n    GQ_WEAK --> FLAGGED[\"PASSED with\\nmonitoring flag\"]\n    GQ_NONE --> FLAGGED\n    FLAGGED --> PASSED\n\n    PASSED --> RETRY_CHECK{{\"Retry threshold\\nexceeded?\\n(> 50% specificity\\nfailures)\"}}\n    RETRY_CHECK -->|\"Yes\"| RETRY_WARN[\"Logged: retry deferred\\nto v1.1\"]\n    RETRY_CHECK -->|\"No\"| DONE[\"Claims proceed\\nto Stage 2\"]\n    RETRY_WARN --> DONE\n\n    style FILTERED fill:#ffcdd2,stroke:#c62828,color:#000\n    style FILTERED2 fill:#ffcdd2,stroke:#c62828,color:#000\n    style FILTERED3 fill:#ffcdd2,stroke:#c62828,color:#000\n    style PASSED fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style FLAGGED fill:#fff3e0,stroke:#e65100,color:#000\n    style EXEMPT fill:#e3f2fd,stroke:#1565c0,color:#000\n    style RETRY_WARN fill:#fff3e0,stroke:#e65100,color:#000\n{{/mermaid}}\n\n=== Gate 1 Checks ===\n\n|= Check |= Field |= Criteria |= Action on Failure\n| **Fidelity** | ##passedFidelity## | Claim must be derivable from original input text, not evidence-contaminated or hallucinated by prior LLM pass | Filtered out (hard filter)\n| **Factuality** | ##passedOpinion## | Not pure opinion or prediction (unless in article thesis). Evaluated by LLM, not keyword matching | Filtered out only if specificity also fails (both must fail)\n| **Specificity** | ##specificityScore## | Score >= UCM ##claimSpecificityMinimum## (default 0.6). Claim must be researchable independently | Filtered if grounded (moderate/strong/weak); **exempt** if ##groundingQuality## is \"none\" (cold extraction  low specificity expected without preliminary evidence)\n| **Grounding Quality** | ##groundingQuality## | 4-level assessment of how well the claim is grounded in preliminary evidence | All levels pass; weak/none receive monitoring flag\n\n=== groundingQuality Levels ===\n\n|= Level |= Meaning |= Gate 1 Treatment\n| **strong** | Fully grounded in preliminary evidence | Pass\n| **moderate** | Evidence themes referenced but claim lacks specifics | Pass\n| **weak** | Acceptable for claims the input article states explicitly | Pass with monitoring flag\n| **none** | Cold extraction  no preliminary evidence available. May indicate poor preliminary search | Pass with monitoring flag. **Exempt from specificity filter** (low specificity expected without grounding)\n\n=== Safety Net ===\n\nIf **all claims** are filtered by Gate 1 (would result in an empty pipeline), the safety net rescues the highest-centrality claim to prevent a completely empty analysis. Rescue priority:\n\n1. Prefer claims that **passed fidelity** (faithful to input)\n1. Then by **centrality** (high > medium > low)\n\nThis ensures the pipeline always produces a result, even when input quality is poor.\n\n=== Verifiability Annotation (B-6) ===\n\nAfter Gate 1 filtering, if UCM ##claimAnnotationMode## is not \"off\", each surviving AtomicClaim receives a **verifiability** assessment:\n\n|= Level |= Meaning\n| **high** | Directly checkable against available evidence (data, studies, official records)\n| **medium** | Partially checkable  some aspects verifiable, others depend on interpretation or unavailable data\n| **low** | Difficult to check  predictions, subjective assessments, or evidence unlikely to be public\n| **none** | Pure value judgment, preference, or unfalsifiable statement\n\n**Key property:** Verifiability is **independent of claim category**. A factual claim can have low verifiability (too vague to research), and an evaluative claim can have high verifiability (contains checkable sub-assertions).\n\n**UCM control:** ##claimAnnotationMode## = \"off\" (default) | \"verifiability\" | \"verifiability_and_misleadingness\". When \"off\", verifiability is computed but stripped from the output.\n\n=== Decomposition Retry Path (deferred to v1.1) ===\n\nWhen > 50% of central claims fail specificity (UCM ##gate1GroundingRetryThreshold##, default 0.5), this indicates poor overall extraction quality. The planned retry path:\n\n1. Trigger second preliminary search with refined queries (using passing claims + rejection reasons)\n1. Re-run Pass 2 with expanded evidence context\n1. Max 1 retry to avoid infinite loops\n\n**Current status (v1.0):** Threshold exceedance is **logged as a warning** but retry is not yet implemented. The warning reads: //\"Gate 1: N% of claims failed specificity (threshold: M%). Retry deferred to v1.1.\"//\n\n----\n\n== Structural Consistency Check ==\n\nRuns after Stage 4 Step 5 (Verdict Validation), before Gate 4. This is a **deterministic structural validation only**  no semantic interpretation, per AGENTS.md LLM Intelligence rule.\n\n=== Structural Check Flow ===\n\n{{mermaid}}\nflowchart TD\n    INPUT[\"CBClaimVerdicts\\n(from Step 5)\"] --> EID{{\"Evidence ID\\nvalidity\"}}\n\n    EID -->|\"Invalid IDs\"| WARN1[\"Warning: evidence ID\\nnot in evidence pool\"]\n    EID -->|\"All valid\"| BID\n\n    EID --> BID{{\"Boundary ID\\nvalidity\"}}\n\n    BID -->|\"Invalid IDs\"| WARN2[\"Warning: boundary ID\\nnot in boundaries\"]\n    BID -->|\"All valid\"| TRUTH\n\n    BID --> TRUTH{{\"Truth %\\nrange check\"}}\n\n    TRUTH -->|\"Out of 0-100\"| WARN3[\"Warning: truthPercentage\\nout of range\"]\n    TRUTH -->|\"In range\"| LABEL\n\n    TRUTH --> LABEL{{\"Label-band\\nmatching\"}}\n\n    LABEL -->|\"Mismatch\"| WARN4[\"Warning: verdict label\\ndoes not match expected\\nfor truth% + confidence%\"]\n    LABEL -->|\"Match\"| COV\n\n    LABEL --> COV{{\"Coverage\\ncompleteness\"}}\n\n    COV -->|\"Gaps found\"| WARN5[\"Warning: claim has\\nverdict but zero evidence\\nin coverage matrix\"]\n    COV -->|\"Complete\"| PASS[\"All structural\\nchecks pass\"]\n\n    WARN1 --> CONTINUE[\"Continue to\\nGate 4\\n(non-blocking)\"]\n    WARN2 --> CONTINUE\n    WARN3 --> CONTINUE\n    WARN4 --> CONTINUE\n    WARN5 --> CONTINUE\n    PASS --> CONTINUE\n\n    style WARN1 fill:#fff3e0,stroke:#e65100,color:#000\n    style WARN2 fill:#fff3e0,stroke:#e65100,color:#000\n    style WARN3 fill:#fff3e0,stroke:#e65100,color:#000\n    style WARN4 fill:#fff3e0,stroke:#e65100,color:#000\n    style WARN5 fill:#fff3e0,stroke:#e65100,color:#000\n    style PASS fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style CONTINUE fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n\n=== Structural Checks ===\n\n|= Check |= What It Validates |= On Failure\n| **Evidence ID validity** | All evidence IDs referenced in ##supportingEvidenceIds## and ##contradictingEvidenceIds## exist in the evidence pool | Warning logged\n| **Boundary ID validity** | All boundary IDs in ##boundaryFindings## are valid ClaimAssessmentBoundary IDs | Warning logged\n| **Truth percentage range** | ##truthPercentage## within 0100 | Warning logged\n| **Label-band matching** | Verdict label matches truth percentage band via ##percentageToClaimVerdict()## (7-point scale mapping, factoring in confidence and ##mixedConfidenceThreshold##) | Warning logged\n| **Coverage completeness** | Every claim has >= 1 evidence item in the coverage matrix, or is explicitly flagged | Warning logged\n\n**Non-blocking:** Structural inconsistencies are captured for debugging and quality monitoring. They do NOT block the pipeline or alter verdicts.\n\n=== What Is NOT Checked (requires LLM) ===\n\nPer AGENTS.md LLM Intelligence rule, the following are **not allowed** as deterministic checks:\n\n* Judging whether a verdict's reasoning is semantically consistent with its truth%\n* Assessing whether boundary findings narratively support the overall verdict\n* Any check that interprets text meaning\n\nIf semantic consistency checking is desired in the future, it must be routed through an LLM call and made a UCM-toggled quality gate.\n\n----\n\n== Gate 4: Confidence Classification ==\n\nGate 4 runs after Stage 4 (Verdict) and the Structural Consistency Check. It classifies the **pre-computed** confidence value from each ##CBClaimVerdict## into tiers using simple numeric thresholds. It does NOT calculate confidence  confidence is determined during the verdict stage by the LLM debate pattern (Steps 14).\n\n=== Gate 4 Decision Flow ===\n\n{{mermaid}}\nflowchart TD\n    INPUT[\"CBClaimVerdict\\nwith pre-computed\\nconfidence value\"] --> TIER{{\"Classify\\nconfidence tier\"}}\n\n    TIER -->|\"confidence >= 70\"| HIGH[\"HIGH\\nPublish with\\nfull confidence\"]\n    TIER -->|\"confidence >= 40\\nand < 70\"| MEDIUM[\"MEDIUM\\nPublish with\\ncaveats\"]\n    TIER -->|\"confidence > 0\\nand < 40\"| LOW[\"LOW\\nFlag for\\nreview\"]\n    TIER -->|\"confidence = 0\"| INSUF[\"INSUFFICIENT\\nMark as\\nUNVERIFIED\"]\n\n    HIGH --> PUB{{\"Publishable?\"}}\n    MEDIUM --> PUB\n    LOW --> PUB\n    INSUF --> PUB\n\n    PUB -->|\"HIGH or MEDIUM\"| PUBLISH[\"Publishable\\n(included in aggregation)\"]\n    PUB -->|\"LOW or INSUFFICIENT\"| REVIEW[\"Not publishable\\n(included with flags)\"]\n\n    style HIGH fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style MEDIUM fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style LOW fill:#fff3e0,stroke:#e65100,color:#000\n    style INSUF fill:#ffcdd2,stroke:#c62828,color:#000\n    style PUBLISH fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style REVIEW fill:#fff3e0,stroke:#e65100,color:#000\n{{/mermaid}}\n\n=== Confidence Tiers ===\n\n|= Confidence Tier |= Threshold |= Action\n| **HIGH** | confidence >= 70 | Publish with full confidence\n| **MEDIUM** | confidence >= 40 and < 70 | Publish with caveats\n| **LOW** | confidence > 0 and < 40 | Flag for review\n| **INSUFFICIENT** | confidence = 0 | Mark as UNVERIFIED\n\n=== What Determines Confidence (during Stage 4, before Gate 4) ===\n\nConfidence is computed during the verdict debate (Steps 14) by the LLM. The following factors influence the LLM's confidence assessment:\n\n|= Factor |= Influence\n| **Evidence count** per claim | More evidence = higher confidence\n| **Average source quality** | Higher ##trackRecordScore## (from Source Reliability) = higher confidence\n| **Evidence agreement** ratio | Strong agreement in one direction = higher confidence; high contradiction = lower confidence\n| **Self-consistency spread** | High spread from Step 2 (parallel Sonnet calls) reduces confidence  indicates unstable verdict\n\n=== High-Harm Confidence Floor (C8) ===\n\nAfter Gate 4 classification, claims with ##harmPotential## \"critical\" or \"high\" are subject to an additional confidence floor (Stammbach/Ash bias mitigation). Claims below the configured minimum confidence (UCM ##highHarmMinConfidence##) are downgraded to UNVERIFIED, regardless of their truth percentage. This prevents low-evidence definitive verdicts on potentially harmful topics.\n\n----\n\n== Statistics and Audit ==\n\nGate 1 and Gate 4 statistics are recorded in ##QualityGates## and persisted in the analysis result:\n\n=== Gate 1 Statistics ===\n\n|= Field |= Description\n| ##totalClaims## | Total claims entering Gate 1\n| ##passedOpinion## | Claims that passed the factuality/opinion check\n| ##passedSpecificity## | Claims that passed the specificity check\n| ##passedFidelity## | Claims that passed the fidelity check\n| ##filteredCount## | Number of claims filtered out\n| ##overallPass## | Boolean  true if at least one claim survived\n\n=== Gate 4 Statistics ===\n\n|= Field |= Description\n| ##totalVerdicts## | Total verdicts entering Gate 4\n| ##highConfidence## | Verdicts classified as HIGH\n| ##mediumConfidence## | Verdicts classified as MEDIUM\n| ##lowConfidence## | Verdicts classified as LOW\n| ##insufficient## | Verdicts classified as INSUFFICIENT\n| ##publishable## | HIGH + MEDIUM count (publishable verdicts)\n\n//The ##QualityGates.passed## flag is true when Gate 1 overall passes (at least one claim survives) AND at least one verdict is publishable (HIGH or MEDIUM confidence).//\n\n----\n\n== Explanation Quality Check (B-8) ==\n\nRuns after Stage 5 aggregation and verdict narrative generation. Validates the quality of the ##VerdictNarrative##  the structured explanation shown to users. Controlled by UCM ##explanationQualityMode##.\n\n=== Modes ===\n\n|= Mode |= Behavior |= LLM Cost\n| **off** (default) | No quality check | 0 calls\n| **structural** | Tier 1 only  deterministic structural checks | 0 calls\n| **rubric** | Tier 1 + Tier 2  structural checks plus LLM rubric evaluation | 1 Haiku call\n\n=== Tier 1: Structural Findings (deterministic) ===\n\nFour boolean checks on the narrative text:\n\n|= Check |= Field |= What It Detects\n| **Evidence cited** | ##hasCitedEvidence## | Narrative references evidence quantities (numeric references like \"14 items\", \"9 sources\")\n| **Verdict stated** | ##hasVerdictCategory## | Verdict label or type is explicitly stated (Unicode-aware: ALL-CAPS tokens, title-case, or percentage)\n| **Confidence statement** | ##hasConfidenceStatement## | Confidence level mentioned (percentage like \"72%\" or fraction like \"4/5\")\n| **Limitations acknowledged** | ##hasLimitations## | Limitations section is substantive (> 5 characters, excluding \"None\" / \"N/A\")\n\n=== Tier 2: Rubric Scores (LLM  Haiku) ===\n\nFive dimensions, each scored 15:\n\n|= Dimension |= What It Measures\n| **Clarity** | Is the explanation understandable? Avoids jargon, ambiguity, convoluted phrasing?\n| **Completeness** | Addresses all claims? Evidence summary references actual quantities?\n| **Neutrality** | Balanced and impartial? Avoids loaded language or implicit bias?\n| **Evidence Support** | Cites specific evidence? Explains how evidence supports the conclusion?\n| **Appropriate Hedging** | Includes appropriate caveats? Avoids overconfidence?\n\n**Overall score:** Mean of the 5 dimension scores.\n\n**Quality flags:** String array detecting specific issues (e.g., \"overconfident_language\", \"missing_counterevidence\", \"vague_key_finding\", \"no_limitations_acknowledged\").\n\n=== Output ===\n\nAttached to ##OverallAssessment.explanationQualityCheck## (optional):\n\n* ##mode##: \"structural\" or \"rubric\"\n* ##structuralFindings##: 4 boolean checks (always present)\n* ##rubricScores##: 5 dimension scores + overall + flags (only when mode = \"rubric\")\n\n----\n\n== Source Files ==\n\n|= File |= Quality Gate Role\n| ##claimboundary-pipeline.ts## | Gate 1 LLM-based validation (##runGate1Validation##), Gate 4 stats (##buildQualityGates##)\n| ##verdict-stage.ts## | Structural Consistency Check (##runStructuralConsistencyCheck##), high-harm confidence floor (##enforceHarmConfidenceFloor##)\n| ##quality-gates.ts## | Gate 1 structural pre-filter (##validateClaimGate1##, ##applyGate1ToClaims##), Gate 4 evidence-based validation (##validateVerdictGate4##, ##applyGate4ToVerdicts##)\n| ##truth-scale.ts## | ##percentageToClaimVerdict##  maps truth% + confidence to verdict label (used by label-band matching)\n| ##types.ts## | ##QualityGates##, ##Gate1Stats##, ##Gate4Stats## type definitions\n", "Product Development.Diagrams.Quality Gates Integration.WebHome": "= Quality Gates Integration =\n\n{{mermaid}}\nflowchart TB\n    subgraph UNDERSTAND[\"Phase 1: UNDERSTAND\"]\n        Input[User Input] --> ClaimExtraction[Claim Extraction]\n        ClaimExtraction --> GATE1[\"Gate 1: Claim Validation<br/>Filter opinions,<br/>predictions,<br/>low-specificity\"]\n        GATE1 -->|Pass| ValidClaims[Valid Claims]\n        GATE1 -.->|Fail| ExcludedClaims[Excluded Claims<br/>with reasons]\n    end\n\n    subgraph RESEARCH[\"Phase 2: RESEARCH\"]\n        ValidClaims --> Search[Web Search]\n        Search --> Sources[Source Documents]\n        Sources --> EvidenceExtraction[Evidence Extraction]\n    end\n\n    subgraph VERDICT[\"Phase 3: VERDICT GENERATION\"]\n        EvidenceExtraction --> VerdictGeneration[Verdict Generation]\n        VerdictGeneration --> GATE4[\"Gate 4: Confidence Assessment<br/>Check source count,<br/>fact count,<br/>reasoning quality\"]\n        GATE4 -->|Pass| PublishableVerdicts[Publishable Verdicts]\n        GATE4 -->|Warn| LowConfidenceVerdicts[Low Confidence<br/>Verdicts]\n    end\n\n    style GATE1 fill:#fff9c4,color:#000\n    style GATE4 fill:#fff9c4,color:#000\n    style ExcludedClaims fill:#ffcdd2,color:#000\n    style ValidClaims fill:#c8e6c9,color:#000\n    style PublishableVerdicts fill:#c8e6c9,color:#000\n    style LowConfidenceVerdicts fill:#ffecb3,color:#000\n{{/mermaid}}\n\n//Yellow = quality gate checkpoints. Green = passed. Red = excluded. Orange = low confidence warning.//\n", "Product Development.Diagrams.Request Lifecycle.WebHome": "= Request Lifecycle =\n\n{{mermaid}}\nsequenceDiagram\n    participant U as User (Browser)\n    participant NUI as Next.js UI\n    participant NAPI as Next.js /api/fh/*\n    participant DAPI as .NET API\n    participant DB as factharbor.db\n    participant RUN as Next.js /api/internal/run-job\n    participant AKEL as AKEL Pipeline\n\n    U->>NUI: Submit article/URL\n    NUI->>NAPI: POST /api/fh/analyze\n    NAPI->>DAPI: POST /v1/analyze\n    DAPI->>DB: Create job (QUEUED)\n    DAPI-->>NAPI: 201 {jobId}\n    NAPI-->>NUI: Redirect to /jobs/{id}\n\n    DAPI->>RUN: POST /api/internal/run-job (async, with retry)\n    RUN->>DB: Update status (RUNNING)\n    RUN->>AKEL: Execute pipeline\n\n    Note over AKEL: Understand  Research  Verdicts  Report\n\n    AKEL-->>RUN: AnalysisResult JSON\n    RUN->>DAPI: PATCH /v1/internal/jobs/{id}\n    DAPI->>DB: Save result (SUCCEEDED)\n\n    U->>NUI: Poll /jobs/{id}\n    NUI->>NAPI: GET /api/fh/jobs/{id}/events\n    NAPI->>DAPI: GET /v1/jobs/{id}/events (SSE)\n    DAPI-->>U: Stream events in real-time\n{{/mermaid}}\n\n//Full request lifecycle from user submission through job queuing, AKEL pipeline execution, and real-time event streaming back to the browser.//\n", "Product Development.Diagrams.Role-Based Access Control.WebHome": "= Role-Based Access Control =\n\n{{mermaid}}\nflowchart TB\n    subgraph Current_Roles[\"Implemented\"]\n        READER[\"Reader\\n(anonymous)\"]\n        ADMIN[\"UCM Admin\\n(via admin key)\"]\n    end\n\n    subgraph Planned_Roles[\"Planned\"]\n        USER[\"Registered User\\n(submit analyses)\"]\n        MODERATOR[\"Moderator\\n(review flagged content)\"]\n    end\n\n    READER -->|\"View analyses,\\nbrowse results\"| VIEW[\"Read-Only\\nAccess\"]\n    ADMIN -->|\"Change configs,\\nresume/pause system\"| CONFIG[\"Admin\\nAccess\"]\n    USER -->|\"Submit claims,\\nview own history\"| SUBMIT[\"Submit\\nAccess\"]\n    MODERATOR -->|\"Review flags,\\nban users\"| MOD[\"Moderation\\nAccess\"]\n\n    style Current_Roles fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style Planned_Roles fill:#fff9c4,stroke:#f9a825,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Security Model.WebHome": "= Security Model =\n\n{{mermaid}}\nflowchart TB\n    subgraph Implemented[\"Implemented (POC)\"]\n        RUNNER_KEY[\"Runner Route Protection\\n/api/internal/run-job\\nrequires x-runner-key header\"]\n        ADMIN_KEY[\"Admin API Protection\\nPOST endpoints\\nrequire X-Admin-Key header\"]\n        TIMING[\"Timing-Safe Comparison\\nfor secret validation\"]\n        HEALTH_AUTH[\"System Health Auth\\nresume/pause require admin key\\nin production\"]\n    end\n\n    subgraph Planned[\"Planned (Alpha/Beta)\"]\n        USER_AUTH[\"User Authentication\\nLogin, registration,\\npassword hashing\"]\n        RBAC[\"Role-Based Access Control\\nReader, User, Admin,\\nModerator roles\"]\n        RATE_LIMIT[\"Rate Limiting\\nPer-user and per-IP\\nquota enforcement\"]\n        SSRF[\"SSRF Protection\\nURL fetching\\nsanitisation\"]\n    end\n\n    subgraph Always[\"Always Active\"]\n        TLS[\"HTTPS/TLS\\nTransport encryption\"]\n        PARAMS[\"Parameterised Queries\\nSQL injection prevention\"]\n        ZOD_VAL[\"Zod Validation\\nAll config and API input\\nschema-validated\"]\n    end\n\n    style Implemented fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style Planned fill:#fff9c4,stroke:#f9a825,color:#000\n    style Always fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Source Reliability Flow.WebHome": "= Source Reliability Flow =\n\n{{mermaid}}\nsequenceDiagram\n    participant User\n    participant Analyzer as claimboundary-pipeline.ts\n    participant Prefetch as prefetchSourceReliability()\n    participant Cache as source-reliability.db\n    participant LLM as LLM Endpoint\n    participant Map as In-Memory Map\n    participant Lookup as getTrackRecordScore()\n    participant Weight as applyEvidenceWeighting()\n\n    Note over User,Weight: PHASE 1: Async Prefetch (before source fetching)\n    User->>Analyzer: Submit claim for analysis\n    Analyzer->>Analyzer: Search for sources\n    Analyzer->>Prefetch: await prefetchSourceReliability(urls)\n\n    loop For each unique domain\n        Prefetch->>Cache: Batch lookup\n        alt Cache Hit\n            Cache-->>Prefetch: Return cached score\n            Prefetch->>Map: Store score\n        else Cache Miss + Important Source\n            Prefetch->>LLM: Evaluate source (internal API)\n            LLM-->>Prefetch: Score + confidence\n            Prefetch->>Cache: Save (TTL: 90 days)\n            Prefetch->>Map: Store score\n        else Cache Miss + Unimportant Source\n            Prefetch->>Map: Store null\n        end\n    end\n\n    Prefetch-->>Analyzer: Done\n\n    Note over User,Weight: PHASE 2: Sync Lookup (during source fetching)\n    loop For each source URL\n        Analyzer->>Lookup: getTrackRecordScore(url)\n        Lookup->>Map: Read from map\n        Map-->>Lookup: Score or null\n        Lookup-->>Analyzer: Return immediately (no I/O)\n        Analyzer->>Analyzer: Assign trackRecordScore to FetchedSource\n    end\n\n    Note over User,Weight: PHASE 3: Evidence Weighting (after verdicts)\n    Analyzer->>Weight: applyEvidenceWeighting(verdicts, evidence, sources)\n    Weight->>Weight: Calculate avg source score per verdict\n    Weight->>Weight: Adjust truthPercentage and confidence\n    Weight-->>Analyzer: Weighted verdicts\n\n    Analyzer-->>User: Analysis complete\n{{/mermaid}}\n", "Product Development.Diagrams.Source Reliability Overview.WebHome": "= Source Reliability Overview =\n\n{{mermaid}}\nflowchart TB\n    subgraph analysis [FactHarbor Analysis Pipeline]\n        AN[claimboundary-pipeline.ts<br/>Analyzer]\n        PF[prefetchSourceReliability<br/>Batch Prefetch]\n        SR[source-reliability.ts<br/>Sync Lookup + Weighting]\n    end\n\n    subgraph cache [Source Reliability Cache]\n        SQLITE[(SQLite<br/>source-reliability.db)]\n        MAP[In-Memory Map<br/>prefetchedScores]\n    end\n\n    subgraph evaluation [LLM Evaluation - Sequential Refinement]\n        EVAL[\"/api/internal/evaluate-source\"]\n        LLM1[Claude<br/>Initial Evaluation]\n        LLM2[OpenAI mini model<br/>Cross-check and Refine]\n        FINAL[Final Result]\n    end\n\n    AN -->|1. Extract URLs| PF\n    PF -->|2. Batch lookup| SQLITE\n    SQLITE -->|3. Cache hits| MAP\n    PF -->|4. Cache miss| EVAL\n    EVAL --> LLM1\n    LLM1 -->|Initial result| LLM2\n    LLM2 -->|Refined result| FINAL\n    FINAL -->|5. Store score| SQLITE\n    FINAL -->|6. Populate| MAP\n    AN -->|7. Sync lookup| SR\n    SR -->|8. Read from| MAP\n    SR -->|9. Apply to| VERDICTS[Verdict Weighting]\n{{/mermaid}}\n", "Product Development.Diagrams.Source Reliability Prefetch Flow.WebHome": "= Source Reliability Prefetch Flow =\n\n{{mermaid}}\nflowchart TD\n    subgraph prefetch [Phase 1: Async Prefetch]\n        URLS[Extract Source URLs] --> DEDUP[Deduplicate Domains]\n        DEDUP --> BATCH[Batch Cache Lookup]\n        BATCH --> LOOP{For Each Domain}\n        LOOP --> HIT{Cache Hit?}\n        HIT -->|Yes| MAP[Add to In-Memory Map]\n        HIT -->|No| RATE{Rate Limit OK?}\n        RATE -->|No| SKIP[Store null in Map]\n        RATE -->|Yes| FILTER{isImportantSource?}\n        FILTER -->|Blog/Spam TLD| SKIP\n        FILTER -->|Legitimate| LLM[Multi-Model LLM<br/>Internal API Only]\n        LLM --> CONF{Confidence >= 0.8?}\n        CONF -->|No| SKIP\n        CONF -->|Yes| CONS{Models Agree?<br/>Diff <= 0.20}\n        CONS -->|No| SKIP\n        CONS -->|Yes| SAVE[Cache + Add to Map]\n    end\n\n    style RATE fill:#f99,color:#000\n    style FILTER fill:#ff9,color:#000\n    style SKIP fill:#ddd,color:#000\n    style SAVE fill:#9f9,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Storage Architecture.WebHome": "= Storage Architecture =\n\n{{mermaid}}\nflowchart TB\n    subgraph NextJS[\"Next.js Web App\"]\n        PIPELINE[\"AKEL Pipeline\"]\n        CONFIG_SVC[\"Config Storage\\n(config-storage.ts)\"]\n        SR_SVC[\"SR Cache\\n(source-reliability-cache.ts)\"]\n    end\n\n    subgraph DotNet[\".NET API\"]\n        EF[\"Entity Framework\\nCore\"]\n    end\n\n    FH_DB[(\"factharbor.db\\nJobs, events, metrics\\n(.NET EF Core)\")]\n    CONFIG_DB[(\"config.db\\nUCM configuration\\n(Next.js better-sqlite3)\")]\n    SR_DB[(\"source-reliability.db\\nSource credibility cache\\n(Next.js better-sqlite3)\")]\n\n    EF --> FH_DB\n    CONFIG_SVC --> CONFIG_DB\n    SR_SVC --> SR_DB\n    PIPELINE -->|\"via REST API\"| EF\n\n    style NextJS fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style DotNet fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Storage Roadmap.WebHome": "= Storage Roadmap =\n\n{{mermaid}}\nflowchart LR\n    subgraph P1[\"POC (Current)\"]\n        S1[\"factharbor.db\\nconfig.db\\nsource-reliability.db\\n+ in-memory Map\"]\n    end\n\n    subgraph P2[\"Alpha\"]\n        S2[\"Add URL cache\\nAdd claim cache\\n(still SQLite)\"]\n    end\n\n    subgraph P3[\"Beta\"]\n        S3[\"PostgreSQL\\nfor factharbor.db\\n(user accounts, search)\"]\n    end\n\n    subgraph P4[\"V1.0+\"]\n        S4[\"Redis IF multi-instance\\nVector DB IF Shadow Mode\\nS3 IF >50GB\"]\n    end\n\n    P1 --> P2 --> P3 --> P4\n\n    style P1 fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style P2 fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style P3 fill:#e3f2fd,stroke:#1565c0,color:#000\n    style P4 fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.System Architecture.WebHome": "= System Architecture =\n\n{{mermaid}}\nflowchart TB\n    subgraph Client[\"Client Layer\"]\n        BROWSER[\"Web Browser\"]\n    end\n\n    subgraph NextJS[\"Next.js Web App (port 3000)\"]\n        UI[\"UI Pages\\n/analyze, /jobs, /admin\"]\n        FH_API[\"Proxy API Routes\\n/api/fh/*\"]\n        INTERNAL[\"Internal API\\n/api/internal/run-job\"]\n        AKEL[\"ClaimAssessmentBoundary Pipeline\\nclaimboundary-pipeline.ts\"]\n        SHARED[\"Shared Modules\\nevidence-filter, aggregation,\\nquality-gates, source-reliability\"]\n        CONFIG_DB[(\"config.db\\nUCM Config\")]\n        SR_DB[(\"source-reliability.db\\nSR Cache\")]\n    end\n\n    subgraph DotNet[\".NET API (port 5000)\"]\n        CONTROLLERS[\"Controllers\\nAnalyze, Jobs, Health\"]\n        SERVICES[\"Services\\nJobService, RunnerClient\"]\n        MAIN_DB[(\"factharbor.db\\nJobs & Events\")]\n    end\n\n    subgraph External[\"External Services\"]\n        LLM[\"LLM Providers\\nAnthropic, OpenAI, Google, Mistral\"]\n        SEARCH[\"Search Providers\\nGoogle CSE, SerpAPI\"]\n    end\n\n    BROWSER --> UI\n    UI --> FH_API\n    FH_API --> CONTROLLERS\n    CONTROLLERS --> SERVICES\n    SERVICES --> MAIN_DB\n    SERVICES -->|\"POST /api/internal/run-job\\n(with retry + backoff)\"| INTERNAL\n    INTERNAL --> AKEL\n    AKEL --> SHARED\n    AKEL --> LLM\n    AKEL --> SEARCH\n    AKEL --> CONFIG_DB\n    SHARED --> SR_DB\n\n    style NextJS fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style DotNet fill:#e3f2fd,stroke:#1565c0,color:#000\n    style External fill:#fff3e0,stroke:#e65100,color:#000\n{{/mermaid}}\n\n//Two-service architecture: Next.js web app (UI + ClaimAssessmentBoundary analysis engine) and .NET API (job management + persistence), communicating via HTTP on a single host.//\n", "Product Development.Diagrams.System Context.WebHome": "= System Context =\n\n{{mermaid}}\nflowchart TB\n    USER[\"Users\\n(Readers, Analysts, Admins)\"]\n\n    subgraph FH[\"FactHarbor Platform\"]\n        direction LR\n        WEB[\"Next.js Web App\\nUI + Analysis Engine\"]\n        API[\".NET API\\nJob Management\"]\n        FH_DB[(\"factharbor.db\\nJobs & Events\")]\n        CFG_DB[(\"config.db\\nUCM Config\")]\n        SR_DB[(\"source-reliability.db\\nSR Cache\")]\n    end\n\n    subgraph LLM[\"LLM Providers\"]\n        ANTHROPIC[\"Anthropic\\n(Claude)\"]\n        OPENAI[\"OpenAI\\n(GPT)\"]\n        GOOGLE[\"Google\\n(Gemini)\"]\n        MISTRAL[\"Mistral\"]\n    end\n\n    subgraph SEARCH[\"Search Providers\"]\n        CSE[\"Google CSE\"]\n        SERP[\"SerpAPI\"]\n    end\n\n    USER --> FH\n    WEB <--> API\n    API --> FH_DB\n    WEB --> CFG_DB\n    WEB --> SR_DB\n    WEB --> LLM\n    WEB --> SEARCH\n\n    style FH fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style LLM fill:#e3f2fd,stroke:#1565c0,color:#000\n    style SEARCH fill:#fff3e0,stroke:#e65100,color:#000\n    style USER fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.Technical and System Users.WebHome": "{{warning}}\n**Partially Implemented (v2.6.33)** - Only AKEL system service is implemented. User system, moderators, background scheduler, and search indexer are **not yet implemented**.\n{{/warning}}\n\n= Target Technical Model =\n\n{{mermaid}}\n\nerDiagram\n    USER {\n        string UserID_PK\n        string role\n        int reputation\n    }\n    MODERATOR {\n        string ModeratorID_PK\n        string UserID_FK\n        string permissions\n    }\n    SYSTEM_SERVICE {\n        string ServiceID_PK\n        string ServiceName\n        string Purpose\n        string Status\n    }\n    AKEL {\n        string InstanceID_PK\n        string ServiceID_FK\n        string Version\n    }\n    BACKGROUND_SCHEDULER {\n        string SchedulerID_PK\n        string ServiceID_FK\n        string ScheduledTasks\n    }\n    SEARCH_INDEXER {\n        string IndexerID_PK\n        string ServiceID_FK\n        string LastSyncTime\n    }\n    USER ||--o| MODERATOR : appointed_as\n    MODERATOR ||--o{ SYSTEM_SERVICE : monitors\n    SYSTEM_SERVICE ||--|| AKEL : AI_processing\n    SYSTEM_SERVICE ||--|| BACKGROUND_SCHEDULER : periodic_tasks\n    SYSTEM_SERVICE ||--|| SEARCH_INDEXER : search_sync\n\n{{/mermaid}}\n\n= Implementation Status =\n\n|= Component |= Target Purpose |= Current Status\n| **USER** | User accounts with reputation | Not implemented (anonymous only)\n| **MODERATOR** | Appointed users with permissions | Not implemented\n| **AKEL** | AI processing engine | Implemented (Twin-Path pipeline)\n| **BACKGROUND_SCHEDULER** | Periodic tasks | Not implemented\n| **SEARCH_INDEXER** | Elasticsearch sync | Not implemented (no Elasticsearch)\n\n= Current Implementation =\n\n**v2.6.33 has only:**\n* AKEL pipeline for analysis\n* .NET API for job persistence\n* No background services\n* No search indexing (uses web search only)", "Product Development.Diagrams.Technology Stack.WebHome": "= Technology Stack =\n\n{{mermaid}}\nflowchart TB\n    subgraph UI_LAYER[\"User Interface\"]\n        REACT[\"React 18\"]\n        NEXTJS[\"Next.js App Router\"]\n        CSS[\"CSS Modules\"]\n    end\n\n    subgraph ENGINE[\"Analysis Engine\"]\n        TS[\"TypeScript\"]\n        AI_SDK[\"Vercel AI SDK\"]\n        ZOD[\"Zod Validation\"]\n        CHEERIO[\"cheerio (HTML)\"]\n        PDF[\"pdf2json (PDF)\"]\n    end\n\n    subgraph API_LAYER[\"API Backend\"]\n        DOTNET[\".NET 8 / ASP.NET Core\"]\n        EF[\"Entity Framework Core\"]\n        SQLITE_LIB[\"Microsoft.Data.Sqlite\"]\n    end\n\n    subgraph STORAGE[\"Storage\"]\n        FH_DB[\"factharbor.db\\nJobs, events, metrics\"]\n        CFG_DB[\"config.db\\nUCM configuration\"]\n        SR_DB[\"source-reliability.db\\nSource credibility cache\"]\n    end\n\n    subgraph EXTERNAL[\"External Services\"]\n        ANTHROPIC[\"Anthropic (Claude Haiku 4.5, Opus 4.6)\"]\n        OPENAI[\"OpenAI (GPT-4.1, GPT-4.1-mini)\"]\n        GOOGLE[\"Google (Gemini 2.5-pro, 2.5-flash)\"]\n        MISTRAL_P[\"Mistral (Large, Small)\"]\n        GCSE[\"Google Custom Search Engine\"]\n        SERP[\"SerpAPI\"]\n    end\n\n    UI_LAYER --> ENGINE\n    ENGINE --> EXTERNAL\n    UI_LAYER --> API_LAYER\n    API_LAYER --> STORAGE\n    ENGINE --> STORAGE\n\n    style UI_LAYER fill:#e8f5e9,stroke:#2e7d32,color:#000\n    style ENGINE fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style API_LAYER fill:#e3f2fd,stroke:#1565c0,color:#000\n    style STORAGE fill:#fff9c4,stroke:#f9a825,color:#000\n    style EXTERNAL fill:#fff3e0,stroke:#e65100,color:#000\n{{/mermaid}}\n\n//Technology stack organised in layers: UI (React/Next.js), Engine (TypeScript/AI SDK), API (.NET/EF Core), Storage (SQLite), and External Services (LLM + search providers with specific model versions).//\n", "Product Development.Diagrams.UCM Config Architecture.WebHome": "= UCM Config Architecture =\n\n{{mermaid}}\nflowchart TB\n    subgraph Admin[\"Admin UI\"]\n        EDITOR[\"Config Editor\\n(JSON / Form view)\"]\n        HISTORY[\"Version History\\n& Compare\"]\n    end\n\n    subgraph Types[\"Configuration Types\"]\n        PIPELINE[\"Pipeline Config\"]\n        SEARCH[\"Search Config\"]\n        CALC[\"Calculation Config\"]\n        SR[\"Source Reliability\"]\n        PROMPTS[\"Prompt Templates\"]\n    end\n\n    subgraph Validation[\"Validation\"]\n        ZOD[\"Zod Schema\\nValidation\"]\n        CANON[\"SHA-256\\nContent Hash\"]\n    end\n\n    subgraph Storage[\"SQLite (config.db)\"]\n        BLOBS[\"config_blobs\\n(immutable, content-addressed)\"]\n        ACTIVE[\"config_active\\n(activation pointers)\"]\n        SNAPSHOTS[\"job_config_snapshots\\n(per-job tracking)\"]\n    end\n\n    subgraph Runtime[\"Runtime\"]\n        CACHE[\"In-memory Map\\n(60s TTL)\"]\n        JOB[\"Analysis Job\\n(reads snapshot)\"]\n    end\n\n    EDITOR --> Types\n    Types --> ZOD\n    ZOD -->|\"valid\"| CANON\n    CANON --> BLOBS\n    BLOBS --> ACTIVE\n    ACTIVE --> SNAPSHOTS\n    ACTIVE --> CACHE\n    CACHE --> JOB\n    HISTORY --> BLOBS\n\n    style Admin fill:#f3e5f5,stroke:#6a1b9a,color:#000\n    style Validation fill:#fff9c4,stroke:#f9a825,color:#000\n    style Storage fill:#e3f2fd,stroke:#1565c0,color:#000\n    style Runtime fill:#e8f5e9,stroke:#2e7d32,color:#000\n{{/mermaid}}\n", "Product Development.Diagrams.UCM Config Precedence.WebHome": "= UCM Config Precedence =\n\n{{mermaid}}\nflowchart TD\n    START([System Startup]) --> CHECK_DB{config.db<br/>has configs?}\n\n    CHECK_DB -->|No - Fresh DB| SEED_FILES[Load from JSON files<br/>apps/web/configs/]\n    CHECK_DB -->|Yes| LOAD_DB[Load from config.db]\n\n    SEED_FILES --> VALIDATE_FILE{Valid JSON<br/>and schema?}\n    VALIDATE_FILE -->|No| FALLBACK_CODE[Use code constants<br/>config-schemas.ts]\n    VALIDATE_FILE -->|Yes| WRITE_DB[Write to config.db]\n    WRITE_DB --> ACTIVATE\n\n    LOAD_DB --> VALIDATE_DB{Valid schema?}\n    VALIDATE_DB -->|No| FALLBACK_FILE[Try JSON files]\n    VALIDATE_DB -->|Yes| ACTIVATE[Activate Config]\n\n    FALLBACK_FILE --> VALIDATE_FILE\n    FALLBACK_CODE --> ACTIVATE\n\n    ACTIVATE --> RUNTIME[Runtime Authority:<br/>config.db]\n\n    subgraph Manual Updates\n        ADMIN[Admin UI Update] --> VALIDATE_ADMIN{Valid JSON<br/>and schema?}\n        VALIDATE_ADMIN -->|No| REJECT[Reject with error]\n        VALIDATE_ADMIN -->|Yes| SAVE_DB[Save to config.db]\n        SAVE_DB --> VERSION[Create new version]\n        VERSION --> ACTIVATE\n\n        REJECT --> ADMIN_RETRY[Admin must fix]\n    end\n\n    subgraph Dev Mode Only\n        DEV_SAVE[Save to File button] --> CHECK_ENV{Development<br/>mode?}\n        CHECK_ENV -->|No| BLOCK[Blocked - production]\n        CHECK_ENV -->|Yes| BACKUP[Create .bak backup]\n        BACKUP --> WRITE_FILE[Write to JSON file]\n    end\n\n    style RUNTIME fill:#c8e6c9\n    style FALLBACK_CODE fill:#ffcdd2\n    style FALLBACK_FILE fill:#fff9c4\n    style BLOCK fill:#ffcdd2\n    style REJECT fill:#ffcdd2\n{{/mermaid}}\n\n//How FactHarbor loads, validates, and activates configurations at runtime. Fallback chain: config.db -> JSON files -> Code constants.//\n", "Product Development.Diagrams.User Class Diagram.WebHome": "= User Class Diagram =\n\n{{mermaid}}\n\nclassDiagram\n    class BaseUser {\n        +view_results()\n        +browse()\n        +search()\n    }\n    class Reader {\n        <<guest>>\n        +browse()\n        +search()\n        +view_results()\n    }\n    class RegisteredUser {\n        +UUID id\n        +String username\n        +Role role\n        +Timestamp created_at\n        +submit_url()\n        +flag_issue()\n        +view_submission_history()\n    }\n    class UCMAdministrator {\n        +manage_config()\n        +view_audit_trail()\n        +activate_config_version()\n        +trigger_reanalysis()\n        +view_system_metrics()\n    }\n    class Moderator {\n        +review_flags()\n        +hide_content()\n        +ban_user()\n    }\n    BaseUser <|-- Reader : anonymous\n    BaseUser <|-- RegisteredUser : logged in\n    RegisteredUser <|-- UCMAdministrator : appointed\n    RegisteredUser <|-- Moderator : appointed\n\n{{/mermaid}}\n\n= Role Permissions =\n\n|= Role |= Capabilities |= Requirements\n| **Reader (Guest)** | Browse, search, view results | No login required\n| **User (Registered)** | Everything Reader can + submit URLs/text (rate-limited), flag content | Free account required\n| **UCM Administrator** | Everything User can + manage UCM config, view audit trail, trigger re-analysis | Appointed by Governing Team\n| **Moderator** | Everything User can + review flags, hide content, ban users | Appointed by Governing Team\n\n= Current Implementation =\n\n* All users are anonymous Readers (no authentication system yet)\n* UCM config management via CLI/direct DB access\n* No moderator tooling\n* No rate limiting (single-user development mode)\n\n= Design Principles =\n\n* **No data editing roles**  analysis outputs are immutable\n* **UCM Administrator** improves the system through configuration, not by editing individual outputs\n* **Submission requires login**  LLM inference and web search are not free; rate limits control costs\n* **Four roles**: Reader (guest), User (registered), UCM Administrator (appointed), Moderator (appointed)\n", "Product Development.Diagrams.Verdict Scale.WebHome": "= Verdict Scale =\n\n{{mermaid}}\nflowchart LR\n    TRUE[\"TRUE\\n86-100%\"]\n    MT[\"MOSTLY\\nTRUE\\n72-85%\"]\n    LT[\"LEANING\\nTRUE\\n58-71%\"]\n    MIXED[\"MIXED /\\nUNVERIFIED\\n43-57%\"]\n    LF[\"LEANING\\nFALSE\\n29-42%\"]\n    MF[\"MOSTLY\\nFALSE\\n15-28%\"]\n    FALSE[\"FALSE\\n0-14%\"]\n\n    TRUE --- MT --- LT --- MIXED --- LF --- MF --- FALSE\n\n    style TRUE fill:#2e7d32,color:#fff\n    style MT fill:#43a047,color:#fff\n    style LT fill:#66bb6a,color:#000\n    style MIXED fill:#fdd835,color:#000\n    style LF fill:#ff9800,color:#000\n    style MF fill:#e65100,color:#fff\n    style FALSE fill:#b71c1c,color:#fff\n{{/mermaid}}\n", "Product Development.Diagrams.WebHome": "= Diagrams =\n\n{{info}}\nNon-trivial diagrams live in dedicated pages here and are **{{include}}d** into the Architecture, Specification, and Planning pages where they are described.\n{{/info}}\n\n== Architecture ==\n\n* [[System Architecture>>FactHarbor.Product Development.Diagrams.System Architecture.WebHome]]  Two-service architecture (API + Web/Runner)\n* [[System Context>>FactHarbor.Product Development.Diagrams.System Context.WebHome]]  Users, platform, LLM and search providers\n* [[Request Lifecycle>>FactHarbor.Product Development.Diagrams.Request Lifecycle.WebHome]]  End-to-end request sequence\n* [[Technology Stack>>FactHarbor.Product Development.Diagrams.Technology Stack.WebHome]]  Frameworks, libraries, and infrastructure\n* [[External Dependencies Map>>FactHarbor.Product Development.Diagrams.External Dependencies Map.WebHome]]  LLM, search, and retrieval dependencies\n* [[LLM Model Tiering>>FactHarbor.Product Development.Diagrams.LLM Model Tiering.WebHome]]  Model selection per pipeline stage\n* [[LLM Abstraction Architecture>>FactHarbor.Product Development.Diagrams.LLM Abstraction Architecture.WebHome]]  Multi-provider abstraction, tiered routing, provider model mapping\n* [[Storage Architecture>>FactHarbor.Product Development.Diagrams.Storage Architecture.WebHome]]  Three-database architecture layout\n* [[Storage Roadmap>>FactHarbor.Product Development.Diagrams.Storage Roadmap.WebHome]]  Storage evolution POC  Alpha  Beta  V1.0+\n* [[Deployment Topology>>FactHarbor.Product Development.Diagrams.Deployment Topology.WebHome]]  POC vs production deployment\n* [[Security Model>>FactHarbor.Product Development.Diagrams.Security Model.WebHome]]  Implemented, planned, and always-active security layers\n* [[Circuit Breaker States>>FactHarbor.Product Development.Diagrams.Circuit Breaker States.WebHome]]  Provider health circuit breaker state machine\n* [[Federation Architecture>>FactHarbor.Product Development.Diagrams.Federation Architecture.WebHome]]  Decentralized node architecture (future)\n\n== AKEL ==\n\n* [[AKEL Engine Overview>>FactHarbor.Product Development.Diagrams.AKEL Engine Overview.WebHome]]  5-step engine summary\n* [[AKEL Analysis Pipeline>>FactHarbor.Product Development.Diagrams.AKEL Analysis Pipeline.WebHome]]  Pipeline with quality gates\n* [[AKEL Pipeline Detail>>FactHarbor.Product Development.Diagrams.AKEL Pipeline Detail.WebHome]]  Detailed 5-step pipeline flow\n* [[AKEL Shared Modules>>FactHarbor.Product Development.Diagrams.AKEL Shared Modules.WebHome]]  Shared analysis modules\n* [[AKEL Quality Assurance>>FactHarbor.Product Development.Diagrams.AKEL Quality Assurance.WebHome]]  Quality gates, evidence defence, source trust pillars\n\n== Data Models ==\n\n**Primary field-level reference:** [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]]  five complementary views (Overview, Result, Target DB, Runtime, UI) with complete field detail per entity.\n\n* [[Analysis Entity Model ERD>>FactHarbor.Product Development.Diagrams.Analysis Entity Model ERD.WebHome]]  Complete entity relationship model\n* [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]]  Core entities overview\n* [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]]  Overview, Result, Target DB, Runtime, UI views\n* [[Job Lifecycle ERD>>FactHarbor.Product Development.Diagrams.Job Lifecycle ERD.WebHome]]  Job and event entity model\n* [[KeyFactor Entity Model>>FactHarbor.Product Development.Diagrams.KeyFactor Entity Model.WebHome]]  KeyFactor entity relationships (historical  Orchestrated era)\n* [[KeyFactor Data Flow>>FactHarbor.Product Development.Diagrams.KeyFactor Data Flow.WebHome]]  KeyFactor data through pipeline phases (historical  Orchestrated era)\n* [[KeyFactor Claim Mapping>>FactHarbor.Product Development.Diagrams.KeyFactor Claim Mapping.WebHome]]  KeyFactor-to-claim relationship graph (historical  Orchestrated era)\n* [[KeyFactor Hierarchy>>FactHarbor.Product Development.Diagrams.KeyFactor Hierarchy.WebHome]]  Thesis  KeyFactors  Claims decomposition (historical  Orchestrated era)\n* [[Verdict Scale>>FactHarbor.Product Development.Diagrams.Verdict Scale.WebHome]]  7-point verdict scale (TRUE to FALSE)\n* [[Audit Trail ERD>>FactHarbor.Product Development.Diagrams.Audit Trail ERD.WebHome]]  Job and UCM audit trail entities\n\n== Config ==\n\n* [[UCM Config Precedence>>FactHarbor.Product Development.Diagrams.UCM Config Precedence.WebHome]]  Configuration source precedence chain\n* [[UCM Config Architecture>>FactHarbor.Product Development.Diagrams.UCM Config Architecture.WebHome]]  Admin UI, validation, storage, and runtime flow\n\n== Pipelines ==\n\n* [[ClaimAssessmentBoundary Pipeline Detail>>FactHarbor.Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail.WebHome]]  5-stage pipeline with LLM debate pattern, sequence diagram, budget table\n* [[Monolithic Dynamic Pipeline Internal>>FactHarbor.Product Development.Diagrams.Monolithic Dynamic Pipeline Internal.WebHome]]  Internal Monolithic Dynamic execution flow\n* [[Pipeline Shared Primitives>>FactHarbor.Product Development.Diagrams.Pipeline Shared Primitives.WebHome]]  Shared primitives architecture\n* [[Pipeline Variant Dispatch>>FactHarbor.Product Development.Diagrams.Pipeline Variant Dispatch.WebHome]]  Analysis dispatch and convergence flow\n\n== Overview ==\n\n* [[Claim and Scenario Lifecycle (Overview)>>FactHarbor.Product Development.Diagrams.Claim and Scenario Lifecycle (Overview).WebHome]]  End-to-end lifecycle overview\n\n== Workflows ==\n\n* [[Claim and Scenario Workflow>>FactHarbor.Product Development.Diagrams.Claim and Scenario Workflow.WebHome]]\n* [[Evidence and Verdict Workflow>>FactHarbor.Product Development.Diagrams.Evidence and Verdict Workflow.WebHome]]\n* [[Quality and Audit Workflow>>FactHarbor.Product Development.Diagrams.Quality and Audit Workflow.WebHome]]\n\n== Quality ==\n\n* [[Quality Gates Flow>>FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome]]  Gate 1 + Gate 4 flow\n* [[Quality Gates Integration>>FactHarbor.Product Development.Diagrams.Quality Gates Integration.WebHome]]  Pipeline integration points\n* [[Evidence Defence in Depth>>FactHarbor.Product Development.Diagrams.Evidence Defence in Depth.WebHome]]  6-layer defence model\n* [[Evidence Quality Filtering Pipeline>>FactHarbor.Product Development.Diagrams.Evidence Quality Filtering Pipeline.WebHome]]  7-layer filtering pipeline\n* [[Context Detection Phases>>FactHarbor.Product Development.Diagrams.Context Detection Phases.WebHome]]  Multi-phase context detection\n* [[Context Detection Decision Tree>>FactHarbor.Product Development.Diagrams.Context Detection Decision Tree.WebHome]]  ~~AnalysisContext~~ vs EvidenceScope decision tree (historical  Orchestrated era)\n* [[Doubted vs Contested Flow>>FactHarbor.Product Development.Diagrams.Doubted vs Contested Flow.WebHome]]  Counter-evidence weight reduction flow\n\n== Source Reliability ==\n\n* [[Source Reliability Overview>>FactHarbor.Product Development.Diagrams.Source Reliability Overview.WebHome]]  System architecture overview\n* [[Source Reliability Flow>>FactHarbor.Product Development.Diagrams.Source Reliability Flow.WebHome]]  3-phase prefetch/lookup/weighting sequence\n* [[Source Reliability Prefetch Flow>>FactHarbor.Product Development.Diagrams.Source Reliability Prefetch Flow.WebHome]]  Phase 1 async prefetch decision flow\n\n== Users and Roles ==\n\n* [[Human User Roles>>FactHarbor.Product Development.Diagrams.Human User Roles.WebHome]]\n* [[Technical and System Users>>FactHarbor.Product Development.Diagrams.Technical and System Users.WebHome]]\n* [[User Class Diagram>>FactHarbor.Product Development.Diagrams.User Class Diagram.WebHome]]\n* [[Role-Based Access Control>>FactHarbor.Product Development.Diagrams.Role-Based Access Control.WebHome]]  Implemented vs planned access levels\n\n== Planning ==\n\n* [[Development Roadmap>>FactHarbor.Product Development.Diagrams.Development Roadmap.WebHome]]  POC  Alpha  Beta  V1.0\n* [[Architecture Roadmap>>FactHarbor.Product Development.Diagrams.Architecture Roadmap.WebHome]]  Phase-by-phase capability evolution\n\n== Testing ==\n\n* [[Promptfoo Test Coverage>>FactHarbor.Product Development.Diagrams.Promptfoo Test Coverage.WebHome]]  Test configurations and provider matrix\n\n== Automation ==\n\n* [[Automation Level>>FactHarbor.Product Development.Diagrams.Automation Level.WebHome]]\n* [[Automation Roadmap>>FactHarbor.Product Development.Diagrams.Automation Roadmap.WebHome]]\n* [[Manual vs Automated Matrix>>FactHarbor.Product Development.Diagrams.Manual vs Automated matrix.WebHome]]\n\n== Outdated ==\n\n* [[Outdated Diagrams>>FactHarbor.Product Development.Diagrams.Outdated.WebHome]]  Superseded diagram pages, retained for historical reference\n\n----\n\n**Navigation:** [[Specification>>FactHarbor.Product Development.Specification.WebHome]] | [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n", "Product Development.Media Source Database.WebHome": "= Media Source Database =\n\n(% class=\"wikigeneratedid\" %)\n(((\n(% class=\"box successmessage\" %)\n(((\n**An open, transparent, AI-powered credibility database for every media source on the internet.**\n)))\n)))\n\nThe Media Source Database evaluates the reliability of media sources using dual-AI cross-checking, evidence-grounded scoring, and a 7-band credibility scale  in 19 languages. It currently powers every FactHarbor analysis and is planned to become a **standalone public service and API** available to journalists, researchers, educators, and anyone who needs to assess source credibility.\n\n**Version**: 1.4 (Multi-Language Support) | **Status**: Operational\n\n----\n\n== The Problem ==\n\nReliable information starts with reliable sources  but assessing source credibility today is surprisingly difficult:\n\n* **Static databases go stale.** Most media rating lists are pre-seeded by editorial teams, updated infrequently, and inevitably fall behind ownership changes, editorial shifts, and evolving track records.\n* **Hidden editorial bias.** Pre-seeded ratings reflect the judgement of whoever compiled them. Users cannot inspect the reasoning or challenge the scores.\n* **English-only coverage.** Most existing tools focus on English-language media, leaving regional sources in dozens of languages unassessed.\n* **Opinion over evidence.** Many rating systems rely on institutional prestige or domain type (.gov, .edu) rather than demonstrated accuracy and correction practices.\n* **No open-source alternative.** There is no transparent, auditable, open-source system for on-demand media source evaluation.\n\nThe Media Source Database addresses all of these.\n\n----\n\n== What Makes This Different ==\n\n=== No Pre-seeded Data ===\n\nThere is no hardcoded list of \"good\" or \"bad\" sources. Every source is evaluated on-demand by AI using the same process  from a local newspaper in So Paulo to a major wire service. Every score is traceable to an LLM evaluation with cited evidence. Nothing is hidden.\n\n=== Dual-AI Cross-Check ===\n\nTwo independent AI models must agree. Claude performs the primary evaluation, then an OpenAI model cross-checks and refines the result. If the models disagree beyond a configurable threshold, the source is marked as \"unknown\" rather than receiving a potentially unreliable score. Skepticism is the default.\n\n=== Evidence-Grounded ===\n\nScores are not based on AI \"feelings\" about a source. The system first searches the web for independent assessments  fact-checker ratings, journalism reviews, correction records, and ownership information  and builds an **evidence pack**. The AI evaluates against this evidence. Scores backed by insufficient evidence are rejected: high reliability ratings require a minimum number of cited sources.\n\n=== Skeptical by Default ===\n\nHigh reliability is harder to earn than low reliability. A source rated \"Highly Reliable\" needs stronger evidence and higher model confidence than one rated \"Mixed\". Unknown sources receive a neutral score (0.50), not a generous one. Absence of negative evidence does not equal reliability.\n\n=== Works in 19 Languages ===\n\nThe system detects the publication language and searches for regional fact-checker assessments alongside international coverage. A German-language source is evaluated against assessments from CORRECTIV, Mimikama, and dpa-Faktencheck  not just English-language fact-checkers.\n\nSupported languages: German, French, Spanish, Portuguese, Italian, Dutch, Polish, Russian, Swedish, Norwegian, Danish, Finnish, Czech, Hungarian, Turkish, Japanese, Chinese, Korean, Arabic.\n\n=== Dynamic and Current ===\n\nSource credibility changes over time. Ownership changes, editorial shifts, political transitions, and corrections records all matter. Evaluations are cached for 90 days and then automatically re-assessed  ensuring scores reflect current performance, not historical reputation. The last 24 months matter most.\n\n=== Fully Transparent ===\n\nEvery evaluation stores the full reasoning, bias indicators, evidence citations, and confidence scores. Users can inspect exactly why a source received its rating, what evidence was considered, and how confident the system is in the result. Evaluations are exportable in JSON, Markdown, HTML, and PDF formats.\n\n----\n\n== How It Works ==\n\n=== 1. Evidence Gathering ===\n\nWhen a source is first encountered, the system searches the web for independent assessments: fact-checker ratings, journalism reviews, track record evidence, ownership information, and correction practices. For non-English sources, it performs dual-language searches  English for international coverage, plus the source's own language for regional fact-checkers.\n\n=== 2. Dual-Model Evaluation ===\n\n**Claude** evaluates the source using the evidence pack, producing a reliability score, confidence level, reasoning, bias classification, and evidence citations. Then an **OpenAI model** independently cross-checks the result using the same evidence.\n\n=== 3. Skeptical Acceptance ===\n\nThe score is only accepted if:\n* Both models agree (score difference  0.20)\n* Confidence meets the threshold ( 0.80, higher for high-reliability ratings)\n* Sufficient evidence was cited\n\nIf any check fails, the source is marked as \"unknown\" (neutral) rather than receiving a potentially unreliable score.\n\n=== 4. Transparent Result ===\n\nThe accepted evaluation is cached for 90 days and includes: reliability score, confidence, full reasoning, bias indicator, all evidence citations, and the evidence pack used during evaluation.\n\n=== Verdict Impact ===\n\nSource reliability scores act as **weights on evidence**. A verdict backed by highly reliable sources keeps its strength. A verdict backed by unreliable sources is pulled toward neutral (50%).\n\n{{code}}\nExample:\n  Original verdict: 80% (Mostly True)\n  Average source score: 0.50 (Mixed reliability)\n  Adjusted verdict: 65% (Leaning True)\n{{/code}}\n\n----\n\n== The 7-Band Credibility Scale ==\n\n|= Score |= Rating |= Meaning |= Verdict Effect\n| 0.86  1.00 | **Highly Reliable** | Verified accuracy, rigorous corrections (e.g., wire services, standards bodies) | Verdict fully preserved\n| 0.72  0.85 | **Reliable** | Consistent accuracy, professional editorial standards | Verdict mostly preserved\n| 0.58  0.71 | **Leaning Reliable** | Often accurate, occasional errors, corrects when notified | Moderate preservation\n| 0.43  0.57 | **Mixed** | Variable accuracy or inconsistent quality by topic/author | Neutral zone\n| 0.29  0.42 | **Leaning Unreliable** | Often inaccurate or bias significantly affects reporting | Pulls toward neutral\n| 0.15  0.28 | **Unreliable** | Pattern of false claims or ignores corrections | Strong pull toward neutral\n| 0.00  0.14 | **Highly Unreliable** | Fabricates content or documented disinformation source | Maximum skepticism\n\n**Unknown sources** (not yet evaluated or evaluation inconclusive) receive a default score of **0.50** (neutral center).\n\n----\n\n== What Users See ==\n\nFor every analysis, users can see:\n\n* **Per-source credibility badge**  color-coded rating (Highly Reliable  Highly Unreliable)\n* **Score and confidence**  the numeric reliability score and how confident the evaluation is\n* **Bias indicator**  political spectrum or other bias classification (noted, not penalized unless it affects accuracy)\n* **Impact on verdict**  how source reliability weighted the final verdict\n\n----\n\n== Who This Is For ==\n\n=== Journalists and Newsrooms ===\n\n* Quick credibility check before citing an unfamiliar source\n* Multi-language coverage for regional and international sources\n* Transparent reasoning that can be cited in editorial discussions\n* Export evaluations for editorial records and source vetting workflows\n\n=== Researchers and Academics ===\n\n* API access for systematic media landscape studies\n* Evidence-based scores suitable for reproducible research\n* Full data export (JSON, Markdown, HTML, PDF) for analysis pipelines\n* No proprietary black box  methodology is open and auditable\n\n=== Fact-Checkers ===\n\n* Cross-reference your own assessments with AI-powered evaluation\n* Identify sources that lack independent assessment coverage\n* Bulk credibility screening for large source lists\n* Multi-language support for cross-border investigations\n\n=== Educators ===\n\n* Teach media literacy with real, transparent evaluations\n* Show students how credibility is assessed with evidence, not opinion\n* Demonstrate how the same source can be evaluated differently over time\n* Open methodology that can be studied and critiqued in the classroom\n\n=== Organizations and Decision-Makers ===\n\n* Vet information sources systematically before acting on them\n* Understand the credibility landscape around a topic\n* Evidence-based source assessments for compliance and due diligence\n\n----\n\n== Standalone Application and Public API ==\n\nThe Media Source Database is planned to become a **separate application and web service**, independent of the FactHarbor analysis pipeline.\n\n=== Why ===\n\nSource reliability evaluation is a general-purpose capability with value far beyond claim analysis. Journalists need quick credibility checks without running a full analysis. Researchers want to query the database directly via API. Other fact-checking tools could integrate source reliability as a service. Decoupling enables independent scaling, deployment, and release cycles.\n\n=== Architecture ===\n\n{{code}}\nCurrent (embedded):\n  FactHarbor Analysis Pipeline  [SR Service (internal)]\n\nFuture (standalone):\n  FactHarbor Analysis Pipeline  Media Source Database API\n  External consumers  Media Source Database API\n  Browser / direct users  Media Source Database Web UI\n{{/code}}\n\n=== Public API (Planned) ===\n\n|= Endpoint |= Method |= Description |= Auth\n| ##/api/v1/sources/{domain}## | GET | Look up source credibility | Reader (guest)\n| ##/api/v1/sources/{domain}/evaluate## | POST | Request fresh evaluation | User (rate-limited)\n| ##/api/v1/sources## | GET | Search/list evaluated sources | Reader (guest)\n| ##/api/v1/sources/{domain}/flag## | POST | Flag incorrect evaluation | User (authenticated)\n\n=== Shared User Account System ===\n\nThe Media Source Database will share FactHarbor's user account system. Users log in once and have access to both.\n\n|= Role |= FactHarbor |= Media Source Database\n| **Reader** (Guest) | Browse and view analyses | Browse and search source evaluations\n| **User** (Registered) | Submit URLs/text for analysis (rate-limited) | Look up source credibility (rate-limited), flag incorrect evaluations\n| **UCM Administrator** | Manage pipeline configuration | Manage SR configuration (evaluation parameters, cache, scoring rules)\n| **Moderator** | Handle abuse and community health | Handle abuse and community health\n\n=== Migration Path ===\n\n1. **Current**: Embedded module within FactHarbor (SQLite cache, internal API)\n1. **Next**: Extracted into separate deployable package (shared database, separate process)\n1. **Future**: Standalone web application with its own UI, public API, and shared authentication\n\n----\n\n== For Sponsors ==\n\n=== The Opportunity ===\n\nMedia trust is in crisis globally. People struggle to distinguish reliable reporting from propaganda, and professionals lack open, transparent tools for source vetting. Existing solutions are proprietary, English-only, or rely on opaque editorial judgement.\n\nThe Media Source Database is **the first open-source, AI-powered, multi-language source credibility service**  built on principles of transparency, evidence, and skepticism.\n\n=== What Funding Enables ===\n\n* **Standalone public service**  Extracting the Media Source Database into an independent application with its own web UI and public API\n* **Expanded language coverage**  Adding more languages and regional fact-checker integrations beyond the current 19\n* **Browser extension**  One-click source credibility checks while browsing\n* **API tiers**  Free public access with rate limits, plus higher-volume tiers for institutional users\n* **Community features**  User flagging, correction workflows, and evaluation quality feedback loops\n\n=== Transparency Commitment ===\n\n* **Open source**  All code is publicly available and auditable\n* **Open methodology**  Evaluation criteria, scoring rules, and AI prompts are documented\n* **Open data**  Evaluation results are exportable and inspectable\n* **No hidden algorithms**  Every score is traceable to specific evidence and reasoning\n\n----\n\n== Design Principles ==\n\n=== Evidence Over Authority ===\n\nSource credibility is **supplementary**, not primary. A low-credibility source with documented evidence should still be considered. A high-credibility source making unsupported claims should be questioned. The evidence itself always matters more than who says it.\n\n=== Track Record Over Prestige ===\n\nDomain type (.gov, .edu, .org) does **not** imply quality. Scores are derived from demonstrated track record: accuracy history, correction practices, editorial independence, and fact-checker assessments. Brand recognition alone does not inflate scores.\n\n=== Entity-Level Evaluation ===\n\nWhen a domain is the primary digital outlet for a larger organization, the evaluation focuses on the reliability of the entire organization  not just the website. This prevents high-quality organizations from being underrated due to narrow domain-focused metrics.\n\n----\n\n== Source Type Score Caps ==\n\nCertain source types have **default score caps** to handle known categories of unreliable content. These are configurable by administrators.\n\n|= Source Type |= Default Cap |= Default Rating\n| Propaganda outlet | 0.14 | Highly Unreliable\n| Known disinformation | 0.14 | Highly Unreliable\n| State-controlled media | 0.42 | Leaning Unreliable\n| User-generated content platform | 0.42 | Leaning Unreliable\n\nThe caps are **prompt-driven**  the AI is instructed to respect them during evaluation. If evidence suggests a source has reformed, the correct action is to **reclassify the source type**, not exceed the cap.\n\n----\n\n== Configuration ==\n\nThe Media Source Database is configured via **UCM** (Admin  Config  Source Reliability). Key settings:\n\n|= Setting |= Default |= Description\n| Enabled | Yes | Enable/disable source evaluation\n| Multi-model | Yes | Use two models for cross-checking\n| Cache TTL | 90 days | How long to cache evaluations\n| Confidence threshold | 0.80 | Minimum AI confidence to accept a score\n| Consensus threshold | 0.20 | Maximum score difference between models\n| Default score | 0.50 | Score for unknown sources (neutral center)\n\n----\n\n{{info}}\n**Technical Reference:** For implementation details  code architecture, batch prefetch pattern, cache strategy, pipeline integration, cost estimates, and test coverage  see [[Source Reliability (Deep Dive)>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].\n\n**Internal module name**: SourceReliability Service | **Key file**: ##apps/web/src/lib/analyzer/source-reliability.ts##\n{{/info}}\n\n----\n\n== Related Pages ==\n\n* [[Source Reliability (Deep Dive)>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]  Technical architecture, code integration, and API details\n* [[Source Reliability Export Guide>>FactHarbor.Product Development.DevOps.Subsystems and Components.Source Reliability Export.WebHome]]  How to export evaluation data\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  How quality gates interact with source reliability\n* [[Evidence and Verdict Workflow>>FactHarbor.Product Development.Diagrams.Evidence and Verdict Workflow.WebHome]]  How evidence flows through the system\n", "Product Development.Planning.Beta0.WebHome": "= Beta: User Testing & Production Readiness =\n\n**Phase Goal:** Real user testing with corrections system and essential security\n\n**Success Metric:** User satisfaction, essential production features functional\n\n== 1. Overview ==\n\nBeta 0 adds user-facing features and essential production infrastructure:\n\n* User corrections notification system\n* Archive.org integration (evidence persistence)\n* Essential security controls\n* Beta tester program\n\n**Key Innovation:** First real users testing the system\n\n== 2. Requirements ==\n\n=== FR45: Corrections Notification System (Basic) ===\n\n**IFCN Compliance Requirement**\n\n**Mechanisms:**\n\n1. In-page banner (persistence period TBD)\n2. Public correction log\n3. Email notifications (opt-in)\n\n**Target:** Corrections visible promptly\n\n=== FR47: Archive.org Integration ===\n\n**Evidence Persistence**\n\n**Functionality:**\n\n* Auto-archive all evidence sources\n* Fallback to archived version if original unavailable\n* Periodic re-archiving for frequently viewed claims\n\n**Target:** High percentage of evidence archived\n\n=== NFR12: Security Controls (Essential) ===\n\n**Production Readiness**\n\n**Requirements:**\n\n1. Input validation (SQL injection, XSS, CSRF prevention)\n2. Rate limiting\n3. HTTPS everywhere\n4. Basic authentication for admin functions\n\n**Target:** 0 critical/high security vulnerabilities\n\n== 3. Beta Testing Program ==\n\n**Recruitment:**\n\n* Multiple beta testers\n* Mix: journalists, researchers, fact-checkers, general users\n\n**Testing Protocol:**\n\n* Structured testing period\n* Guided tasks + free exploration\n* Satisfaction survey\n* Bug reporting system\n\n**Success Metric:** Satisfactory user feedback\n\n== 4. Success Criteria ==\n\n*  Beta testers recruited and completed testing\n*  User satisfaction meets target\n*  Corrections system working (all mechanisms)\n*  Archive.org integration operational\n*  Security audit: 0 critical/high issues\n*  Quality gates still maintaining <5% hallucinations\n\n== Related Pages ==\n\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  Phase redefinition\n* [[Alpha (previously POC2)>>FactHarbor.Product Development.Planning.POC2.WebHome]]  Previous phase (superseded)\n* [[V1.0>>FactHarbor.Product Development.Planning.V10.WebHome]]  Next phase (launch)\n* [[Roadmap Overview>>FactHarbor.Product Development.Planning.WebHome]]\n* [[Storage Architecture>>FactHarbor.Product Development.Diagrams.Storage Architecture.WebHome]]  PostgreSQL migration planned for Beta\n\n**Document Status:** Beta Specification\n**Version:** V0.9.70", "Product Development.Planning.POC to Alpha Transition.WebHome": "= POC to Alpha Transition =\n\n== 1. POC1 Completion Scorecard ==\n\n=== 1.1 Original Success Criteria (from POC1 Section 4) ===\n\n|= Criterion |= Target |= Status |= Evidence |= Blocker?\n| Processes diverse articles without crashes | Works | **MET** | Pipeline operational, SSE lifecycle management, retry with backoff | No\n| Generates verdicts for all factual claims | Works | **MET** | 7-point verdict scale, per-claim and article verdicts | No\n| Blocks non-factual claims (0% pass through) | 0% | **MET (different approach)** | Gate 1 uses LLM text analysis for factuality classification rather than rule-based opinion detection | No\n| Blocks insufficient-evidence verdicts | 0% with <2 sources | **CODE-ENFORCED** | Gate 4 enforces minimum 2 sources (##quality-gates.ts##); central claims exempt | No\n| Hallucination rate <10% | <10% | **UNKNOWN** | No baseline test run; 38 promptfoo test cases built but not executed | **YES**\n| 0 opinions published as facts | 0% | **MET (different approach)** | LLM classifies input types at Gate 1; not rule-based | No\n| Average quality score >=7.0/10 | >=7.0 | **UNKNOWN** | No baseline test run | **YES**\n| Identified prompt improvements | Documented | **PARTIAL** | Prompt Optimization v2.8 completed and code-reviewed; not validated against baseline | No\n| Validated threshold values | Validated | **PARTIAL** | Gate thresholds implemented; not validated against real data | Soft\n| Clear path to POC2 | Defined | **MET** | POC2 spec complete | No\n\n=== 1.2 Functional Requirements Status ===\n\n|= FR |= Title |= Original POC1 Scope |= Status |= Notes\n| FR1 | Claim Intake | In scope | **COMPLETE** | URL + text + PDF input, format validation\n| FR2 | Claim Normalization | Implicit | **COMPLETE** | Input neutrality (question == statement within +/-5%)\n| FR3 | Claim Classification | Gate 1 | **COMPLETE** | LLM text analysis classifies claim types\n| FR4 | Analysis Summary | In scope | **COMPLETE+** | Two-panel summary, multi-context, article verdict\n| FR5 | Evidence Linking | In scope | **COMPLETE+** | Web search, source extraction, provenance validation, probative value scoring\n| FR6 | Scenario Comparison | Deferred | **EVOLVED** | Now uses AnalysisContext multi-context detection (more sophisticated)\n| FR7 | Automated Verdicts | In scope | **COMPLETE+** | 7-point scale, dependency tracking, contestation, harm potential\n| FR12 | Two-Panel Summary | In scope | **COMPLETE** | Summary + analysis panels in UI\n| NFR11 | Quality Gates | Gates 1+4 | **COMPLETE** | Gate 1 (Claim Validation) + Gate 4 (Verdict Confidence)\n| NFR14 | LLM Abstraction | In scope | **COMPLETE+** | 4 providers via Vercel AI SDK with model tiering\n\n=== 1.3 Non-Functional Requirements Status ===\n\n|= NFR |= Title |= Status |= Notes\n| Performance | Processing time | **MET** | 30s-15min depending on article complexity\n| Scalability | Single-instance | **BASIC** | Sufficient for POC; scaling needs PostgreSQL + Redis\n| Transparency | Evidence + reasoning visible | **MET** | Sources cited, reasoning shown, source reliability scores\n| Quality | Quality gates enforced | **MET** | Gate 1 + Gate 4 operational\n\n== 2. Beyond-POC Features Already Implemented ==\n\nDevelopment has significantly exceeded the original POC1 scope. Many features originally planned for POC2, Beta 0, or V1.0 are already operational.\n\n=== 2.1 Features Originally Planned for POC2 ===\n\n|= Feature |= POC2 Spec Reference |= Current Status\n| Multi-provider LLM | NFR14 enhancement | **COMPLETE**  Anthropic, OpenAI, Google, Mistral via Vercel AI SDK\n| Source Reliability Scoring | Part of evidence quality | **COMPLETE**  LLM evaluation + multi-model consensus + in-memory cache\n| Evidence Quality Filtering | Part of quality framework | **COMPLETE**  7-layer defense with probativeValue, sourceType classification\n| Context-Aware Analysis | Conditional on POC1 experiments | **COMPLETE**  AnalysisContext detection is standard feature, not experimental\n| Quality Metrics Dashboard | NFR13 | **BUILT**  At ##/admin/metrics##, but no data (metrics not integrated)\n| LLM Text Analysis Pipeline | Part of quality framework | **COMPLETE**  4 analysis points: input classification, evidence quality, context similarity, verdict validation\n\n=== 2.2 Features Originally Planned for Beta 0 / V1.0 ===\n\n|= Feature |= Original Phase |= Current Status\n| A/B Testing Framework | V1.0 | **BUILT**  Framework + 38 promptfoo test cases (not executed)\n| API Endpoints | V1.0 (UN-14) | **COMPLETE**  ASP.NET Core 8.0 with 6 controllers\n| Admin Dashboard | Beta 0 | **COMPLETE**  Config management, quality, metrics\n| Unified Config Management | Future | **COMPLETE**  All 4 phases: DB-backed config, hot-reload, snapshots, admin UI\n| Job Config Snapshots | Future | **COMPLETE**  Full auditability per job\n\n=== 2.3 Implication ===\n\n{{warning}}\n**Continuing to call this \"POC1\" understates what has been built.** The phase boundaries defined in V0.9.70 are meaningless against v2.10.2 reality. Documentation says \"deferred to POC2\" for features that are already implemented and operational. This creates confusion in every document that references the original roadmap.\n{{/warning}}\n\n== 3. POC Completion ==\n\n{{success}}\n**POC declared complete on 2026-02-19.** Tagged as ##v1.0.0-poc##.\n{{/success}}\n\n=== 3.1 Basis for Closure ===\n\nThe POC was closed based on demonstrated working software rather than formal baseline tests:\n\n|= Criterion |= Evidence\n| End-to-end pipeline works | ClaimAssessmentBoundary pipeline v1.0  all 5 stages operational\n| Automated claim analysis | Claims extracted, evidence gathered, verdicts generated with LLM debate pattern\n| Quality gates enforced | Gate 1 (claim validation) + Gate 4 (confidence) operational\n| Source reliability | LLM-based evaluation with multi-model consensus and caching\n| Input neutrality | Question vs statement within +/-4% tolerance (empirically tested)\n| Code quality | 853 unit tests passing, build clean\n| Architecture | Evidence-emergent boundaries, UCM configuration, multi-provider LLM\n\nThe original POC1 success criteria (##hallucination rate <10%##, ##average quality score >=7.0##) were defined for a much simpler system. The current system has evolved far beyond those original goals (see Section 2). Formal baseline testing is deferred to Alpha as a quality-improvement exercise, not a POC gate.\n\n=== 3.2 Deferred to Alpha ===\n\n|= Item |= Rationale |= Alpha Priority\n| Baseline quality test (promptfoo) | Quality measurement, not POC proof | med\n| Metrics integration | Observability concern | med\n| Claim fidelity validation | Quality refinement | med\n| Dead code cleanup | Cosmetic | med\n\n== 4. Phase Redefinition ==\n\n=== 4.1 The Problem ===\n\nThe current roadmap was defined at V0.9.70 when:\n\n* The project was a simple single-prompt analyzer\n* No LLM abstraction existed\n* No config management existed\n* No source reliability existed\n* Quality gates were planned but not implemented\n* \"Scenarios\" were the core analytical concept\n\nNow at v2.10.2:\n\n* Multi-step orchestrated pipeline with 7 phases\n* Multi-provider LLM with model tiering\n* Full config management with hot-reload\n* LLM-based source reliability with multi-model consensus\n* Quality gates implemented and code-enforced\n* AnalysisContexts + KeyFactors (bounded analytical frames with decomposition questions)\n\n**The original phase boundaries (POC1 -> POC2 -> Beta 0 -> V1.0) are meaningless.**\n\n=== 4.2 DECIDED: Compress POC2 into Alpha ===\n\n{{info}}\n**Decision**: Skip the original POC2 specification. Run baseline test, declare POC done, transition directly to Alpha. Cherry-pick remaining POC2 goals based on empirical data.\n{{/info}}\n\n**Rationale**:\n\n* Honest about where the project actually is\n* Data-driven decisions on remaining work\n* Avoids implementing obsolete features (Gate 3 references \"Scenarios\" which are replaced)\n* Matches terminology the industry expects\n* Original POC2 spec was designed for a simpler system that no longer exists\n\n**What \"Alpha\" means**:\n\n* System works end-to-end\n* Quality is measured and tracked\n* Features may still change\n* Not production-hardened\n* No user-facing stability guarantees\n* Security not yet hardened\n\n=== 4.3 Minimum Steps to Enter Alpha ===\n\n1. **RUN baseline quality test**  **BLOCKER**\n1. **INTEGRATE metrics collection**  SHOULD\n1. **DOCUMENT baseline results**  SHOULD\n1. **UPDATE roadmap pages** in xWiki to reflect new phase definitions  SHOULD\n1. **ARCHIVE outdated POC documentation** or add status warnings  NICE-TO-HAVE\n\n=== 4.4 What Changes Between \"POC\" and \"Alpha\" ===\n\n|= Aspect |= POC (Current) |= Alpha (Proposed)\n| Quality | Unknown baseline | Measured, tracked, targets set\n| Caching | None beyond SR | Claim + URL caching (see [[Storage Architecture>>FactHarbor.Product Development.Diagrams.Storage Architecture.WebHome]])\n| Testing | Manual only | Promptfoo + baseline + metrics\n| Config | Hot-reload ready | Validated in production-like conditions\n| Phase label | \"POC1 v2.10.2\" | \"Alpha 0.1\"\n| Documentation | Outdated POC specs | Updated to reflect actual state\n\n== 5. Revised POC2 Assessment ==\n\n=== 5.1 Original POC2 Goals vs Current State ===\n\n|= POC2 Goal |= Status |= Still Needed? |= Recommendation\n| **Gate 2: Evidence Relevance** | NOT IMPLEMENTED | MAYBE | Reassess after baseline  may be handled by existing probativeValue filtering\n| **Gate 3: Scenario Coherence** | NOT IMPLEMENTED | PROBABLY NOT | Now uses AnalysisContexts with overlap detection already built in.\n| **Evidence Deduplication (FR54)** | NOT IMPLEMENTED | YES (eventually) | Defer to Beta  needs normalized DB schema (PostgreSQL)\n| **Quality Metrics Dashboard (NFR13)** | BUILT (not integrated) | YES | Integrate metrics now (15-30 min); dashboard already exists\n| **Context-Aware Analysis** | COMPLETE | No | Standard feature since v2.6.40+\n| **<5% hallucination rate** | UNKNOWN | YES  as a target | Measure first, then improve if needed\n\n=== 5.2 Recommendation ===\n\n{{warning}}\n**Do NOT pursue the original POC2 specification as written.** It was designed for a simpler system that no longer exists.\n{{/warning}}\n\nInstead:\n\n1. Run baseline test to establish quality metrics\n1. Based on data, decide which quality improvements matter most\n1. Gate 2 (Evidence Relevance)  may be partially covered by probativeValue scoring; validate empirically\n1. Gate 3 (Scenario Coherence)  obsolete as written; AnalysisContext overlap detection serves a similar purpose\n1. Evidence deduplication  valuable but needs PostgreSQL; defer to Beta\n\n== 6. Agreed Roadmap ==\n\n=== Previous Roadmap (Superseded) ===\n\n{{code}}\nPOC1 (Specification Complete) -> POC2 (Quality & Reliability) -> Beta 0 (User Testing) -> V1.0\n{{/code}}\n\n=== Current Roadmap ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Development Roadmap.WebHome\"/}}\n\n**POC Complete** (after baseline test): Run baseline quality test, integrate metrics, document results.\n\n**Alpha** (quality measurement + optimization):\n\n|= Milestone |= Focus |= Dependencies\n| **Alpha 0.1** | Baseline test + metrics integration | LLM API costs\n| **Alpha 0.2** | URL cache + claim cache | None\n| **Alpha 0.3** | Quality improvements based on baseline | Alpha 0.1 results\n| **Alpha 0.4** | Parallel verdicts + tiered LLM routing | Code already built, needs integration\n| **Alpha 0.5** | Security assessment + hardening plan | None\n\n**Beta** (user testing with real users): PostgreSQL migration + normalized schema, user accounts / authentication, browse/search interface, evidence deduplication (FR54), user flagging system.\n\n**V1.0** (public launch): Production-hardened, scaled infrastructure, continuous quality improvement.\n\n== 7. Decision Summary ==\n\n|= Decision |= Status |= Outcome |= Urgency\n| **Declare POC complete** | **DONE** | ##v1.0.0-poc## tagged 2026-02-19 | \n| **Phase redefinition** | **DONE** | POC -> Alpha -> Beta -> V1.0 | \n| **Roadmap** | **DONE** | See Section 6 | \n| **Baseline test** | Deferred to Alpha | Quality measurement, not POC gate | med\n| **Integrate metrics** | Deferred to Alpha | Observability concern | med\n| **Gates 2 & 3** | Deferred | May not be needed with CB pipeline | low\n| **Evidence dedup (FR54)** | Deferred to Beta | Needs PostgreSQL | low\n\n{{success}}\n**POC is complete.** All remaining work is Alpha scope. See ##Docs/STATUS/Backlog.md## for the prioritized task list.\n{{/success}}\n\n== Related Pages ==\n\n* [[Storage Architecture>>FactHarbor.Product Development.Diagrams.Storage Architecture.WebHome]]  Database and caching decisions (Alpha milestone 0.2)\n* [[POC1>>FactHarbor.Product Development.Planning.POC1.WebHome]]  Original POC1 goals and current implementation status\n* [[POC2>>FactHarbor.Product Development.Planning.POC2.WebHome]]  Original POC2 goals (superseded by this document)\n* [[Roadmap Overview>>FactHarbor.Product Development.Planning.WebHome]]  Implementation roadmap\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture\n\n**Document Status:** POC COMPLETE (2026-02-19, ##v1.0.0-poc##)\n**Next Steps:** Alpha phase  see ##Docs/STATUS/Backlog.md## for prioritized task list.\n", "Product Development.Planning.POC1.WebHome": "= POC1: Core Workflow with Quality Gates =\n\n**Phase Goal:** Prove AKEL can produce credible, quality outputs without manual intervention\n\n**Success Metric:** <10% hallucination rate, quality gates prevent low-confidence publications\n\n== 1. Overview ==\n\nPOC1 validates that the core AKEL workflow (Article  Claims  Verdicts) can produce trustworthy fact-checking analyses automatically. This phase implements **2 critical quality gates** to prevent low-quality outputs from being published.\n\n**Key Innovation:** Quality validation BEFORE publication, not after\n\n**What We're Proving:**\n\n* AKEL can reliably extract factual claims from articles\n* AKEL can generate credible verdicts with proper evidence\n* **AKEL can assess article credibility beyond simple claim averaging** (context-aware analysis)\n* Quality gates prevent hallucinations and low-confidence outputs\n* Fully automated approach is viable\n\n== 2. Scope ==\n\n=== In Scope ===\n\n* Core AKEL workflow (claim extraction, verdict generation)\n* **Gate 1:** Claim Validation (factual vs. opinion/prediction)\n* **Gate 4:** Verdict Confidence Assessment (minimum 2 sources, quality thresholds)\n* Basic UI to display results\n* Manual quality tracking\n\n=== Out of Scope (Deferred to POC2+) ===\n\n* User accounts / authentication\n* Corrections system\n* Search engine optimization (ClaimReview schema)\n* Image verification\n* Archive.org integration\n* Security hardening\n* Gates 2 & 3 (Evidence relevance, AnalysisContext coherence)\n\n{{info}}\n**Now Implemented in POC1:** The following items were originally deferred but have been implemented:\n* **API endpoints**  ASP.NET Core 8.0 API (`apps/api/`)\n* **A/B testing**  Built-in A/B testing framework (`apps/web/src/lib/analyzer/ab-testing.ts`)\n{{/info}}\n\n=== Context-Aware Analysis (Implemented) ===\n\n{{info}}\n**Status: Production Feature.** Context-aware analysis was originally listed as experimental. It is now a core production feature implemented as **AnalysisContext detection** in the orchestrated pipeline.\n{{/info}}\n\n**What was implemented:**\n* Claims are decomposed into **AnalysisContexts**  bounded analytical frames that identify distinct verifiable aspects\n* Each AnalysisContext contains **KeyFactors**  specific questions to investigate, with contestation tracking\n* The pipeline evaluates evidence per-context, producing independent verdicts that are then aggregated\n* This is more sophisticated than the original \"single-pass holistic\" proposal  it uses structured multi-context decomposition rather than a simple prompt enhancement\n\n**See:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] and [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] for details\n\n== 3. Requirements ==\n\n=== 3.1 NFR11: Quality Assurance Framework (POC1 Lite Version) ===\n\n**Importance:** CRITICAL - Core POC1 Requirement \n**Fulfills:** AI safety, credibility, prevents embarrassing failures\n\n**Specification:**\n\nAKEL must validate outputs before displaying to users. POC1 implements a **2-gate subset** of the full NFR11 framework.\n\n==== Gate 1: Claim Validation ====\n\n**Purpose:** Ensure extracted claims are factual assertions, not opinions or predictions\n\n**Validation Checks:**\n\n1. **Factual Statement Test:** Can this be verified with evidence?\n2. **Opinion Detection:** Contains hedging language? (\"I think\", \"probably\", \"best\", \"worst\")\n3. **Specificity Score:** Contains concrete details? (names, numbers, dates, locations)\n4. **Future Prediction Test:** Makes claims about future events?\n\n**Pass Criteria:**\n{{code}}- isFactual: true\n- opinionScore:  0.3\n- specificityScore:  0.3\n- claimType: FACTUAL{{/code}}\n\n**Action if Failed:**\n\n* Flag as \"Non-verifiable: Opinion/Prediction/Ambiguous\"\n* Do NOT generate analysis contexts or verdicts\n* Display explanation to user\n\n**Target:** 0% opinion statements processed as facts\n\n==== Gate 4: Verdict Confidence Assessment ====\n\n**Purpose:** Only publish verdicts with sufficient evidence and confidence\n\n**Validation Checks:**\n\n1. **Evidence Count:** Minimum 2 independent sources\n2. **Source Quality:** Average reliability  0.6 (on 0-1 scale)\n3. **Evidence Agreement:** % supporting vs. contradicting  0.6\n4. **Uncertainty Factors:** Count of hedging statements in reasoning\n\n**Confidence Tiers:**\n{{code}}HIGH (80-100%):\n - 3 sources\n - 0.7 average quality\n - 80% agreement\n \nMEDIUM (50-79%):\n - 2 sources\n - 0.6 average quality\n - 60% agreement\n \nLOW (0-49%):\n - 2 sources BUT low quality/agreement\n \nINSUFFICIENT:\n - <2 sources  DO NOT PUBLISH{{/code}}\n\n**POC1 Publication Rule:**\n\n* Minimum **MEDIUM** confidence required\n* Blocked verdicts show \"Insufficient Evidence\" message\n\n**Target:** 0% verdicts published with <2 sources\n\n=== 3.2 Modified FR7: Automated Verdicts (Enhanced) ===\n\n**Enhancement for POC1:**\n\nAfter AKEL generates a verdict, it must pass through the quality validation pipeline:\n\n{{code}}\nAKEL Workflow (POC1):\n\n1. Extract claims from article\n \n2. [GATE 1] Validate each claim is fact-checkable\n  (pass claims only)\n3. Generate verdicts for each claim\n \n4. [GATE 4] Validate verdict has sufficient evidence\n  (pass verdicts only)\n5. Display to user\n\nFailed claims/verdicts:\n- Store in database with failure reason\n- Display explanatory message to user\n- Log for quality metrics tracking\n{{/code}}\n\n**Updated Verdict States:**\n\n* PUBLISHED - Passed all gates\n* INSUFFICIENT_EVIDENCE - Failed Gate 4\n* NON_FACTUAL_CLAIM - Failed Gate 1\n* PROCESSING - In progress\n* ERROR - System failure\n\n=== 3.3 Modified FR4: Analysis Summary (Enhanced) ===\n\n**Enhancement for POC1:**\n\nAnalysis Summary must now display quality metadata:\n\n{{code}}\nAnalysis Summary:\n Total Claims Found: 5\n Verifiable Claims: 3\n Non-verifiable (Opinion): 1\n Non-verifiable (Prediction): 1\n \n Verdicts Generated: 3\n High Confidence: 1\n Medium Confidence: 2\n Insufficient Evidence: 0\n \n Evidence Sources: 12 total\n Average Source Quality: 0.73\n \n Quality Score: 8.5/10\n{{/code}}\n\n== 4. Success Criteria ==\n\nPOC1 is considered **SUCCESSFUL** if:\n\n** Functional:**\n\n* Processes diverse test articles without crashes\n* Generates verdicts for all factual claims\n* Blocks all non-factual claims (0% pass through)\n* Blocks all insufficient-evidence verdicts (0% with <2 sources)\n\n** Quality:**\n\n* Hallucination rate <10% (manual verification)\n* 0 verdicts with <2 sources published\n* 0 opinion statements published as facts\n* Average quality score 7.0/10\n\n** Performance:**\n\n* Processing time reasonable for POC demonstration\n* Quality gates execute efficiently\n* UI displays results clearly\n\n** Learnings:**\n\n* Identified prompt engineering improvements\n* Documented AKEL strengths/weaknesses\n* Validated threshold values\n* Clear path to POC2 defined\n\n== 5. Decision Gates ==\n\n**POC  Alpha Decision:**\n\n* **IF** hallucination rate >10%  Pause, improve prompts before Alpha\n* **IF** majority of claims non-processable  Rethink claim extraction approach\n* **IF** quality gates too strict (excessive blocking)  Adjust thresholds\n* **IF** quality gates too loose (hallucinations pass)  Tighten criteria\n\n**Only proceed to Alpha if all success criteria met**\n\n== 6. Architecture Notes ==\n\n**Actual POC1 Architecture (Implemented):**\n\n{{code}}\nUser Input  STEP 1: UNDERSTAND (claim decomposition, AnalysisContext detection)\n            STEP 2-4: RESEARCH & EVIDENCE (web search, evidence extraction, quality filtering)\n            STEP 4.4-4.6: CONTEXT REFINEMENT (supplemental claims/contexts)\n            STEP 5: VERDICTS (per-context verdicts with 7-point truth scale)\n            STEP 6: SUMMARY (aggregation across contexts)\n            STEP 7: REPORT (final output with quality gates)\n{{/code}}\n\n**vs. Full System (Future):**\n\n{{code}}\nAbove pipeline + Review Queue  Publication workflow  User contributions\n{{/code}}\n\n**Current POC1 Simplifications (vs. full production):**\n\n* 2 quality gates (Gate 1 + Gate 4) instead of 4\n* No review queue or publication workflow\n* No user accounts or contribution system\n* SQLite storage (not PostgreSQL)\n* Single-instance deployment\n\n**See:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] for details\n\n== Related Pages ==\n\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  Phase redefinition and transition assessment\n* [[Roadmap Overview>>FactHarbor.Product Development.Planning.WebHome]]  All phases\n* [[POC2 Requirements>>FactHarbor.Product Development.Planning.POC2.WebHome]]  Superseded by POC to Alpha Transition\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]  Full system requirements\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture\n* [[NFR11 Full Specification>>FactHarbor.Product Development.Requirements.WebHome#NFR11]]  Complete quality framework\n\n**Document Status:** POC1 Feature-Complete (v2.10.2)  Pending baseline quality test to declare POC complete and enter Alpha\n**Version:** v2.10.2", "Product Development.Planning.POC2.WebHome": "{{warning}}\n**Superseded:** POC2 has been superseded by [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]. This page is retained for historical reference.\n{{/warning}}\n\n= POC2: Robust Quality & Reliability =\n\n**Phase Goal:** Prove AKEL produces high-quality outputs consistently at scale\n\n**Success Metric:** <5% hallucination rate, all 4 quality gates operational\n\n== 1. Overview ==\n\nPOC2 extends POC1 by implementing the full quality assurance framework (all 4 gates), adding evidence deduplication, and processing significantly more test articles to validate system reliability at scale.\n\n**Key Innovation:** Complete quality validation pipeline catches all categories of errors\n\n**What We're Proving:**\n\n* All 4 quality gates work together effectively\n* Evidence deduplication prevents artificial inflation\n* System maintains quality at larger scale\n* Quality metrics dashboard provides actionable insights\n\n== 2. New Requirements ==\n\n=== 2.1 NFR11: Complete Quality Assurance Framework ===\n\n**Add Gates 2 & 3** (POC1 had only Gates 1 & 4)\n\n==== Gate 2: Evidence Relevance Validation ====\n\n**Purpose:** Ensure AI-linked evidence actually relates to the claim\n\n**Validation Checks:**\n\n1. **Semantic Similarity:** Cosine similarity between claim and evidence embeddings  0.6\n2. **Entity Overlap:** At least 1 shared named entity between claim and evidence\n3. **Topic Relevance:** Evidence discusses the claim's subject matter (score  0.5)\n\n**Action if Failed:**\n\n* Discard irrelevant evidence (don't count it)\n* If <2 relevant evidence items remain  \"Insufficient Evidence\" verdict\n* Log discarded evidence for quality review\n\n**Target:** 0% of evidence cited is off-topic\n\n==== Gate 3: Scenario Coherence Check ====\n\n**Purpose:** Validate scenarios are logical, complete, and meaningfully different\n\n**Validation Checks:**\n\n1. **Completeness:** All required fields populated (assumptions, scope, evidence context)\n2. **Internal Consistency:** Assumptions don't contradict each other (score <0.3)\n3. **Distinctiveness:** Scenarios are meaningfully different (similarity <0.8)\n4. **Minimum Detail:** At least 1 specific assumption per scenario\n\n**Action if Failed:**\n\n* Merge duplicate scenarios\n* Flag contradictory assumptions for review\n* Reduce confidence score by 20%\n* Do not publish if <2 distinct scenarios\n\n**Target:** 0% duplicate scenarios, all scenarios internally consistent\n\n=== 2.2 FR54: Evidence Deduplication (NEW) ===\n\n**Importance:** HIGH \n**Fulfills:** Accurate evidence counting, prevents artificial inflation\n\n**Purpose:** Prevent counting the same evidence multiple times when cited by different sources\n\n**Problem:**\n\n* Wire services (AP, Reuters) redistribute same content\n* Different sites cite the same original study\n* Aggregators copy primary sources\n* AKEL might count this as \"5 sources\" when it's really 1\n\n**Solution: Content Fingerprinting**\n\n* Generate SHA-256 hash of normalized text\n* Detect near-duplicates (85% similarity) using fuzzy matching\n* Track which sources cited each unique piece of evidence\n* Display provenance chain to user\n\n**Target:** Duplicate detection >95% accurate, evidence counts reflect reality\n\n=== 2.3 NFR13: Quality Metrics Dashboard (Internal) ===\n\n**Importance:** HIGH \n**Fulfills:** Real-time quality monitoring during development\n\n**Dashboard Metrics:**\n\n* Claim processing statistics\n* Gate performance (pass/fail rates for each gate)\n* Evidence quality metrics\n* Hallucination rate tracking\n* Processing performance\n\n**Target:** Dashboard functional, all metrics tracked, exportable\n\n== 3. Success Criteria ==\n\n** Quality:**\n\n* Hallucination rate <5% (target: <3%)\n* Average quality rating 8.0/10\n* 0 critical failures (publishable falsities)\n* Gates correctly identify >95% of low-quality outputs\n\n** All 4 Gates Operational:**\n\n* Gate 1: Claim validation working\n* Gate 2: Evidence relevance filtering working\n* Gate 3: Scenario coherence checking working\n* Gate 4: Verdict confidence assessment working\n\n** Evidence Deduplication:**\n\n* Duplicate detection >95% accurate\n* Evidence counts reflect reality\n* Provenance tracked correctly\n\n** Metrics Dashboard:**\n\n* All metrics implemented and tracking\n* Dashboard functional and useful\n* Alerts trigger appropriately\n\n== 4. Architecture Notes ==\n\n**POC2 Enhanced Architecture:**\n\n{{code}}\nInput  AKEL Processing  All 4 Quality Gates  Display\n (claims + scenarios (1: Claim validation\n + evidence linking 2: Evidence relevance\n + verdicts) 3: Scenario coherence\n 4: Verdict confidence)\n{{/code}}\n\n**Key Additions from POC1:**\n\n* Scenario generation component\n* Evidence deduplication system\n* Gates 2 & 3 implementation\n* Quality metrics collection\n\n**Still Simplified vs. Full System:**\n\n* Single AKEL orchestration (not multi-component pipeline)\n* No review queue\n* No federation architecture\n\n**See:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] for details\n\n== 5. Context-Aware Analysis (Conditional Feature) ==\n\n**Status:** Depends on POC1 experimental test results\n\n**Background:**\n\nPOC1 tested context-aware analysis as an experimental feature using Approach 1 (Single-Pass Holistic Analysis). The goal is to detect when articles use accurate individual claims but reach misleading conclusions through faulty logic or selective presentation.\n\n**See:** [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]] for complete investigation\n\n=== 5.1 POC2 Implementation Path ===\n\n**Decision based on POC1 test results (30-article test set):**\n\n==== If POC1 Accuracy 70% (Success) ====\n\n**Action:** Implement as standard feature (no longer experimental)\n\n**Enhancement to FR4:**\n* Context-aware analysis becomes part of standard Analysis Summary\n* Article verdict may differ from simple claim average\n* AI evaluates logical structure and reasoning quality\n\n**Potential Upgrade to Approach 6 (Hybrid):**\n* Add weighted claim importance (some claims more central than others)\n* Add rule-based fallacy detection alongside AI reasoning\n* Combine AI judgment with heuristic checks for robustness\n\n**Target:** Maintain 70% accuracy at detecting misleading articles\n\n==== If POC1 Accuracy 50-70% (Promising) ====\n\n**Action:** Implement alternative Approach 4 (Weighted Aggregation)\n\n**Instead of holistic analysis:**\n* AI assigns importance weights (0-1) to each claim\n* Weight based on: claim centrality, evidence strength, logical role\n* Article verdict = weighted average of claim verdicts\n* More structured than pure AI reasoning\n\n**Rationale:** If holistic reasoning is inconsistent, structured weighting may work better\n\n==== If POC1 Accuracy <50% (Insufficient) ====\n\n**Action:** Defer context-aware analysis to post-POC2\n\n**Fallback:**\n* Focus on individual claim accuracy only\n* Article verdict = simple average of claim verdicts\n* Note limitation: May miss misleading articles built from accurate claims\n\n**Future consideration:** Try Approach 7 (LLM-as-Judge) with better models in future releases\n\n=== 5.2 Testing in POC2 ===\n\n**If context-aware feature is implemented:**\n\n* Expand test set from 30 to 100 articles\n* Include more diverse article types (op-eds, news, analysis, advocacy)\n* Track false positive rate (flagging good articles as misleading)\n* Validate with subject matter experts when possible\n\n**Success Metrics:**\n* 70% accuracy on misleading article detection\n* <15% false positive rate\n* Reasoning is comprehensible to users\n\n=== 5.3 Architecture Notes ===\n\n**Context-aware analysis adds NO additional API calls**\n\nThe enhanced analysis happens within the existing AKEL workflow:\n\n{{code}}\nStandard Flow: Context-Aware Enhancement:\n1. Extract claims 1. Extract claims + mark central claims\n2. Find evidence 2. Find evidence\n3. Generate verdicts 3. Generate verdicts\n4. Write summary 4. Write context-aware summary\n (evaluates article structure)\n{{/code}}\n\n**Cost:** $0 increase (same API calls, enhanced prompt only)\n\n**See:** [[POC Requirements>>FactHarbor.Product Development.Specification.POC.Requirements]] Component 1 for implementation details\n\n== Related Pages ==\n\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  Supersedes this document\n* [[POC1>>FactHarbor.Product Development.Planning.POC1.WebHome]]  Previous phase\n* [[Beta>>FactHarbor.Product Development.Planning.Beta0.WebHome]]  Follows Alpha in revised roadmap\n* [[Roadmap Overview>>FactHarbor.Product Development.Planning.WebHome]]\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n\n**Version:** V0.9.70 (original specification)", "Product Development.Planning.Project Status.WebHome": "= Project Status =\n\n**FactHarbor is an operational fact-analysis platform. POC phase is complete.**\n\n----\n\n== Current Status ==\n\n|= |= |\n| **Version** | v2.11.0 (Code)  3.0.0-cb (Schema)  ##v1.0.0-poc## (Tag) |\n| **Phase** | **POC Complete**  entering Alpha |\n| **Codebase** | Next.js (UI + AI orchestrator) + ASP.NET Core (API + persistence) |\n| **Tests** | 853 passing (45 test files, all mocked  no LLM API calls) |\n| **Build** | Clean |\n\n== POC Closure (2026-02-19) ==\n\n{{success}}\n**The Proof of Concept is complete.** The ClaimAssessmentBoundary pipeline v1.0 demonstrates end-to-end automated claim analysis: claim extraction, evidence gathering from web sources, evidence-emergent boundary clustering, LLM-based verdicts with debate pattern, and quality-gated aggregation. Tagged as ##v1.0.0-poc##.\n{{/success}}\n\n== What's Operational ==\n\n* **ClaimAssessmentBoundary pipeline**  5-stage architecture (extract claims, research evidence, cluster boundaries, generate verdicts, aggregate assessment)\n* **LLM debate pattern**  advocate/challenger/reconciliation for verdict quality\n* **Multi-provider AI**  Anthropic, OpenAI, Google, Mistral via Vercel AI SDK with tiered model routing\n* **7-point verdict scale**  TRUE to FALSE with confidence, distinguishing contested from insufficient evidence\n* **Quality gates**  claim validation (Gate 1) and verdict confidence (Gate 4)\n* **Source reliability scoring**  LLM-based evaluation with multi-model consensus and caching\n* **Evidence quality filtering**  probative value assessment, source authority, extraction confidence\n* **Input neutrality**  question vs statement phrasing within +/-4% tolerance\n* **Unified Configuration Management**  runtime-configurable parameters, admin UI, hot-reload\n* **Real-time progress**  Server-Sent Events for live analysis updates\n* **PDF/HTML/text input**  extract and analyze content from URLs or pasted text\n* **Rich HTML report export**  self-contained dark-themed reports\n\n== Development Phases ==\n\n|= Phase |= Status |= Focus\n| **Proof of Concept** | **Complete** (##v1.0.0-poc##) | Core AI workflow, CB pipeline, quality gates, multi-provider support, UCM\n| **Alpha** | Starting | Cost optimization, quality validation, UI polish, dead code cleanup\n| **Beta** | Planned | User testing, production hardening, security, PostgreSQL migration\n| **V1.0** | Planned | Public launch, API access, continuous quality improvement\n\n== Alpha Priorities ==\n\n1. Cost reduction (Batch API, prompt caching, NPO/OSS credits)\n2. Quality validation with real LLM calls (claim fidelity, AtomicClaim extraction)\n3. UI polish (quality gate display, CB admin config, coverage matrix)\n4. Dead code cleanup\n5. Security hardening  before any public deployment\n\n== Links ==\n\n* [[Detailed Roadmap>>FactHarbor.Product Development.Planning.WebHome]]  full phase definitions, success criteria, requirements mapping\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  phase redefinition and transition assessment\n* [[Specification>>FactHarbor.Product Development.Specification.WebHome]]  architecture, data models, requirements\n* [[Known Issues>>https://github.com/robertschaub/FactHarbor]]  bug tracking and limitations\n", "Product Development.Planning.Requirements-Roadmap-Matrix.WebHome": "= Requirements Roadmap Matrix =\n\n**Last Updated:** 2026-02-07\n**Version:** 4.1 (Updated phase definitions)\n**Status:** Authoritative Source\n\n{{info}}\n**Purpose:** This is the single source of truth for:\n* Requirement-to-phase mapping\n* Implementation levels (Basic  Enhanced  Complete)\n* Importance and Urgency prioritization\n* User Needs fulfillment by phase\n\nAll other documents reference this matrix rather than duplicating information.\n{{/info}}\n\n== Phase Definitions ==\n\n|= Phase |= Goal |= Key Focus\n| **POC** | Prove AI can analyze claims automatically | Core workflow + quality gates + AnalysisContext detection (**Feature-complete at v2.10.2**, pending baseline test)\n| **POC2** | Improve reliability and robustness | Merged into Alpha/Beta\n| **Alpha** | Quality measurement + optimization | Baseline testing, caching, quality improvements based on empirical data\n| **Beta** | User testing with limited audience | Production readiness, user features, security, PostgreSQL\n| **V1.0** | Public production launch | IFCN compliance, search visibility\n| **V1.x** | Feature expansion | Advanced capabilities, browser extensions\n| **V2.0+** | Advanced capabilities | Video verification, OSINT, federation\n\n**Phase Philosophy:** Each phase builds on proven capabilities from previous phases. We validate AI quality before adding production features.\n\n== Requirement Categories ==\n\n**This matrix covers ALL requirements from the baseline:**\n\n**1. Functional Requirements (24 total)**\n* **FR1-FR13**: Core workflow and POC requirements\n* **FR44-FR54**: Advanced features and future enhancements\n* **FR14-FR43**: Numbers reserved (not used)\n\n**2. Non-Functional Requirements (9 total)**\n* **NFR1-NFR5**: Basic quality attributes (Performance, Scalability, Transparency, Security & Privacy, Maintainability)\n* **NFR11-NFR14**: Formal quality, security, metrics, and LLM abstraction requirements\n\n**3. User Needs (20 total)**\n* **UN-1 to UN-28**: User requirements (with some gaps in numbering)\n\n{{warning}}\n**Future Requirement Numbering:** To avoid conflicts with existing requirements, new requirements will use:\n* **FR55-FR84** for future functional requirements\n* **NFR14-NFR18** for future non-functional requirements \n* **UN-29 to UN-36** for future user needs\n\nSee [[Gap Analysis>>FactHarbor.Product Development.Requirements.GapAnalysis]] for features not yet assigned formal requirement numbers.\n{{/warning}}\n\n== Requirements by Phase ==\n\n**Note:** Requirements organized by phase, with Formal requirements listed first, followed by POC-specific implementation details.\n\n=== POC1 ===\n\n**Goal:** Prove that AI can extract claims and determine verdicts automatically\n\n**Experimental Feature:** Context-aware analysis (tests if article credibility differs from claim average)\n\n**Requirements:** 2 formal + 9 POC-specific\n\n==== Formal Requirements ====\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **FR7** | Automated Verdicts (Enhanced with Quality Gates) | CRITICAL | HIGH | Core AKEL capability - system cannot function without automated verdict generation\n| **FR4** | Analysis Summary (Enhanced with Context-Aware Analysis) | HIGH | HIGH | Enhanced in POC1 to test context-aware analysis (experimental feature). If successful (70% accuracy), ships in POC2. See [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]]\n\n==== POC-Specific Requirements ====\n\n| **NFR14** | LLM Provider Abstraction | HIGH | HIGH | Provider-agnostic architecture, no vendor lock-in, POC1 implements with Anthropic primary\n|= ID |= Title |= Notes\n| **FR1** | Claim Intake | Part of FR7 implementation\n| **FR2** | Claim Normalization | Part of FR7 implementation\n| **FR3** | Claim Classification | Part of FR7 implementation\n| **FR5** | Evidence Linking | Part of FR7 implementation\n| **FR6** | Scenario Comparison | Part of FR7 implementation\n| **FR12** | Two-Panel Summary View (Article Summary with FactHarbor Analysis Summary) | Part of FR4 implementation\n| **NFR1** | Performance | Basic implementation\n| **NFR2** | Scalability | Basic implementation\n| **NFR3** | Transparency | Basic implementation\n\n**Context-Aware Analysis Details:**\n* **Approach:** Single-Pass Holistic Analysis (Approach 1)\n* **Implementation:** Enhanced AI prompt to evaluate logical structure\n* **Testing:** 30-article test set (10 straightforward, 10 misleading, 10 complex)\n* **Success Criteria:** 70% accuracy detecting misleading articles\n* **Decision Path:** If 70%  ship in POC2; if 50-70%  try weighted aggregation; if <50%  defer\n* **Cost:** Zero increase (no additional API calls, no architecture changes)\n* **Documentation:** [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]]\n\n=== POC2 ===\n\n**Goal:** Improve system reliability and add essential quality features\n\n**Requirements:** 2 formal\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **NFR11** | AKEL Quality Assurance Framework | CRITICAL | HIGH | Without quality gates, automated analysis cannot be trusted. POC1 implements 2 gates (Gates 1 & 4), POC2 implements all gates\n| **FR54** | Evidence Deduplication | CRITICAL | LOW | Essential for data integrity and preventing duplicate processing\n\n=== Beta 0 ===\n\n**Goal:** Prepare for public launch with essential user-facing features and security\n\n**Requirements:** 2 formal + 6 POC-specific\n\n==== Formal Requirements ====\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **NFR12** | Security Controls | CRITICAL | MEDIUM | Essential for production deployment (moved from V1.0 to Beta 0 for earlier security hardening)\n| **NFR13** | Quality Metrics Transparency | HIGH | MEDIUM | Users need to understand quality levels\n\n==== POC-Specific Requirements ====\n\n|= ID |= Title |= Notes\n| **FR9** | Publication Workflow | Production-ready publishing system\n| **FR10** | Moderation | Moderation tools and processes\n| **FR11** | Audit Trail | Production-ready audit logging\n| **FR13** | In-Article Claim Highlighting | User-facing feature for article analysis\n| **NFR4** | Security & Privacy | Basic security and privacy controls\n| **NFR5** | Maintainability | Code quality and maintainability standards\n\n=== V1.0 ===\n\n**Goal:** Full production launch with IFCN compliance and search engine visibility\n\n**Requirements:** 4 formal\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **FR44** | ClaimReview Schema Implementation | HIGH | MEDIUM | Required for search engine discoverability (Google/Bing fact-check visibility)\n| **FR45** | User Corrections Notification System | HIGH | MEDIUM | Critical for user trust and transparency, IFCN compliance\n| **FR48** | Contributor Safety Framework | HIGH | MEDIUM | Protects contributors from harassment\n| **FR49** | A/B Testing Framework | MEDIUM | MEDIUM | Enables continuous system improvement through experimentation\n\n=== V1.1 ===\n\n**Goal:** Add advanced capabilities\n\n**Requirements:** 2 formal\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **FR50** | OSINT Toolkit Integration | HIGH | LOW | Valuable capability (geolocation, chronolocation, social media analysis)\n| **FR52** | Interactive Detection Training | MEDIUM | LOW | Education and media literacy features\n\n=== V1.5 ===\n\n**Goal:** Media verification expansion (image, video, audio)\n\n**Requirements:** 4 formal\n\n|= ID |= Title |= Importance |= Urgency |= Notes\n| **FR46** | Image Verification System | HIGH | MEDIUM | Moved from V1.0 to V1.5 to focus V1.0 on core text-based fact-checking. Includes reverse image search, manipulation detection, EXIF analysis\n| **FR51** | Video Verification System | HIGH | LOW | Moved from V1.1 to V1.5. Includes keyframe extraction, deepfake detection, metadata analysis\n| **FR47** | Archive.org Integration | MEDIUM | MEDIUM | Moved from Beta 0 to V1.5. Ensures evidence persistence even if original sources deleted\n| **FR53** | Cross-Organizational Sharing | MEDIUM | LOW | Collaboration with other fact-checkers (IFCN/EFCSN members)\n\n=== Deferred (Not in V1.5) ===\n\n**Requirements deferred beyond V1.5:**\n\n|= ID |= Title |= Notes\n| **FR8** | Time Evolution | Version history for claims and verdicts. Deferred - UCM config audit trail in V1.0\n\n== Requirements Summary by Phase ==\n\n|= Phase |= Formal Requirements |= POC-Specific |= Total |= Cumulative\n| **POC1** | 3 (FR4, FR7, NFR14) | 9 | 12 | 12\n| **POC2** | 2 (FR54, NFR11) | 0 | 2 | 14\n| **Beta 0** | 2 (NFR12, NFR13) | 6 | 8 | 22\n| **V1.0** | 4 (FR44, FR45, FR48, FR49) | 0 | 4 | 26\n| **V1.1** | 2 (FR50, FR52) | 0 | 2 | 28\n| **V1.5** | 4 (FR46, FR47, FR51, FR53) | 0 | 4 | 32\n| **Deferred** | 0 | 1 (FR8) | 1 | 33\n\n**Total Requirements:** 33 (24 FR + 9 NFR)\n\n== User Needs by Phase ==\n\n**Summary of when each User Need is fulfilled:**\n\n|= User Need |= Title |= Fulfilled in Phase |= Via Requirements\n| **UN-1** | Trust Assessment at a Glance | POC1 | FR7, NFR13\n| **UN-2** | Claim Extraction and Verification | POC1 | FR7\n| **UN-3** | Article Summary with FactHarbor Analysis Summary | POC1 | FR4 (enhanced with context-aware analysis), FR12, NFR14 (cost sustainability)\n| **UN-4** | Social Media Fact-Checking | POC1 | FR7\n| **UN-5** | Source Provenance and Track Records | POC1 | FR4, FR6\n| **UN-6** | Publisher Reliability History | POC1 | FR4, FR6\n| **UN-7** | Evidence Transparency | POC1 | FR4, NFR13, NFR14 (no hidden vendor dependencies)\n| **UN-8** | Understanding Disagreement and Consensus | POC1 | FR7\n| **UN-9** | Methodology Transparency | POC1 | NFR13\n| **UN-10** | Manipulation Tactics Detection | V1.5 | FR52\n| **UN-11** | Filtered Research | POC2 | FR54\n| **UN-12** | Submit Unchecked Claims | POC1 | FR7, FR1\n| **UN-13** | Cite FactHarbor Verdicts | Beta 0 | FR13, FR44\n| **UN-14** | API Access for Integration | V1.0 | Production API infrastructure\n| **UN-15** | Verdict Evolution Timeline | Deferred (Not in V1.0) | FR8 - UCM config evolution tracking, NFR14 (modular design enables evolution)\n| **UN-16** | AI vs. Human Review Status | POC1 | NFR13\n| **UN-17** | In-Article Claim Highlighting | Beta 0 | FR13\n| **UN-26** | Search Engine Visibility | V1.0 | FR44\n| **UN-27** | Visual Claim Verification | V1.5 (images), V1.5 (video) | FR46, FR51\n| **UN-28** | Safe Contribution Environment | V1.0 | FR48\n\n**Total User Needs:** 20 (UN-1 to UN-28, with gaps)\n\n**Note:** UN-29 to UN-36 reserved for future user needs from Gap Analysis\n\n== All Requirements List ==\n\n**For reference, complete list of all requirements:**\n\n**Functional Requirements (24):**\n* POC1: FR1, FR2, FR3, FR4, FR5, FR6, FR7, FR12\n* POC2: FR54\n* Beta 0: FR9, FR10, FR11, FR13\n* V1.0: FR44, FR45, FR48, FR49\n* V1.1: FR50, FR52\n* V1.5: FR46, FR47, FR51, FR53\n* Deferred: FR8\n\n**Non-Functional Requirements (8):**\n* POC1: NFR1, NFR2, NFR3, NFR14\n* POC2: NFR11\n* Beta 0: NFR4, NFR5, NFR12, NFR13\n\n**User Needs (20):**\nUN-1, UN-2, UN-3, UN-4, UN-5, UN-6, UN-7, UN-8, UN-9, UN-10, UN-11, UN-12, UN-13, UN-14, UN-15, UN-16, UN-17, UN-26, UN-27, UN-28\n\n== Key Changes in V4.0 (COMPLETE) ==\n\n**This version is COMPLETE and includes ALL requirements from baseline:**\n\n1. **All 32 Requirements Now Included**\n - **Added:** FR3, FR8, FR9, FR10, FR12, NFR4, NFR5 (were missing in V3.0)\n - **Total:** 24 FR + 8 NFR = 32 requirements\n\n2. **Context-Aware Analysis in POC1**\n - FR4 enhanced to test context-aware analysis (experimental)\n - See [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]]\n\n3. **Media Verification in V1.5**\n - FR46 (Image), FR51 (Video), FR47 (Archive.org)\n - Consolidated in V1.5 release\n\n4. **Security Earlier (Beta 0)**\n - NFR12 moved from V1.0 to Beta 0\n - NFR4 (Security & Privacy) also in Beta 0\n\n5. **Future Numbering Reserved**\n - FR55-FR84, NFR14-NFR18, UN-29 to UN-36\n\n== Gap Analysis Features (Not Yet Formal Requirements) ==\n\nThe following features from [[Gap Analysis>>FactHarbor.Product Development.Requirements.GapAnalysis]] are not yet assigned formal requirement numbers but would use FR55+, NFR14+, UN-29+ when formalized:\n\n**15 gap features across 7 categories:**\n* Accessibility (2): WCAG compliance, Multilingual support\n* Platform Integration (2): Browser extensions, Embeddable widgets\n* Media Verification (3): Image/Video (FR46/FR51), Audio (not yet formalized)\n* Mobile & Offline (2): Mobile apps/PWA, Offline access\n* Education (2): Educational resources, Media literacy\n* Collaboration (2): Professional tools, Community discussion\n* Advanced Features (2): User analytics, Personalization\n\n== References ==\n\n**This matrix is referenced by:**\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] - Main requirements specification\n* [[POC Requirements>>FactHarbor.Product Development.Specification.POC.Requirements]] - POC1 & POC2 detailed specifications\n* [[Implementation Roadmap>>FactHarbor.Product Development.Planning.WebHome]] - High-level phase descriptions\n* [[Gap Analysis>>FactHarbor.Product Development.Requirements.GapAnalysis]] - Features not yet in V1.5\n* [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]] - Context-aware analysis solution\n\n**Maintenance:**\n* Update this matrix when adding new requirements\n* Update this matrix when changing phase assignments or importance/urgency\n* Verify other documents still reference correctly after updates\n* Use FR55+, NFR14+, UN-29+ for new requirements\n\n**Last Review:** 2025-12-23 (V4.0 - COMPLETE) \n**Next Review:** After POC1 completion or when adding new requirements\n", "Product Development.Planning.V10.WebHome": "= V1.0: Public Launch =\n\n**Phase Goal:** Production-ready public launch with IFCN compliance\n\n**Success Metric:** IFCN compliant, Google indexed, sustained quality\n\n== 1. Overview ==\n\nV1.0 is the public launch with all production requirements:\n\n* ClaimReview schema (Google/Bing visibility)\n* Complete corrections system\n* Full security audit\n* IFCN/EFCSN compliance\n* Public quality metrics\n\n**This is the FIRST public release**\n\n== 2. Launch Blockers ==\n\n**MUST HAVE before public launch:**\n\n1.  FR44: ClaimReview Schema Implementation\n2.  FR45: Complete Corrections System\n3.  NFR11: Hardened Quality Gates (<5% hallucinations sustained)\n4.  NFR12: Full Security Audit (0 critical/high vulnerabilities)\n5.  NFR13: Public Quality Metrics Dashboard\n6.  IFCN Compliance Documentation\n7.  Privacy Policy & Terms of Service\n\n**Cannot launch without ALL of these**\n\n== 3. Key Requirements ==\n\n=== FR44: ClaimReview Schema ===\n\n**Google Fact Check Visibility**\n\nSchema.org structured data for every published analysis.\n\n**Mapping:**\n\n* 80-100% likelihood  5 (Highly Supported)\n* 60-79%  4 (Supported)\n* 40-59%  3 (Mixed/Uncertain)\n* 20-39%  2 (Questionable)\n* 0-19%  1 (Refuted)\n\n**Target:** Indexed in Google Fact Check Explorer\n\n=== NFR12: Full Security Audit ===\n\n**Production Security**\n\n* Professional penetration testing\n* Code review for vulnerabilities\n* Dependency vulnerability scanning\n* GDPR compliance audit (if EU users)\n\n**Target:** 0 critical/high vulnerabilities, audit report published\n\n=== NFR13: Public Quality Metrics ===\n\n**Transparency Dashboard**\n\nPublic-facing metrics:\n\n* Claims analyzed\n* Quality gates performance\n* Hallucination rate\n* Evidence quality metrics\n* User feedback scores\n\n**Target:** Dashboard live, transparent, trustworthy\n\n== 4. Success Criteria ==\n\n*  IFCN certified (or compliant)\n*  Google Fact Check Explorer indexed\n*  Security audit passed\n*  <5% hallucination rate sustained\n*  User satisfaction target met\n*  System stability demonstrated\n*  All launch blockers resolved\n\n== 5. Post-Launch Roadmap ==\n\n**Near-term (V1.x):**\n\n* Enhanced contributor safety (FR48)\n* Browser extensions\n* A/B testing framework (FR49)\n* Image verification advanced (FR46)\n\n**Long-term (V2.0+):**\n\n* Video verification (FR51)\n* OSINT toolkit integration (FR50)\n* Interactive detection training (FR52)\n* Cross-organizational sharing (FR53)\n\n== Related Pages ==\n\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  Phase redefinition\n* [[Beta>>FactHarbor.Product Development.Planning.Beta0.WebHome]]  Previous phase\n* [[Roadmap Overview>>FactHarbor.Product Development.Planning.WebHome]]\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]\n\n**Document Status:** V1.0 Specification\n**Version:** V0.9.70", "Product Development.Planning.WebHome": "= Planning =\n\n**FactHarbor development follows a phased approach from Proof of Concept to production launch.**\n\n== Development Phases ==\n\n{{info}}\n**Detailed Requirement Mapping:** See [[Requirements Roadmap Matrix>>FactHarbor.Product Development.Planning.Requirements-Roadmap-Matrix.WebHome]] for complete phase-to-requirement mapping, implementation levels, and User Needs fulfillment by phase.\n{{/info}}\n\n* [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]]  **Phase redefinition: POC -> Alpha -> Beta -> V1.0**\n* [[POC1: Core Workflow with Quality Gates>>FactHarbor.Product Development.Planning.POC1.WebHome]]\n* [[POC2: Robust Quality & Reliability>>FactHarbor.Product Development.Planning.POC2.WebHome]]  Superseded by POC to Alpha Transition\n* [[Beta 0: User Testing & Production Readiness>>FactHarbor.Product Development.Planning.Beta0.WebHome]]\n* [[V1.0: Public Launch>>FactHarbor.Product Development.Planning.V10.WebHome]]\n\n== Supporting Resources ==\n\n* [[Tooling>>FactHarbor.Product Development.DevOps.Tooling.WebHome]]  Setup checklists, tool decisions, and environment configuration\n* [[Zero-Cost Hosting Implementation Guide>>FactHarbor.Product Development.DevOps.Deployment.Zero-Cost Hosting Implementation Guide.WebHome]]  Hosting strategy using free-tier cloud services\n* [[Requirements Roadmap Matrix>>FactHarbor.Product Development.Planning.Requirements-Roadmap-Matrix.WebHome]]  Complete phase-to-requirement mapping\n\n== Philosophy ==\n\n**Validate AI automation quality BEFORE building production features**\n\nEach phase builds on proven capabilities from the previous phase. We never skip ahead - if AI quality is insufficient, we improve prompts and algorithms rather than adding manual workarounds.\n\n== Development Phases Overview ==\n\n**POC1: Core Workflow with Quality Gates**\n\nBuild:\n* Automated claim analysis\n* Confidence scoring\n* Source evaluation\n* Browse/search interface\n* User flagging system\n\n* **LLM Provider Abstraction (NFR-14):** Provider-agnostic AI access\n * POC1: Multi-provider via Vercel AI SDK (Anthropic, OpenAI, Google, Mistral)\n * Architecture: UCM Pipeline Config with per-phase model tiering (modelUnderstand, modelExtractEvidence, modelVerdict)\n * Future: Admin UI for provider management, cost comparison dashboards (Beta+)\n\n**Goal**: Prove AI quality through automated analysis\n\n**User Needs fulfilled in POC1**: UN-1, UN-2, UN-3, UN-4, UN-5, UN-6, UN-7, UN-8, UN-9, UN-12\n\n**POC2/Beta 0: Enhanced Features**\n\nAdd only if needed:\n* UCM admin UI for configuration management\n* Basic moderation tooling\n* In-article claim highlighting (FR13)\n\n**Additional User Needs fulfilled**: UN-13, UN-17\n\n**V1.0+: Continuous Improvement**\n\n* Continuous quality improvement\n* Feature additions based on real usage\n* Scale infrastructure\n\n**Additional User Needs fulfilled**: UN-14 (API access)\n\n**Deferred Beyond V1.0**:\n* Federation (until multiple successful instances exist)\n* Complex contribution workflows (focus on automation)\n* Extensive role hierarchy (keep simple)\n\n== Success Metrics Framework ==\n\n**Note**: Each phase defines specific success criteria in detail. This section provides the overall metrics framework.\n\n**System Quality**:\n* Error rate by category (target: continuous improvement)\n* Average confidence score (target: increase)\n* Source quality distribution (target: more high-quality)\n* Contradiction detection rate (target: increase)\n\n**Efficiency**:\n* Claims processed per hour (target: increase)\n* Human hours per claim (target: decrease)\n* Automation coverage (target: >90%)\n* Re-work rate (target: <5%)\n\n**User Satisfaction**:\n* User flag rate (issues found)\n* Correction acceptance rate (flags valid)\n* Return user rate\n* Trust indicators (surveys)\n\n**User Needs Metrics**:\n* UN-1: % users who understand trust scores\n* UN-4: Time to verify social media claim\n* UN-7: % users who access evidence details\n* UN-8: % users who view multiple analysis contexts\n* UN-15: % users who check evolution timeline\n* UN-17: % users who enable in-article highlighting; avg. time spent on highlighted vs. non-highlighted articles\n\n== Status ==\n\n**Current Phase:** **Alpha** (POC complete, tagged ##v1.0.0-poc## on 2026-02-19)\n**Status:** ClaimAssessmentBoundary pipeline v1.0 operational. 853 tests passing, build clean. Concept proven.\n**Version:** v2.11.0 (Code)  3.0.0-cb (Schema)\n**Roadmap:** POC (complete) -> Alpha (current) -> Beta -> V1.0 (see [[POC to Alpha Transition>>FactHarbor.Product Development.Planning.POC to Alpha Transition.WebHome]])\n", "Product Development.Requirements.GapAnalysis": "= Gap Analysis =\n\n**Status:**  Analysis Complete \n**Purpose:** Identify features NOT YET planned for releases up to V1.0\n\n{{info}}\n**Scope:** This analysis only includes gaps - features that are NOT addressed in existing requirements (FR1-FR54, NFR1-NFR13, UN-1 to UN-28) or planned future requirements.\n\n**Already Planned for V1.0:** ClaimReview Schema (FR44) is NOT listed here as it's already planned for V1.0.\n\n**Deferred to V1.5:** Image/Video Verification (FR46, FR51) and Archive.org (FR47) have been moved to V1.5, so they ARE included in this Gap Analysis.\n\n**Requirement Numbers:** Proposed new requirements use numbers that avoid conflicts: FR55-FR84, NFR14-NFR18, UN-29 to UN-36\n{{/info}}\n\n== 1. Analysis Framework ==\n\n=== 1.1 Importance Formula ===\n\n**Importance = f(risk, impact, strategy)**\n\n* **Risk:** What are the consequences if we don't have this feature?\n* **Impact:** How many users affected? How severe?\n* **Strategy:** How well does this align with FactHarbor's mission and strategic goals?\n\n**Importance Levels:**\n* **VERY HIGH:** Critical to mission, high risk if missing, affects majority of users\n* **HIGH:** Important for success, significant impact, strong strategic alignment\n* **MEDIUM:** Valuable but not critical, moderate impact\n* **LOW:** Nice-to-have, limited impact\n\n=== 1.2 Urgency Formula ===\n\n**Urgency = f(fail fast and learn, legal, promises made)**\n\n* **Fail fast and learn:** Do we need to validate assumptions quickly?\n* **Legal:** Are there legal requirements or external deadlines?\n* **Promises made:** Have we committed this to stakeholders, funders, or partners?\n\n**Urgency Levels:**\n* **HIGH:** External deadlines, legal requirements, or critical testing needed\n* **MEDIUM:** Strategic opportunity, growing trends, competitive pressure\n* **LOW:** No external pressure, can add anytime\n\n=== 1.3 Context Matters ===\n\n**Important principle:** Importance and urgency change based on milestone context.\n\n* **POC:** Only basic features urgent\n* **Beta:** More features become urgent for user testing\n* **Release:** Legal/compliance becomes critical\n\n**Priorities are not absolute - they're contextual.**\n\n== 2. Gap Categories ==\n\nWe identified **15 true gaps** (features NOT in current roadmap) across **7 categories**:\n\n=== Category 1: Accessibility & Inclusivity ===\n* Gap 1.1: WCAG 2.1 Compliance\n* Gap 1.2: Multilingual Support\n\n=== Category 2: Platform Integration & Distribution ===\n* Gap 2.1: Browser Extensions\n* Gap 2.2: Embeddable Widgets\n\n=== Category 3: Media Verification ===\n* Gap 3.1: Image Verification\n* Gap 3.2: Video Verification\n* Gap 3.3: Audio Verification\n\n=== Category 4: Mobile & Offline Access ===\n* Gap 3.1: Mobile Apps / PWA\n* Gap 3.2: Offline Access\n\n=== Category 5: Education & Media Literacy ===\n* Gap 4.1: Educational Resources\n* Gap 4.2: Media Literacy Integration\n\n=== Category 6: Collaboration & Community ===\n* Gap 5.1: Professional Collaboration Tools\n* Gap 5.2: Community Discussion\n\n=== Category 7: Advanced Features ===\n* Gap 6.1: User Analytics\n* Gap 6.2: Personalization\n\n== 3. Critical Gaps (VERY HIGH Importance) ==\n\n=== 3.1 Gap: WCAG 2.1 Accessibility Compliance ===\n\n**Status:**  Not addressed in current requirements \n**Importance:** VERY HIGH \n**Urgency:** HIGH (legal requirement)\n\n**Why Important:**\n* **Risk:** CRITICAL\n * Legal liability (European Accessibility Act enforced June 28, 2025)\n * Lawsuits, fines up to $250,000 (Accessible Canada Act)\n * Cannot operate in EU market without compliance\n * Retrofitting is 100x more expensive than building in from start\n* **Impact:** 15-20% of population (1+ billion people) excluded without accessibility\n * Affects blind, low-vision, deaf, motor impairments, cognitive disabilities\n * \"86% of companies report improved customer satisfaction after implementing accessibility\" (Forrester)\n* **Strategy:** CRITICAL ALIGNMENT\n * Mission is \"a world where decisions are grounded in evidence\" - not \"for sighted people only\"\n * Non-profit public interest mission requires serving ALL publics\n\n**Why Urgent:**\n* **Fail fast:** HIGH - Legal deadlines approaching (EU Accessibility Act June 2025)\n* **Legal:** CRITICAL - Required by law in EU, Canada, US (Section 508, ADA)\n* **Promises:** HIGH if mission emphasizes \"public interest\" or \"for all\"\n\n**Missing Requirements:**\n* WCAG 2.1 Level AA compliance (minimum legal standard)\n* Screen reader compatibility\n* Keyboard-only navigation\n* Sufficient color contrast ratios\n* Alternative text for all images\n* Closed captions for videos\n* Accessible forms and error messages\n\n**Recommended:**\n* **NFR14: Accessibility Compliance** - Platform must conform to WCAG 2.1 Level AA standards\n* **NFR15: Assistive Technology Support** - Compatible with screen readers, voice navigation, keyboard-only usage\n* **FR55: Accessibility Settings** - User-configurable contrast, text size, reduced motion options\n\n**When to Address:** Build in from MVP/POC1 - retrofitting is 100x more expensive\n\n**Research Evidence:**\n* \"1 in 4 adults in the US has a disability\" (CDC 2023)\n* \"Legal actions increased 14% in 2023\" (Forrester accessibility report)\n* European Accessibility Act mandatory June 28, 2025\n\n=== 3.2 Gap: Educational Resources & Onboarding ===\n\n**Status:**  Not addressed \n**Importance:** VERY HIGH \n**Urgency:** MEDIUM\n\n**Why Important:**\n* **Risk:** HIGH - Users won't understand methodology, will distrust results\n* **Impact:** VERY HIGH - Affects all users, especially first-time visitors\n* **Strategy:** CRITICAL - Transparency requires education\n\n**Why Urgent:**\n* **Fail fast:** MEDIUM - Need to test what educational content resonates\n* **Legal:** None\n* **Promises:** HIGH if emphasizing \"transparent\" methodology\n\n**Missing Requirements:**\n* Interactive first-time user tutorial\n* Video explanations of how AKEL works\n* Glossary of terms (scenario, verdict, evidence quality)\n* FAQ addressing common questions\n* Educational resources hub\n* Teacher/educator curriculum materials\n\n**Recommended:**\n* **UN-34: Learn How to Fact-Check** - Educational resources for understanding methodology\n* **FR67: Onboarding Tutorial** - Interactive first-time user walkthrough\n* **FR68: Educational Resources Hub** - Guides, videos, FAQs, glossary\n* **FR69: Curriculum Materials** - Resources for educators to use FactHarbor in classrooms\n\n**When to Address:** Beta 0 (before public users)\n\n**Research Evidence:**\n* \"Users need 3+ exposures to new concept before trusting it\" (UX research)\n* Educational fact-checking platforms have 3x higher user retention (MediaWise 2024)\n\n== 4. High Importance Gaps ==\n\n=== 4.1 Gap: Browser Extensions ===\n\n**Status:**  Not addressed \n**Importance:** HIGH \n**Urgency:** MEDIUM\n\n**Why Important:**\n* **Risk:** MEDIUM - Competitive disadvantage, reduced adoption\n* **Impact:** MEDIUM-HIGH - Significantly improves UX for active fact-checkers\n* **Strategy:** HIGH ALIGNMENT - Meet users where misinformation spreads (in their browsers)\n\n**Why Urgent:**\n* **Fail fast:** MEDIUM - Should validate that users actually want browser extensions\n* **Legal:** None\n* **Promises:** LOW unless explicitly promised to early adopters\n\n**Missing Requirements:**\n* Chrome/Firefox/Safari browser extensions\n* Right-click context menu for selected text\n* Inline highlighting of claims on any webpage\n* Quick verdict tooltips without leaving page\n* Save/bookmark fact-checks\n\n**Recommended:**\n* **UN-29: In-Context Fact-Checking** - Browser extension for real-time verification\n* **FR58: Browser Extensions** - Chrome, Firefox, Safari with context menu\n* **FR59: Cross-Site Highlighting** - Highlight and analyze claims on any website\n\n**When to Address:** Test web platform first, then build extension MVP if user demand validated\n\n**Research Evidence:**\n* \"3-click verification: Select  Right-click  Verify\" is standard UX pattern\n* Extensions like UnCovered, Pino, InVID/WeVerify widely adopted\n* NewsGuard browser extension demonstrates market acceptance\n\n=== 4.2 Gap: Multilingual Support ===\n\n**Status:**  Not addressed \n**Importance:** HIGH \n**Urgency:** MEDIUM\n\n**Why Important:**\n* **Risk:** HIGH - Mission limited to English speakers (~20% of world)\n* **Impact:** VERY HIGH - Excludes 80% of world population\n* **Strategy:** CRITICAL ALIGNMENT - Vision of \"a world where decisions are grounded in evidence\" - not just English-speaking world\n\n**Why Urgent:**\n* **Fail fast:** MEDIUM - Test which languages users need, validate translation quality early\n* **Legal:** None\n* **Promises:** MEDIUM-HIGH if mission statement emphasizes \"global\" or \"world\"\n\n**Missing Requirements:**\n* Interface available in multiple languages\n* Content translation/analysis in non-English languages\n* Right-to-left (RTL) language support (Arabic, Hebrew)\n* Locale-specific formatting (dates, numbers, currencies)\n* Character encoding for non-Latin scripts\n\n**Recommended:**\n* **FR56: Multilingual Interface** - UI in 10+ languages\n* **FR57: Multilingual Content Analysis** - AKEL analyzes claims in multiple languages\n* **NFR16: Internationalization (i18n)** - RTL support, character encodings, locale formatting\n\n**When to Address:** Post-V1.0 (after English version stable)\n\n**Research Evidence:**\n* Only 25% of internet users speak English (Internet World Stats)\n* Misinformation spreads in all languages, many underserved\n\n=== 4.3 Gap: Mobile Apps (Native) ===\n\n**Status:**  Not addressed (PWA might be planned) \n**Importance:** HIGH \n**Urgency:** MEDIUM\n\n**Why Important:**\n* **Risk:** MEDIUM-HIGH - 60% of web traffic is mobile, but not addressing native app expectations\n* **Impact:** HIGH - Affects majority of users who expect native mobile experience\n* **Strategy:** MEDIUM-HIGH - Mobile-first is standard for modern platforms\n\n**Why Urgent:**\n* **Fail fast:** MEDIUM - Test if responsive web is sufficient or if native apps needed\n* **Legal:** None\n* **Promises:** LOW unless explicitly promised\n\n**Missing Requirements:**\n* iOS native app\n* Android native app\n* Offline capabilities\n* Push notifications\n* Camera integration for on-the-spot verification\n\n**Recommended:**\n* **UN-32: Mobile-Native Experience** - Native apps for iOS/Android\n* **FR66: Native Mobile Apps** - Full mobile capabilities\n* **NFR17: Progressive Web App** - Installable, offline, push notifications\n\n**When to Address:** Post-V1.0 (test web-first, then native if needed)\n\n=== 4.2 Gap: Media Verification (Images, Videos, Audio) ===\n\n**Status:**  Not addressed for V1.0 (deferred to V1.5) \n**Importance:** VERY HIGH \n**Urgency:** MEDIUM\n\n**Why Important:**\n* **Risk:** HIGH - Cannot address major category of misinformation (visual/audio)\n* **Impact:** VERY HIGH - Visual misinformation is primary vector\n* **Strategy:** CRITICAL ALIGNMENT - Mission incomplete without multimedia fact-checking\n\n**Why Urgent:**\n* **Fail fast:** VERY HIGH - Should test approach quickly (partner vs. build?)\n* **Legal:** None\n* **Promises:** MEDIUM (depends on mission statements)\n\n**Missing Requirements:**\n* Reverse image search integration\n* Video frame extraction and analysis\n* Audio deepfake detection\n* EXIF metadata extraction\n* Synthetic media detection (AI-generated content)\n\n**Recommended:**\n* **UN-31: Media Verification** - Image, video, audio fact-checking\n* **FR63: Image Verification** - Reverse search, EXIF, synthetic detection\n* **FR64: Video Verification** - Frame analysis, metadata, deepfake detection\n* **FR65: Audio Verification** - Voice deepfakes, audio forensics\n\n**When to Address:** V1.5 (pilot with existing tools like InVID, TinEye before building in-house)\n\n**Note:** Originally planned as FR46, FR51, FR47 for V1.0, now deferred to V1.5 to focus V1.0 on core text-based fact-checking.\n\n**Research Evidence:**\n* \"Most deception relies on decontextualization\" of images (Cazzamatta 2025)\n* \"Deepfakes targeting political figures raise concerns\" (Corsi et al. 2024)\n* InVID/WeVerify used by professional fact-checkers (AFP 2024)\n\n== 5. Medium Importance Gaps ==\n\n=== 5.1 Gap: Embeddable Widgets ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* JavaScript widget for embedding fact-checks in third-party sites\n* WordPress plugin\n* Customizable styling to match publisher branding\n\n**Recommended:**\n* **FR61: Embeddable Widgets** - JavaScript widgets for third-party sites\n* **FR62: CMS Plugins** - WordPress, Drupal, Ghost plugins\n\n**When to Address:** Post-V1.0 (after core platform stable)\n\n=== 5.2 Gap: Educational Partnerships ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* Curriculum materials for K-12 and university educators\n* Classroom discussion guides\n* Student exercises and assignments\n* Teacher training materials\n\n**Recommended:**\n* Include in **FR69: Curriculum Materials**\n\n**When to Address:** V2.0+ (after platform established)\n\n=== 5.3 Gap: Professional Collaboration Tools ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* Shared workspaces for fact-checking teams\n* Assignment and workflow management\n* Internal notes and discussion threads\n* Collaborative editing\n\n**Recommended:**\n* **FR72: Collaboration Workspace** - Team features for professional fact-checkers\n\n**When to Address:** Post-V1.0 (for professional fact-checkers)\n\n=== 5.4 Gap: Community Discussion ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* Public comment threads on fact-checks\n* Community forums\n* Upvoting/downvoting\n\n**Recommended:**\n* **FR75: Community Discussion** - Forums and comment threads\n\n**When to Address:** V2.0+ (requires moderation capacity)\n\n=== 5.5 Gap: User Analytics ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* Anonymous usage analytics\n* A/B testing framework\n* User satisfaction surveys\n* Funnel analysis\n\n**Recommended:**\n* **FR78: Analytics Dashboard** - Usage tracking and insights\n\n**When to Address:** Beta 0 (to measure product-market fit)\n\n=== 5.6 Gap: Personalization ===\n\n**Status:**  Not addressed \n**Importance:** MEDIUM \n**Urgency:** LOW\n\n**Missing Requirements:**\n* Saved searches\n* Custom feeds based on topics of interest\n* Personalized recommendations\n* User preference settings\n\n**Recommended:**\n* **FR81: User Personalization** - Custom feeds, saved searches, preferences\n\n**When to Address:** V2.0+ (after core platform stable)\n\n== 6. Summary: What to Address When ==\n\n=== POC/Beta 0 (Urgent) ===\n* **NFR14-NFR15, FR55:** Accessibility (legal requirement)\n* **UN-34, FR67-FR69:** Educational resources (transparency requires education)\n\n=== V1.0 (Important) ===\n* **UN-29, FR58-FR59:** Browser extensions (competitive necessity)\n* **FR56-FR57, NFR16:** Multilingual support (mission alignment)\n* **UN-32, FR66, NFR17:** Mobile experience (user expectation)\n\n=== Post-V1.0 (Valuable) ===\n* **FR61-FR62:** Embeddable widgets\n* **FR72:** Professional collaboration\n* **FR75:** Community discussion\n* **FR78:** Analytics\n* **FR81:** Personalization\n\n== 7. Requirement Summary ==\n\n**Existing Requirements (From Baseline):**\n* Functional: FR1 through FR54 (includes ClaimReview FR44; FR46, FR51, FR47 moved to V1.5)\n* Non-Functional: NFR1 through NFR13\n* User Needs: UN-1 through UN-28\n\n**Proposed New Requirements (From This Gap Analysis):**\n* Functional: FR55 through FR84 (30 new)\n* Non-Functional: NFR14 through NFR18 (5 new)\n* User Needs: UN-29 through UN-36 (8 new)\n\n**Total After Implementation:**\n* Functional: FR1-FR84 (84 total)\n* Non-Functional: NFR1-NFR18 (18 total)\n* User Needs: UN-1 through UN-36 (36 total)\n", "Product Development.Requirements.Roles.WebHome": "= Roles & Content States =\n\n**Version:** 0.10.0\n**Last Updated:** February 2026\n**Status:** Current\n\nThis page defines user roles and content states in FactHarbor.\n\n== 1. User Roles ==\n\n=== 1.1 Readers (Guest) ===\n\n**Who**: Anonymous visitors (no registration required)\n\n**Can:**\n* Browse and search published analyses\n* View analysis results and evidence\n* Explore the Evidence Landscape for any claim\n\n**Cannot:**\n* Submit URLs/text for analysis (requires login)\n* Flag quality issues (requires login)\n* Manage UCM configuration\n* Moderate content\n\n=== 1.2 Users (Registered) ===\n\n**Who**: Logged-in users with a registered account\n\n**Can:**\n* Everything Readers can do\n* Submit URLs/text for analysis (**rate-limited**  LLM and web search usage is not free)\n* Flag quality issues\n* View their own submission history\n\n**Rate Limits:**\n* Submission quotas per user per day/month (configurable via UCM)\n* Prevents abuse and controls operational costs (LLM inference + web search API fees)\n\n**Cannot:**\n* Modify analysis output data (data is immutable)\n* Manage UCM configuration\n* Moderate content\n\n=== 1.3 UCM Administrators ===\n\n**Who**: Appointed by Governing Team to manage system configuration\n\n**Responsibilities:**\n* Manage UCM configuration (prompt templates, quality thresholds, model selection, source reliability rules)\n* Monitor quality metrics and identify systematic improvement opportunities\n* Version and activate configuration changes\n* Trigger re-analysis when configuration improves\n* Review UCM config audit trail\n\n**Can:**\n* Create and activate UCM config versions\n* View config change history and audit trail\n* Trigger re-analysis with updated config\n* Access detailed system metrics\n* Participate in RFC (Request for Comments) processes\n\n**Cannot:**\n* Edit individual analysis outputs (data is immutable)\n* Override quality gates for specific analyses\n* Act as editorial gatekeepers\n\n**Key Principle**: UCM Administrators improve THE SYSTEM through configuration, not individual outputs.\n\n=== 1.4 Moderators ===\n\n**Who**: Team members focused on community health and abuse prevention\n\n**Responsibilities:**\n* Handle abuse, spam, and harassment\n* Enforce community guidelines\n* Respond to user reports\n* Manage bans and appeals\n\n**Can:**\n* Hide abusive content\n* Ban users for policy violations\n* Review appeals\n* Escalate serious issues to Governing Team\n\n**Cannot:**\n* Approve content for publication\n* Review content quality before publication\n* Override quality gates for content\n* Act as editorial gatekeepers\n\n**Critical Distinction:**\n{{{\nModerators handle: ABUSE (spam, harassment, violations)\nModerators DO NOT handle: CONTENT QUALITY (that's automated)\n}}}\n\n=== 1.5 Domain Experts (Optional, Task-Specific) ===\n\n**Who**: Subject matter specialists consulted for specific high-stakes disputes\n\n**Not a permanent role**: Contacted externally when needed for contested claims in their domain\n\n**When used:**\n* Medical claims with life/safety implications\n* Legal interpretations with significant impact\n* Scientific claims with high controversy\n* Technical claims requiring specialized knowledge\n\n**Process:**\n* Moderator identifies need for expert input\n* Contact expert externally (don't require them to be users)\n* Expert provides written opinion with sources\n* Opinion may inform UCM config improvements (e.g., domain-specific prompt adjustments)\n\n**Important**: This is CONSULTATION, not APPROVAL. Expert input may lead to UCM config improvements that benefit all future analyses in that domain.\n\n**User Needs served**: UN-16 (Expert validation status)\n\n== 2. Content States ==\n\n**Fulfills**: UN-1 (Trust indicators), UN-16 (Review status transparency)\n\nFactHarbor uses **two content states**: Published and Hidden. Analysis data is immutable  quality improvements flow through UCM configuration changes, not data edits. Analyses that fail quality gates are recorded internally but never published; the system is improved to address recurring failures.\n\n=== 2.1 Published ===\n\n**Status**: Visible to all users\n\n**When:**\n* Quality gates passed\n* Confidence  threshold\n* Meets structural requirements\n* Sufficient evidence found\n\n**Displayed with content:**\n* Confidence score (0-100%)\n* Source Quality Score (0-100%)\n* Risk tier badge (A/B/C)\n* Controversy flag (if high dispute among sources)\n* Completeness score (% of expected fields filled)\n* Analysis date and UCM Config Version\n* Clear \"AI-Generated\" labeling\n\n**Labels by Risk Tier:**\n* **Tier A (High Risk):** \" AI-Generated - High Impact Topic - Seek Professional Advice\"\n* **Tier B (Medium Risk):** \" AI-Generated - May Contain Errors\"\n* **Tier C (Low Risk):** \" AI-Generated\"\n\n**Automatic Warnings:**\n* Confidence < 60%: \"Low confidence - use caution\"\n* Source quality < 40%: \"Sources may be unreliable\"\n* High controversy: \"Disputed - multiple interpretations exist\"\n* Medical/Legal/Safety domain: \"Seek professional advice\"\n\n**System Improvement:**\n* Quality improvements flow through UCM configuration changes\n* All config changes versioned and auditable\n* Every analysis references the UCM config snapshot used\n* Sampling audits identify systematic issues for UCM config improvement\n\n**User Needs served**: UN-1 (Trust score), UN-9 (Methodology transparency), UN-16 (Review status)\n\n=== 2.2 Hidden ===\n\n**Status**: Not visible to regular users (only to moderators)\n\n**Reasons:**\n* Spam or advertising\n* Personal attacks or harassment\n* Illegal content\n* Privacy violations\n* Deliberate misinformation (verified)\n* Abuse or harmful content\n\n**Process:**\n* Automated detection flags for moderator review\n* Moderator confirms and hides\n* Original author notified with reason\n* Can appeal to board if disputes moderator decision\n\n**Note**: Content is hidden, not deleted (for audit trail)\n\n== 3. Roles Overview ==\n\n**Four roles:**\n\n1. **Reader** (default, guest)  Anonymous visitors. Browse, search, view results. No registration required.\n2. **User** (registered)  Logged-in users. Submit URLs/text for analysis (rate-limited), flag issues. Registration required.\n3. **UCM Administrator** (appointed)  Manage system configuration to improve analysis quality. Appointed by Governing Team.\n4. **Moderator** (appointed)  Handle abuse, spam, harassment. Does NOT manage content quality (that is automated).\n5. **Domain Expert** (optional, task-specific)  Consulted externally for high-stakes disputes. Not a platform role.\n\n**Key Principle**: Analysis output data is immutable. Quality improvements flow through UCM configuration changes, not data edits.\n\n== 4. Principles ==\n\n* **Improve the system, not the data**  analysis outputs are immutable\n* **Low barrier to entry**  anyone can browse and search without registration; submission requires a free account\n* **Clear separation** between content quality (automated) and behavioral moderation (human)\n* **UCM-driven improvement**  quality improvements flow through configuration changes\n* **No gatekeeping** for content publication\n* **Every report references its config**  full reproducibility via UCM config snapshots\n\n== 5. Related Pages ==\n\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] - AI system\n* [[Workflows>>FactHarbor.Product Development.Specification.Workflows.WebHome]] - Process workflows\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] - System architecture\n* [[Decision Processes>>FactHarbor.Organisation.Governance.Decision Processes.WebHome]] - Governance\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] - All requirements\n", "Product Development.Requirements.User Needs.WebHome": "= User Needs =\n\nThis page defines user needs that drive FactHarbor's requirements and design decisions.\n\n**Template**: As a <specific user role>, I want to <action/goal>, so that I can <benefit/outcome>\n\n**Purpose**: User needs inform functional requirements (FR) and non-functional requirements (NFR). Each need maps to one or more requirements that fulfill it.\n\n== 1. Core Reading & Discovery ==\n\n=== UN-1: Trust Assessment at a Glance ===\n**As** an article reader (any content type), \n**I want** to see a trust score and overall verdict summary at a glance, \n**so that** I can quickly decide if the content is worth my time to read in detail.\n\n**Maps to**: FR7 (Automated Verdicts), NFR3 (Transparency)\n\n=== UN-2: Claim Extraction and Verification ===\n**As** an article reader, \n**I want** to see the key factual claims extracted from content with verification verdicts (likelihood ranges + uncertainty ratings) for each relevant scenario, \n**so that** I can distinguish proven facts from speculation and understand context-dependent truth.\n\n**Maps to**: FR1 (Claim Intake), FR4 (Scenario Generation), FR7 (Automated Verdicts)\n\n=== UN-3: Article Summary with FactHarbor Analysis Summary ===\n**As** an article reader, \n**I want** to see an article summary (the document's position, key claims, and reasoning) side-by-side with FactHarbor's analysis summary (source credibility assessment, claim-by-claim verdicts, methodology evaluation, and overall quality verdict), \n**so that** I can quickly understand both what the document claims and FactHarbor's complete analysis of its credibility without reading the full detailed report.\n\n**Maps to**: FR7 (Automated Verdicts), FR6 (Scenario Comparison), FR12 (Two-Panel Summary View - Article Summary with FactHarbor Analysis Summary)\n\n==== Example: Two-Panel Summary Layout ====\n\n|=**ARTICLE SUMMARY**|=**FACTHARBOR ANALYSIS SUMMARY**\n|(((\n**FactHarbor Summary: AHA Alcohol & Heart Health Statement (2025)**\n\n**Source:** American Heart Association Scientific Statement, //Circulation//, June 2025\n**Credibility:** Very High (peer-reviewed expert consensus)\n\n=== The Big Picture ===\n**Old belief:** \"A glass of wine is good for your heart\"\n**New position:** We're no longer sure that's true\n\n=== Key Findings ===\n\n|=**Drinking Level**|=**Verdict**\n|Heavy (3 drinks/day)|(% style=\"color:red\" %) **Harmful**  consistent across ALL studies\n|Moderate (1-2 drinks/day)|(% style=\"color:orange\" %) **Uncertain**  benefits may have been overstated\n|None|(% style=\"color:green\" %) **Don't start drinking for heart health**\n\n=== Why the Shift? ===\nNewer genetic studies (Mendelian randomization) found **no evidence** that moderate drinking protects the heart. The apparent benefits in older studies were likely due to lifestyle differences and methodological bias.\n\n=== AHA Bottom Line ===\n(% class=\"box\" %)\n(((\nIf you don't drink, don't start. If you do drink, keep it to 2/day (men) or 1/day (women). Focus on proven healthy behaviors insteadexercise, diet, not smoking.\n\n//The \"wine for heart health\" era appears to be over.//\n)))\n)))|(((\n**FactHarbor Analysis Summary**\n\n**Document:** AHA Scientific Statement on Alcohol and Cardiovascular Disease (2025)\n\n=== Source Assessment ===\n**Credibility:** (% style=\"color:green\" %)**VERY HIGH**(%%)  Official AHA statement, peer-reviewed, expert panel, published in top journal (//Circulation//)\n\n=== Analysis Findings ===\n\n|=**Claim in Document**|=**FactHarbor Verdict**|=**Confidence**\n|Heavy drinking harms heart health|(% style=\"color:green\" %)**STRONGLY SUPPORTED**|(% style=\"color:green\" %)**95%**\n|Moderate drinking benefits uncertain|(% style=\"color:green\" %)**WELL SUPPORTED**|(% style=\"color:green\" %)**85%**\n|Prior \"cardioprotective\" claims overstated|(% style=\"color:green\" %)**SUPPORTED**|(% style=\"color:green\" %)**80%**\n|More research needed|**APPROPRIATE**|N/A\n\n=== Assessment ===\n\n(% style=\"color:green\" %)(%%) **Strengths:** Transparent about methodological limitations, incorporates newer Mendelian randomization evidence, appropriately cautious, avoids overstatement\n\n(% style=\"color:green\" %)(%%) **Methodology:** Sound synthesis of observational and genetic evidence\n\n(% style=\"color:orange\" %)(%%) **Limitation:** Still relies heavily on observational data; RCT evidence limited\n\n=== Verdict on the Statement Itself ===\n\n(% class=\"box successmessage\" %)\n(((\n**WELL-SUPPORTED SCIENTIFIC SYNTHESIS**  The AHA statement is credible, balanced, and appropriately reflects the current state of evidence. It correctly signals a shift away from previous assumptions about moderate drinking benefits without overclaiming in either direction.\n)))\n\n**Analysis ID:** FH-AHA-ALCO-2025-12-17\n)))\n\n**Key Elements of Two-Panel Layout**:\n\n**Left Panel (Article Summary)**:\n* Document title and source\n* Source credibility (document's own authority)\n* \"The Big Picture\" - old belief vs. new position\n* \"Key Findings\" - document's main claims in structured format\n* \"Why the Shift?\" - document's reasoning\n* \"Bottom Line\" - document's conclusion\n\n**Right Panel (FactHarbor Analysis Summary)**:\n* FactHarbor's source assessment (independent credibility check)\n* Claim-by-claim analysis with verdicts and confidence scores\n* Assessment of methodology (strengths/limitations)\n* Overall verdict on the document itself\n* Analysis ID for reference\n\n**Design Principle**: User sees **what they claim** and **FactHarbor's complete analysis** side-by-side without scrolling.\n\n=== UN-4: Social Media Fact-Checking ===\n**As** a social media user, \n**I want** to check claims in posts before sharing, \n**so that** I can avoid spreading misinformation.\n\n**Maps to**: FR1 (Claim Intake), FR7 (Automated Verdicts), NFR1 (Performance - fast processing)\n\n=== UN-17: In-Article Claim Highlighting ===\n**As** a reader viewing an article, \n**I want** to see factual claims highlighted with color-coded credibility indicators (green for well-supported, yellow for uncertain, red for refuted), \n**so that** I can immediately identify which statements are trustworthy and which require skepticism without interrupting my reading flow.\n\n**Maps to**: FR7 (Automated Verdicts), FR13 (In-Article Claim Highlighting), NFR1 (Performance - real-time highlighting)\n\n==== Visual Concept ====\n\nWhen reading an article on FactHarbor:\n\n(% style=\"font-family:monospace; background-color:#f5f5f5; padding:10px; display:block;\" %)\n(((\nRegular article text flows normally...\n\n(% style=\"background-color:#90EE90; padding:2px 5px;\" %)This claim is well-supported by evidence(%%) and you can continue reading...\n\nMore context and explanation...\n\n(% style=\"background-color:#FFD700; padding:2px 5px;\" %)This claim is uncertain with conflicting evidence(%%) but the article continues...\n\nAdditional information...\n\n(% style=\"background-color:#FFB6C6; padding:2px 5px;\" %)This claim has been refuted by research(%%) and understanding that helps readers...\n)))\n\n**Hover/Click on any highlighted claim**  See verdict, confidence score, and evidence summary\n\n== 2. Source Tracing & Credibility ==\n\n=== UN-5: Source Provenance and Track Records ===\n**As** an article reader, \n**I want** to trace each piece of evidence back to its original source and see that source's historical track record, \n**so that** I can assess the reliability of the information chain and learn which sources are consistently trustworthy.\n\n**Maps to**: FR5 (Evidence Linking), Section 4.1 (Source Requirements - track record system)\n\n=== UN-6: Publisher Reliability History ===\n**As** an article reader, \n**I want** to see historical accuracy track records for sources and publishers, \n**so that** I can learn which outlets are consistently reliable over time.\n\n**Maps to**: Section 4.1 (Source Requirements), Data Model (Source entity with track_record_score)\n\n== 3. Understanding the Analysis ==\n\n=== UN-7: Evidence Transparency ===\n**As** a skeptical reader, \n**I want** to see the evidence and reasoning behind each verdict, \n**so that** I can judge whether I agree with the assessment and form my own conclusions.\n\n**Maps to**: FR5 (Evidence Linking), NFR3 (Transparency)\n\n=== UN-8: Understanding Disagreement and Consensus ===\n**As** an article reader, \n**I want** to see which scenarios have strong supporting evidence versus which have conflicting evidence or high uncertainty, \n**so that** I can understand where legitimate disagreement exists versus where consensus is clear.\n\n**Maps to**: FR6 (Scenario Comparison), FR7 (Automated Verdicts - uncertainty factors), AKEL Gate 2 (Contradiction Search)\n\n=== UN-9: Methodology Transparency ===\n**As** an article reader, \n**I want** to understand how likelihood ranges and confidence scores are calculated, \n**so that** I can trust the verification process itself.\n\n**Maps to**: NFR3 (Transparency), Architecture (documented algorithms), AKEL (Quality Gates)\n\n== 4. Pattern Recognition & Learning ==\n\n=== UN-10: Manipulation Tactics Detection ===\n**As** an article reader, \n**I want** to see common manipulation tactics or logical fallacies identified in content, \n**so that** I can recognize them elsewhere and become a more critical consumer of information.\n\n**Maps to**: AKEL (Bubble Detection), Section 5 (Automated Risk Scoring)\n\n=== UN-11: Filtered Research ===\n**As** a researcher, \n**I want** to filter content by verification status, confidence levels, and source quality, \n**so that** I can work only with reliable information appropriate for my research needs.\n\n**Maps to**: FR1 (Claim Classification), Section 4.4 (Confidence Scoring), NFR1 (Performance)\n\n== 5. Taking Action ==\n\n=== UN-12: Submit Unchecked Claims ===\n**As** a reader who finds unchecked claims, \n**I want** to submit them for verification, \n**so that** I can help expand fact-checking coverage and contribute to the knowledge base.\n\n**Maps to**: FR1 (Claim Intake), Section 1.1 (Reader role)\n\n=== UN-13: Cite FactHarbor Verdicts ===\n**As** a content creator, \n**I want** to cite FactHarbor verdicts when sharing content, \n**so that** I can add credibility to what I publish and help my audience distinguish fact from speculation.\n\n**Maps to**: FR7 (Automated Verdicts), NFR3 (Transparency - exportable data)\n\n== 6. Professional Use ==\n\n=== UN-14: API Access for Integration ===\n**As** a journalist/researcher, \n**I want** API access to verification data and claim histories, \n**so that** I can integrate fact-checking into my professional workflow without manual lookups.\n\n**Maps to**: Architecture (REST API), NFR2 (Scalability), FR11 (Audit Trail)\n\n== 7. Understanding Evolution & Trust Labels ==\n\n=== UN-15: Verdict Evolution Timeline ===\n\n{{warning}}\n**Status:** Deferred (Not in V1.0)\n\nFull verdict evolution timeline has been **dropped from V1.0**. The system tracks UCM config versions and per-job config snapshots. Full evolution tracking is deferred to future releases.\n{{/warning}}\n\n**As** an article reader, \n**I want** to see how a claim's verdict has evolved over time with clear timestamps, \n**so that** I can understand whether the current assessment is stable or recently changed based on new evidence.\n\n**Maps to**: ~~FR8 (Deferred)~~, Data Model (UCM config tracking), NFR3 (Transparency)\n\n=== UN-16: Analysis Origin and Review Status ===\n**As** an article reader,\n**I want** to know how the verdict was generated (AI pipeline, configuration version) and whether expert input was involved,\n**so that** I can gauge the appropriate level of trust and understand the analysis process used.\n\n**Maps to**: AKEL (Content States), Section 5 (Risk Tiers), Data Model (AuthorType field)\n\n== 8. User Need  Requirements Mapping Summary ==\n\nThis section provides a consolidated view of how user needs drive system requirements.\n\n=== 8.1 Functional Requirements Coverage ===\n\n(% style=\"width:100%\" %)\n|=(% style=\"width:10%\" %)FR#|=(% style=\"width:35%\" %)Requirement|=(% style=\"width:55%\" %)Fulfills User Needs\n|(% style=\"width:10%\" %)FR1|(% style=\"width:35%\" %)Claim Intake|(% style=\"width:55%\" %)UN-2, UN-4, UN-12\n|(% style=\"width:10%\" %)FR4|(% style=\"width:35%\" %)Scenario Generation|(% style=\"width:55%\" %)UN-2, UN-3\n|(% style=\"width:10%\" %)FR5|(% style=\"width:35%\" %)Evidence Linking|(% style=\"width:55%\" %)UN-5, UN-7\n|(% style=\"width:10%\" %)FR6|(% style=\"width:35%\" %)Scenario Comparison|(% style=\"width:55%\" %)UN-3, UN-8\n|(% style=\"width:10%\" %)FR7|(% style=\"width:35%\" %)Automated Verdicts|(% style=\"width:55%\" %)UN-1, UN-2, UN-3, UN-4, UN-13, UN-17\n|(% style=\"width:10%\" %)FR8|(% style=\"width:35%\" %)Time Evolution|(% style=\"width:55%\" %)UN-15\n|(% style=\"width:10%\" %)FR11|(% style=\"width:35%\" %)Audit Trail|(% style=\"width:55%\" %)UN-14, UN-16\n|(% style=\"width:10%\" %)FR12|(% style=\"width:35%\" %)Two-Panel Summary View|(% style=\"width:55%\" %)UN-3\n|(% style=\"width:10%\" %)FR13|(% style=\"width:35%\" %)In-Article Claim Highlighting|(% style=\"width:55%\" %)UN-17\n\n=== 8.2 Non-Functional Requirements Coverage ===\n\n(% style=\"width:100%\" %)\n|=(% style=\"width:10%\" %)NFR#|=(% style=\"width:35%\" %)Requirement|=(% style=\"width:55%\" %)Fulfills User Needs\n|(% style=\"width:10%\" %)NFR1|(% style=\"width:35%\" %)Performance|(% style=\"width:55%\" %)UN-4 (fast fact-checking), UN-11 (responsive filtering), UN-17 (real-time highlighting)\n|(% style=\"width:10%\" %)NFR2|(% style=\"width:35%\" %)Scalability|(% style=\"width:55%\" %)UN-14 (API access at scale)\n|(% style=\"width:10%\" %)NFR3|(% style=\"width:35%\" %)Transparency|(% style=\"width:55%\" %)UN-1, UN-7, UN-9, UN-13, UN-15\n\n=== 8.3 AKEL System Coverage ===\n\n(% style=\"width:100%\" %)\n|=(% style=\"width:45%\" %)AKEL Component|=(% style=\"width:55%\" %)Fulfills User Needs\n|(% style=\"width:45%\" %)Quality Gates|(% style=\"width:55%\" %)UN-9 (methodology transparency)\n|(% style=\"width:45%\" %)Contradiction Search (Gate 2)|(% style=\"width:55%\" %)UN-8 (understanding disagreement)\n|(% style=\"width:45%\" %)Bubble Detection|(% style=\"width:55%\" %)UN-10 (manipulation tactics)\n|(% style=\"width:45%\" %)Content States|(% style=\"width:55%\" %)UN-16 (Analysis origin and review status)\n|(% style=\"width:45%\" %)Risk Tiers|(% style=\"width:55%\" %)UN-16 (appropriate review level)\n\n=== 8.4 Data Model Coverage ===\n\n(% style=\"width:100%\" %)\n|=(% style=\"width:45%\" %)Entity|=(% style=\"width:55%\" %)Fulfills User Needs\n|(% style=\"width:45%\" %)Source (with track_record_score)|(% style=\"width:55%\" %)UN-5, UN-6 (source reliability)\n|(% style=\"width:45%\" %)Scenario|(% style=\"width:55%\" %)UN-2, UN-3, UN-8 (context-dependent truth)\n|(% style=\"width:45%\" %)Verdict (with likelihood_range, uncertainty_factors)|(% style=\"width:55%\" %)UN-1, UN-2, UN-3, UN-8 (detailed assessment)\n|(% style=\"width:45%\" %)UCM config snapshots|(% style=\"width:55%\" %)UN-15 (evolution timeline)\n|(% style=\"width:45%\" %)AuthorType field|(% style=\"width:55%\" %)UN-16 (AI vs. human status)\n\n== 9. User Need Gaps & Future Considerations ==\n\nThis section identifies user needs that may emerge as the platform matures:\n\n**Potential Future Needs**:\n* **Collaborative annotation**: Users want to discuss verdicts with others\n* **Personal tracking**: Users want to track claims they're following\n* **Custom alerts**: Users want notifications when tracked claims are updated\n* **Export capabilities**: Users want to export claim analyses for their own documentation\n* **Comparative analysis**: Users want to compare how different fact-checkers rate the same claim\n\n**When to address**: These needs should be considered when:\n1. User feedback explicitly requests them\n2. Usage metrics show users attempting these workflows\n3. Competitive analysis shows these as differentiators\n\n**Principle**: Start simple (current User Needs), add complexity only when metrics prove necessity.\n\n== 10. Related Pages ==\n\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] - Parent page with roles, rules, and functional requirements\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] - How requirements are implemented\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] - Data structures supporting user needs\n* [[AKEL (AI Knowledge Extraction Layer)>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] - AI system fulfilling automation needs\n* [[Workflows>>FactHarbor.Product Development.Specification.Workflows.WebHome]] - User interaction workflows\n\n== Additional User Needs (V0.9.70) ==\n\n=== UN-26: Search Engine Visibility ===\n\n**As a** content consumer \n**I want** FactHarbor analyses to appear in Google search results \n**So that** I can find fact-checks when searching\n\n**Requirements:** FR44 (ClaimReview schema)\n\n=== UN-27: Visual Claim Verification ===\n\n**As a** social media user \n**I want** to verify images shared with claims \n**So that** I can detect manipulated photos\n\n**Requirements:** FR46 (Image Verification)\n\n=== UN-28: Safe Contribution Environment ===\n\n**As a** fact-checking contributor \n**I want** protection from harassment \n**So that** I can contribute without fear\n\n**Requirements:** FR48 (Safety Framework)\n", "Product Development.Requirements.WebHome": "= Requirements =\n\n{{info}}\n**Phase Assignments:** See [[Requirements Roadmap Matrix>>FactHarbor.Product Development.Planning.Requirements-Roadmap-Matrix.WebHome]] for which requirements are implemented in which phases.\n{{/info}}\n\n**This page defines Roles, Content States, Rules, and System Requirements for FactHarbor.**\n\n**Core Philosophy:** Invest in system improvement, not manual data correction. When AI makes errors, improve the algorithm and re-process automatically.\n\n== Navigation ==\n\n* **[[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]]** - What users need from FactHarbor (drives these requirements)\n* **[[Roles & Content States>>FactHarbor.Product Development.Requirements.Roles.WebHome]]** - User roles and content states\n* **This page** - How we fulfill those needs through system design\n\n(% class=\"box infomessage\" %)\n(((\n**How to read this page:**\n\n1. **User Needs drive Requirements**: See [[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]] for what users need\n2. **Requirements define implementation**: This page shows how we fulfill those needs\n3. **Functional Requirements (FR)**: Specific features and capabilities\n4. **Non-Functional Requirements (NFR)**: Quality attributes (performance, security, etc.)\n\nEach requirement references which User Needs it fulfills.\n)))\n\n== 1. Roles ==\n\n**Fulfills**: UN-12 (Submit claims), UN-13 (Cite verdicts), UN-14 (API access)\n\nFactHarbor uses four roles.\n\n=== 1.1 Reader (Guest) ===\n\n**Who**: Anonymous visitors (no login required)\n\n**Can**:\n* Browse and search claims\n* View ClaimAssessmentBoundaries, evidence, verdicts, and confidence scores\n* Use filters, search, and visualization tools\n\n**Cannot**:\n* Submit URLs/text for analysis (requires login)\n* Flag issues (requires login)\n* Modify analysis output data (data is immutable)\n* Manage UCM configuration\n\n=== 1.1b User (Registered) ===\n\n**Who**: Logged-in users (free account required)\n\n**Can**:\n* Everything Reader can do\n* Submit URLs/text for analysis (rate-limited  LLM and web search costs)\n* Flag issues or errors\n* View own submission history\n\n**Cannot**:\n* Modify analysis output data (data is immutable)\n* Manage UCM configuration\n\n**User Needs served**: UN-1 (Trust assessment), UN-2 (Claim verification), UN-3 (Article summary with FactHarbor analysis summary), UN-4 (Social media fact-checking), UN-5 (Source tracing), UN-7 (Evidence transparency), UN-8 (Understanding disagreement), UN-12 (Submit claims), UN-17 (In-article highlighting)\n\n=== 1.2 UCM Administrator ===\n\n**Who**: Appointed by Governing Team to manage system configuration\n\n**Can**:\n* Everything a Reader can do\n* Manage UCM configuration (prompt templates, quality thresholds, model selection)\n* View config change history and audit trail\n* Activate/deactivate config versions\n* Trigger re-analysis with updated config\n* View detailed system metrics\n\n**Cannot**:\n* Edit individual analysis outputs (data is immutable)\n* Delete or hide content (only moderators)\n* Override quality gates for specific analyses\n\n**Key Principle**: UCM Administrators improve the system through configuration, not by editing individual outputs.\n\n=== 1.3 Moderator ===\n\n**Who**: Trusted community members with proven track record, appointed by governance board\n\n**Can**:\n* Review flagged content\n* Hide harmful or abusive content\n* Resolve disputes between users\n* Issue warnings or temporary bans\n* Make final decisions on content disputes\n* Access full audit logs\n\n**Cannot**:\n* Change governance rules\n* Permanently ban users without board approval\n* Override technical quality gates\n\n**Note**: Small team (3-5 initially), supported by automated moderation tools.\n\n=== 1.4 Domain Experts (Optional, Task-Specific) ===\n\n**Who**: Subject matter specialists invited for specific high-stakes disputes\n\n**Not a permanent role**: Contacted externally when needed for contested claims in their domain\n\n**When used**:\n* Medical claims with life/safety implications\n* Legal interpretations with significant impact\n* Scientific claims with high controversy\n* Technical claims requiring specialized knowledge\n\n**Process**:\n* Moderator identifies need for expert input\n* Contact expert externally (don't require them to be users)\n* Expert provides written opinion with sources\n* Opinion may inform UCM config improvements for the domain\n\n**User Needs served**: UN-16 (Expert validation status)\n\n== 2. Content States ==\n\n**Fulfills**: UN-1 (Trust indicators), UN-16 (Review status transparency)\n\nFactHarbor uses two content states. Focus is on transparency and confidence scoring, not gatekeeping.\n\n=== 2.1 Published ===\n\n**Status**: Visible to all users\n\n**Includes**:\n* AI-generated analyses (default state)\n\n**Quality Indicators** (displayed with content):\n* **Confidence Score**: 0-100% (AI's confidence in analysis)\n* **Source Quality Score**: 0-100% (based on source track record)\n* **Controversy Flag**: If high dispute among sources\n* **Completeness Score**: % of expected fields filled\n* **Analysis Date**: When the analysis was performed\n* **UCM Config Version**: Which configuration version produced this analysis\n\n**Automatic Warnings**:\n* Confidence < 60%: \"Low confidence - use caution\"\n* Source quality < 40%: \"Sources may be unreliable\"\n* High controversy: \"Disputed - multiple interpretations exist\"\n* Medical/Legal/Safety domain: \"Seek professional advice\"\n\n**User Needs served**: UN-1 (Trust score), UN-9 (Methodology transparency), ~~UN-15 (Evolution timeline - Deferred)~~, UN-16 (Review status)\n\n=== 2.2 Hidden ===\n\n**Status**: Not visible to regular users (only to moderators)\n\n**Reasons**:\n* Spam or advertising\n* Personal attacks or harassment\n* Illegal content\n* Privacy violations\n* Deliberate misinformation (verified)\n* Abuse or harmful content\n\n**Process**:\n* Automated detection flags for moderator review\n* Moderator confirms and hides\n* Original author notified with reason\n* Can appeal to board if disputes moderator decision\n\n**Note**: Content is hidden, not deleted (for audit trail)\n\n== 3. Contribution Rules ==\n\n=== 3.1 All Contributors Must ===\n\n* Provide sources for factual claims\n* Use clear, neutral language in FactHarbor's own summaries\n* Respect others and maintain civil discourse\n* Accept community feedback constructively\n* Focus on improving quality, not protecting ego\n\n=== 3.2 AKEL (AI System) ===\n\n**AKEL is the primary system**. Human contributions supplement and train AKEL.\n\n**AKEL Must**:\n* Mark all outputs as AI-generated\n* Display confidence scores prominently\n* Provide source citations\n* Flag uncertainty clearly\n* Identify contradictions in evidence\n* Learn from human corrections\n\n**When AKEL Makes Errors**:\n1. Capture the error pattern (what, why, how common)\n2. Improve the system (better prompt, model, validation)\n3. Re-process affected claims automatically\n4. Measure improvement (did quality increase?)\n\n**Human Role**: Train AKEL through corrections, not replace AKEL\n\n=== 3.3 Contributors Should ===\n\n* Improve clarity and structure\n* Add missing sources\n* Flag errors for system improvement\n* Suggest better ways to present information\n* Participate in quality discussions\n\n=== 3.4 Moderators Must ===\n\n* Be impartial\n* Document moderation decisions\n* Respond to appeals promptly\n* Use automated tools to scale efforts\n* Focus on abuse/harm, not routine quality control\n\n== 4. Quality Standards ==\n\n**Fulfills**: UN-5 (Source reliability), UN-6 (Publisher track records), UN-7 (Evidence transparency), UN-9 (Methodology transparency)\n\n=== 4.1 Source Requirements ===\n\n**Track Record Over Credentials**:\n* Sources evaluated by historical accuracy\n* Correction policy matters\n* Independence from conflicts of interest\n* Methodology transparency\n\n**Source Quality Database**:\n* Automated tracking of source accuracy\n* Correction frequency\n* Reliability score (updated continuously)\n* Users can see source track record\n\n**No automatic trust** for government, academia, or media - all evaluated by track record.\n\n**User Needs served**: UN-5 (Source provenance), UN-6 (Publisher reliability)\n\n=== 4.2 Claim Requirements ===\n\n* Clear subject and assertion\n* Verifiable with available information\n* Sourced (or explicitly marked as needing sources)\n* Neutral language in FactHarbor summaries\n* Appropriate context provided\n\n**User Needs served**: UN-2 (Claim extraction and verification)\n\n=== 4.3 Evidence Requirements ===\n\n* Publicly accessible (or explain why not)\n* Properly cited with attribution\n* Relevant to claim being evaluated\n* Original source preferred over secondary\n\n**User Needs served**: UN-7 (Evidence transparency)\n\n=== 4.4 Confidence Scoring ===\n\n**Automated confidence calculation based on**:\n* Source quality scores\n* Evidence consistency\n* Contradiction detection\n* Completeness of analysis\n* Historical accuracy of similar claims\n\n**Thresholds**:\n* < 40%: Too low to publish (needs improvement)\n* 40-60%: Published with \"Low confidence\" warning\n* 60-80%: Published as standard\n* 80-100%: Published as \"High confidence\"\n\n**User Needs served**: UN-1 (Trust assessment), UN-9 (Methodology transparency)\n\n== 5. Automated Risk Scoring ==\n\n**Fulfills**: UN-10 (Manipulation detection), UN-16 (Appropriate review level)\n\n**Replace manual risk tiers with continuous automated scoring**.\n\n=== 5.1 Risk Score Calculation ===\n\n**Factors** (weighted algorithm):\n* **Domain sensitivity**: Medical, legal, safety auto-flagged higher\n* **Potential impact**: Views, citations, spread\n* **Controversy level**: Flags, disputes, edit wars\n* **Uncertainty**: Low confidence, contradictory evidence\n* **Source reliability**: Track record of sources used\n\n**Score**: 0-100 (higher = more risk)\n\n=== 5.2 Automated Actions ===\n\n* **Score > 80**: Flag for moderator review before publication\n* **Score 60-80**: Publish with prominent warnings\n* **Score 40-60**: Publish with standard warnings\n* **Score < 40**: Publish normally\n\n**Continuous monitoring**: Risk score recalculated as new information emerges\n\n**User Needs served**: UN-10 (Detect manipulation tactics), UN-16 (Review status)\n\n== 6. System Improvement Process ==\n\n**Core principle**: Fix the system, not just the data.\n\n=== 6.1 Error Capture ===\n\n**When users flag errors or make corrections**:\n1. What was wrong? (categorize)\n2. What should it have been?\n3. Why did the system fail? (root cause)\n4. How common is this pattern?\n5. Store in ErrorPattern table (improvement queue)\n\n=== 6.2 Continuous Improvement Cycle ===\n\n1. **Review**: Analyze top error patterns\n2. **Develop**: Create fix (prompt, model, validation)\n3. **Test**: Validate fix on sample claims\n4. **Deploy**: Roll out if quality improves\n5. **Re-process**: Automatically update affected claims\n6. **Monitor**: Track quality metrics\n\n=== 6.3 Quality Metrics Dashboard ===\n\n**Track continuously**:\n* Error rate by category\n* Source quality distribution\n* Confidence score trends\n* User flag rate (issues found)\n* Correction acceptance rate\n* Re-work rate\n* Claims processed per hour\n\n**Goal**: continuous improvement in error rate\n\n== 7. Automated Quality Monitoring ==\n\n**Replace manual audit sampling with automated monitoring**.\n\n=== 7.1 Continuous Metrics ===\n\n* **Source quality**: Track record database\n* **Consistency**: Contradiction detection\n* **Clarity**: Readability scores\n* **Completeness**: Field validation\n* **Accuracy**: User corrections tracked\n\n=== 7.2 Anomaly Detection ===\n\n**Automated alerts for**:\n* Sudden quality drops\n* Unusual patterns\n* Contradiction clusters\n* Source reliability changes\n* User behavior anomalies\n\n=== 7.3 Targeted Review ===\n\n* Review only flagged items\n* Random sampling for calibration (not quotas)\n* Learn from corrections to improve automation\n\n== 8. Functional Requirements ==\n\nThis section defines specific features that fulfill user needs.\n\n=== 8.1 Claim Intake & Normalization ===\n\n==== FR1  Claim Intake ====\n\n**Fulfills**: UN-2 (Claim extraction), UN-4 (Quick fact-checking), UN-12 (Submit claims)\n\n* Users submit claims via simple form or API\n* Claims can be text, URL, or image\n* Duplicate detection (semantic similarity)\n* Auto-categorization by domain\n\n==== FR2  Claim Normalization ====\n\n**Fulfills**: UN-2 (Claim verification)\n\n* Standardize to clear assertion format\n* Extract key entities (who, what, when, where)\n* Identify claim type (factual, predictive, evaluative)\n* Link to existing similar claims\n\n==== FR3  Claim Classification ====\n\n**Fulfills**: UN-11 (Filtered research)\n\n* Domain: Politics, Science, Health, etc.\n* Type: Historical fact, current stat, prediction, etc.\n* Risk score: Automated calculation\n* Complexity: Simple, moderate, complex\n\n=== 8.2 Scenario System ===\n\n==== FR4  Scenario Generation ====\n\n**Fulfills**: UN-2 (Context-dependent verification), UN-3 (Article summary with FactHarbor analysis summary), UN-8 (Understanding disagreement)\n\n**Automated scenario creation**:\n* AKEL analyzes claim and generates likely scenarios (use-cases and contexts)\n* Each scenario includes: assumptions, definitions, boundaries, evidence context\n* Users can flag incorrect scenarios\n* System learns from corrections\n\n**Key Concept**: Scenarios represent different interpretations or contexts (e.g., \"Clinical trials with healthy adults\" vs. \"Real-world data with diverse populations\")\n\n==== FR5  Evidence Linking ====\n\n**Fulfills**: UN-5 (Source tracing), UN-7 (Evidence transparency)\n\n* Automated evidence discovery from sources\n* Relevance scoring\n* Contradiction detection\n* Source quality assessment\n\n==== FR6  Scenario Comparison ====\n\n**Fulfills**: UN-3 (Article summary with FactHarbor analysis summary), UN-8 (Understanding disagreement)\n\n* Side-by-side comparison interface\n* Highlight key differences between scenarios\n* Show evidence supporting each scenario\n* Display confidence scores per scenario\n\n=== 8.3 Verdicts & Analysis ===\n\n==== FR7  Automated Verdicts ====\n\n**Fulfills**: UN-1 (Trust score), UN-2 (Verification verdicts), UN-3 (Article summary with FactHarbor analysis summary), UN-13 (Cite verdicts)\n\n* AKEL generates verdict based on evidence within each scenario\n* **Likelihood range** displayed (e.g., \"0.70-0.85 (likely true)\") - NOT binary true/false\n* **Uncertainty factors** explicitly listed (e.g., \"Small sample sizes\", \"Long-term effects unknown\")\n* Confidence score displayed prominently\n* Source quality indicators shown\n* Contradictions noted\n* Uncertainty acknowledged\n\n**Key Innovation**: Detailed probabilistic verdicts with explicit uncertainty, not binary judgments\n\n==== FR8  Time Evolution ====\n\n{{warning}}\n**Status:** Deferred (Not in V1.0)\n\nThis requirement has been **dropped from the current architecture and design**. UCM config versioning and per-job config snapshots provide basic tracking. Full evolution timeline functionality is deferred to future releases beyond V1.0.\n{{/warning}}\n\n**Fulfills**: UN-15 (Verdict evolution timeline)\n\n* Claims and verdicts update as new evidence emerges\n* Version history maintained for all verdicts\n* Changes highlighted\n* Confidence score trends visible\n* Users can see \"as of date X, what did we know?\"\n\n=== 8.4 User Interface & Presentation ===\n\n==== FR12  Two-Panel Summary View (Article Summary with FactHarbor Analysis Summary) ====\n\n**Fulfills**: UN-3 (Article Summary with FactHarbor Analysis Summary)\n\n**Purpose**: Provide side-by-side comparison of what a document claims vs. FactHarbor's complete analysis of its credibility\n\n**Left Panel: Article Summary**:\n* Document title, source, and claimed credibility\n* \"The Big Picture\" - main thesis or position change\n* \"Key Findings\" - structured summary of document's main claims\n* \"Reasoning\" - document's explanation for positions\n* \"Conclusion\" - document's bottom line\n\n**Right Panel: FactHarbor Analysis Summary**:\n* FactHarbor's independent source credibility assessment\n* Claim-by-claim verdicts with confidence scores\n* Methodology assessment (strengths, limitations)\n* Overall verdict on document quality\n* Analysis ID for reference\n\n**Design Principles**:\n* No scrolling required - both panels visible simultaneously\n* Visual distinction between \"what they say\" and \"FactHarbor's analysis\"\n* Color coding for verdicts (supported, uncertain, refuted)\n* Confidence percentages clearly visible\n* Mobile responsive (panels stack vertically on small screens)\n\n**Implementation Notes**:\n* Generated automatically by AKEL for every analyzed document\n* Updates when verdict evolves (maintains version history)\n* Exportable as standalone summary report\n* Shareable via permanent URL\n\n==== FR13  In-Article Claim Highlighting ====\n\n**Fulfills**: UN-17 (In-article claim highlighting)\n\n**Purpose**: Enable readers to quickly assess claim credibility while reading by visually highlighting factual claims with color-coded indicators\n\n==== Visual Example: Article with Highlighted Claims ====\n\n(% class=\"box\" %)\n(((\n**Article: \"New Study Shows Benefits of Mediterranean Diet\"**\n\nA recent study published in the Journal of Nutrition has revealed new findings about the Mediterranean diet.\n\n(% class=\"box successmessage\" style=\"margin:10px 0;\" %)\n(((\n **Researchers found that Mediterranean diet followers had a 25% lower risk of heart disease compared to control groups**\n\n(% style=\"font-size:0.9em; color:#666;\" %)\n WELL SUPPORTED  87% confidence\n[[Click for evidence details ]]\n(%%)\n)))\n\nThe study, which followed 10,000 participants over five years, showed significant improvements in cardiovascular health markers.\n\n(% class=\"box warningmessage\" style=\"margin:10px 0;\" %)\n(((\n **Some experts believe this diet can completely prevent heart attacks**\n\n(% style=\"font-size:0.9em; color:#666;\" %)\n UNCERTAIN  45% confidence\nOverstated - evidence shows risk reduction, not prevention\n[[Click for details ]]\n(%%)\n)))\n\nDr. Maria Rodriguez, lead researcher, recommends incorporating more olive oil, fish, and vegetables into daily meals.\n\n(% class=\"box errormessage\" style=\"margin:10px 0;\" %)\n(((\n **The study proves that saturated fats cause heart disease**\n\n(% style=\"font-size:0.9em; color:#666;\" %)\n REFUTED  15% confidence\nClaim not supported by study design; correlation  causation\n[[Click for counter-evidence ]]\n(%%)\n)))\n\nParticipants also reported feeling more energetic and experiencing better sleep quality, though these were secondary measures.\n)))\n\n**Legend:**\n*  = Well-supported claim (confidence 75%)\n*  = Uncertain claim (confidence 40-74%)\n*  = Refuted/unsupported claim (confidence <40%)\n* Plain text = Non-factual content (context, opinions, recommendations)\n\n==== Tooltip on Hover/Click ====\n\n(% class=\"box infomessage\" %)\n(((\n**FactHarbor Analysis**\n\n**Claim:**\n\"Researchers found that Mediterranean diet followers had a 25% lower risk of heart disease\"\n\n**Verdict:** WELL SUPPORTED\n**Confidence:** 87%\n\n**Evidence Summary:**\n* Meta-analysis of 12 RCTs confirms 23-28% risk reduction\n* Consistent findings across multiple populations\n* Published in peer-reviewed journal (high credibility)\n\n**Uncertainty Factors:**\n* Exact percentage varies by study (20-30% range)\n\n[[View Full Analysis ]]\n)))\n\n**Color-Coding System**:\n* **Green**: Well-supported claims (confidence 75%, strong evidence)\n* **Yellow/Orange**: Uncertain claims (confidence 40-74%, conflicting or limited evidence)\n* **Red**: Refuted or unsupported claims (confidence <40%, contradicted by evidence)\n* **Gray/Neutral**: Non-factual content (opinions, questions, procedural text)\n\n==== Interactive Highlighting Example (Detailed View) ====\n\n(% style=\"width:100%; border-collapse:collapse;\" %)\n|=**Article Text**|=**Status**|=**Analysis**\n|(((A recent study published in the Journal of Nutrition has revealed new findings about the Mediterranean diet.)))|(% style=\"text-align:center;\" %)Plain text|(% style=\"font-style:italic; color:#888;\" %)Context - no highlighting\n|(((//Researchers found that Mediterranean diet followers had a 25% lower risk of heart disease compared to control groups//)))|(% style=\"background-color:#D4EDDA; text-align:center; padding:8px;\" %) **WELL SUPPORTED**|(((\n**87% confidence**\n\nMeta-analysis of 12 RCTs confirms 23-28% risk reduction\n\n[[View Full Analysis]]\n)))\n|(((The study, which followed 10,000 participants over five years, showed significant improvements in cardiovascular health markers.)))|(% style=\"text-align:center;\" %)Plain text|(% style=\"font-style:italic; color:#888;\" %)Methodology - no highlighting\n|(((//Some experts believe this diet can completely prevent heart attacks//)))|(% style=\"background-color:#FFF3CD; text-align:center; padding:8px;\" %) **UNCERTAIN**|(((\n**45% confidence**\n\nOverstated - evidence shows risk reduction, not prevention\n\n[[View Details]]\n)))\n|(((Dr. Rodriguez recommends incorporating more olive oil, fish, and vegetables into daily meals.)))|(% style=\"text-align:center;\" %)Plain text|(% style=\"font-style:italic; color:#888;\" %)Recommendation - no highlighting\n|(((//The study proves that saturated fats cause heart disease//)))|(% style=\"background-color:#F8D7DA; text-align:center; padding:8px;\" %) **REFUTED**|(((\n**15% confidence**\n\nClaim not supported by study; correlation  causation\n\n[[View Counter-Evidence]]\n)))\n\n**Design Notes:**\n* Highlighted claims use italics to distinguish from plain text\n* Color backgrounds match XWiki message box colors (success/warning/error)\n* Status column shows verdict prominently\n* Analysis column provides quick summary with link to details\n\n**User Actions**:\n* **Hover** over highlighted claim  Tooltip appears\n* **Click** highlighted claim  Detailed analysis modal/panel\n* **Toggle** button to turn highlighting on/off\n* **Keyboard**: Tab through highlighted claims\n\n**Interaction Design**:\n* Hover/click on highlighted claim  Show tooltip with:\n * Claim text\n * Verdict (e.g., \"WELL SUPPORTED\")\n * Confidence score (e.g., \"85%\")\n * Brief evidence summary\n * Link to detailed analysis\n* Toggle highlighting on/off (user preference)\n* Adjustable color intensity for accessibility\n\n**Technical Requirements**:\n* Real-time highlighting as page loads (non-blocking)\n* Claim boundary detection (start/end of assertion)\n* Handle nested or overlapping claims\n* Preserve original article formatting\n* Work with various content formats (HTML, plain text, PDFs)\n\n**Performance Requirements**:\n* Highlighting renders within 500ms of page load\n* No perceptible delay in reading experience\n* Efficient DOM manipulation (avoid reflows)\n\n**Accessibility**:\n* Color-blind friendly palette (use patterns/icons in addition to color)\n* Screen reader compatible (ARIA labels for claim credibility)\n* Keyboard navigation to highlighted claims\n\n**Implementation Notes**:\n* Claims extracted and analyzed by AKEL during initial processing\n* Highlighting data stored as annotations with byte offsets\n* Client-side rendering of highlights based on verdict data\n* Mobile responsive (tap instead of hover)\n\n=== 8.5 Workflow & Moderation ===\n\n==== FR9  Publication Workflow ====\n\n**Fulfills**: UN-1 (Fast access to verified content), UN-16 (Clear review status)\n\n**Simple flow**:\n1. Claim submitted\n2. AKEL processes (automated)\n3. If confidence > threshold: Publish (labeled as AI-generated)\n4. If confidence < threshold: Flag for improvement\n5. If risk score > threshold: Flag for moderator\n\n**No multi-stage approval process**\n\n==== FR10  Moderation ====\n\n**Focus on abuse, not routine quality**:\n* Automated abuse detection\n* Moderators handle flags\n* Quick response to harmful content\n* Minimal involvement in routine content\n\n==== FR11  Audit Trail ====\n\n**Fulfills**: UN-14 (API access to histories), UN-15 (Evolution tracking)\n\n* All UCM config changes logged\n* UCM config version history public\n* Moderation decisions documented\n* System improvements tracked\n\n== 9. Non-Functional Requirements ==\n\n=== 9.1 NFR1  Performance ===\n\n**Fulfills**: UN-4 (Fast fact-checking), UN-11 (Responsive filtering)\n\n* Claim processing: < 30 seconds\n* Search response: < 2 seconds\n* Page load: < 3 seconds\n* 99% uptime\n\n=== 9.2 NFR2  Scalability ===\n\n**Fulfills**: UN-14 (API access at scale)\n\n* Handle 10,000 claims initially\n* Scale to 1M+ claims\n* Support 100K+ concurrent users\n* Automated processing scales linearly\n\n=== 9.3 NFR3  Transparency ===\n\n**Fulfills**: UN-7 (Evidence transparency), UN-9 (Methodology transparency), UN-13 (Citable verdicts), UN-15 (Evolution visibility)\n\n* All algorithms open source\n* All data exportable\n* All decisions documented\n* Quality metrics public\n\n=== 9.4 NFR4  Security & Privacy ===\n\n* Follow [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n* Secure authentication\n* Data encryption\n* Regular security audits\n\n=== 9.5 NFR5  Maintainability ===\n\n* Modular architecture\n* Automated testing\n* Continuous integration\n* Comprehensive documentation\n\n=== NFR11: AKEL Quality Assurance Framework ===\n\n**Fulfills:** AI safety, IFCN methodology transparency\n\n**Specification:**\n\nMulti-layer AI quality gates to detect hallucinations, low-confidence results, and logical inconsistencies.\n\n==== Quality Gate 1: Claim Extraction Validation ====\n\n**Purpose:** Ensure extracted claims are factual assertions (not opinions/predictions)\n\n**Checks:**\n1. **Factual Statement Test:** Is this verifiable? (Yes/No)\n2. **Opinion Detection:** Contains hedging language? (\"I think\", \"probably\", \"best\")\n3. **Future Prediction Test:** Makes claims about future events?\n4. **Specificity Score:** Contains specific entities, numbers, dates?\n\n**Thresholds:**\n* Factual: Must be \"Yes\"\n* Opinion markers: <2 hedging phrases\n* Specificity: 3 specific elements\n\n**Action if Failed:** Flag as \"Non-verifiable\", do NOT generate verdict\n\n==== Quality Gate 2: Evidence Relevance Validation ====\n\n**Purpose:** Ensure AI-linked evidence actually relates to claim\n\n**Checks:**\n1. **Semantic Similarity Score:** Evidence vs. claim (embeddings)\n2. **Entity Overlap:** Shared people/places/things?\n3. **Topic Relevance:** Discusses claim subject?\n\n**Thresholds:**\n* Similarity: 0.6 (cosine similarity)\n* Entity overlap: 1 shared entity\n* Topic relevance: 0.5\n\n**Action if Failed:** Discard irrelevant evidence\n\n==== Quality Gate 3: Scenario Coherence Check ====\n\n**Purpose:** Validate scenario assumptions are logical and complete\n\n**Checks:**\n1. **Completeness:** All required fields populated\n2. **Internal Consistency:** Assumptions don't contradict\n3. **Distinguishability:** Scenarios meaningfully different\n\n**Thresholds:**\n* Required fields: 100%\n* Contradiction score: <0.3\n* Scenario similarity: <0.8\n\n**Action if Failed:** Merge duplicates, reduce confidence -20%\n\n==== Quality Gate 4: Verdict Confidence Assessment ====\n\n**Purpose:** Only publish high-confidence verdicts\n\n**Checks:**\n1. **Evidence Count:** Minimum 2 sources\n2. **Source Quality:** Average reliability 0.6\n3. **Evidence Agreement:** Supporting vs. contradicting 0.6\n4. **Uncertainty Factors:** Hedging in reasoning\n\n**Confidence Tiers:**\n* **HIGH (80-100%):** 3 sources, 0.7 quality, 80% agreement\n* **MEDIUM (50-79%):** 2 sources, 0.6 quality, 60% agreement\n* **LOW (0-49%):** <2 sources OR low quality/agreement\n* **INSUFFICIENT:** <2 sources  DO NOT PUBLISH\n\n**Implementation Phases:**\n* **POC1:** Gates 1 & 4 only (basic validation)\n* **POC2:** All 4 gates (complete framework)\n* **V1.0:** Hardened with <5% hallucination rate\n\n**Acceptance Criteria:**\n*  All gates operational\n*  Hallucination rate <5%\n*  Quality metrics public\n\n=== NFR12: Security Controls ===\n\n**Fulfills:** Data protection, system integrity, user privacy, production readiness\n\n**Purpose:** Protect FactHarbor systems, user data, and operations from security threats, ensuring production-grade security posture.\n\n**Specification:**\n\n==== API Security ====\n\n**Rate Limiting:**\n* **Analysis endpoints:** 100 requests/hour per IP\n* **Read endpoints:** 1,000 requests/hour per IP\n* **Search:** 500 requests/hour per IP\n* **Authenticated users:** 5x higher limits\n* **Burst protection:** Max 10 requests/second\n\n**Authentication & Authorization:**\n* **API Keys:** Required for programmatic access\n* **JWT tokens:** For user sessions (1-hour expiry)\n* **OAuth2:** For third-party integrations\n* **Role-Based Access Control (RBAC):**\n * Reader (Guest): Browse, search, view published analyses\n * User (Registered): Submit URLs/text (rate-limited), flag issues\n * UCM Administrator: Manage UCM configuration, view audit trail\n * Moderator: Handle abuse, manage community health\n * Admin: System infrastructure, user management\n\n**CORS Policies:**\n* Whitelist approved domains only\n* No wildcard origins in production\n* Credentials required for sensitive endpoints\n\n**Input Sanitization:**\n* Validate all user input against schemas\n* Sanitize HTML/JavaScript in text submissions\n* Prevent SQL injection (use parameterized queries)\n* Prevent command injection (no shell execution of user input)\n* Max request size: 10MB\n* File upload restrictions: Whitelist file types, scan for malware\n\n---\n\n==== Data Security ====\n\n**Encryption at Rest:**\n* Database encryption using AES-256\n* Encrypted backups\n* Key management via cloud provider KMS (AWS KMS, Google Cloud KMS)\n* Regular key rotation (90-day cycle)\n\n**Encryption in Transit:**\n* HTTPS/TLS 1.3 only (no TLS 1.0/1.1)\n* Strong cipher suites only\n* HSTS (HTTP Strict Transport Security) enabled\n* Certificate pinning for mobile apps\n\n**Secure Credential Storage:**\n* Passwords hashed with bcrypt (cost factor 12+)\n* API keys encrypted in database\n* Secrets stored in environment variables (never in code)\n* Use secrets manager (AWS Secrets Manager, HashiCorp Vault)\n\n**Data Privacy:**\n* Minimal data collection (privacy by design)\n* User data deletion on request (GDPR compliance)\n* PII encryption in database\n* Anonymize logs (no PII in log files)\n\n---\n\n==== Application Security ====\n\n**OWASP Top 10 Compliance:**\n\n1. **Broken Access Control:** RBAC implementation, path traversal prevention\n2. **Cryptographic Failures:** Strong encryption, secure key management\n3. **Injection:** Parameterized queries, input validation\n4. **Insecure Design:** Security review of all features\n5. **Security Misconfiguration:** Hardened defaults, security headers\n6. **Vulnerable Components:** Dependency scanning (see below)\n7. **Authentication Failures:** Strong password policy, MFA support\n8. **Data Integrity Failures:** Signature verification, checksums\n9. **Security Logging Failures:** Comprehensive audit logs\n10. **Server-Side Request Forgery:** URL validation, whitelist domains\n\n**Security Headers:**\n* `Content-Security-Policy`: Strict CSP to prevent XSS\n* `X-Frame-Options`: DENY (prevent clickjacking)\n* `X-Content-Type-Options`: nosniff\n* `Referrer-Policy`: strict-origin-when-cross-origin\n* `Permissions-Policy`: Restrict browser features\n\n**Dependency Vulnerability Scanning:**\n* **Tools:** Snyk, Dependabot, npm audit, pip-audit\n* **Frequency:** Automated scans (continuous)\n* **Action:** Patch critical vulnerabilities within 24 hours\n* **Policy:** No known high/critical CVEs in production\n\n**Security Audits:**\n* **Internal:** Periodic security reviews\n* **External:** Penetration testing by certified firm\n* **Bug Bounty:** Public bug bounty program (V1.1+)\n* **Compliance:** SOC 2 Type II certification target (V1.5)\n\n---\n\n==== Operational Security ====\n\n**DDoS Protection:**\n* CloudFlare or AWS Shield\n* Rate limiting at CDN layer\n* Automatic IP blocking for abuse patterns\n\n**Monitoring & Alerting:**\n* Real-time security event monitoring\n* Alerts for:\n * Failed login attempts (>5 in 10 minutes)\n * API abuse patterns\n * Unusual data access patterns\n * Security scan detections\n* Integration with SIEM (Security Information and Event Management)\n\n**Incident Response:**\n* Documented incident response plan\n* Security incident classification (P1-P4)\n* On-call rotation for security issues\n* Post-mortem for all security incidents\n* Public disclosure policy (coordinated disclosure)\n\n**Backup & Recovery:**\n* Encrypted backups (automated)\n* 30-day retention period\n* Tested recovery procedures (periodic)\n* Disaster recovery plan (RTO: 4 hours, RPO: 1 hour)\n\n---\n\n==== Compliance & Standards ====\n\n**GDPR Compliance:**\n* User consent management\n* Right to access data\n* Right to deletion\n* Data portability\n* Privacy policy published\n\n**Accessibility:**\n* WCAG 2.1 AA compliance\n* Screen reader compatibility\n* Keyboard navigation\n* Alt text for images\n\n**Browser Support:**\n* Modern browsers only (Chrome/Edge/Firefox/Safari latest 2 versions)\n* No IE11 support\n\n**Acceptance Criteria:**\n\n*  Passes OWASP ZAP security scan (no high/critical findings)\n*  All dependencies with known vulnerabilities patched\n*  Penetration test completed with no critical findings\n*  Rate limiting blocks abuse attempts\n*  Encryption at rest and in transit verified\n*  Security headers scored A+ on securityheaders.com\n*  Incident response plan documented and tested\n*  95% uptime over 30-day period\n\n=== NFR13: Quality Metrics Transparency ===\n\n**Fulfills:** User trust, transparency, continuous improvement, IFCN methodology transparency\n\n**Purpose:** Provide transparent, measurable quality metrics that demonstrate AKEL's performance and build user trust in automated fact-checking.\n\n**Specification:**\n\n==== Component: Public Quality Dashboard ====\n\n**Core Metrics to Display:**\n\n**1. Verdict Quality Metrics**\n\n**TIGERScore (Fact-Checking Quality):**\n* **Definition:** Measures how well generated verdicts match expert fact-checker judgments\n* **Scale:** 0-100 (higher is better)\n* **Calculation:** Using TIGERScore framework (Truth-conditional accuracy, Informativeness, Generality, Evaluativeness, Relevance)\n* **Target:** Average 80 for production release\n* **Display:**\n{{code}}\nVerdict Quality (TIGERScore):\nOverall: 84.2  (+2.1 from last month)\n\nDistribution:\n Excellent (>80): 67%\n Good (60-80): 28%\n Needs Improvement (<60): 5%\n\nTrend: [Graph showing improvement over time]\n{{/code}}\n\n**2. Hallucination & Faithfulness Metrics**\n\n**AlignScore (Faithfulness to Evidence):**\n* **Definition:** Measures how well verdicts align with actual evidence content\n* **Scale:** 0-1 (higher is better)\n* **Purpose:** Detect AI hallucinations (making claims not supported by evidence)\n* **Target:** Average 0.85, hallucination rate <5%\n* **Display:**\n{{code}}\nEvidence Faithfulness (AlignScore):\nAverage: 0.87  (-0.02 from last month)\n\nHallucination Rate: 4.2%\n - Claims without evidence support: 3.1%\n - Misrepresented evidence: 1.1%\n\nAction: Prompt engineering review scheduled\n{{/code}}\n\n**3. Evidence Quality Metrics**\n\n**Source Reliability:**\n* Average source quality score (0-1 scale)\n* Distribution of high/medium/low quality sources\n* Publisher track record trends\n\n**Evidence Coverage:**\n* Average number of sources per claim\n* Percentage of claims with 2 sources (EFCSN minimum)\n* Geographic diversity of sources\n\n**Display:**\n{{code}}\nEvidence Quality:\n\nAverage Sources per Claim: 4.2\nClaims with 2 sources: 94% (EFCSN compliant)\n\nSource Quality Distribution:\n High quality (>0.8): 48%\n Medium quality (0.5-0.8): 43%\n Low quality (<0.5): 9%\n\nGeographic Diversity: 23 countries represented\n{{/code}}\n\n**4. Contributor Consensus Metrics** (when human reviewers involved)\n\n**Inter-Rater Reliability (IRR):**\n* **Calculation:** Cohen's Kappa or Fleiss' Kappa for multiple raters\n* **Scale:** 0-1 (higher is better)\n* **Interpretation:**\n * >0.8: Almost perfect agreement\n * 0.6-0.8: Substantial agreement\n * 0.4-0.6: Moderate agreement\n * <0.4: Poor agreement\n* **Target:** Maintain 0.7 (substantial agreement)\n\n**Display:**\n{{code}}\nContributor Consensus:\n\nInter-Rater Reliability (IRR): 0.73 (Substantial agreement)\n - Verdict agreement: 78%\n - Evidence quality agreement: 71%\n - Scenario structure agreement: 69%\n\nCases requiring moderator review: 12\nModerator override rate: 8%\n{{/code}}\n\n---\n\n==== Quality Dashboard Implementation ====\n\n**Dashboard Location:** `/quality-metrics`\n\n**Update Frequency:**\n* **POC2:** Manual updates\n* **Beta 0:** Automated updates\n* **V1.0:** Real-time metrics\n\n**Dashboard Sections:**\n\n1. **Overview:** Key metrics at a glance\n2. **Verdict Quality:** TIGERScore trends and distributions\n3. **Evidence Analysis:** Source quality and coverage\n4. **AI Performance:** Hallucination rates, AlignScore\n5. **Human Oversight:** Contributor consensus, review rates\n6. **System Health:** Processing times, error rates, uptime\n\n**Example Dashboard Layout:**\n\n{{code}}\n\n FactHarbor Quality Metrics Last updated: \n Public Dashboard 2 hours ago \n\n\n KEY METRICS\n\nTIGERScore (Verdict Quality): 84.2  (+2.1)\nAlignScore (Faithfulness): 0.87  (-0.02)\nHallucination Rate: 4.2%  (Target: <5%)\nAverage Sources per Claim: 4.2  (+0.3)\n\n TRENDS (30 days)\n\n[Graph: TIGERScore trending upward]\n[Graph: Hallucination rate declining]\n[Graph: Evidence quality stable]\n\n IMPROVEMENT TARGETS\n\n1. Reduce hallucination rate to <3% (Current: 4.2%)\n2. Increase TIGERScore average to >85 (Current: 84.2)\n3. Maintain IRR >0.75 (Current: 0.73)\n\n DETAILED REPORTS\n\n Quality Report (PDF)\n Methodology Documentation\n AKEL Performance Analysis\n Contributor Agreement Analysis\n\n{{/code}}\n\n---\n\n==== Continuous Improvement Feedback Loop ====\n\n**How Metrics Inform AKEL Improvements:**\n\n1. **Identify Weak Areas:**\n * Low TIGERScore  Review prompt engineering\n * High hallucination  Strengthen evidence grounding\n * Low IRR  Clarify evaluation criteria\n\n2. **A/B Testing Integration:**\n * Test prompt variations\n * Measure impact on quality metrics\n * Deploy winners automatically\n\n3. **Alert Thresholds:**\n * TIGERScore drops below 75  Alert team\n * Hallucination rate exceeds 7%  Pause auto-publishing\n * IRR below 0.6  Moderator training needed\n\n4. **Periodic Quality Reviews:**\n * Analyze trends\n * Identify systematic issues\n * Plan prompt improvements\n * Update AKEL models\n\n---\n\n==== Metric Calculation Details ====\n\n**TIGERScore Implementation:**\n* Reference: https://github.com/TIGER-AI-Lab/TIGERScore\n* Input: Generated verdict + reference verdict (from expert)\n* Output: 0-100 score across 5 dimensions\n* Requires: Test set of expert-reviewed claims (minimum 100)\n\n**AlignScore Implementation:**\n* Reference: https://github.com/yuh-zha/AlignScore\n* Input: Generated verdict + source evidence text\n* Output: 0-1 faithfulness score\n* Calculation: Semantic alignment between claim and evidence\n\n**Source Quality Scoring:**\n* Use existing source reliability database (e.g., NewsGuard, MBFC)\n* Factor in: Publication history, corrections record, transparency\n* Scale: 0-1 (weighted average across sources)\n\n---\n\n==== Integration Points ====\n\n* **NFR11: AKEL Quality Assurance** - Metrics validate quality gate effectiveness\n* **FR49: A/B Testing** - Metrics measure test success\n* **FR11: Audit Trail** - Source of quality data\n* **NFR3: Transparency** - Public metrics build trust\n\n**Acceptance Criteria:**\n\n*  All core metrics implemented and calculating correctly\n*  Dashboard updates automatically\n*  Alerts trigger when metrics degrade beyond thresholds\n*  Quality report auto-generates\n*  Dashboard is publicly accessible (no login required)\n*  Mobile-responsive dashboard design\n*  Metrics inform periodic AKEL improvement planning\n\n== 13. Requirements Traceability ==\n\nFor full traceability matrix showing which requirements fulfill which user needs, see:\n\n* [[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]] - Section 8 includes comprehensive mapping tables\n\n== 14. Related Pages ==\n\n**Non-Functional Requirements (see Section 9):**\n* [[NFR11  AKEL Quality Assurance Framework>>#NFR11]]\n* [[NFR12  Security Controls>>#NFR12]]\n* [[NFR13  Quality Metrics Transparency>>#NFR13]]\n\n**Other Requirements:**\n* [[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]]\n* [[V1.0 Requirements>>FactHarbor.Product Development.Planning.V10.WebHome]]\n* [[Gap Analysis>>FactHarbor.Product Development.Requirements.GapAnalysis]]\n\n* **[[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]]** - What users need (drives these requirements)\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] - How requirements are implemented\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] - Data structures supporting requirements\n* [[Workflows>>FactHarbor.Product Development.Specification.Workflows.WebHome]] - User interaction workflows\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] - AI system fulfilling automation requirements\n* [[Global Rules>>FactHarbor.Organisation.How-We-Work-Together.GlobalRules.WebHome]]\n* [[Privacy Policy>>FactHarbor.Organisation.How-We-Work-Together.Privacy-Policy]]\n\n= V0.9.70 Additional Requirements =\n\n== Functional Requirements (Additional) ==\n\n=== FR44: ClaimReview Schema Implementation ===\n\n**Fulfills:** UN-13 (Cite FactHarbor Verdicts), UN-14 (API Access for Integration), UN-26 (Search Engine Visibility)\n\n**Purpose:** Generate valid ClaimReview structured data for every published analysis to enable Google/Bing search visibility and fact-check discovery.\n\n**Specification:**\n\n==== Component: Schema.org Markup Generator ====\n\nFactHarbor must generate valid ClaimReview structured data following Schema.org specifications for every published claim analysis.\n\n**Required JSON-LD Schema:**\n\n{{code language=\"json\"}}\n{\n \"@context\": \"https://schema.org\",\n \"@type\": \"ClaimReview\",\n \"datePublished\": \"YYYY-MM-DD\",\n \"url\": \"https://factharbor.org/claims/{claim_id}\",\n \"claimReviewed\": \"The exact claim text\",\n \"author\": {\n \"@type\": \"Organization\",\n \"name\": \"FactHarbor\",\n \"url\": \"https://factharbor.org\"\n },\n \"reviewRating\": {\n \"@type\": \"Rating\",\n \"ratingValue\": \"1-5\",\n \"bestRating\": \"5\",\n \"worstRating\": \"1\",\n \"alternateName\": \"FactHarbor likelihood score\"\n },\n \"itemReviewed\": {\n \"@type\": \"Claim\",\n \"author\": {\n \"@type\": \"Person\",\n \"name\": \"Claim author if known\"\n },\n \"datePublished\": \"YYYY-MM-DD if known\",\n \"appearance\": {\n \"@type\": \"CreativeWork\",\n \"url\": \"Original claim URL if from article\"\n }\n }\n}\n{{/code}}\n\n**FactHarbor-Specific Mapping:**\n\n**Likelihood Score to Rating Scale:**\n* 80-100% likelihood  5 (Highly Supported)\n* 60-79% likelihood  4 (Supported)\n* 40-59% likelihood  3 (Mixed/Uncertain)\n* 20-39% likelihood  2 (Questionable)\n* 0-19% likelihood  1 (Refuted)\n\n**Multiple Scenarios Handling:**\n* If claim has multiple scenarios with different verdicts, generate **separate ClaimReview** for each scenario\n* Add `disambiguatingDescription` field explaining scenario context\n* Example: \"Scenario: If interpreted as referring to 2023 data...\"\n\n==== Implementation Requirements ====\n\n1. **Auto-generate** on claim publication\n2. **Embed** in HTML `<head>` section as JSON-LD script\n3. **Validate** against Schema.org validator before publishing\n4. **Submit** to Google Search Console for indexing\n5. **Update** automatically when verdict changes (integrate with FR8: Time Evolution)\n\n==== Integration Points ====\n\n* **FR7: Automated Verdicts** - Source of rating data and claim text\n* **FR8: Time Evolution** - Triggers schema updates when verdicts change\n* **FR11: Audit Trail** - Logs all schema generation and update events\n\n==== Resources ====\n\n* ClaimReview Project: https://www.claimreviewproject.com\n* Schema.org ClaimReview: https://schema.org/ClaimReview\n* Google Fact Check Guidelines: https://developers.google.com/search/docs/appearance/fact-check\n\n**Acceptance Criteria:**\n\n*  Passes Google Structured Data Testing Tool\n*  Appears in Google Fact Check Explorer within 48 hours of publication\n*  Valid JSON-LD syntax (no errors)\n*  All required fields populated with correct data types\n*  Handles multi-scenario claims correctly (separate ClaimReview per scenario)\n\n=== FR45: User Corrections Notification System ===\n\n**Fulfills:** IFCN Principle 5 (Open & Honest Corrections), EFCSN compliance\n\n**Purpose:** When any claim analysis is corrected, notify users who previously viewed the claim to maintain transparency and build trust.\n\n**Specification:**\n\n==== Component: Corrections Visibility Framework ====\n\n**Correction Types:**\n\n1. **Major Correction:** Verdict changes category (e.g., \"Supported\"  \"Refuted\")\n2. **Significant Correction:** Likelihood score changes >20%\n3. **Minor Correction:** Evidence additions, source quality updates\n4. **Scenario Addition:** New scenario added to existing claim\n\n==== Notification Mechanisms ====\n\n**1. In-Page Banner:**\n\nDisplay prominent banner on claim page:\n\n{{code}}\n[!] CORRECTION NOTICE\nThis analysis was updated on [DATE]. [View what changed] [Dismiss]\n\nMajor changes:\n Verdict changed from \"Likely True (75%)\" to \"Uncertain (45%)\"\n New contradicting evidence added from [Source]\n Scenario 2 updated with additional context\n\n[See full correction log]\n{{/code}}\n\n**2. Correction Log Page:**\n\n* Public changelog at `/claims/{id}/corrections`\n* Displays for each correction:\n * Date/time of correction\n * What changed (before/after comparison)\n * Why changed (reason if provided)\n * Who made change (AKEL auto-update vs. contributor override)\n\n**3. Email Notifications (opt-in):**\n\n* Send to users who bookmarked or shared the claim\n* Subject: \"FactHarbor Correction: [Claim title]\"\n* Include summary of changes\n* Link to updated analysis\n\n**4. RSS/API Feed:**\n\n* Corrections feed at `/corrections.rss`\n* API endpoint: `GET /api/corrections?since={timestamp}`\n* Enables external monitoring by journalists and researchers\n\n==== Display Rules ====\n\n* Show banner on **ALL pages** displaying the claim (search results, related claims, embeddings)\n* Banner persists for **30 days** after correction\n* **\"Corrections\" count badge** on claim card\n* **Timestamp** on every verdict: \"Last updated: [datetime]\"\n\n==== IFCN Compliance Requirements ====\n\n* Corrections policy published at `/corrections-policy`\n* User can report suspected errors via `/report-error/{claim_id}`\n* Link to IFCN complaint process (if FactHarbor becomes signatory)\n* **Scrupulous transparency:** Never silently edit analyses\n\n==== Integration Points ====\n\n* **FR8: Time Evolution** - Triggers corrections when verdicts change\n* **FR11: Audit Trail** - Source of correction data and change history\n* **NFR3: Transparency** - Public correction log demonstrates commitment\n\n**Acceptance Criteria:**\n\n*  Banner appears within 60 seconds of correction\n*  Correction log is permanent and publicly accessible\n*  Email notifications deliver within 5 minutes\n*  RSS feed updates in real-time\n*  Mobile-responsive banner design\n*  Accessible (screen reader compatible)\n\n=== FR46: Image Verification System ===\n\n**Fulfills:** UN-27 (Visual Claim Verification)\n\n**Purpose:** Verify authenticity and context of images shared with claims to detect manipulation, misattribution, and out-of-context usage.\n\n**Specification:**\n\n==== Component: Multi-Method Image Verification ====\n\n**Method 1: Reverse Image Search**\n\n**Purpose:** Find earlier uses of the image to verify context\n\n**Implementation:**\n* Integrate APIs:\n * **Google Vision AI** (reverse search)\n * **TinEye** (oldest known uses)\n * **Bing Visual Search** (broad coverage)\n\n**Process:**\n1. Extract image from claim or user upload\n2. Query multiple reverse search services\n3. Analyze results for:\n * Earliest known publication\n * Original context (what was it really showing?)\n * Publication timeline\n * Geographic spread\n\n**Output:**\n{{code}}\nReverse Image Search Results:\n\nEarliest known use: 2019-03-15 (5 years before claim)\nOriginal context: \"Photo from 2019 flooding in Mumbai\"\nThis claim uses it for: \"2024 hurricane damage in Florida\"\n\n Image is OUT OF CONTEXT\n\nFound in 47 other articles:\n 2019-03-15: Mumbai floods (original)\n 2020-07-22: Bangladesh monsoon\n 2024-10-15: Current claim (misattributed)\n\n[View full timeline]\n{{/code}}\n\n---\n\n**Method 2: AI Manipulation Detection**\n\n**Purpose:** Detect deepfakes, face swaps, and digital alterations\n\n**Implementation:**\n* Integrate detection services:\n * **Sensity AI** (deepfake detection)\n * **Reality Defender** (multimodal analysis)\n * **AWS Rekognition** (face detection inconsistencies)\n\n**Detection Categories:**\n1. **Face Manipulation:**\n * Deepfake face swaps\n * Expression manipulation\n * Identity replacement\n\n2. **Image Manipulation:**\n * Copy-paste artifacts\n * Clone stamp detection\n * Content-aware fill detection\n * JPEG compression inconsistencies\n\n3. **AI Generation:**\n * Detect fully AI-generated images\n * Identify generation artifacts\n * Check for model signatures\n\n**Confidence Scoring:**\n* **HIGH (80-100%):** Strong evidence of manipulation\n* **MEDIUM (50-79%):** Suspicious artifacts detected\n* **LOW (0-49%):** Minor inconsistencies or inconclusive\n\n**Output:**\n{{code}}\nManipulation Analysis:\n\nFace Manipulation: LOW RISK (12%)\nImage Editing: MEDIUM RISK (64%)\n  Clone stamp artifacts detected in sky region\n  JPEG compression inconsistent between objects\n\nAI Generation: LOW RISK (8%)\n\n Possible manipulation detected. Manual review recommended.\n{{/code}}\n\n---\n\n**Method 3: Metadata Analysis (EXIF)**\n\n**Purpose:** Extract technical details that may reveal manipulation or misattribution\n\n**Extracted Data:**\n* **Camera/Device:** Make, model, software\n* **Timestamps:** Original date, modification dates\n* **Location:** GPS coordinates (if present)\n* **Editing History:** Software used, edit count\n* **File Properties:** Resolution, compression, format conversions\n\n**Red Flags:**\n* Metadata completely stripped (suspicious)\n* Timestamp conflicts with claimed date\n* GPS location conflicts with claimed location\n* Multiple edit rounds (hiding something?)\n* Creation date after modification date (impossible)\n\n**Output:**\n{{code}}\nImage Metadata:\n\nCamera: iPhone 14 Pro\nOriginal date: 2023-08-12 14:32:15\nLocation: 40.7128N, 74.0060W (New York City)\nModified: 2024-10-15 08:45:22\nSoftware: Adobe Photoshop 2024\n\n Location conflicts with claim\nClaim says: \"Taken in Los Angeles\"\nEXIF says: New York City\n\n Edited 14 months after capture\n{{/code}}\n\n---\n\n==== Verification Workflow ====\n\n**Automatic Triggers:**\n1. User submits claim with image\n2. Article being analyzed contains images\n3. Social media post includes photos\n\n**Process:**\n1. Extract images from content\n2. Run all 3 verification methods in parallel\n3. Aggregate results into confidence score\n4. Generate human-readable summary\n5. Display prominently in analysis\n\n**Display Integration:**\n\nShow image verification panel in claim analysis:\n\n{{code}}\n IMAGE VERIFICATION\n\n[Image thumbnail]\n\n Reverse Search: Original context verified\n Manipulation: Possible editing detected (64% confidence)\n Metadata: Consistent with claim details\n\nOverall Assessment: CAUTION ADVISED\nThis image may have been edited. Original context appears accurate.\n\n[View detailed analysis]\n{{/code}}\n\n==== Integration Points ====\n\n* **FR7: Automated Verdicts** - Image verification affects claim credibility\n* **FR4: Analysis Summary** - Image findings included in summary\n* **UN-27: Visual Claim Verification** - Direct fulfillment\n\n==== Cost Considerations ====\n\n**API Costs (estimated per image):**\n* Google Vision AI: $0.001-0.003\n* TinEye: $0.02 (commercial API)\n* Sensity AI: $0.05-0.10\n* AWS Rekognition: $0.001-0.002\n\n**Total per image:** ~$0.07-0.15\n\n**Mitigation Strategies:**\n* Cache results for duplicate images\n* Use free tier quotas where available\n* Prioritize higher-value claims for deep analysis\n* Offer premium verification as paid tier\n\n**Acceptance Criteria:**\n\n*  Reverse image search finds original sources\n*  Manipulation detection accuracy >80% on test dataset\n*  EXIF extraction works for major image formats (JPEG, PNG, HEIC)\n*  Results display within 10 seconds\n*  Mobile-friendly image comparison interface\n*  False positive rate <15%\n\n=== FR47: Archive.org Integration ===\n\n**Importance:** CRITICAL \n**Fulfills:** Evidence persistence, FR5 (Evidence linking) \n\n**Purpose:** Ensure evidence remains accessible even if original sources are deleted.\n\n**Specification:**\n\n**Automatic Archiving:**\n\nWhen AKEL links evidence:\n1. Check if URL already archived (Wayback Machine API)\n2. If not, submit for archiving (Save Page Now API)\n3. Store both original URL and archive URL\n4. Display both to users\n\n**Archive Display:**\n\n{{code}}\nEvidence Source: [Original URL]\nArchived: [Archive.org URL] (Captured: [date])\n\n[View Original] [View Archive]\n{{/code}}\n\n**Fallback Logic:**\n\n* If original URL unavailable  Auto-redirect to archive\n* If archive unavailable  Display warning\n* If both unavailable  Flag for manual review\n\n**API Integration:**\n\n* Use Wayback Machine Availability API\n* Use Save Page Now API (SPNv2)\n* Rate limiting: 15 requests/minute (Wayback limit)\n\n**Acceptance Criteria:**\n\n*  All evidence URLs auto-archived\n*  Archive links displayed to users\n*  Fallback to archive if original unavailable\n*  API rate limits respected\n*  Archive status visible in evidence display\n\n== Category 4: Community Safety ===== FR48: Contributor Safety Framework ===\n\n**Importance:** CRITICAL \n**Fulfills:** UN-28 (Safe contribution environment) \n\n**Purpose:** Protect contributors from harassment, doxxing, and coordinated attacks.\n\n**Specification:**\n\n**1. Privacy Protection:**\n\n* **Optional Pseudonymity:** Contributors can use pseudonyms\n* **Email Privacy:** Emails never displayed publicly\n* **Profile Privacy:** Contributors control what's public\n* **IP Logging:** Only for abuse prevention, not public\n\n**2. Harassment Prevention:**\n\n* **Automated Toxicity Detection:** Flag abusive comments\n* **Personal Information Detection:** Auto-block doxxing attempts\n* **Coordinated Attack Detection:** Identify brigading patterns\n* **Rapid Response:** Moderator alerts for harassment\n\n**3. Safety Features:**\n\n* **Block Users:** Contributors can block harassers\n* **Private Contributions:** Option to contribute anonymously\n* **Report Harassment:** One-click harassment reporting\n* **Safety Resources:** Links to support resources\n\n**4. Moderator Tools:**\n\n* **Quick Ban:** Immediately block abusers\n* **Pattern Detection:** Identify coordinated attacks\n* **Appeal Process:** Fair review of moderation actions\n* **Escalation:** Serious threats escalated to authorities\n\n**5. UCM Administrator Protection:**\n\n* **Enhanced Privacy:** Additional protection for system administrators\n* **Verification:** Optional identity verification (not public)\n* **Legal Support:** Resources for administrators facing legal threats\n\n**Acceptance Criteria:**\n\n*  Pseudonyms supported\n*  Toxicity detection active\n*  Doxxing auto-blocked\n*  Harassment reporting functional\n*  Moderator tools implemented\n*  Safety policy published\n\n== Category 5: Continuous Improvement ===== FR49: A/B Testing Framework ===\n\n**Importance:** CRITICAL \n**Fulfills:** Continuous system improvement \n\n**Purpose:** Test and measure improvements to AKEL prompts, algorithms, and workflows.\n\n**Specification:**\n\n**Test Capabilities:**\n\n1. **Prompt Variations:**\n * Test different claim extraction prompts\n * Test different verdict generation prompts\n * Measure: Accuracy, clarity, completeness\n\n2. **Algorithm Variations:**\n * Test different source scoring algorithms\n * Test different confidence calculations\n * Measure: Audit accuracy, user satisfaction\n\n3. **Workflow Variations:**\n * Test different quality gate thresholds\n * Test different risk tier assignments\n * Measure: Publication rate, quality scores\n\n**Implementation:**\n\n* **Traffic Split:** 50/50 or 90/10 splits\n* **Randomization:** Consistent per claim (not per user)\n* **Metrics Collection:** Automatic for all variants\n* **Statistical Significance:** Minimum sample size calculation\n* **Rollout:** Winner promoted to 100% traffic\n\n**A/B Test Workflow:**\n\n{{code}}\n1. Hypothesis: \"New prompt improves claim extraction\"\n2. Design test: Control vs. Variant\n3. Define metrics: Extraction accuracy, completeness\n4. Run test: minimum 100 claims each\n5. Analyze results: Statistical significance?\n6. Decision: Deploy winner or iterate\n{{/code}}\n\n**Acceptance Criteria:**\n\n*  A/B testing framework implemented\n*  Can test prompt variations\n*  Can test algorithm variations\n*  Metrics automatically collected\n*  Statistical significance calculated\n*  Results inform system improvements\n\n=== FR54: Evidence Deduplication ===\n\n**Importance:** CRITICAL (POC2/Beta) \n**Fulfills:** Accurate evidence counting, quality metrics \n\n**Purpose:** Avoid counting the same source multiple times when it appears in different forms.\n\n**Specification:**\n\n**Deduplication Logic:**\n\n1. **URL Normalization:**\n * Remove tracking parameters (?utm_source=...)\n * Normalize http/https\n * Normalize www/non-www\n * Handle redirects\n\n2. **Content Similarity:**\n * If two sources have >90% text similarity  Same source\n * If one is subset of other  Same source\n * Use fuzzy matching for minor differences\n\n3. **Cross-Domain Syndication:**\n * Detect wire service content (AP, Reuters)\n * Mark as single source if syndicated\n * Count original publication only\n\n**Display:**\n\n{{code}}\nEvidence Sources (3 unique, 5 total):\n\n1. Original Article (NYTimes)\n - Also appeared in: WashPost, Guardian (syndicated)\n\n2. Research Paper (Nature)\n\n3. Official Statement (WHO)\n{{/code}}\n\n**Acceptance Criteria:**\n\n*  URL normalization works\n*  Content similarity detected\n*  Syndicated content identified\n*  Unique vs. total counts accurate\n*  Improves evidence quality metrics\n\n== Additional Requirements (Lower Importance) ===== FR50: OSINT Toolkit Integration ===\n\n**Fulfills:** Advanced media verification \n\n**Purpose:** Integrate open-source intelligence tools for advanced verification.\n\n**Tools to Integrate:**\n* InVID/WeVerify (video verification)\n* Bellingcat toolkit\n* Additional TBD based on V1.0 learnings\n\n=== FR51: Video Verification System ===\n\n**Fulfills:** UN-27 (Visual claims), advanced media verification \n\n**Purpose:** Verify video-based claims.\n\n**Specification:**\n* Keyframe extraction\n* Reverse video search\n* Deepfake detection (AI-powered)\n* Metadata analysis\n* Acoustic signature analysis\n\n=== FR52: Interactive Detection Training ===\n\n**Fulfills:** Media literacy education \n\n**Purpose:** Teach users to identify misinformation.\n\n**Specification:**\n* Interactive tutorials\n* Practice exercises\n* Detection quizzes\n* Gamification elements\n\n=== FR53: Cross-Organizational Sharing ===\n\n**Fulfills:** Collaboration with other fact-checkers \n\n**Purpose:** Share findings with IFCN/EFCSN members.\n\n**Specification:**\n* API for fact-checking organizations\n* Structured data exchange\n* Privacy controls\n* Attribution requirements\n\n== Summary ==\n\n**V1.0 Critical Requirements (Must Have):**\n\n* FR44: ClaimReview Schema \n* FR45: Corrections Notification \n* FR46: Image Verification \n* FR47: Archive.org Integration \n* FR48: Contributor Safety \n* FR49: A/B Testing \n* FR54: Evidence Deduplication \n* NFR11: Quality Assurance Framework \n* NFR12: Security Controls \n* NFR13: Quality Metrics Dashboard \n\n**V1.1+ (Future):**\n\n* FR50: OSINT Integration\n* FR51: Video Verification\n* FR52: Detection Training\n* FR53: Cross-Org Sharing\n\n**Total:** 11 critical requirements for V1.0\n\n=== FR54: Evidence Deduplication ===\n\n**Fulfills:** Accurate evidence counting, quality metrics \n\n**Purpose:** Avoid counting the same source multiple times when it appears in different forms.\n\n**Specification:**\n\n**Deduplication Logic:**\n\n1. **URL Normalization:**\n * Remove tracking parameters (?utm_source=...)\n * Normalize http/https\n * Normalize www/non-www\n * Handle redirects\n\n2. **Content Similarity:**\n * If two sources have >90% text similarity  Same source\n * If one is subset of other  Same source\n * Use fuzzy matching for minor differences\n\n3. **Cross-Domain Syndication:**\n * Detect wire service content (AP, Reuters)\n * Mark as single source if syndicated\n * Count original publication only\n\n**Display:**\n\n{{code}}\nEvidence Sources (3 unique, 5 total):\n\n1. Original Article (NYTimes)\n - Also appeared in: WashPost, Guardian (syndicated)\n\n2. Research Paper (Nature)\n\n3. Official Statement (WHO)\n{{/code}}\n\n**Acceptance Criteria:**\n\n*  URL normalization works\n*  Content similarity detected\n*  Syndicated content identified\n*  Unique vs. total counts accurate\n*  Improves evidence quality metrics\n\n== Additional Requirements (Lower Importance) ===== FR7: Automated Verdicts (Enhanced with Quality Gates) ===\n\n**POC1+ Enhancement:**\n\nAfter AKEL generates verdict, it passes through quality gates:\n\n{{code}}\nWorkflow:\n1. Extract claims\n \n2. [GATE 1] Validate fact-checkable\n \n3. Generate scenarios\n \n4. Generate verdicts\n \n5. [GATE 4] Validate confidence\n \n6. Display to user\n{{/code}}\n\n**Updated Verdict States:**\n* PUBLISHED\n* INSUFFICIENT_EVIDENCE\n* NON_FACTUAL_CLAIM\n* PROCESSING\n* ERROR\n\n=== FR4: Analysis Summary (Enhanced with Quality Metadata) ===\n\n**POC1+ Enhancement:**\n\nDisplay quality indicators:\n\n{{code}}\nAnalysis Summary:\n Verifiable Claims: 3/5\n High Confidence Verdicts: 1\n Medium Confidence: 2\n Evidence Sources: 12\n Avg Source Quality: 0.73\n Quality Score: 8.5/10\n{{/code}}\n", "Product Development.Specification.Architecture.AKEL Pipeline.WebHome": "= AKEL Pipeline =\n\nThe **AI Knowledge Extraction Layer (AKEL)** is FactHarbor's core analysis engine. It takes an article or claim as input and produces a structured verdict report through the ClaimAssessmentBoundary pipeline's 5-stage workflow: ##extractClaims  researchEvidence  clusterBoundaries  generateVerdicts  aggregateAssessment##.\n\n== Pipeline Overview ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.AKEL Pipeline Detail.WebHome\"/}}\n\n=== Step-by-Step ===\n\n**Stage 1: ##extractClaims##**  Extracts ##AtomicClaims## from user input.\n* Two-pass LLM extraction: initial pass (##CLAIM_EXTRACTION_PASS1##) then consolidation and deduplication (##CLAIM_EXTRACTION_PASS2##)\n* Sets ##passedFidelity## flag per claim (LLM-assessed verifiability)\n* Applies **Gate 1** (Claim Validation): filters opinions, predictions, and non-verifiable claims\n* Safety net: rescues highest-centrality claim if all claims would be filtered\n\n**Stage 2: ##researchEvidence##**  Iteratively searches the web for evidence per claim.\n* Generates targeted search queries per ##AtomicClaim## (##GENERATE_QUERIES##)\n* Fetches and parses source content (HTML via cheerio, PDF via pdf2json)\n* Classifies search result relevance and extracts ##EvidenceItems## from source content\n* Applies evidence quality filtering (probative value, deduplication, provenance)\n* Iterates per ##ClaimAssessmentBoundary## (up to 3 rounds) until research complete or token budget exhausted (750K tokens)\n\n**Stage 3: ##clusterBoundaries##**  Groups evidence into ##ClaimAssessmentBoundaries##.\n* LLM clusters compatible ##EvidenceScopes## into evidence groupings (##BOUNDARY_CLUSTERING##)\n* Boundaries emerge from evidence  not predefined analytical templates\n\n**Stage 4: ##generateVerdicts##**  Generates per-claim verdicts via structured debate.\n* Advocate  Challenger  Reconciliation pattern (3 Sonnet calls per boundary)\n* Advisory validation: grounding check and direction check (Haiku, non-blocking)\n* Produces ##CBClaimVerdicts## with confidence scores and evidence citations\n\n**Stage 5: ##aggregateAssessment##**  Aggregates verdicts and produces the final result.\n* Applies **Gate 4** (Confidence Assessment): flags low-confidence verdicts\n* Weighted aggregation of ##CBClaimVerdicts## (centrality, harm potential, confidence, triangulation)\n* Generates verdict narrative and final ##AnalysisResult##\n\n== Pipeline Variants ==\n\nFactHarbor supports two pipeline variants. The **ClaimAssessmentBoundary** pipeline is the comprehensive default; the **monolithic dynamic** variant is a fast, lower-cost alternative.\n\n{{mermaid}}\nflowchart LR\n    INPUT[\"User Input\"] --> DISPATCH{{\"Pipeline\\nDispatch\"}}\n\n    DISPATCH -->|\"claimboundary\\n(default)\"| CB[\"ClaimAssessmentBoundary\\n5-stage workflow\\nFull quality gates\"]\n    DISPATCH -->|\"monolithic_dynamic\"| DYN[\"Monolithic Dynamic\\nSingle LLM call\\nFlexible output\"]\n\n    CB --> RESULT[\"AnalysisResult\\nJSON\"]\n    DYN --> RESULT\n\n    style CB fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style DYN fill:#f3e5f5,stroke:#6a1b9a,color:#000\n{{/mermaid}}\n\n//Both variants produce an AnalysisResult JSON output. The ClaimAssessmentBoundary pipeline is the default and most capable; the monolithic dynamic variant trades depth for speed and flexibility.//\n\n|= Variant |= Approach |= Quality |= Speed |= Use Case\n| **ClaimAssessmentBoundary** | 5-stage workflow (extract  research  cluster  verdict  aggregate) with iterative research, quality gates, evidence filtering | Highest | Thorough (multiple LLM calls) | Production analysis\n| **Monolithic Dynamic** | Single LLM tool-loop call, flexible output | Streamlined | Fastest | Fast analysis, second opinion\n\n== Shared Analysis Modules ==\n\nAll pipeline variants share a common set of analysis modules. This ensures consistency in verdict calculations, evidence quality, and source reliability across pipelines.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.AKEL Shared Modules.WebHome\"/}}\n\n|= Module |= Used By |= Purpose\n| ##aggregation.ts## | CB | Verdict weighting by centrality/harm/confidence/triangulation, contestation weight reduction\n| ##claim-decomposition.ts## | CB | Claim text parsing and normalisation\n| ##evidence-filter.ts## | CB | Probative value filtering, false positive rate calculation\n| ##quality-gates.ts## | CB | Gate 1 (claim fidelity + specificity) and Gate 4 (verdict confidence)\n| ##source-reliability.ts## | CB, Dyn | LLM-based source credibility evaluation with SQLite cache\n| ##truth-scale.ts## | CB, Dyn | Percentage-to-verdict label mapping (7-point scale)\n| ##budgets.ts## | CB, Dyn | Token and cost budget tracking and enforcement\n\n== Budget Controls ==\n\nThe ClaimAssessmentBoundary pipeline enforces budgets to prevent runaway LLM costs:\n\n|= Budget |= Default |= Purpose\n| Max research iterations | 3 per boundary | Limits research rounds per ClaimAssessmentBoundary\n| Max total tokens | 750,000 | Caps total LLM token consumption across all stages\n| Max boundaries | UCM-configurable | Hard limit on ClaimAssessmentBoundaries processed in parallel\n\n== Deep Dives ==\n\nFor detailed implementation references:\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  CB pipeline (5 stages) and Monolithic Dynamic: invariants, shared primitives, result model\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  Gate 1 (claim fidelity) and Gate 4 (verdict confidence) criteria, configuration\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[System Design>>FactHarbor.Product Development.Specification.Architecture.System Design.WebHome]] | Next: [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]\n", "Product Development.Specification.Architecture.Data Model.WebHome": "= Data Model =\n\nFactHarbor's data model centres on the **analysis result**  a structured representation of how claims are decomposed, researched, and evaluated. This page defines the complete entity landscape derived from the source code, the 7-point verdict scale, and the job lifecycle.\n\n== Entity Overview ==\n\nThe following diagram shows all major entity groups and their primary relationships at a glance. For detailed views (result fields, target database, runtime entities, UI visibility), see [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Entity Views.WebHome\" section=\"HOverviewERD\"/}}\n\n== Analysis Entity Model ==\n\nThe ClaimAssessmentBoundary pipeline produces a hierarchy of entities: the input is decomposed into AtomicClaims (Stage 1), evidence is gathered from web sources (Stage 2), evidence is clustered into ClaimAssessmentBoundaries (Stage 3), verdicts are generated per-claim via LLM debate (Stage 4), and aggregated into an OverallAssessment (Stage 5).\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Analysis Entity Model ERD.WebHome\"/}}\n\n//For complete field-level detail per entity and per pipeline phase, see [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].//\n\n== Entity Descriptions ==\n\n|= Entity |= Purpose |= Source Interface |= Key Relationships\n| **CBClaimUnderstanding** | Stage 1 output: decomposition of input into AtomicClaims with Gate 1 stats and preliminary evidence | ##types.ts:942## | Parent of AtomicClaim[]; contains gate1Stats, preliminaryEvidence\n| **AtomicClaim** | Single verifiable assertion extracted from user input; the analytical unit. Only central claims (high/medium centrality) survive extraction. | ##types.ts:715## | Receives CBClaimVerdict[]; has centrality, harmPotential, groundingQuality, expectedEvidenceProfile\n| **ClaimAssessmentBoundary** | Evidence-emergent grouping of compatible EvidenceScopes; the top-level analytical frame. Created post-research by clustering EvidenceScopes. | ##types.ts:747## | Contains EvidenceItem[] and constituentScopes[]; referenced by BoundaryFinding[]; has internalCoherence\n| **CBClaimVerdict** | Per-claim verdict with truth percentage, confidence, reasoning, boundary findings, challenge responses, and misleadingness assessment | ##types.ts:791## | References EvidenceItem[] (supporting + contradicting); contains BoundaryFinding[], ConsistencyResult, ChallengeResponse[], TriangulationScore, TruthPercentageRange\n| **EvidenceItem** | Extracted statement from a web source with quality metadata (probativeValue, scopeQuality, sourceType, derivative tracking) | ##types.ts:385## | References FetchedSource; assigned to ClaimAssessmentBoundary via claimBoundaryId; has EvidenceScope\n| **FetchedSource** | Web source with URL, content, and source reliability score (0.0-1.0 from LLM evaluation, with confidence and consensus) | ##types.ts:447## | Referenced by EvidenceItem[]\n| **OverallAssessment** | Final aggregated result: weighted truth percentage, verdict, confidence, narrative, coverage matrix, quality gates, explanation quality check | ##types.ts:1035## | Aggregates CBClaimVerdict[], ClaimAssessmentBoundary[]; has VerdictNarrative, CoverageMatrix, QualityGates, ExplanationQualityCheck, TruthPercentageRange\n| **VerdictNarrative** | Structured narrative: headline, evidence base summary, key finding, boundary disagreements, limitations | ##types.ts:927## | Contained by OverallAssessment\n\n=== Additional Entities ===\n\n* **EvidenceScope** (##types.ts:226##)  Per-evidence methodology metadata (methodology [optional], temporal [optional], boundaries, geographic, sourceType, additionalDimensions). All fields except ##name## are optional; populated when available from the source. Embedded in EvidenceItem.\n* **BoundaryFinding** (##types.ts:773##)  Per-boundary quantitative signals within a CBClaimVerdict: boundaryId, boundaryName, truthPercentage, confidence, evidenceDirection, evidenceCount.\n* **ConsistencyResult** (##types.ts:820##)  Self-consistency check output: verdict stability across multiple LLM runs (percentages[], average, spread, stable flag, assessed flag).\n* **ChallengeDocument** (##types.ts:845##)  Output of the adversarial challenge step (Stage 4, Step 3): contains per-claim arrays of ChallengePoints.\n* **ChallengePoint** (##types.ts:857##)  A single adversarial challenge against a verdict: id, type (assumption/missing_evidence/methodology_weakness/independence_concern), description, evidenceIds, severity, challengeValidation.\n* **ChallengeValidation** (##types.ts:874##)  Structural validation of a challenge point's evidence references: evidenceIdsValid, validIds[], invalidIds[]. Populated by ##validateChallengeEvidence()## before reconciliation.\n* **ChallengeResponse** (##types.ts:885##)  How adversarial challenges were addressed in reconciliation: challengeType, response, verdictAdjusted, adjustmentBasedOnChallengeIds.\n* **TriangulationScore** (##types.ts:898##)  Cross-boundary agreement assessment: supporting/contradicting boundary counts, level (strong/moderate/weak/conflicted), factor.\n* **CoverageMatrix** (##types.ts:912##)  Claims x boundaries evidence distribution: claim IDs (rows), boundary IDs (columns), counts[][] per cell. Provides ##getBoundariesForClaim()## and ##getClaimsForBoundary()## accessors.\n* **TruthPercentageRange** (##types.ts:835##)  Plausible range for a truth percentage verdict (min, max), computed from self-consistency spread and optionally widened by boundary variance. Attached to both CBClaimVerdict and OverallAssessment.\n* **CBResearchState** (##types.ts:971##)  Top-level research container holding CBClaimUnderstanding, EvidenceItem[], FetchedSource[], SearchQuery[], query budget tracking, iteration tracking, ClaimAssessmentBoundary[], and accumulated AnalysisWarning[].\n\n=== Explanation Quality Entities (B-8) ===\n\n* **ExplanationQualityCheck** (##types.ts:1023##)  Explanation quality check result, attached to OverallAssessment: mode (structural or rubric), structuralFindings, rubricScores (when mode is rubric).\n* **ExplanationStructuralFindings** (##types.ts:998##)  Tier 1 deterministic structural check: hasCitedEvidence, hasVerdictCategory, hasConfidenceStatement, hasLimitations.\n* **ExplanationRubricScores** (##types.ts:1009##)  Tier 2 LLM-powered rubric evaluation: clarity, completeness, neutrality, evidenceSupport, appropriateHedging (each scored 1-5), overallScore (weighted average), flags[].\n\n== Quality Gate Entities ==\n\nTwo quality gates produce validation entities that attach to the analysis result.\n\n|= Entity |= Gate |= Purpose |= Key Fields |= Source\n| **ClaimValidationResult** | Gate 1 | Per-claim factuality + fidelity check | ##isFactual##, ##opinionScore##, ##specificityScore##, ##claimType##, ##passed## | ##types.ts:105##\n| **VerdictValidationResult** | Gate 4 | Per-verdict confidence assessment | ##evidenceCount##, ##averageSourceQuality##, ##evidenceAgreement##, ##confidenceTier## | ##types.ts:121##\n| **QualityGates** | Both | Aggregate pass/fail with Gate1Stats and Gate4Stats | ##passed##, ##gate1Stats##, ##gate4Stats##, ##summary## | ##types.ts:172##\n\nGate 1 statistics (##Gate1Stats##, ##types.ts:137##) track total claims evaluated, passed, filtered, and central claims kept despite failing validation. The CB pipeline's ##CBClaimUnderstanding.gate1Stats## additionally tracks ##passedFidelity##  the count of claims passing the fidelity check.\n\nGate 4 statistics (##Gate4Stats##, ##types.ts:148##) track verdict confidence distribution across HIGH, MEDIUM, LOW, and INSUFFICIENT tiers.\n\nFor detailed quality gate criteria and examples, see [[Quality Gates Deep Dive>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]].\n\n== Configuration Entities ==\n\nFour configuration profiles are stored as immutable JSON blobs in the UCM (Unified Config Management) system. Each analysis job records the exact config snapshot used for reproducibility.\n\n|= Config |= Purpose |= Key Settings |= Source\n| **PipelineConfig** | Pipeline operational settings | LLM provider selection, model tiering, budget controls, context detection, confidence calibration | ##config-schemas.ts:86##\n| **CalcConfig** | Calculation and aggregation | Verdict bands, centrality weights, contestation penalties, quality gate thresholds, deduplication | ##config-schemas.ts:751##\n| **SearchConfig** | Search provider settings | Provider selection, max results, timeout, domain whitelist/blacklist | ##config-schemas.ts:52##\n| **SourceReliabilityConfig** | Source reliability service | Multi-model consensus, confidence/consensus thresholds, cache TTL, platform skip lists | ##config-schemas.ts:598##\n\nFor configuration storage and versioning, see [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]].\n\n== 7-Point Verdict Scale ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Verdict Scale.WebHome\"/}}\n\n//The 7-point verdict scale maps truth percentages to verdict labels. MIXED (evidence on both sides, confidence >= 40%) and UNVERIFIED (insufficient evidence, confidence < 40%) share the same percentage range but differ in their confidence interpretation.//\n\n|= Verdict |= Truth % Range |= Meaning\n| **TRUE** | 86-100% | Claim is well-supported by strong evidence\n| **MOSTLY TRUE** | 72-85% | Claim is largely supported with minor caveats\n| **LEANING TRUE** | 58-71% | More evidence supports than contradicts, but not conclusive\n| **MIXED** | 43-57% (confidence >= 40%) | Significant evidence on both sides\n| **UNVERIFIED** | 43-57% (confidence < 40%) | Insufficient evidence to determine truth\n| **LEANING FALSE** | 29-42% | More evidence contradicts than supports\n| **MOSTLY FALSE** | 15-28% | Claim is largely contradicted by evidence\n| **FALSE** | 0-14% | Claim is strongly contradicted by evidence\n\n=== MIXED vs. UNVERIFIED ===\n\nBoth occupy the 43-57% truth range. The distinction is **confidence-based**:\n* **MIXED**: The system found substantial evidence but it points in opposing directions (high confidence in the conflict)\n* **UNVERIFIED**: The system could not find enough evidence to make a determination (low confidence due to insufficient data)\n\n== Job Lifecycle ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Job Lifecycle ERD.WebHome\"/}}\n\n=== Job Status Transitions ===\n\n|= Status |= Meaning\n| **QUEUED** | Job submitted, waiting for runner capacity\n| **RUNNING** | ClaimAssessmentBoundary pipeline is executing\n| **SUCCEEDED** | Analysis complete, results available\n| **FAILED** | Analysis failed (provider error, timeout, or input issue)\n| **PAUSED** | System auto-paused due to provider outage (jobs resume when system recovers)\n\n== Audit Trail ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Audit Trail ERD.WebHome\"/}}\n\n== Storage ==\n\nAnalysis results are currently stored as **JSON blobs** in SQLite (via the .NET Entity Framework ##ResultJson## field). The full entity model above lives within this JSON blob  it is not normalised into separate database tables.\n\n|= Database |= Technology |= Contents\n| ##factharbor.db## | .NET Entity Framework Core | Jobs, events, analysis results (JSON), metrics\n| ##config.db## | Next.js better-sqlite3 | UCM configuration blobs, activation pointers, usage tracking\n| ##source-reliability.db## | Next.js better-sqlite3 | Source credibility evaluation cache\n\n**Target Evolution:** PostgreSQL for primary storage (enabling full-text search, user accounts), with the JSON blob approach preserved for backwards compatibility during migration. See [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]] for details, and [[Target Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] for the normalised design specification.\n\n== Deep Dives ==\n\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict calculation formulas, aggregation hierarchy, weighting factors\n* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]  4-layer confidence system, penalty calculations\n* [[KeyFactors Design>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome]]  //Historical (Orchestrated pipeline).// Retained for design context on how CB pipeline's boundary-based approach evolved.\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]] | Next: [[External Dependencies>>FactHarbor.Product Development.Specification.Architecture.External Dependencies.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome": "= Calculations and Verdicts =\n\n{{info}}\n**Developer Reference**  Verdict scale, counter-evidence handling, aggregation hierarchy, weighting formulas, and special-case guards that produce FactHarbor's final verdicts.\n\n**Key Files**: ##apps/web/src/lib/analyzer/aggregation.ts##, ##apps/web/src/lib/analyzer/truth-scale.ts##, ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts##, ##apps/web/src/lib/analyzer/source-reliability.ts##\n{{/info}}\n\n**Version**: 2.11.0\n**Status**: Operational\n\n----\n\n== 1. Verdict Scale (7-Point System) ==\n\nFactHarbor uses a symmetric 7-point scale with truth percentages from 0-100%. The 43-57% range distinguishes between **MIXED** (high confidence, evidence on both sides) and **UNVERIFIED** (low confidence, insufficient evidence).\n\n|= Verdict |= Range |= Confidence |= Description\n| **TRUE** | 86-100% | - | Strong support, no credible counter-evidence\n| **MOSTLY-TRUE** | 72-85% | - | Mostly supported, minor gaps\n| **LEANING-TRUE** | 58-71% | - | Mixed evidence, leans positive\n| **MIXED** | 43-57% | >= 40% | Evidence on both sides, roughly equal\n| **UNVERIFIED** | 43-57% | < 40% | Insufficient evidence to judge\n| **LEANING-FALSE** | 29-42% | - | More counter-evidence than support\n| **MOSTLY-FALSE** | 15-28% | - | Strong counter-evidence\n| **FALSE** | 0-14% | - | Direct contradiction\n\n=== 1.1 MIXED vs UNVERIFIED ===\n\n* **MIXED** (blue in UI): Substantial evidence exists, but is roughly equal on both sides. High confidence in the mixed state.\n* **UNVERIFIED** (orange in UI): Not enough evidence to make any judgement. Low confidence due to insufficient information.\n\n=== 1.2 percentageToClaimVerdict ===\n\n**File**: ##apps/web/src/lib/analyzer/truth-scale.ts## (line 138)\n\nBand thresholds and the ##mixedConfidenceThreshold## are UCM-configurable via ##CalcConfig.verdictBands##.\n\n{{code language=\"typescript\"}}\n// Simplified  actual signature takes VerdictBandConfig + mixedConfidenceThreshold from UCM\nfunction percentageToClaimVerdict(truthPercentage: number, confidence?: number): ClaimVerdict7Point {\n  if (truthPercentage >= 86) return \"TRUE\";\n  if (truthPercentage >= 72) return \"MOSTLY-TRUE\";\n  if (truthPercentage >= 58) return \"LEANING-TRUE\";\n  if (truthPercentage >= 43) {\n    const conf = confidence !== undefined ? normalizePercentage(confidence) : 0;\n    return conf >= mixedConfidenceThreshold ? \"MIXED\" : \"UNVERIFIED\";\n  }\n  if (truthPercentage >= 29) return \"LEANING-FALSE\";\n  if (truthPercentage >= 15) return \"MOSTLY-FALSE\";\n  return \"FALSE\";\n}\n{{/code}}\n\n=== 1.3 truthFromBand ===\n\nConverts confidence-adjusted bands to truth percentages:\n\n{{code language=\"typescript\"}}\nfunction truthFromBand(band: \"strong\" | \"partial\" | \"uncertain\" | \"refuted\", confidence: number): number {\n  const conf = normalizePercentage(confidence) / 100;\n  switch (band) {\n    case \"strong\":    return Math.round(72 + 28 * conf);  // 72-100%\n    case \"partial\":   return Math.round(50 + 35 * conf);  // 50-85%\n    case \"uncertain\": return Math.round(35 + 30 * conf);  // 35-65%\n    case \"refuted\":   return Math.round(28 * (1 - conf)); // 0-28%\n  }\n}\n{{/code}}\n\n**Example (\"strong\" band with varying confidence)**:\n\n|= Confidence |= Calculation |= Result |= Verdict\n| High (90%) | 72 + 28 x 0.9 | 97% | **TRUE**\n| Medium (60%) | 72 + 28 x 0.6 | 89% | **TRUE**\n| Low (30%) | 72 + 28 x 0.3 | 80% | **MOSTLY-TRUE**\n\nSame evidence band, but lower confidence pulls the verdict down within the band.\n\n----\n\n== 2. Counter-Evidence Handling ==\n\nCounter-evidence is distinguished from mere contestation and influences verdict calculations. The v2.8 system separates **DOUBTED** (political criticism without documented evidence) from **CONTESTED** (actual documented counter-evidence).\n\n=== 2.1 Doubted vs Contested ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Doubted vs Contested Flow.WebHome\"/}}\n\n//Orange = doubted (opinion only). Yellow = disputed evidence. Red = established counter-evidence. Green = full weight preserved.//\n\n**Key Distinction:**\n\n* **DOUBTED** = Political criticism, rhetoric, accusations WITHOUT documented evidence -> Full weight (claim remains credible)\n* **CONTESTED** = Has actual documented counter-evidence -> Reduced weight (genuine uncertainty)\n\n**Implementation (v2.8):**\n\n* ##detectClaimContestation()## in ##aggregation.ts## -- Claim-level contestation detection (CB pipeline)\n* Contestation classification is LLM-derived (##factualBasis## field, set during verdict generation); weight reduction applied via ##getClaimWeight()## in ##aggregation.ts##\n\n=== 2.2 Evidence Item Categorization ===\n\n**File**: ##apps/web/src/lib/analyzer/types.ts## (##EvidenceItem## interface)\n\nEvidence items are categorized during extraction:\n\n|= Category |= Description\n| ##\"evidence\"## | Supporting evidence\n| ##\"criticism\"## | Counter-evidence or opposing views\n| ##\"expert_quote\"## | Expert testimony\n| ##\"statistic\"## | Numerical data\n| ##\"legal_provision\"## | Legal framework\n| ##\"event\"## | Factual events\n\n=== 2.3 Contestation Fields ===\n\n{{code language=\"typescript\"}}\ninterface EvidenceItem {\n  category: \"legal_provision\" | \"evidence\" | \"expert_quote\" | \"statistic\" | \"event\" | \"criticism\";\n  isContestedClaim?: boolean;  // True if this evidence item contests a claim\n  claimSource?: string;         // Who makes the contested claim\n}\n{{/code}}\n\n=== 2.4 Evidence-Based Contestation Weight Reduction ===\n\n**File**: ##apps/web/src/lib/analyzer/aggregation.ts## (##getClaimWeight()##)\n\nContestation with documented evidence reduces a claim's **weight** in the overall verdict (v2.9.0  changed from point deductions to weight multipliers to avoid double-penalising contested claims whose ##truthPercentage## already reflects counter-evidence):\n\n{{code language=\"typescript\"}}\nif (claim.isContested) {\n  const ctw = weights?.contestationWeights ?? { established: 0.5, disputed: 0.7, opinion: 1.0 };\n  const basis = claim.factualBasis || \"unknown\";\n  if (basis === \"established\") weight *= ctw.established;      // default 0.5x\n  else if (basis === \"disputed\") weight *= ctw.disputed;       // default 0.7x\n  // \"opinion\", \"unknown\" = doubted only  full weight\n}\n{{/code}}\n\n|= factualBasis |= Weight Multiplier |= Rationale\n| ##\"established\"## | 0.5x (default) | Strong documented counter-evidence reduces influence\n| ##\"disputed\"## | 0.7x (default) | Some documented counter-evidence moderately reduces influence\n| ##\"opinion\"## | 1.0x | Rhetoric only, no documented evidence  full weight\n| ##\"unknown\"## | 1.0x | No evidence of contestation  full weight\n\nAll multipliers are UCM-configurable via ##CalcConfig.aggregation.contestationWeights##.\n\n----\n\n== 3. Aggregation Hierarchy ==\n\nThe aggregation system produces the final verdict by rolling up from individual evidence items through four levels to the overall answer.\n\n{{mermaid}}\ngraph TD\n    Evidence[\"Evidence Items<br/>(per source)\"] --> ClaimVerdicts[\"AtomicClaim Verdicts<br/>(per claim  Stage 4 LLM debate:<br/>Advocate  Challenger  Reconciliation)\"]\n    ClaimVerdicts --> WeightCalc[\"Weight Calculation<br/><br/>centrality: high=3.0x / med=2.0x / low=1.0x<br/>harmPotential: high=1.5x<br/>confidence: 01 factor<br/>triangulation + derivative adjustments<br/>contestation: established=0.5x / disputed=0.7x\"]\n    WeightCalc --> OverallAnswer[\"Overall Answer<br/>(Stage 5: aggregateAssessment)\"]\n    OverallAnswer --> ArticleVerdict[\"Article Verdict<br/>(verdict label + narrative)\"]\n\n    style Evidence fill:#e8f5e9,color:#000\n    style ClaimVerdicts fill:#c8e6c9,color:#000\n    style WeightCalc fill:#e3f2fd,color:#000\n    style OverallAnswer fill:#fff3e0,color:#000\n    style ArticleVerdict fill:#f3e5f5,color:#000\n{{/mermaid}}\n\n//Green = evidence and claim verdicts. Blue = weight calculation (Stage 5). Orange = overall answer. Purple = article verdict (label + narrative).//\n\n=== 3.1 Weight Calculation ===\n\n**Function**: ##getClaimWeight()## in ##aggregation.ts## (v3.1) + inline in ##aggregateAssessment()## for final triangulation/derivative factors.\n\nAll multipliers are UCM-configurable via ##CalcConfig.aggregation##.\n\n{{code language=\"typescript\"}}\n// Base weight formula (v3.1  aggregateAssessment, Stage 5):\n// finalWeight = centralityWeight  harmWeight  confidenceFactor\n//              (1 + triangulationFactor)  derivativeFactor\n//\n// + contestation reduction applied via getClaimWeight() in aggregation.ts\n\n// thesisRelevance guard  tangential/irrelevant claims excluded\nif (claim.thesisRelevance && claim.thesisRelevance !== \"direct\") return 0;\n\nconst centralityWeight =\n  centrality === \"high\"   ? cw.high   :   // default 3.0\n  centrality === \"medium\" ? cw.medium :   // default 2.0\n                            cw.low;       // default 1.0\n\nconst harmWeight = harmLevel === \"high\" ? 1.5 : 1.0;  // UCM-configurable\nconst confidenceFactor = verdict.confidence / 100;\n{{/code}}\n\n|= Factor |= Default |= Condition\n| Centrality  high | 3.0x | Claim is ##centrality: \"high\"##\n| Centrality  medium | 2.0x | Claim is ##centrality: \"medium\"##\n| Centrality  low | 1.0x | Claim is ##centrality: \"low\"## (or unset)\n| ##thesisRelevance##  ##\"direct\"## | 0x | Tangential/irrelevant claims excluded entirely\n| Harm potential  high | 1.5x | ##harmPotential: \"high\"## (death, injury, safety claims)\n| Confidence | 01 factor | ##verdict.confidence / 100## (e.g. 80% confidence  0.8x)\n| Triangulation | 1 + factor | Multi-source corroboration bonus (UCM-configurable)\n| Derivative | 0.5x1.0x | Derivative/secondary claims down-weighted\n| Contested (established) | 0.5x | Counter-evidence is ##\"established\"## (UCM-configurable)\n| Contested (disputed) | 0.7x | Counter-evidence is ##\"disputed\"## (UCM-configurable)\n| Doubted (opinion/unknown) | 1.0x | No documented counter-evidence  full weight\n\n=== 3.2 Level 1: Claim Verdicts ===\n\n**Source**: LLM verdict generation + source reliability weighting (see [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]).\n\nSource reliability adjusts verdicts based on the credibility of evidence sources. See Section 7 below for the full formula.\n\n=== 3.3 Level 2: Weighted Average Computation ===\n\n**File**: ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts## (##aggregateAssessment()##, Stage 5)\n\nThe CB pipeline has no \"Key Factor\" intermediate layer. All ##AtomicClaim## verdicts are aggregated directly using the weighted formula from 3.1. Counter-claims have their truth percentage inverted before aggregation:\n\n{{code language=\"typescript\"}}\n// Counter-claims evaluate the OPPOSITE position  invert their truth%\n// If counter-claim is TRUE (85%), it means the user's thesis is FALSE (15%)\nconst effectiveTruthPct = claim.isCounterClaim\n  ? 100 - claim.truthPercentage\n  : claim.truthPercentage;\n\n// Weighted sum across all direct AtomicClaims\nconst weightedTruthPct = (effectiveTruthPct  weight) / (weight);\n{{/code}}\n\n=== 3.4 Level 3: Overall Verdict ===\n\n**File**: ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts## (##aggregateAssessment()##, Stage 5) + ##apps/web/src/lib/analyzer/truth-scale.ts## (##percentageToArticleVerdict()##)\n\nThe weighted truth percentage from Level 2 is mapped to a verdict label and then enriched with a narrative:\n\n{{code language=\"typescript\"}}\n// Map weighted truth% to 7-point verdict label\nconst verdictLabel = percentageToArticleVerdict(\n  weightedTruthPercentage,\n  weightedConfidence,\n  undefined,\n  mixedConfidenceThreshold,  // UCM-configurable\n);\n\n// Generate human-readable narrative (VERDICT_NARRATIVE prompt  Sonnet)\nconst verdictNarrative = await generateVerdictNarrative(\n  weightedTruthPercentage, verdictLabel, weightedConfidence,\n  claimVerdicts, boundaries, ...\n);\n{{/code}}\n\nClaimAssessmentBoundaries group related claims during research (Stage 3) and drive separate LLM debates (Stage 4), but the final aggregation in Stage 5 operates over all ##CBClaimVerdict[]## directly  there is no per-boundary averaging step before the overall verdict.\n\n=== 3.5 Multi-Boundary Caveat ===\n\nWhen a claim spans multiple distinct ##ClaimAssessmentBoundaries## (e.g., \"Legal fairness in period A\" and \"Scientific validity in period B\"), the weighted average across all claim verdicts may produce a less intuitive overall number than individual boundary-level answers would. The ##articleVerdictReliability## flag and ##articleVerdictReason## field in the output signal this to the UI when boundaries have significantly divergent verdicts. See [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] for details on multi-boundary scenarios.\n\n----\n\n== 4. Near-Duplicate Claim Handling ==\n\n=== 4.1 Problem ===\n\nIf the LLM generates multiple claims expressing the same idea, both would influence the overall verdict, effectively double-counting the same evidence.\n\n=== 4.2 Solution: LLM-Based Claim Deduplication ===\n\nIn the CB pipeline, near-duplicate claims are deduplicated **upstream** by the LLM during ##CLAIM_EXTRACTION_PASS2## (see [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]). The LLM consolidates semantically equivalent claims before they enter the aggregation stage, eliminating the need for downstream Jaccard-similarity clustering.\n\nThe deterministic ##dedupeWeightedAverageTruth()## function from the prior Orchestrated pipeline is not present in the CB pipeline. If near-duplicate claims survive extraction, the weighted average in Stage 5 (3.3) dilutes their combined influence through confidence-based weighting.\n\n**Current de-duplication point**: ##CLAIM_EXTRACTION_PASS2## LLM call in Stage 1 (##extractClaims##).\n\n=== 4.3 Example ===\n\n{{code}}\nCluster 1: [Claim A (85%), Claim B (82%), Claim C (80%)]\n  - Claim A: 85% x 1.0  = 85.0\n  - Claim B: 82% x 0.25 = 20.5\n  - Claim C: 80% x 0.25 = 20.0\n  - Total weight: 1.5\n  - Contribution: 125.5 / 1.5 = 83.7%\n\nCluster 2: [Claim D (90%)]\n  - Claim D: 90% x 1.0 = 90.0\n  - Total weight: 1.0\n  - Contribution: 90.0 / 1.0 = 90.0%\n\nOverall: (83.7 + 90.0) / 2 = 86.9%\n{{/code}}\n\n**UI impact**: All claims are still displayed. De-duplication only affects aggregation calculations, not visibility.\n\n----\n\n== 5. Dependency Handling ==\n\n**Field**: ##dependencyFailed## on ##CBClaimVerdict## (##apps/web/src/lib/analyzer/types.ts##, line 477)\n\nClaims can depend on other claims (e.g., \"timing\" depends on \"attribution\"). If a dependency is false, the dependent claim is excluded from aggregation to avoid double-counting the false prerequisite.\n\n{{code language=\"typescript\"}}\n// Check if any dependency is false (truthPercentage < 43%)\nconst failedDeps = dependencies.filter((depId: string) => {\n  const depVerdict = verdictMap.get(depId);\n  return depVerdict && depVerdict.truthPercentage < 43;\n});\n\nif (failedDeps.length > 0) {\n  verdict.dependencyFailed = true;\n  verdict.failedDependencies = failedDeps;\n}\n{{/code}}\n\n**Threshold**: ##truthPercentage < 43%## (below the LEANING-FALSE / UNVERIFIED boundary).\n\nClaims with ##dependencyFailed = true## are excluded from aggregation (independent verdicts only).\n\n----\n\n== 6. Pseudoscience Escalation ==\n\n{{warning}}\n**Orchestrated pipeline only  not implemented in the CB pipeline.** The ##escalatePseudoscienceVerdict()## function and associated keyword-pattern logic existed in ##orchestrated.ts##, which was removed in v2.11.0.\n{{/warning}}\n\nIn the ClaimAssessmentBoundary pipeline, pseudoscience claims are handled through LLM reasoning in the Advocate/Challenger/Reconciliation debate (Stage 4). The Challenger role is prompted to surface scientific consensus and credible counter-evidence, which naturally drives pseudoscience claims toward low truth percentages without requiring hardcoded keyword lists or deterministic escalation logic.\n\n----\n\n== 7. Benchmark Guard (Proportionality Claims) ==\n\n{{warning}}\n**Orchestrated pipeline only  not implemented in the CB pipeline.** The ##hasBenchmarkEvidence## heuristic and ##EVALUATIVE_OUTCOME_RE## regex existed in ##orchestrated.ts##, which was removed in v2.11.0.\n{{/warning}}\n\nIn the ClaimAssessmentBoundary pipeline, proportionality claims are handled through the ##VERDICT_RECONCILIATION## LLM step (Stage 4). The reconciliation prompt instructs the model to flag insufficient comparative evidence and to reflect that uncertainty in the ##confidence## score, rather than deterministically forcing ##truthPct = 50##. This approach generalises across languages and domains without requiring hardcoded numeric patterns or regex matching.\n\n----\n\n== 8. Source Reliability Weighting ==\n\n**Version**: v2.11.0+\n**Files**: ##apps/web/src/lib/analyzer/source-reliability.ts##, ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts##\n\nSource reliability scores influence verdict calculations by adjusting truth percentages based on the credibility of evidence sources. For full details on the reliability evaluation system, see [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].\n\n=== 8.1 Score = Weight ===\n\nWith the 7-band credibility scale, the LLM score directly represents reliability and is used as-is for verdict weighting:\n\n{{code language=\"typescript\"}}\nfunction calculateEffectiveWeight(data: SourceReliabilityData): number {\n  // Simple: score IS the weight\n  // Confidence already filtered out low-quality evaluations (threshold gate)\n  return data.score;\n}\n{{/code}}\n\n|= Component |= Purpose\n| **Score** | LLM-evaluated reliability (7-band scale, 0.0-1.0) -- used directly as weight\n| **Confidence** | Quality gate (threshold: 65%) -- scores below threshold are rejected\n| **Consensus** | Multi-model agreement (Claude + GPT-4 must agree within 15%)\n\n=== 8.2 Verdict Adjustment Formula ===\n\n{{code language=\"typescript\"}}\n// Average effective weight across all sources for a verdict\nconst avgWeight = sources.map(s => calculateEffectiveWeight(s))\n  .reduce((a, b) => a + b) / sources.length;\n\n// Pull verdict toward neutral (50) based on reliability\nadjustedTruth = Math.round(50 + (originalTruth - 50) * avgWeight);\n\n// Scale confidence by reliability\nadjustedConfidence = Math.round(originalConfidence * (0.5 + avgWeight / 2));\n{{/code}}\n\n=== 8.3 Impact Examples ===\n\n**High Reliability Source (Reuters, 95% score)**\n\n{{code}}\nOriginal verdict: 85% (MOSTLY-TRUE)\nAdjusted: 50 + (85 - 50) x 0.95 = 83.3% -> 83% (MOSTLY-TRUE)\nImpact: Verdict mostly preserved\n{{/code}}\n\n**Unknown Source (50% score -- neutral default)**\n\n{{code}}\nOriginal verdict: 85% (MOSTLY-TRUE)\nAdjusted: 50 + (85 - 50) x 0.50 = 67.5% -> 68% (LEANING-TRUE)\nImpact: Strong pull toward neutral (appropriate skepticism)\n{{/code}}\n\n**Low Reliability Source (27% score)**\n\n{{code}}\nOriginal verdict: 85% (MOSTLY-TRUE)\nAdjusted: 50 + (85 - 50) x 0.27 = 59.5% -> 60% (LEANING-TRUE)\nImpact: Strong pull toward neutral (unreliable source)\n{{/code}}\n\n**Multi-Source Averaging**\n\n{{code}}\nVerdict evidence from:\n  - reuters.com:       95% score\n  - bild.de:           44% score\n  - unknown-blog.xyz:  50% score (default)\n\nAverage weight: (95 + 44 + 50) / 3 = 63%\n\nOriginal: 85% -> Adjusted: 50 + (85 - 50) x 0.63 = 72% (LEANING-TRUE)\n{{/code}}\n\n=== 8.4 Unknown Source Handling ===\n\nSources not in the reliability cache are assigned ##defaultScore = 0.5## (symmetric scale centre), resulting in 50% weight (neutral). This applies appropriate skepticism without completely discounting evidence.\n\n----\n\n== 9. Related Documentation ==\n\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] -- Gate 1 (claim validation) and Gate 4 (confidence assessment)\n* [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] -- Full source reliability evaluation system\n* [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] -- ClaimAssessmentBoundary clustering, multi-boundary averaging reliability\n* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] -- 4-layer calibration, recency penalty, low-source penalty\n* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] -- 7-layer evidence filtering strategy\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] -- CB pipeline (default) and Monolithic Dynamic pipeline\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] -- Architecture overview\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] | Next: [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]] | [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome": "= Confidence Calibration =\n\n{{info}}\n**Developer Reference**  The 4-layer confidence calibration system that transforms raw LLM confidence into calibrated scores. Includes graduated recency penalty, low-source penalty, and confidence floor.\n\n**Key File**: ##apps/web/src/lib/analyzer/confidence-calibration.ts##\n{{/info}}\n\n== Overview ==\n\nRaw LLM confidence scores are unreliable  models tend to be overconfident or inconsistent across providers. FactHarbor applies a **4-layer calibration pipeline** that adjusts confidence based on evidence density, verdict band alignment, context consistency, and external penalties.\n\n{{mermaid}}\nflowchart LR\n    RAW[\"Raw LLM<br/>Confidence\"] --> D[\"Layer 1:<br/>Density Anchor\"]\n    D --> B[\"Layer 2:<br/>Band Snapping\"]\n    B --> V[\"Layer 3:<br/>Verdict Coupling\"]\n    V --> C[\"Layer 4:<br/>Context Consistency\"]\n    C --> PEN[\"Penalties:<br/>Recency + Low-Source\"]\n    PEN --> FLOOR[\"Floor: 10%<br/>minimum\"]\n    FLOOR --> FINAL[\"Calibrated<br/>Confidence\"]\n\n    style RAW fill:#ffcdd2,color:#000\n    style FINAL fill:#c8e6c9,color:#000\n    style PEN fill:#fff9c4,color:#000\n{{/mermaid}}\n\n//Raw LLM confidence (red) passes through 4 calibration layers, then penalty adjustments, and finally the confidence floor to produce calibrated confidence (green).//\n\n== Layer 1: Density Anchor ==\n\nAnchors confidence to the **density of supporting evidence**. More evidence = higher confidence baseline.\n\n|= Evidence Count |= Confidence Anchor\n| 0 sources | Floor (10%)\n| 1 source, 1-2 facts | Max 40%\n| 2 sources, 3-4 facts | Max 60%\n| 3+ sources, 5+ facts | No cap (LLM confidence preserved)\n\n**Rationale**: A claim supported by a single blog post should not have 95% confidence, regardless of what the LLM says.\n\n== Layer 2: Band Snapping ==\n\nEnsures confidence aligns with the verdict band. Prevents \"HIGH confidence in UNVERIFIED\" contradictions.\n\n|= Verdict Band |= Max Confidence\n| TRUE / FALSE | 100%\n| MOSTLY TRUE / MOSTLY FALSE | 90%\n| LEANING TRUE / LEANING FALSE | 80%\n| MIXED | 70%\n| UNVERIFIED | 50%\n\n**Rationale**: If the system couldn't determine truth (UNVERIFIED), confidence in the verdict should reflect that uncertainty.\n\n== Layer 3: Verdict Coupling ==\n\nCouples confidence to the **distance from the band midpoint**. Verdicts near band boundaries get lower confidence than those squarely within a band.\n\n{{code language=\"typescript\"}}\n// Example: truthPercentage = 72% is at the bottom of MOSTLY-TRUE (72-85%)\n// Band midpoint = 78.5%\n// Distance from midpoint = |72 - 78.5| / (85 - 72) = 0.5\n// Confidence reduced proportionally to distance from midpoint\n{{/code}}\n\n**Rationale**: A verdict at 72% (just barely MOSTLY TRUE) should have lower confidence than one at 80% (solidly MOSTLY TRUE).\n\n== Layer 4: Context Consistency ==\n\nFor multi-context analyses, checks whether verdict confidence is consistent across analysis contexts. Inconsistent contexts reduce overall confidence.\n\n|= Scenario |= Adjustment\n| Single context | No adjustment\n| Multiple contexts, consistent verdicts | No adjustment\n| Multiple contexts, divergent verdicts | Confidence reduced by divergence factor\n| ##articleVerdictReliability = \"low\"## | Additional reduction\n\n**Rationale**: When contexts answer different questions and produce wildly different verdicts, the overall confidence in the aggregated result should be lower.\n\n== Post-Calibration Penalties ==\n\nAfter the 4-layer calibration, two additional penalties may apply:\n\n=== Graduated Recency Penalty (v2.11) ===\n\n**Purpose**: Reduce confidence when time-sensitive claims lack recent evidence.\n\n**Trigger**: Topic is recency-sensitive (via LLM ##temporalContext## or heuristic detection) AND no evidence found within the recency window.\n\n**Formula**:\n{{code}}\neffectivePenalty = round(maxPenalty x staleness x volatility x volume)\n{{/code}}\n\n**Three independent factors**:\n\n==== Factor 1: Staleness Curve ====\n\nHow far outside the recency window is the evidence?\n\n|= Evidence Age |= Staleness Multiplier\n| Within window (default: 6 months) | 0 (no penalty)\n| Outside window, within 2x window | Linear ramp 0  1\n| Beyond 2x window | 1.0 (capped)\n| No dates found | 1.0 (full staleness)\n\n==== Factor 2: Topic Volatility ====\n\nHow time-critical is the topic? Derived from ##temporalContext.granularity##:\n\n|= Granularity |= Multiplier |= Example\n| ##week## | 1.0 | Breaking news\n| ##month## | 0.8 | Monthly-cycle topics\n| ##year## | 0.4 | Institutional / annual\n| ##none## | 0.2 | Enduring / structural\n| //undefined// | 0.7 | Fallback when LLM didn't assess\n\n==== Factor 3: Evidence Volume ====\n\nMore evidence (even if dated) attenuates the penalty:\n\n|= Date Candidates |= Multiplier\n| 0 | 1.0\n| 1-10 | 0.9\n| 11-25 | 0.7\n| 26+ | 0.5\n\n==== Example ====\n\nInstitutional topic (annual cycle):\n* Evidence 14 months old, window = 6 months  staleness = 1.0 (capped)\n* Granularity = \"year\"  volatility = 0.4\n* 35 date candidates  volume = 0.5\n* **Effective penalty = round(20 x 1.0 x 0.4 x 0.5) = 4 points** (vs. 20 flat)\n\n**Backwards compatibility**: Set ##recencyGraduatedPenalty: false## to revert to flat binary penalty.\n\n=== Low-Source Confidence Penalty ===\n\n**Purpose**: Reduce confidence when evidence base is thin.\n\n|= Parameter |= Default |= Description\n| ##lowSourceThreshold## | 2 | Unique source count threshold\n| ##lowSourceConfidencePenalty## | 15 | Flat penalty (points)\n\n**Trigger**: Unique source count <= threshold.\n\n=== Confidence Floor ===\n\nAfter all penalties, confidence cannot drop below ##minConfidenceFloor## (default: **10%**).\n\n== Configuration Reference ==\n\nAll settings are in UCM Pipeline Config:\n\n{{code language=\"json\"}}\n{\n  \"recencyWindowMonths\": 6,\n  \"recencyConfidencePenalty\": 20,\n  \"recencyGraduatedPenalty\": true,\n  \"lowSourceThreshold\": 2,\n  \"lowSourceConfidencePenalty\": 15,\n  \"minConfidenceFloor\": 10\n}\n{{/code}}\n\n|= Setting |= Default |= Rationale\n| ##recencyWindowMonths## | 6 | Time-sensitive evidence window\n| ##recencyConfidencePenalty## | 20 | Max penalty for recency gap\n| ##recencyGraduatedPenalty## | true | Multi-factor graduated penalty\n| ##lowSourceThreshold## | 2 | Source count for thin-evidence penalty\n| ##lowSourceConfidencePenalty## | 15 | Penalty for thin evidence base\n| ##minConfidenceFloor## | 10 | Absolute minimum confidence\n\n== Confidence in Verdict Calculation ==\n\nCalibrated confidence modulates the final truth percentage within each verdict band via ##truthFromBand()##:\n\n{{code language=\"typescript\"}}\nfunction truthFromBand(band: \"strong\" | \"partial\" | \"uncertain\" | \"refuted\", confidence: number): number {\n  const conf = normalizePercentage(confidence) / 100;\n  switch (band) {\n    case \"strong\":    return Math.round(72 + 28 * conf);  // 72-100%\n    case \"partial\":   return Math.round(50 + 35 * conf);  // 50-85%\n    case \"uncertain\": return Math.round(35 + 30 * conf);  // 35-65%\n    case \"refuted\":   return Math.round(28 * (1 - conf)); // 0-28%\n  }\n}\n{{/code}}\n\n**Example  \"strong\" band with varying confidence**:\n* High confidence (90%): 72 + 28x0.9 = **97% (TRUE)**\n* Medium confidence (60%): 72 + 28x0.6 = **89% (TRUE)**\n* Low confidence (30%): 72 + 28x0.3 = **80% (MOSTLY-TRUE)**\n\nSame evidence band, but lower confidence pulls the verdict down within the band.\n\n== MIXED vs UNVERIFIED ==\n\nConfidence determines whether the 43-57% range is **MIXED** or **UNVERIFIED**:\n\n{{code language=\"typescript\"}}\nconst MIXED_CONFIDENCE_THRESHOLD = 60;\n\nif (truthPercentage >= 43 && truthPercentage <= 57) {\n  return confidence >= 60 ? \"MIXED\" : \"UNVERIFIED\";\n}\n{{/code}}\n\n* **MIXED** (confidence >= 60%): Evidence on both sides, high confidence in mixed state\n* **UNVERIFIED** (confidence < 60%): Insufficient evidence, low confidence\n\n----\n\n== Related Documentation ==\n\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  Gate 4 confidence tiers\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict calculation formulas\n* [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]]  Overview of confidence calibration in context\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome": "= Boundary Clustering and EvidenceScope Detection =\n\n{{info}}\n**Developer Reference**  How FactHarbor clusters evidence into ClaimBoundaries and detects EvidenceScope metadata across the pipeline.\n\n**Key File**: ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts## (Stage 3: CLUSTER BOUNDARIES)\n{{/info}}\n\n----\n\n== 1. Overview ==\n\nThis guide explains:\n\n* **What** ClaimBoundaries and EvidenceScopes are (definitions)\n* **When** to extract EvidenceScope vs create ClaimBoundary (stage timing)\n* **How** boundaries are clustered from evidence (congruence-based flow)\n* **Why** the system uses evidence-emergent boundaries (approach)\n* **Where** the implementation lives (code references)\n\n----\n\n== 2. Terminology ==\n\n=== 2.1 Core Definitions ===\n\n**ClaimBoundary** (Evidence-Emergent Grouping):\n* An evidence-emergent grouping of compatible EvidenceScopes\n* **Created**: AFTER research (Stage 3: CLUSTER BOUNDARIES)\n* **Example**: \"Methodology A Studies\" (evidence using compatible Framework A scopes)\n* **Storage**: ##claimBoundaries## array, ##evidenceItem.claimBoundaryId##\n* **Purpose**: Organize evidence for per-claim verdict generation with per-boundary findings\n\n**EvidenceScope** (Per-Evidence Source Metadata):\n* Metadata about a single evidence item's methodology, boundaries, temporal, and geographic constraints\n* **Created**: DURING research (Stage 2: RESEARCH, per evidence item)\n* **Example**: \"Standard S, Period P data, Full system boundary\"\n* **Storage**: ##evidenceItem.evidenceScope## object (MANDATORY)\n* **Purpose**: Capture source's analytical frame so compatible evidence can be clustered\n\n**Key Distinction**:\n* **EvidenceScope** = \"What methodology/boundaries did THIS source use?\" (per-evidence metadata)\n* **ClaimBoundary** = \"Which evidence items can be grouped together?\" (cluster of compatible EvidenceScopes)\n\n=== 2.2 Terminology Usage Rules ===\n\n**FactHarbor ClaimBoundary Pipeline Entities** (use in prompts and code):\n* **AtomicClaim**: Single verifiable assertion extracted from user input\n* **EvidenceItem**: Information extracted from sources (with EvidenceScope)\n* **EvidenceScope**: Per-evidence source methodology metadata\n* **ClaimBoundary**: Evidence-emergent grouping of compatible EvidenceScopes\n* **ClaimVerdict**: Per-claim verdict with boundaryFindings[]\n* **BoundaryFinding**: Per-boundary quantitative assessment for a specific claim\n\n**NEVER use in new code**:\n* ~~AnalysisContext~~  Use **ClaimBoundary**\n* ~~contextId~~  Use **claimBoundaryId**\n* ~~analysisContexts~~  Use **claimBoundaries**\n* ~~scope~~ (ambiguous)  Use **EvidenceScope** or **ClaimBoundary** explicitly\n\n----\n\n== 3. EvidenceScope vs ClaimBoundary Decision Tree ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Context Detection Decision Tree.WebHome\"/}}\n\n=== 3.1 When to Extract EvidenceScope (Always) ===\n\nExtract EvidenceScope metadata for EVERY evidence item during Stage 2: RESEARCH:\n\n1. **Source states methodology**: \"This study uses Standard S\"\n1. **Source has temporal boundaries**: \"Data from Period P to Period Q\"\n1. **Source has system boundaries**: \"Full system boundary\" vs \"Subsystem only\"\n1. **Source has geographic scope**: \"Jurisdiction J\" vs \"Region R\"\n\n**Primary fields** (always extracted when source provides them; optional in TypeScript type):\n* ##methodology##  The analytical approach used by the source\n* ##temporal##  When the source data was collected or applies\n\n**Optional fields**:\n* ##boundaries##  What was included/excluded in the analysis\n* ##geographic##  Geographic scope of the source data\n* ##additionalDimensions##  Domain-specific scope data (e.g., sample size, blinding)\n\n=== 3.2 When ClaimBoundaries Are Created (Stage 3) ===\n\nClaimBoundaries are created AFTER research by clustering compatible EvidenceScopes:\n\n1. **Compatible EvidenceScopes**: Cluster into single ClaimBoundary\n1*. All evidence uses \"Standard S, Period P\"  1 boundary\n1. **Incompatible methodologies**: Create separate ClaimBoundaries\n1*. \"Methodology A (full system)\" vs \"Methodology B (subsystem)\"  2 boundaries\n1. **Different jurisdictions**: Create separate ClaimBoundaries\n1*. \"Jurisdiction J\" vs \"Jurisdiction K\"  2 boundaries\n1. **Different temporal periods**: May create separate boundaries if temporal is primary subject\n1*. \"Period P\" vs \"Period Q\"  potentially 2 boundaries\n\n----\n\n== 4. Pipeline Flow ==\n\n=== 4.1 Five-Stage ClaimBoundary Pipeline ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Context Detection Phases.WebHome\"/}}\n\n=== 4.2 Stage Breakdown ===\n\n|= Stage |= EvidenceScope Action |= ClaimBoundary Action |= Output\n| **Stage 1: EXTRACT CLAIMS** | None | None | AtomicClaim[] (central claims only)\n| **Stage 2: RESEARCH** | **Extract per-item** | None | EvidenceItem[] (each with EvidenceScope)\n| **Stage 3: CLUSTER BOUNDARIES** | None | **Cluster compatible scopes** | ClaimBoundary[] + assignments\n| **Stage 4: VERDICT** | None | Use for per-boundary findings | ClaimVerdict[] (with boundaryFindings[])\n| **Stage 5: AGGREGATE** | None | Use in coverage matrix | OverallAssessment + VerdictNarrative\n\n=== 4.3 Data Flow ===\n\n**Stage 2: EvidenceScope Extraction**  attaches EvidenceScope metadata to each evidence item.\n\n**Stage 3: Boundary Clustering**  groups compatible EvidenceScopes into ClaimBoundaries.\n\nFor full field definitions see [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]. Key fields for boundary clustering:\n\n|= Entity |= Field |= Role in Clustering\n| EvidenceScope | ##methodology## | Primary congruence signal (when available)\n| EvidenceScope | ##temporal## | Primary congruence signal (when available)\n| EvidenceScope | ##boundaries## | Secondary congruence signal (optional)\n| EvidenceScope | ##geographic## | Secondary congruence signal (optional)\n| EvidenceScope | ##additionalDimensions## | Supplementary congruence context (optional)\n| ClaimBoundary | ##id## | Unique identifier (CB_01, CB_02, ...)\n| ClaimBoundary | ##name## / ##shortName## | Human-readable labels\n| ClaimBoundary | ##description## | What this boundary represents\n| ClaimBoundary | ##methodology## | Dominant methodology (if applicable)\n| ClaimBoundary | ##internalCoherence## | 0-1: consistency of evidence within boundary\n| ClaimBoundary | ##evidenceCount## | Number of evidence items in this boundary\n\n----\n\n== 5. Congruence-Based Clustering Rules ==\n\n=== 5.1 The Congruence Assessment Test ===\n\nInstead of pre-creating boundaries from input, FactHarbor uses **congruence assessment**:\n\n> **\"Are these EvidenceScopes compatible enough to cluster into a single ClaimBoundary, or should they be separated?\"**\n\n* **Compatible**  Merge into single ClaimBoundary\n* **Incompatible**  Create separate ClaimBoundaries\n\n=== 5.2 Key Principles ===\n\n1. **Evidence-emergent**: Boundaries emerge from gathered evidence, not from input analysis\n1. **LLM-driven clustering**: Sonnet-tier LLM assesses congruence across 5 factors\n1. **Selective clustering**: Most analyses have 1-3 boundaries (default 1), max 5 (rare)\n1. **Explicit statements only**: Don't invent boundaries the evidence doesn't support\n1. **Congruence focus**: Only separate when evidence scopes are genuinely incompatible\n\n=== 5.3 Congruence Factors (LLM-Evaluated) ===\n\n|= Factor |= Merge If... |= Separate If...\n| **Methodology** | Same or compatible methodology | Fundamentally different approach (e.g., Method A vs Method B)\n| **Boundaries** | Overlapping scope boundaries | Non-overlapping system boundaries (e.g., full system vs subsystem)\n| **Geographic** | Same or overlapping regions | Distinct jurisdictions with different rules (e.g., Jurisdiction J vs K)\n| **Temporal** | Overlapping time periods | Non-overlapping periods (if temporal is primary subject)\n| **Contradiction** | Low contradiction between items | High contradiction driven by scope differences\n\n=== 5.4 Clustering Heuristics (LLM Guidance) ===\n\n**Compatible EvidenceScopes** (merge into single boundary):\n* All evidence uses \"Standard S\" with \"Period P data\"\n* Methodologies are variants of same framework (e.g., \"Framework A v1\" and \"Framework A v2\")\n* Geographic scopes overlap (e.g., \"Region R\" and \"Sub-region within R\")\n* Temporal periods overlap and not primary subject\n\n**Incompatible EvidenceScopes** (separate boundaries):\n* \"Methodology A (full system)\" vs \"Methodology B (subsystem)\"  different methodological approaches\n* \"Jurisdiction J (Framework F)\" vs \"Jurisdiction K (Framework G)\"  different regulatory frameworks\n* \"Period P\" vs \"Period Q\"  if temporal period is primary subject (e.g., \"compare effectiveness in Period P vs Period Q\")\n\n----\n\n== 6. Stage 3: CLUSTER BOUNDARIES Implementation ==\n\n=== 6.1 Clustering Process ===\n\n**Input**: EvidenceItem[] (each with EvidenceScope), AtomicClaim[]\n\n**Output**: ClaimBoundary[] + evidence assignments (claimBoundaryId per item)\n\n**Process**:\n\n1. **Collect unique EvidenceScopes**  extract all distinct scopes from gathered evidence\n1. **LLM clustering call**  single Sonnet-tier call that:\n1*. Groups EvidenceScopes with compatible methodology, boundaries, geography, and temporal period\n1*. Separates scopes where evidence is contradictory due to different methodological assumptions\n1*. Names each cluster as a ClaimBoundary with human-readable label\n1*. Provides scopeToBoundaryMapping\n1. **Assign evidence to boundaries**  set claimBoundaryId on each EvidenceItem\n1. **Coherence assessment**  for each boundary, compute internalCoherence (0-1)\n1. **Post-clustering validation**  deterministic checks:\n1*. Every boundary has non-empty id, name, at least 1 evidence item\n1*. Every evidence item assigned to exactly one boundary\n1*. No duplicate boundary IDs\n1*. If validation fails  fallback to single \"General\" boundary\n\n**Default boundary**: If evidence doesn't have meaningful scope distinctions (all sources use similar methodology/temporal/boundaries), all evidence clusters into a single \"General\" boundary.\n\n=== 6.2 Clustering Criteria Examples (Generic) ===\n\n==== Example 1: Methodological Split ====\n\n**EvidenceScopes**:\n* 5 items: ##{ methodology: \"Methodology A\", temporal: \"Period P\", boundaries: \"Full system\" }##\n* 3 items: ##{ methodology: \"Methodology B\", temporal: \"Period P\", boundaries: \"Subsystem\" }##\n\n**Result**: 2 ClaimBoundaries\n* CB_01: \"Methodology A Studies (Full System)\"\n* CB_02: \"Methodology B Studies (Subsystem)\"\n\n**Rationale**: Different methodologies + different boundaries  incompatible\n\n==== Example 2: Geographic Split ====\n\n**EvidenceScopes**:\n* 4 items: ##{ methodology: \"Framework F\", temporal: \"Period P\", geographic: \"Jurisdiction J\" }##\n* 2 items: ##{ methodology: \"Framework G\", temporal: \"Period P\", geographic: \"Jurisdiction K\" }##\n\n**Result**: 2 ClaimBoundaries\n* CB_01: \"Jurisdiction J Proceedings\"\n* CB_02: \"Jurisdiction K Proceedings\"\n\n**Rationale**: Different jurisdictions + different frameworks  incompatible\n\n==== Example 3: Compatible Evidence (Single Boundary) ====\n\n**EvidenceScopes**:\n* All 8 items: ##{ methodology: \"Standard S\", temporal: \"Period P-Q\", boundaries: \"Measurement Boundary B\" }##\n\n**Result**: 1 ClaimBoundary\n* CB_01: \"General Evidence (Standard S)\"\n\n**Rationale**: All scopes compatible  merge into single boundary\n\n----\n\n== 7. Boundary Count and Reliability ==\n\n=== 7.1 Thresholds and Limits ===\n\n|= Boundary Count |= Status |= Meaning\n| **1** | Default | Most common  all evidence uses compatible scopes\n| **2-3** | Healthy | Typical for analyses with distinct methodological or jurisdictional frames\n| **4-5** | High | Complex analytical frame with multiple incompatible boundaries\n| **5+** | Limit | System enforces soft cap via prompt guidance (merge most similar)\n\n=== 7.2 Multi-Boundary Verdict Structure ===\n\n**Single Boundary (Most Common)**: ClaimVerdict has 1 BoundaryFinding. Per-boundary assessment = overall assessment.\n\n**Multiple Boundaries (Distinct Frames)**: ClaimVerdict has multiple BoundaryFindings. Each shows how evidence within that boundary supports/contradicts the claim.\n\n**Example** (generic):\n* Claim: \"Process X is effective\"\n* Boundary CB_01 (Methodology A): 85% support\n* Boundary CB_02 (Methodology B): 40% support\n* Overall: Weighted average with triangulation factor\n* Narrative: \"Evidence diverges across methodologies  Methodology A shows strong support, Methodology B shows weak support\"\n\n----\n\n== 8. Implementation Reference ==\n\n=== 8.1 Core Files ===\n\n|= File |= Purpose\n| ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts## | Main pipeline entry point (5 stages)\n| ##apps/web/src/lib/analyzer/verdict-stage.ts## | Verdict stage module (5-step debate pattern)\n| ##apps/web/src/lib/analyzer/types.ts## | TypeScript types (ClaimBoundary, EvidenceScope, AtomicClaim, etc.)\n| ##apps/web/prompts/claimboundary.prompt.md## | UCM-managed prompts (BOUNDARY_CLUSTERING section)\n\n**Prompt Templates**:\n* ##BOUNDARY_CLUSTERING##  Congruence-based scope clustering guidance\n* ##CLAIM_EXTRACTION_PASS1## / ##PASS2##  Evidence-grounded claim extraction\n* ##VERDICT_ADVOCATE## / ##CHALLENGER## / ##RECONCILIATION##  5-step debate pattern\n\n=== 8.2 Key Functions ===\n\n{{code language=\"typescript\"}}\n// Stage 3: CLUSTER BOUNDARIES\nasync function clusterBoundaries(\n  evidenceItems: EvidenceItem[],\n  atomicClaims: AtomicClaim[]\n): Promise<ClaimBoundary[]>\n\n// Collect unique scopes\nfunction collectUniqueScopes(items: EvidenceItem[]): EvidenceScope[]\n\n// LLM call for clustering\nasync function llm.clusterEvidenceScopes(params): Promise<ClusteringResult>\n\n// Post-clustering validation\nfunction validateBoundaries(boundaries: ClaimBoundary[], evidence: EvidenceItem[]): boolean\n{{/code}}\n\n=== 8.3 Configuration ===\n\n{{code language=\"json\"}}\n{\n  \"pipeline\": {\n    \"maxClaimBoundaries\": 5,  // Soft cap (prompt guidance)\n    \"boundaryClusteringModel\": \"sonnet\",  // LLM tier for clustering\n    \"congruenceGuidance\": \"from_architecture_doc_11_5\"  // Genericized examples\n  }\n}\n{{/code}}\n\n----\n\n== 9. Examples ==\n\n=== 9.1 Methodological Boundary  Distinct ClaimBoundaries ===\n\n**Input**: \"Process X is more effective than Process Y\"\n\n**EvidenceScopes**:\n* Source A: Methodology A / Full system boundary\n* Source B: Methodology B / Subsystem boundary\n\n**Result**: 2 ClaimBoundaries\n* CB_01: \"Methodology A Studies (Full System)\"\n* CB_02: \"Methodology B Studies (Subsystem)\"\n\n=== 9.2 Geographic Boundary  Distinct ClaimBoundaries ===\n\n**Input**: \"Entity A violated regulations\"\n\n**EvidenceScopes**:\n* Source A: Jurisdiction J / Framework F\n* Source B: Jurisdiction K / Framework G\n\n**Result**: 2 ClaimBoundaries\n* CB_01: \"Jurisdiction J Proceedings\"\n* CB_02: \"Jurisdiction K Proceedings\"\n\n=== 9.3 Compatible Evidence  Single ClaimBoundary ===\n\n**Input**: \"Trend T is accelerating\"\n\n**EvidenceScopes**:\n* All sources: Standard S / Period P-Q / Measurement Boundary B\n\n**Result**: 1 ClaimBoundary\n* CB_01: \"General Evidence (Standard S)\"\n\n=== 9.4 Temporal Subject  Distinct ClaimBoundaries ===\n\n**Input**: \"Policy effectiveness changed from Period P to Period Q\"\n\n**EvidenceScopes**:\n* Source A: Standard S / Period P data\n* Source B: Standard S / Period Q data\n\n**Result**: 2 ClaimBoundaries (temporal is primary subject)\n* CB_01: \"Period P Evidence\"\n* CB_02: \"Period Q Evidence\"\n\n----\n\n== 10. Related Documentation ==\n\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict calculation, boundary aggregation\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  Gate 1 (claim validation), Gate 4 (confidence)\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  Pipeline architecture (ClaimBoundary is now default)\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]  ClaimBoundary vs EvidenceScope definitions\n* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]  Evidence filtering and classification\n* [[Boundary Definition Guidelines>>FactHarbor.Product Development.DevOps.Guidelines.Scope Definition Guidelines.WebHome]]  When to use EvidenceScope vs ClaimBoundary\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] | Next: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Direction Semantics.WebHome": "= Direction Semantics =\n\n{{info}}\n**Developer Reference**  This page documents the multi-layer direction semantics in FactHarbor's analysis pipeline. Understanding these semantics is critical for maintaining LLM-code alignment and avoiding direction confusion bugs.\n\n**Key Files**: ##apps/web/src/lib/analyzer/types.ts##, ##apps/web/src/lib/analyzer/verdict-stage.ts##, ##apps/web/prompts/claimboundary.prompt.md##\n{{/info}}\n\n**Version**: 2.11.0 (ClaimAssessmentBoundary pipeline, 2026-02-19)\n**Status**: Operational (Stage 4 advisory LLM validation  grounding + direction; no auto-correct in CB pipeline)\n\n----\n\n== 1. The Core Concern\n\n**LLM interpretation must match source-code interpretation exactly**, or the system will produce inconsistent results. Direction semantics are the most common source of LLM-code misalignment.\n\nThe system has multiple \"direction\" concepts that can easily conflate, leading to:\n- Wrong verdict flips during auto-correction\n- Scope mismatch between evidence direction and sub-claim verdicts\n- Counter-claims being treated incorrectly\n\n----\n\n== 2. The Direction Stack ==\n\nThe system has **four distinct direction layers**:\n\n{{mermaid}}\ngraph TB\n    subgraph Layer4[\"Layer 4: VERDICT DIRECTION\"]\n        V[\"verdict.truthPercentage: 0-100<br/>'Is this sub-claim TRUE or FALSE?'\"]\n    end\n\n    subgraph Layer3[\"Layer 3: EVIDENCE DIRECTION\"]\n        E[\"claimDirection: supports | contradicts | neutral<br/>'Does this evidence SUPPORT or CONTRADICT<br/>the ORIGINAL claim?'\"]\n    end\n\n    subgraph Layer2[\"Layer 2: CLAIM DIRECTION\"]\n        C[\"isCounterClaim: boolean<br/>'Does this sub-claim support or oppose<br/>the main thesis?'\"]\n    end\n\n    subgraph Layer1[\"Layer 1: THESIS DIRECTION\"]\n        T[\"impliedClaim: string<br/>'What is the original user claim?'\"]\n    end\n\n    Layer1 --> Layer2\n    Layer2 --> Layer3\n    Layer3 --> Layer4\n\n    style Layer1 fill:#e8f5e9,color:#000\n    style Layer2 fill:#fff9c4,color:#000\n    style Layer3 fill:#e3f2fd,color:#000\n    style Layer4 fill:#f3e5f5,color:#000\n{{/mermaid}}\n\n|= Layer |= Field |= Question Answered |= Scope\n| **1. Thesis** | ##impliedClaim## | What is the original user claim? | Entire analysis\n| **2. Claim** | ##isCounterClaim## | Does this sub-claim support or oppose the thesis? | Per sub-claim\n| **3. Evidence** | ##claimDirection## | Does evidence support or contradict the ORIGINAL claim? | Per evidence item\n| **4. Verdict** | ##truthPercentage## | Is this sub-claim TRUE or FALSE? | Per sub-claim verdict\n\n**Critical**: Each layer has a different scope. Conflating them causes direction confusion bugs.\n\n----\n\n== 3. The Confusion Problem ==\n\n=== 3.1 Example Scenario ===\n\n**Original Claim (Thesis)**: \"The trial was fair\"\n\n**Sub-claim SC1**: \"The judge recused themselves due to conflict of interest\"\n- ##isCounterClaim: true## (opposes thesis that trial was fair)\n- Evidence confirms recusal\n- Evidence ##claimDirection: \"supports\"## (supports SC1 being true)\n- **But SC1 being TRUE means the trial was NOT fair**\n\n**Sub-claim SC2**: \"The defendant received legal representation\"\n- ##isCounterClaim: false## (supports thesis that trial was fair)\n- Evidence confirms representation\n- Evidence ##claimDirection: \"supports\"## (supports SC2 being true)\n- **SC2 being TRUE means the trial WAS fair**\n\n=== 3.2 The Direction Matrix ===\n\n|= Evidence Direction |= Claim Type |= Verdict Implication\n| ##supports## SC1 | Counter-claim | Thesis = FALSE\n| ##contradicts## SC1 | Counter-claim | Thesis = TRUE\n| ##supports## SC2 | Thesis-aligned | Thesis = TRUE\n| ##contradicts## SC2 | Thesis-aligned | Thesis = FALSE\n\n**The LLM must understand this matrix. If it conflates layers, verdicts flip incorrectly.**\n\n----\n\n== 4. The Scope Mismatch Problem ==\n\n{{mermaid}}\ngraph TD\n    subgraph Evidence[\"EVIDENCE claimDirection\"]\n        Dir[\"claimDirection: 'contradicts'<br/>(relative to ORIGINAL CLAIM)\"]\n        E1[\"Evidence E1: 'Judge recused'<br/> contradicts 'trial was fair'\"]\n    end\n\n    subgraph Verdict[\"SUB-CLAIM VERDICT\"]\n        SC[\"SC1: 'Judge recused'<br/> verdict should be HIGH (true)\"]\n        Mis[\" MISMATCH:<br/>Evidence 'contradicts' but supports SC1\"]\n    end\n\n    Evidence --> Verdict\n\n    style Evidence fill:#e3f2fd,color:#000\n    style Verdict fill:#f3e5f5,color:#000\n    style Mis fill:#ffcdd2,color:#000\n{{/mermaid}}\n\n**The Problem**:\n1. Evidence ##claimDirection## is relative to the ORIGINAL claim\n2. Sub-claim verdict validation uses this direction\n3. But sub-claims may have different direction semantics (counter-claims)\n4. Validator sees \"contradicts\" evidence + HIGH verdict  flags as mismatch\n5. **Wrong flip** if auto-correct is enabled\n\n---\n\n== 5. Implementation ==\n\n=== 5.1 Evidence Direction (claimDirection) ===\n\n**File**: ##apps/web/src/lib/analyzer/types.ts:477##\n\n{{code language=\"typescript\"}}\ninterface EvidenceItem {\n  // Direction relative to ORIGINAL USER CLAIM\n  claimDirection: \"supports\" | \"contradicts\" | \"neutral\";\n}\n{{/code}}\n\n**Prompt Definition**: ##apps/web/prompts/claimboundary.prompt.md## EXTRACT_EVIDENCE section\n\n{{code}}\nFor EVERY extracted evidence item, evaluate claimDirection:\n- \"supports_thesis\": This evidence item provides evidence that SUPPORTS the user's claim being TRUE\n- \"contradicts_thesis\": This evidence item provides evidence that CONTRADICTS the user's claim\n- \"contextual\": This evidence item is contextual/background information\n\nCRITICAL: claimDirection is ALWAYS relative to the ORIGINAL USER CLAIM, not sub-claims.\n{{/code}}\n\n=== 5.2 Counter-Claim Detection (isCounterClaim) ===\n\n**File**: ##apps/web/src/lib/analyzer/types.ts:563##\n\n{{code language=\"typescript\"}}\ninterface SubClaim {\n  // Does this claim test the OPPOSITE of the thesis?\n  isCounterClaim: boolean;\n}\n{{/code}}\n\n**Prompt Definition**: ##apps/web/prompts/claimboundary.prompt.md## CLAIM_EXTRACTION_PASS2 section\n\n{{code}}\nisCounterClaim = true when the claim evaluates the OPPOSITE position:\n- Thesis: \"X is fair\"  Claim: \"X violated due process\"  isCounterClaim: true\n\nWHY THIS MATTERS: Counter-claims have their verdicts INVERTED during aggregation.\n{{/code}}\n\n=== 5.3 Verdict Direction Validation ===\n\n**Version**: v2.11.0 (ClaimAssessmentBoundary pipeline, Stage 4)\n\n**File**: ##apps/web/src/lib/analyzer/verdict-stage.ts:421## (##validateVerdicts##)\n\nThe CB pipeline runs **two advisory LLM checks** as part of Stage 4 verdict generation  both are non-blocking. Verdicts are returned unchanged regardless of validation findings:\n\n{{code language=\"typescript\"}}\n// verdict-stage.ts:421\nexport async function validateVerdicts(\n  verdicts: CBClaimVerdict[],\n  evidence: EvidenceItem[],\n  llmCall: LLMCallFn,\n): Promise<CBClaimVerdict[]> {\n  // Check A: Grounding validation (Haiku)\n  // Prompt: VERDICT_GROUNDING_VALIDATION\n  // Checks: reasoning  evidence alignment\n  const groundingResult = await llmCall(\"VERDICT_GROUNDING_VALIDATION\", { ... }, { tier: \"haiku\" });\n\n  // Check B: Direction validation (Haiku)\n  // Prompt: VERDICT_DIRECTION_VALIDATION\n  // Checks: verdict % direction vs evidence direction\n  const directionResult = await llmCall(\"VERDICT_DIRECTION_VALIDATION\", { ... }, { tier: \"haiku\" });\n\n  // Validation is advisory  verdicts are returned unchanged\n  return verdicts;\n}\n{{/code}}\n\n**Prompt Definition**: ##apps/web/prompts/claimboundary.prompt.md## VERDICT_DIRECTION_VALIDATION section\n\n{{code}}\nFor each verdict:\n- Read the sub-claim being rated\n- Read the evidence statements (ignore direction labels  assess meaning directly)\n- Determine whether the evidence supports or contradicts the sub-claim\n- Compare against the verdict percentage\n\nA verdict is MISALIGNED if:\n- Evidence mostly supports the sub-claim BUT verdict is below 43%\n- Evidence mostly contradicts the sub-claim BUT verdict is above 72%\n- Note: Contestation or dispute about interpretation does NOT make a factual claim false\n\nSemantic rules:\n- Requirement evidence vs outcome evidence must be interpreted explicitly\n- Verification mechanisms (peer review/audit/independent checks) count as corroboration\n- Thesis-critical qualifiers (\"without\", \"requires\", \"only if\", temporal scope) must be preserved\n- Interpretation dispute lowers confidence but does not invert factual direction by itself\n{{/code}}\n\n**Critical**: \"ignore direction labels  assess meaning directly\"  This resolves the scope mismatch by having the LLM evaluate the sub-claim directly, not via original-claim direction labels.\n\n=== 5.4 Validation Mode  Advisory Only ===\n\n**Direction validation in the CB pipeline is advisory. No auto-correct.**\n\n{{code language=\"typescript\"}}\n// verdict-stage.ts  validation issues are logged but do not modify verdicts\nfor (const dr of directionResults) {\n  if (dr.directionValid === false) {\n    console.warn(`[VerdictStage] Direction issue for claim ${dr.claimId}:`, dr.issues);\n  }\n}\n// Verdicts are returned unchanged  validation is advisory (8.4)\nreturn verdicts;\n{{/code}}\n\n**Design rationale**: The CB pipeline uses a 5-step LLM debate pattern (Advocate  Challenger  Reconciliation  Grounding Validation  Direction Validation). By the time direction validation runs, the verdict has already been through adversarial challenge and reconciliation. Direction validation is a final sanity check  mismatch issues are logged for monitoring, not corrected automatically.\n\n**Note on the Orchestrated pipeline**: The removed Orchestrated pipeline had auto-correct enabled (##suggestedPct## from LLM, capped fallback formula). The CB pipeline deliberately removed auto-correct in favour of the debate pattern producing a more reliable initial verdict.\n\n=== 5.5 Validation Failure Behavior ===\n\nIf the VERDICT_DIRECTION_VALIDATION or VERDICT_GROUNDING_VALIDATION LLM calls fail, issues are emitted as ##console.warn## and the pipeline continues:\n\n{{code language=\"typescript\"}}\n// verdict-stage.ts  non-blocking validation\nfor (const gr of groundingResults) {\n  if (gr.groundingValid === false) {\n    console.warn(`[VerdictStage] Grounding issue for claim ${gr.claimId}:`, gr.issues);\n  }\n}\n{{/code}}\n\n**Operational impact**: A validation failure (LLM call error or parse failure) does not affect verdict output. The pipeline completes normally. Monitor server logs for ##[VerdictStage]## warnings.\n\n=== 5.6 Qualifier Preservation in Claim Decomposition (2026-02-13) ===\n\nDirection consistency depends on preserving thesis-critical qualifiers from understanding to verdict validation.\n\nPrompt contract in ##claimboundary.prompt.md##:\n- ##CLAIM_EXTRACTION_PASS2##: \"QUALIFIER PRESERVATION (CRITICAL)\" block requires preserving negation/modality/independence/temporal qualifiers in atomic claims.\n- ##CLAIM_VALIDATION##: Gate 1 validation checks fidelity of extracted claims to original user input  prevents qualifier loss.\n- ##VERDICT_DIRECTION_VALIDATION##: Semantic interpretation rules to prevent implication inversion.\n\n**Why this matters**:\n- If qualifiers are dropped (for example \"without independent corroboration\"), direction-validation mismatch warnings can become false positives.\n- Preserving qualifiers keeps Layer 1 thesis semantics aligned with Layer 4 verdict interpretation.\n\n----\n\n== 6. Contestation vs Contradiction ==\n\n**CRITICAL DISTINCTION**:\n\n|= Concept | Meaning | Effect on Verdict\n| **Contestation** | Stakeholders dispute interpretation/completeness | Reduces confidence, NOT verdict direction\n| **Contradiction** | Evidence directly refutes the factual claim | Affects verdict direction\n\n**Contestation Fields** (assertion-level, NOT source-level):\n\n{{code language=\"typescript\"}}\n// ClaimVerdict: types.ts:315-318\ninterface ClaimVerdict {\n  isContested: boolean;\n  contestedBy: string;\n  factualBasis: \"established\" | \"disputed\" | \"opinion\" | \"unknown\";\n}\n\n// EvidenceItem: types.ts:471\ninterface EvidenceItem {\n  isContestedClaim: boolean;\n  claimSource: string;\n}\n{{/code}}\n\n**Prompt Definition**: ##apps/web/prompts/claimboundary.prompt.md## VERDICT_RECONCILIATION section\n\n{{code}}\nCONTESTED  FALSE:\n- If evidence confirms the factual component being rated BUT stakeholders dispute\n  interpretation  verdict should be >=50% with reduced confidence\n- If evidence directly refutes the factual component  verdict should be <50%\n- Contestation affects confidence, not direction, for the factual component being rated\n{{/code}}\n\n**Example**:\n- Claim: \"Company released 3M pages of documents\"\n- Evidence: Company released 3M pages (confirmed)\n- Contestation: \"Only half of what was expected\"\n- **Correct verdict**: 70% (Mostly True - factual claim confirmed)\n- **Correct confidence**: 65% (reduced due to completeness dispute)\n\n----\n\n== 7. LLM-Prompt Contract Requirements ==\n\nFor the LLM to interpret direction exactly as code does, the prompt must document semantics explicitly:\n\n=== 7.1 Define Direction Relative to Original Claim ===\n\n{{code}}\nclaimDirection is ALWAYS relative to the ORIGINAL USER CLAIM, not sub-claims.\n\nExample:\n- Original claim: \"Trial was fair\"\n- Evidence: \"Judge recused due to conflict\"\n- claimDirection: \"contradicts\" (contradicts \"trial was fair\")\n\nEven if this evidence is cited for a sub-claim about recusal, the direction\nis still relative to the ORIGINAL claim.\n{{/code}}\n\n=== 7.2 Explain Counter-Claim Semantics ===\n\n{{code}}\nCounter-claims (isCounterClaim: true) test the OPPOSITE of the thesis.\n\nIf evidence supports a counter-claim:\n- The counter-claim is TRUE\n- The thesis is FALSE\n- verdict should be HIGH for the counter-claim\n- This does NOT mean the evidence \"contradicts\" the counter-claim\n{{/code}}\n\n=== 7.3 Require Explicit Evidence-Claim Mapping ===\n\n{{code}}\nsupportingEvidenceIds (REQUIRED): Cite at least one evidence ID when supporting\nevidence exists; if none exists, use [] and explicitly state insufficiency in reasoning.\n{{/code}}\n\n=== 7.4 Multilingual Semantic Invariance ===\n\n{{code}}\nCRITICAL: Direction semantics must hold across languages, not just English keywords.\n\nThe semantic interpretation of:\n- claimDirection: supports/contradicts/neutral\n- isCounterClaim: true/false\n- Contestation vs Contradiction\n\nMust be invariant across all languages. The LLM must assess MEANING, not just\nkeyword matching. A claim that \"X tait injuste\" (French: \"X was unfair\") must\nbe treated identically to \"X was unfair\" in English for direction purposes.\n\nPrompt instructions use English keywords for consistency, but LLM interpretation\nmust be language-agnostic at the semantic level.\n{{/code}}\n\n----\n\n== 8. Anti-Pattern Warning ==\n\n{{warning}}\n**DO NOT** use Layer 3 evidence direction counters alone to auto-correct Layer 4 verdicts.\n\n**Why**: Layer 3 (##claimDirection##) is scoped to the ORIGINAL claim. Layer 4 (##truthPercentage##) is scoped to the SUB-CLAIM. These are different scopes.\n\n**Wrong approach** (causes direction confusion bugs):\n{{code language=\"typescript\"}}\n//  WRONG: Auto-correct based on claimDirection counters\nif (supportCount > contradictCount && verdictPct < 43) {\n  verdictPct = 65; // Wrong flip!\n}\n{{/code}}\n\n**Correct approach** (use LLM semantic validation):\n{{code language=\"typescript\"}}\n//  CORRECT: LLM evaluates sub-claim + evidence semantically (verdict-stage.ts:421)\n// The CB pipeline runs VERDICT_DIRECTION_VALIDATION with:\n// - Full evidence pool (id, statement, claimDirection)\n// - Verdict truth percentages per claim\n// - LLM assesses meaning directly, ignoring direction label scope issues\nawait validateVerdicts(verdicts, evidence, llmCall);\n{{/code}}\n\n**Current status**: Direction validation in the CB pipeline is **advisory** (non-blocking). The 5-step LLM debate pattern (Advocate  Challenger  Reconciliation) produces reliable initial verdicts; direction validation is a final logging check. No auto-correct. See ##verdict-stage.ts:421##.\n{{/warning}}\n\n----\n\n== 9. Warning Metadata ==\n\nWhen direction mismatches are detected, warning payloads include **legacy directional metadata** from the old counter-based approach. This metadata is kept for debugging but is **NOT reliable for sub-claim direction assessment**:\n\n{{code language=\"typescript\"}}\ninterface DirectionMismatchWarning {\n  type: \"verdict_direction_mismatch\";\n  severity: \"warning\";\n  message: string;\n  details: {\n    claimId: string;\n    verdictPct: number;\n    legacyDirectionalMetadata: {\n      supportPct: number;       // Based on claimDirection counters\n      contradictPct: number;    // RELATIVE TO ORIGINAL CLAIM\n      neutralPct: number;       // NOT reliable for sub-claim direction\n      note: \"Based on original-claim direction labels, not semantic evaluation\";\n    };\n    llmAssessment?: string;     // LLM's semantic evaluation reason\n    evidenceIds: string[];      // Which evidence was considered\n  };\n}\n{{/code}}\n\n**Important**: Use ##llmAssessment## for accurate direction evaluation. The ##legacyDirectionalMetadata## is kept for backward compatibility and debugging only.\n\n----\n\n== 10. Verification Checklist ==\n\nTo ensure LLM-code alignment:\n\n|= Check |= Code Location |= Prompt Location\n| ##claimDirection## defined relative to original claim | ##types.ts:711## (CBEvidenceItem) | ##claimboundary.prompt.md## EXTRACT_EVIDENCE\n| Counter-claim direction semantics | ##types.ts:514## (isCounterClaim); inverted in ##aggregation.ts:132## | ##claimboundary.prompt.md## CLAIM_EXTRACTION_PASS2\n| Qualifier preservation for decomposition | N/A (prompt contract) | ##claimboundary.prompt.md## CLAIM_EXTRACTION_PASS2 + CLAIM_VALIDATION\n| Evidence-to-verdict mapping (advisory) | ##verdict-stage.ts:421## (validateVerdicts) | ##claimboundary.prompt.md## VERDICT_DIRECTION_VALIDATION\n| Auto-correct: N/A  advisory validation only | ##verdict-stage.ts:468## (verdicts returned unchanged) | N/A\n| Contestation  False | ##types.ts:269-275## (factualBasis), ##types.ts:407## (isContestedClaim) | ##claimboundary.prompt.md## VERDICT_RECONCILIATION\n| LLM per-claim direction validation | ##verdict-stage.ts:438## (VERDICT_DIRECTION_VALIDATION call) | ##claimboundary.prompt.md## VERDICT_DIRECTION_VALIDATION\n| Semantic interpretation rules for direction | ##verdict-stage.ts:421## | ##claimboundary.prompt.md## VERDICT_DIRECTION_VALIDATION\n| Contestation fields separate from direction | ##types.ts:407## (isContestedClaim, contestedBy) | ##claimboundary.prompt.md## VERDICT_RECONCILIATION\n| Validation failure handling | ##verdict-stage.ts:456-466## (console.warn, non-blocking) | N/A (code-only)\n\n----\n\n== 11. Common Pitfalls ==\n\n|= Pitfall |= Symptom |= Fix\n| **Conflating layers** | Wrong verdict flips | Use LLM per-claim validation\n| **Counter-based auto-correct** | Correct verdicts flipped to wrong | Use LLM ##suggestedPct## for correction magnitude, not deterministic inversion\n| **Uncapped deterministic correction** | Extreme swings (5%95%) from binary signal | Cap fallback formula to moderate range (55-75 / 25-45)\n| **Contestation treated as contradiction** | High verdicts become low | Separate contestation from direction in prompts\n| **Evidence direction used for sub-claim validation** | Counter-claims flagged incorrectly | Use semantic LLM validation instead\n| **Ignoring degraded mode** | Silent validation failures | Monitor ##direction_validation_degraded## warnings\n\n----\n\n== 12. Related Documentation ==\n\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] -- Verdict scale, aggregation, counter-evidence handling\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] -- Gate 1 (claim validation) and Gate 4 (confidence assessment)\n* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] -- 7-layer evidence filtering strategy\n* [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] -- ClaimAssessmentBoundary and EvidenceScope detection methodology\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] -- ClaimAssessmentBoundary vs Monolithic Dynamic pipeline overview\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] | Next: [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome": "= Evidence Quality Filtering Architecture =\n\n{{info}}\n**Developer Reference**  Deterministic post-processing filter that enforces probative value standards on LLM-extracted evidence, ensuring only well-attributed, specific evidence reaches verdict aggregation.\n\n**Key File**: ##apps/web/src/lib/analyzer/evidence-filter.ts##\n{{/info}}\n\n**Version**: 2.6.42\n**Date**: 2026-02-02\n\n----\n\n== 1. Introduction ==\n\n=== 1.1 Purpose ===\n\nThe Evidence Quality Filter is a **deterministic post-processing layer** that removes low-quality evidence items that slip through the LLM extraction process. It enforces **probative value standards** to ensure only well-attributed, specific evidence reaches the verdict aggregation stage.\n\n=== 1.2 Problem Statement ===\n\nDuring evidence extraction, LLMs may occasionally extract items that:\n\n* Use vague attribution (\"some say\", \"many believe\")\n* Lack concrete source excerpts or URLs\n* Are too short to be meaningful\n* Duplicate existing evidence\n* Fail category-specific quality checks\n\n=== 1.3 Solution ===\n\nA **multi-layer defense strategy** combines LLM instruction (soft enforcement) with deterministic filtering (hard enforcement) across 7 layers. Layers 1-2 operate pre-verdict (evidence quality); Layers 3-7 operate post-verdict during aggregation. For the architect-level overview, see [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]].\n\n----\n\n== 2. Multi-Layer Claim Filtering Defense ==\n\n=== 2.1 7-Layer Defense ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Evidence Quality Filtering Pipeline.WebHome\"/}}\n\n=== 2.2 Layer Protection Summary ===\n\n|= Layer |= What It Filters |= Verdict Impact\n| **1** | Vague attribution, missing sources | Evidence never reaches aggregation\n| **2** | LLM-generated text, synthetic URLs | Hallucinated \"evidence\" rejected\n| **3** | Tangential claims with 0 evidence | Claims removed from report\n| **4** | All tangential claims | Claims contribute weight=0 to verdict\n| **5** | Opinion-only keyFactors | Factors removed from report\n| **6** | Opinion-based contestation | Full weight retained (doubt does not equal contestation)\n| **7** | Cross-context evidence | Claims evaluated in correct analytical frame\n\n----\n\n== 3. Two-Layer Enforcement Strategy ==\n\n=== 3.1 Layer 1: LLM Prompts (Soft Enforcement) ===\n\n**Location**: ##apps/web/src/lib/analyzer/prompts/base/extract-evidence-base.ts##\n\nApproximately 85-90% compliance, cost-effective, but inconsistent across providers.\n\n=== 3.2 Layer 2: Deterministic Filter (Hard Enforcement) ===\n\n**Location**: ##apps/web/src/lib/analyzer/evidence-filter.ts##\n\n100% consistent enforcement via ##filterByProbativeValue()##.\n\n=== 3.3 Combined Effect ===\n\n{{mermaid}}\nflowchart LR\n    RAW[\"Raw Evidence<br/>~50% false positive rate\"] --> SOFT[\"Layer 1: LLM Prompts<br/>~85-90% compliance\"]\n    SOFT --> DET[\"Layer 2: Deterministic Filter<br/>filterByProbativeValue()\"]\n    DET --> CLEAN[\"Clean Evidence<br/>~0% false positive rate\"]\n\n    style RAW fill:#ffcdd2,color:#000\n    style SOFT fill:#fff9c4,color:#000\n    style DET fill:#c8e6c9,color:#000\n    style CLEAN fill:#c8e6c9,color:#000\n{{/mermaid}}\n\n//Red = unfiltered evidence with high false positive rate. Yellow = LLM soft enforcement reduces rate to ~10%. Green = deterministic filter brings false positive rate to ~0%.//\n\n**Result**: Layer 1 reduces false positive rate from ~50% to ~10%. Layer 2 reduces from ~10% to ~0%.\n\n----\n\n== 4. Filter Rules ==\n\n=== 4.1 Statement Quality ===\n\n|= Rule |= Value |= Description\n| ##minStatementLength## | 20 characters | Minimum length for a statement to be considered meaningful\n| ##maxVaguePhraseCount## | 2 | Maximum vague phrases allowed (13 vague phrase patterns detected)\n\n**Detected Vague Phrases** include patterns such as: \"some say\", \"many believe\", \"it is thought\", \"reportedly\", \"allegedly\", \"sources say\", and similar attributions lacking specificity.\n\n=== 4.2 Source Linkage ===\n\n|= Rule |= Value |= Description\n| ##requireSourceExcerpt## | ##true## | Every evidence item must include a source excerpt\n| ##minExcerptLength## | 30 characters | Minimum length for a source excerpt\n| ##requireSourceUrl## | ##true## | Every evidence item must link to a source URL\n\n=== 4.3 Category-Specific Rules ===\n\n|= Category |= Requirement |= Rationale\n| **statistic** | ##requireNumber=true##, ##minExcerptLength=50## | Statistics must contain actual numbers and longer excerpts for context\n| **expert_quote** | ##requireAttribution=true## | Quotes must name the expert or institution\n| **event** | ##requireTemporalAnchor=true## | Events must include dates or temporal references\n| **legal_provision** | ##requireCitation=true## | Legal references must cite specific provisions\n\n=== 4.4 Deduplication ===\n\n|= Rule |= Value |= Description\n| ##deduplicationThreshold## | 0.85 | Jaccard similarity threshold; evidence items exceeding this are considered duplicates\n\n----\n\n== 5. Configuration ==\n\nThe ##ProbativeFilterConfig## interface contains all settings, which are admin-editable via UCM CalcConfig.\n\n{{code language=\"typescript\"}}\ninterface ProbativeFilterConfig {\n  minStatementLength: number;        // default: 20\n  maxVaguePhraseCount: number;       // default: 2\n  requireSourceExcerpt: boolean;     // default: true\n  minExcerptLength: number;          // default: 30\n  requireSourceUrl: boolean;         // default: true\n  deduplicationThreshold: number;    // default: 0.85\n  categoryRules: {\n    statistic: { requireNumber: boolean; minExcerptLength: number };\n    expert_quote: { requireAttribution: boolean };\n    event: { requireTemporalAnchor: boolean };\n    legal_provision: { requireCitation: boolean };\n  };\n}\n{{/code}}\n\nAll settings are managed through the UCM CalcConfig administrative interface. Changes take effect on the next analysis run without redeployment.\n\n----\n\n== 6. Classification Fallbacks ==\n\nWhen the LLM fails to classify an evidence item, the system applies **safe defaults** that are conservative without being alarmist:\n\n|= Field |= Fallback Value |= Rationale\n| ##harmPotential## | ##\"medium\"## | Neutral -- does not inflate or deflate harm assessment\n| ##factualBasis## | ##\"unknown\"## | Conservative -- avoids asserting factual basis without evidence\n| ##isContested## | ##false## | Does not reduce weight without evidence of contestation\n| ##sourceAuthority## | ##\"secondary\"## | Neutral middle tier -- neither boosts nor penalizes\n| ##evidenceBasis## | ##\"anecdotal\"## | Weakest credible type -- avoids overstating evidence strength\n\n=== 6.1 Fallback Rate Monitoring ===\n\n|= Fallback Rate |= Status |= Action\n| < 5% | Healthy | Normal operation, no intervention needed\n| 5-10% | Investigate | Review LLM extraction prompts for classification gaps\n| > 10% | Warning | Prompt engineering review recommended\n| > 20% | Critical | Immediate investigation required; may indicate LLM provider regression\n\n----\n\n== 7. Examples ==\n\n=== 7.1 Vague Attribution Filtered ===\n\n{{code}}\nInput evidence:\n  statement: \"Some experts believe this is wrong\"\n  sourceExcerpt: \"\"\n  sourceUrl: \"\"\n\nFilter result: REMOVED\nReasons:\n  - Vague phrase detected: \"some experts believe\" (1 of max 2)\n  - Missing source excerpt (requireSourceExcerpt=true)\n  - Missing source URL (requireSourceUrl=true)\n{{/code}}\n\n=== 7.2 Statistic Without Number Filtered ===\n\n{{code}}\nInput evidence:\n  statement: \"Studies show a significant increase in outcomes\"\n  category: \"statistic\"\n  sourceExcerpt: \"The report indicates improvement\"\n  sourceUrl: \"https://example.com/report\"\n\nFilter result: REMOVED\nReasons:\n  - Category 'statistic' requires a number (requireNumber=true)\n  - Excerpt too short for statistic (30 < minExcerptLength 50)\n{{/code}}\n\n=== 7.3 Well-Formed Evidence Passes ===\n\n{{code}}\nInput evidence:\n  statement: \"The 2024 audit found 14 compliance violations across 3 departments\"\n  category: \"statistic\"\n  sourceExcerpt: \"According to the annual audit report published March 2024, inspectors identified 14 separate compliance violations spanning the finance, operations, and HR departments.\"\n  sourceUrl: \"https://example.gov/audit-2024\"\n\nFilter result: PASS\n  - Statement length: 71 chars (>= 20)\n  - No vague phrases detected\n  - Source excerpt: 162 chars (>= 50 for statistic)\n  - Source URL present\n  - Number detected in statement (\"14\", \"3\")\n{{/code}}\n\n=== 7.4 Duplicate Evidence Filtered ===\n\n{{code}}\nEvidence A: \"The report found 14 compliance violations in 3 departments\"\nEvidence B: \"14 compliance violations were found across three departments per the report\"\n\nJaccard similarity: 0.89 (> threshold 0.85)\nFilter result: Evidence B REMOVED as duplicate of Evidence A\n{{/code}}\n\n----\n\n== 8. Troubleshooting ==\n\n=== 8.1 Common Issues ===\n\n|= Issue |= Cause |= Solution\n| Too much evidence filtered | Filter rules too strict for domain | Review ##minStatementLength## and ##maxVaguePhraseCount## in UCM CalcConfig; consider domain-specific tuning\n| Duplicate evidence not caught | Low similarity between paraphrased items | Lower ##deduplicationThreshold## (default 0.85); consider semantic deduplication in future\n| Category rules rejecting valid evidence | LLM misclassifying evidence categories | Check evidence category assignment in extraction prompts; classification fallbacks may be masking the issue\n| High fallback rate (>10%) | LLM provider not returning classification fields | Review extraction prompt templates in ##extract-evidence-base.ts##; check provider response schemas\n| Statistics passing without numbers | Category misclassified as general evidence | Verify category assignment; ##requireNumber## only applies to ##statistic## category\n\n=== 8.2 Testing ===\n\n**Test Files**:\n* ##apps/web/test/unit/lib/analyzer/evidence-filter.test.ts## -- Evidence filter unit tests\n* ##apps/web/test/unit/lib/analyzer/v2.8-verification.test.ts## -- 7-layer defense enhancement tests\n\n**Coverage**: 53 tests for evidence filter + 30 tests for 7-layer defense enhancements = **83 total tests**.\n\n----\n\n== 9. Related Documentation ==\n\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] -- Quality gate checkpoints (Gate 1, Gate 4)\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] -- Verdict calculation, confidence modulation\n* [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] -- Context-aware routing (Layer 7)\n* [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] -- Source credibility evaluation\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] -- Pipeline architecture and filter integration\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Next: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome": "= KeyFactors Design Decision =\n\n{{warning}}\n**HISTORICAL DESIGN DOCUMENT  Orchestrated Pipeline (v2.6.x era)**\n\nThe KeyFactor concept was implemented in the Orchestrated pipeline (##orchestrated.ts##, removed in v2.11.0). It is **not present** in the ClaimAssessmentBoundary pipeline. In the CB pipeline, the equivalent analytical grouping is the ##ClaimAssessmentBoundary##  evidence-emergent groupings of ##EvidenceScopes## that replace the KeyFactor intermediate layer. See [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] for the current architecture.\n\nThis document is retained for historical context on the design decisions that shaped the current CB pipeline's approach to claim decomposition.\n{{/warning}}\n\n{{info}}\n**Developer Reference**  Design decisions around KeyFactors: decomposition questions that break down a thesis into verifiable dimensions. Covers entity relationships, data flow, contestation structure, and implementation status.\n\n**Key File**: ##apps/web/src/lib/analyzer/orchestrated.ts## (removed in v2.11.0  historical)\n{{/info}}\n\n**Date**: January 2026\n**Version**: 2.6.33\n**Status**: Historical Design Decision Document (Orchestrated pipeline  superseded by CB pipeline)\n\n----\n\n== Executive Summary ==\n\nThis document captures the design decisions around **KeyFactors** in FactHarbor's fact-checking architecture:\n\n1. **KeyFactors are decomposition questions** that break down a thesis into verifiable dimensions\n1. **KeyFactors should be optional and emergent**, not forced templates\n1. **KeyFactors are discovered during understanding**, not verdict generation\n1. **Scenario was rejected** as a first-class entity (derivable from existing data)\n\n----\n\n== 1. What is a KeyFactor? ==\n\n=== Definition ===\n\nA **KeyFactor** is an **evaluation dimension**  a question that must be answered to verify a thesis. It represents a structured decomposition of \"what must be true for the thesis to be true?\"\n\n=== Examples ===\n\n|= Thesis |= KeyFactors (Decomposition Questions)\n| \"The Bolsonaro trial was fair\" | Was due process followed? Was evidence properly considered? Was the judge impartial? Was the outcome proportionate?\n| \"Vaccine X causes autism\" | Is there a documented causal mechanism? Do controlled studies support this? What does scientific consensus say?\n| \"Company Y committed fraud\" | Were financial statements misrepresented? Was there intent to deceive? Were stakeholders harmed?\n\n=== KeyFactor vs Claim ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.KeyFactor Hierarchy.WebHome\"/}}\n\n|= Entity |= Level |= What it represents\n| **Thesis** | Highest | The main assertion to verify\n| **KeyFactor** | Middle | Evaluation dimension / decomposition question\n| **Claim** | Lowest | Atomic verifiable assertion\n\n----\n\n== 2. Entity Relationship Model ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.KeyFactor Entity Model.WebHome\"/}}\n\n----\n\n== 3. Data Flow ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.KeyFactor Data Flow.WebHome\"/}}\n\n----\n\n== 4. Design Decisions ==\n\n=== Decision 1: KeyFactors are Optional and Emergent ===\n\n**Rejected Approach**: Force a fixed template of 5 factors for every analysis.\n\n**Adopted Approach**: KeyFactors emerge from thesis decomposition. The number and type depend on the specific claim being analyzed.\n\n{{code language=\"typescript\"}}\n// BAD: Forced template\nconst FACTORS = [\"Process\", \"Evidence\", \"Impartiality\", \"Proportionality\", \"Compliance\"];\n\n// GOOD: Emergent from thesis\nconst factors = await decomposeThesis(thesis);\n// Could return 2 factors, 5 factors, or none\n{{/code}}\n\n**Rationale**:\n* Different claims require different evaluation dimensions\n* Forcing irrelevant factors creates noise\n* LLM can identify what dimensions actually matter\n\n**Optional Hints (v2.6.18+)**:\nWhile KeyFactors remain emergent, you can optionally provide hints via ##FH_KEYFACTOR_HINTS## environment variable. These are **suggestions only**  the LLM can consider them but is not required to use them. This allows domain-specific guidance without enforcing irrelevant factors.\n\n{{code language=\"bash\"}}\n# In .env.local or environment\nFH_KEYFACTOR_HINTS='[{\"question\":\"Was due process followed?\",\"factor\":\"Due Process\",\"category\":\"procedural\"},{\"question\":\"Was evidence properly considered?\",\"factor\":\"Evidence Basis\",\"category\":\"evidential\"}]'\n{{/code}}\n\n=== Decision 2: KeyFactors are Discovered During Understanding ===\n\n**Rejected Approach**: Generate KeyFactors during verdict generation.\n\n**Adopted Approach**: Discover KeyFactors in Step 1 (Understand), so research can be directed toward answering them.\n\n{{mermaid}}\nflowchart LR\n    subgraph \"Understanding Phase\"\n        T[Thesis] --> KF[KeyFactors]\n        KF --> C[Claims mapped to factors]\n    end\n\n    subgraph \"Research Phase\"\n        C --> R[Research guided by factors]\n    end\n\n    subgraph \"Verdict Phase\"\n        R --> V[Verdicts per factor]\n    end\n{{/mermaid}}\n\n//KeyFactors guide the entire pipeline: discovered early so research and verdict phases benefit from the decomposition.//\n\n**Rationale**:\n* KeyFactors guide what evidence to search for\n* Claims should be organized by the factor they address\n* Verdict is aggregation, not discovery\n\n=== Decision 3: Scenario Rejected as First-Class Entity ===\n\n**Rejected**: Creating a ##Scenario## entity to represent competing narratives.\n\n**Rationale**:\n* Scenarios are derivable from existing data (contestation metadata)\n* Adding Scenario increases schema complexity\n* Risk of creating false equivalence between well-supported and poorly-supported interpretations\n* Contestation already captures \"who disputes what\"\n\n**Alternative**: Display \"Competing Interpretations\" as a derived view when relevant.\n\n=== Decision 4: KeyFactor Contestation Structure ===\n\nContestation attaches to KeyFactors (not Claims) because:\n* Political disputes are usually about evaluation dimensions (\"was it fair?\")\n* Not about atomic facts (\"court met on date X\")\n\n{{code language=\"typescript\"}}\n// KeyFactor verdict (per AnalysisContextAnswer)  types.ts\ninterface KeyFactor {\n  factor: string;                // Short label (matches KEY_FACTOR_DEF.factor)\n  supports: \"yes\" | \"no\" | \"neutral\";\n  explanation: string;\n\n  // Contestation\n  isContested: boolean;\n  contestedBy: string;           // e.g. \"stakeholder group\", \"opposition coalition\"\n  contestationReason: string;    // Why the finding is contested\n  factualBasis: \"established\" | \"disputed\" | \"opinion\" | \"unknown\";\n}\n{{/code}}\n\n**factualBasis Rules (v2.8  Affects Weight Calculation):**\n\n|= Value |= Meaning |= Weight (v3.1) |= Example\n| ##established## | Strong documented counter-evidence | 0.5x | Court transcripts showing bias\n| ##disputed## | Some factual counter-evidence | 0.7x | Conflicting expert opinions\n| ##opinion## | No factual counter-evidence (DOUBTED) | 1.0x | \"Critics say it was unfair\"\n| ##alleged## | Accusation without evidence (DOUBTED) | 1.0x | Political statements\n| ##unknown## | Cannot determine | 1.0x | Insufficient information\n\n//Note: Weights above reflect v3.1 contestation defaults in ##aggregation.ts##. The Orchestrated pipeline used 0.3x/0.5x; updated to 0.5x/0.7x in v2.9.0 to avoid double-penalising contested claims.//\n\n**Key Distinction (v2.8):**\n* **DOUBTED** (##opinion##/##alleged##/##unknown##): Political rhetoric without evidence -> Full weight\n* **CONTESTED** (##established##/##disputed##): Has documented counter-evidence -> Reduced weight\n\n----\n\n== 5. Relationship to Claims ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.KeyFactor Claim Mapping.WebHome\"/}}\n\n//Blue = KeyFactors (evaluation dimensions). Orange = Claims (atomic verifiable assertions). Each factor is answered by one or more claims.//\n\n**KeyFactor verdict** = Aggregation of claim verdicts that address that factor.\n\n----\n\n== 6. Implementation Status ==\n\n|= Feature |= Status |= Notes\n| Discovery in Understanding | Complete | Schema includes ##keyFactors## array\n| Emergent factors | Complete | No forced template\n| Optional hints | Complete | ##FH_KEYFACTOR_HINTS## env var provides suggestions (not enforced)\n| Claim-to-factor mapping | Complete | ##keyFactorId## preserved in ##ClaimVerdict## (fixed 2026-01-06)\n| Factor aggregation | Fixed | Aggregation logic finds mapped claims (validation needed)\n| Report display | Complete | Both question and article modes\n| Contestation | Partial | Schema supports it but not fully implemented\n\n----\n\n== Summary ==\n\n|= Aspect |= Decision\n| **What is KeyFactor?** | Evaluation dimension / decomposition question\n| **When discovered?** | During Understanding phase\n| **Required?** | No  optional, 0-N per analysis\n| **Fixed template?** | No  emergent from thesis\n| **Optional hints?** | Yes  ##FH_KEYFACTOR_HINTS## provides suggestions (not enforced)\n| **Contestation?** | Attaches to KeyFactors\n| **Scenario entity?** | Rejected  derivable from contestation\n| **Relationship to Claims** | Parent-child (factor answered by claims)\n\n----\n\n== Related Documentation ==\n\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict aggregation including KeyFactor-level verdicts\n* [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]  Entity relationships\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  Current CB pipeline (5-stage) and Monolithic Dynamic\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome": "= Pipeline Variants =\n\n{{info}}\n**Developer Reference**  Twin-path pipeline architecture: the two selectable analysis variants, their shared primitives, invariants, result model, and configuration.\n\n**Key Files**: ##claimboundary-pipeline.ts##, ##verdict-stage.ts##, ##monolithic-dynamic.ts##, ##apps/web/prompts/claimboundary.prompt.md##, ##apps/web/prompts/monolithic-dynamic.prompt.md##\n{{/info}}\n\n== Architecture Overview ==\n\nFactHarbor supports two analysis pipeline variants. Both share the same infrastructure primitives but differ in orchestration approach and result flexibility.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Pipeline Variant Dispatch.WebHome\"/}}\n\n//Green = comprehensive (default), Blue = fast alternative. Both variants converge on a common result envelope for auditability.//\n\n== Non-Negotiable Invariants ==\n\nBoth variants must preserve these architectural invariants:\n\n|= Invariant |= Requirement\n| **Pipeline integrity** | Understand  Research  Verdict (no stage skipping)\n| **Input neutrality** | Question vs. statement divergence target <= 4 points (avg absolute)\n| **Quality gates** | Gate 1 (claim validation) and Gate 4 (confidence) are mandatory\n| **Generic by design** | No domain-specific hardcoding or keyword lists\n| **No synthetic evidence** | Evidence must be attributable to fetched sources with real URLs + excerpts\n| **Fail closed** | Missing/invalid provenance degrades confidence or triggers fallback  never hallucinate\n\n== Variant Comparison ==\n\n|= Criterion |= ClaimAssessmentBoundary |= Monolithic Dynamic\n| **Maturity** | Production-ready (default) | Production-ready (fast alternative)\n| **Result Schema** | Canonical (stable) | Dynamic (flexible)\n| **UI Compatibility** | Full support | Separate viewer needed\n| **Quality Gates** | Gate 1 + Gate 4 | Minimum safety contract\n| **Pipeline Stages** | Fixed 5-stage (Understand  Research  Scope  Boundary  Verdict) | LLM tool-loop\n| **Provenance** | Strict | Minimum requirements\n| **Budget Control** | Iterations / sources | maxSteps / sources\n| **Cost Predictability** | High | Low\n| **Typical Latency** | 2-5 min | 20-60s\n| **Use Case** | Comprehensive analysis (default) | Fast analysis, second opinion\n\n=== Decision Tree ===\n\n{{mermaid}}\nflowchart TD\n    START{\"What is your goal?\"}\n\n    START -->|\"Comprehensive<br/>fact-checking\"| CB[\"Use ClaimAssessmentBoundary<br/>Default choice\"]\n    START -->|\"Fast check or<br/>second opinion\"| FAST{\"Want LLM-driven<br/>result structure?\"}\n\n    FAST -->|\"Yes  streamlined\"| DYN[\"Use Monolithic Dynamic<br/>Fast alternative\"]\n    FAST -->|\"No  standard format\"| CB\n\n    style CB fill:#c8e6c9,color:#000\n    style DYN fill:#e3f2fd,color:#000\n{{/mermaid}}\n\n{{info}}\nThe Orchestrated pipeline (previously the default) was removed in v2.11.0 (2026-02-17) and replaced by ClaimAssessmentBoundary. Historical comparison data for the Orchestrated era is available in the [[archive>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome_arch]].\n{{/info}}\n\n== Monolithic Dynamic Internals ==\n\nSystem and user prompts are loaded from UCM database via ##loadAndRenderSection()## from ##apps/web/prompts/monolithic-dynamic.prompt.md##. Provider-specific structured output sections (##STRUCTURED_OUTPUT_ANTHROPIC##, etc.) are dynamically selected based on the detected LLM provider. Internal execution flow, budget constraints, safety contract, and output schema:\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Monolithic Dynamic Pipeline Internal.WebHome\"/}}\n\n== Shared Primitives ==\n\nBoth variants reuse stable infrastructure primitives. Orchestration logic stays isolated per variant to prevent cross-contamination.\n\n=== What Is Shared ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Pipeline Shared Primitives.WebHome\"/}}\n\n=== What Is Kept Separate ===\n\n* ClaimAssessmentBoundary pipeline orchestration logic (##claimboundary-pipeline.ts##, ##verdict-stage.ts##)\n* Dynamic monolithic tool-loop orchestration logic (##monolithic-dynamic.ts##)\n\n=== Text Analysis Service (v2.9+) ===\n\nLLM-only text analysis (no heuristic fallback), shared by both pipelines:\n\n|= Analysis Point |= Pipeline Phase |= Purpose\n| Input Classification | Understand | Decompose claims, detect comparative/compound\n| Evidence Quality | Research | Filter low-quality evidence, assess probative value\n| Context Similarity | Organize | Merge similar contexts, infer phase buckets\n| Verdict Validation | Aggregate | Detect inversions, harm potential, contestation\n\n== Result Model ==\n\n=== Common Result Envelope ===\n\nEvery job result includes a common envelope regardless of variant:\n\n|= Field |= Type |= Description\n| ##pipelineVariant## | string | ##\"claimboundary\"## / ##\"monolithic_dynamic\"##\n| ##pipelineVersion## | string | Schema version / build info\n| ##budgets## | object | Configured caps (iterations, maxSteps, maxSources, tokens)\n| ##budgetStats## | object | Observed usage\n| ##warnings## | array | Fallback events, provenance rejections\n| ##providerInfo## | object | Model and provider identifiers used\n\n=== Dynamic Payload (Minimum Safety Contract) ===\n\nUsed by ##monolithic_dynamic##. Allows flexible structure but requires:\n\n|= Field |= Required |= Description\n| ##rawJson## | Yes | The model's full JSON output\n| ##citations## | Yes | Array with ##url## (HTTP(S)), ##excerpt## (non-trivial), ##title## (optional)\n| ##narrativeMarkdown## | No | Human-readable explanation\n| ##toolTrace## | No | Tool-call trace (queries, fetched URLs, timestamps)\n\n== Configuration ==\n\n=== Pipeline Selection (UCM) ===\n\n{{code language=\"json\"}}\n{\n  \"defaultPipelineVariant\": \"claimboundary\",\n  \"allowedVariants\": [\n    \"claimboundary\",\n    \"monolithic_dynamic\"\n  ]\n}\n{{/code}}\n\n=== Per-Job Override (API) ===\n\n{{code language=\"json\"}}\nPOST /api/fh/jobs\n{\n  \"inputType\": \"text\",\n  \"inputValue\": \"...\",\n  \"pipelineVariant\": \"claimboundary\"\n}\n{{/code}}\n\nThe selected variant is **persisted on the job at creation time**, ensuring reproducibility even if defaults change later.\n\n== Fallback Behavior ==\n\n|= Variant |= Trigger |= Action\n| **Monolithic Dynamic** | Citation missing | Reject result, require minimum contract\n| **Monolithic Dynamic** | Budget exceeded | Mark job as failed with budget stats\n| **Monolithic Dynamic** | Timeout | Return partial results with warning\n\n== Performance Characteristics ==\n\nTypical analysis time for a 5-10 claim article:\n\n|= Variant |= p50 |= p95 |= Est. Cost\n| **ClaimAssessmentBoundary** | 2-3 min | 5-7 min | $0.15-0.30\n| **Monolithic Dynamic** | 1-2 min | 3-10 min | $0.10-0.50\n\n== Risks and Mitigations ==\n\n|= Risk |= Description |= Mitigation\n| **Complexity creep** | Multiple variants can explode configuration and branching | Single dispatcher + strict shared-primitive boundary\n| **CB regressions** | Refactoring shared primitives can change ClaimAssessmentBoundary behavior | Phase unifications behind thin wrappers; contract tests\n| **Dynamic output safety** | Dynamic payload could omit evidence or mislead users | Minimum safety contract; clear pipeline labelling in UI\n| **Cost/latency tail risk** | Uncontrolled tool loops cause runaway cost | maxSteps + maxSources + timeouts + budget enforcement\n| **Security/abuse risk** | Users pick dynamic path consuming resources differently | Budget enforcement; optionally gate variants later\n\n== Search Provider Requirements ==\n\nBoth pipelines use ##searchWebWithProvider()## and require at least one search provider:\n\n|= Provider |= Environment Variables |= Notes\n| **SerpAPI** | ##SERPAPI_API_KEY## | Pay-per-use (~$0.002/search)\n| **Google CSE** | ##GOOGLE_CSE_API_KEY## + ##GOOGLE_CSE_ID## | Free tier: 100 queries/day\n\nWithout search credentials, analysis continues without external sources (LLM internal knowledge only if ##FH_ALLOW_MODEL_KNOWLEDGE=true##).\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Next: [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome": "= Prompt Architecture =\n\n{{info}}\n**Developer Reference**  UCM-managed prompt system: all runtime LLM prompts load from ##.prompt.md## files via ##loadAndRenderSection()##. TypeScript prompt modules under ##prompts/## are retained for testing only.\n\n**Key Files**: ##apps/web/prompts/*.prompt.md## (runtime), ##apps/web/src/lib/analyzer/prompt-loader.ts## (loader), ##apps/web/src/lib/analyzer/prompts/## (testing harness only)\n{{/info}}\n\n----\n\n== 1. Overview ==\n\nFactHarbor uses **UCM-managed prompt files** (##.prompt.md##) as the single source of truth for all LLM prompts at runtime (**v2.8.2**). Per AGENTS.md String Usage Boundary, all text that goes into LLM prompts must be UCM-managed, not hardcoded in TypeScript.\n\n=== 1.1 Runtime Architecture (Production) ===\n\nBoth pipelines load prompts via ##loadAndRenderSection()## from UCM-managed ##.prompt.md## files:\n\n1. **Section-based prompts**  Each ##.prompt.md## file contains named sections (##~#~# SECTION_NAME##) with variable substitution (##$~{variableName}##)\n1. **Provider-specific structured output**  Separate sections per provider (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##)\n1. **UCM database backing**  Prompt files are seeded into SQLite config DB on first use and can be edited via Admin UI\n\n=== 1.2 Legacy Testing Harness (Test-Only) ===\n\nA separate TypeScript prompt composition system (##buildPrompt()## in ##prompt-builder.ts##) exists under ##apps/web/src/lib/analyzer/prompts/## but is **only used by the ##prompt-testing.ts## test harness**. It is NOT called by any production pipeline.\n\n----\n\n== 2. Directory Structure ==\n\n=== 2.1 Runtime Prompts (UCM-Managed) ===\n\n{{code language=\"none\"}}\napps/web/prompts/\n+-- claimboundary.prompt.md         # ClaimAssessmentBoundary pipeline prompts (13 sections)\n+-- monolithic-dynamic.prompt.md    # Monolithic Dynamic pipeline prompts (7 sections)\n+-- source-reliability.prompt.md    # Source reliability evaluation prompts\n+-- text-analysis/                  # LLM text analysis pipeline prompts\n    +-- *.prompt.md\n    +-- README.md\n{{/code}}\n\nEach ##.prompt.md## file has YAML frontmatter (version, pipeline, variables, requiredSections) followed by ##~#~# SECTION_NAME## headers with prompt content. The ##loadAndRenderSection(pipeline, sectionName, variables)## function extracts and renders sections with variable substitution.\n\n=== 2.2 Testing Harness (TypeScript, Not Used in Production) ===\n\n{{code language=\"none\"}}\napps/web/src/lib/analyzer/prompts/  # TESTING HARNESS ONLY\n+-- prompt-builder.ts              # Composition engine (testing only)\n+-- prompt-testing.ts              # Testing utilities\n+-- OUTPUT_SCHEMAS.md              # Schema documentation\n+-- base/                          # Task-specific base prompts (testing only)\n+-- providers/                     # LLM-specific variants (testing only)\n+-- config-adaptations/            # Configuration adaptations (testing only)\n{{/code}}\n\n----\n\n== 3. Prompt Loading Flow (Runtime) ==\n\n{{mermaid}}\nflowchart LR\n    UCM[\"UCM Database<br/>(config_blobs)\"] --> LOAD[\"loadPromptConfig()<br/>Load active prompt\"]\n    LOAD --> PARSE[\"extractSections()<br/>Parse ## headers\"]\n    PARSE --> RENDER[\"renderSection()<br/>Variable substitution\"]\n    RENDER --> FINAL[\"Rendered Prompt<br/>Section\"]\n\n    style UCM fill:#e3f2fd,color:#000\n    style LOAD fill:#c8e6c9,color:#000\n    style PARSE fill:#fff9c4,color:#000\n    style RENDER fill:#ffe0b2,color:#000\n    style FINAL fill:#e1bee7,color:#000\n{{/mermaid}}\n\n//Blue = UCM database, Green = config loader, Yellow = section parser, Orange = variable substitution, Purple = final output.//\n\nProvider-specific structured output is loaded as a separate section (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##) and concatenated to the main prompt:\n\n{{code language=\"typescript\"}}\nconst provider = detectProvider(model.modelName);\nconst section = `STRUCTURED_OUTPUT_${provider.toUpperCase()}`;\nconst rendered = await loadAndRenderSection(\"monolithic-dynamic\", section, {});\n{{/code}}\n\n----\n\n== 4. Pipeline Prompt Sections ==\n\n=== 4.1 ClaimAssessmentBoundary Pipeline ===\n\n**File**: ##apps/web/prompts/claimboundary.prompt.md## (13 sections, pipeline: \"claimboundary\", version: 1.0.0)\n\nEach section covers exactly one LLM call point. Sections are mapped to the 5 pipeline stages:\n\n|= Section |= Purpose |= Stage |= Model Tier\n| ##CLAIM_EXTRACTION_PASS1## | Initial atomic claim extraction from user input | Stage 1: ##extractClaims## | Haiku\n| ##CLAIM_EXTRACTION_PASS2## | Claim consolidation, dependency analysis, counter-claim tagging | Stage 1: ##extractClaims## | Haiku\n| ##CLAIM_VALIDATION## | Gate 1: filter opinions, predictions, unanswerable claims | Stage 1: ##extractClaims## | Haiku\n| ##GENERATE_QUERIES## | Generate research queries per ##AtomicClaim## | Stage 2: ##researchEvidence## | Haiku\n| ##RELEVANCE_CLASSIFICATION## | Assess search result relevance to each claim | Stage 2: ##researchEvidence## | Haiku\n| ##EXTRACT_EVIDENCE## | Extract ##EvidenceItems## from fetched source content | Stage 2: ##researchEvidence## | Haiku\n| ##BOUNDARY_CLUSTERING## | Cluster ##EvidenceScopes## into ##ClaimAssessmentBoundaries## | Stage 3: ##clusterBoundaries## | Haiku\n| ##VERDICT_ADVOCATE## | Construct pro-thesis verdict for a boundary | Stage 4: ##generateVerdicts## | Sonnet\n| ##VERDICT_CHALLENGER## | Identify counter-evidence and weaknesses in Advocate verdict | Stage 4: ##generateVerdicts## | Sonnet\n| ##VERDICT_RECONCILIATION## | Synthesize Advocate/Challenger into final verdict + confidence | Stage 4: ##generateVerdicts## | Sonnet\n| ##VERDICT_GROUNDING_VALIDATION## | Advisory: verify verdict reasoning traces back to evidence | Stage 4: ##generateVerdicts## | Haiku\n| ##VERDICT_DIRECTION_VALIDATION## | Advisory: verify ##claimDirection## alignment with verdict | Stage 4: ##generateVerdicts## | Haiku\n| ##VERDICT_NARRATIVE## | Generate human-readable verdict narrative | Stage 5: ##aggregateAssessment## | Sonnet\n\nThe Advocate  Challenger  Reconciliation pattern in Stage 4 is the CB pipeline's core design: reliable initial verdicts emerge from structured debate, eliminating the need for post-hoc auto-correction.\n\n{{info}}\n**Note**: ##orchestrated.prompt.md## (40+ sections) was removed in v2.11.0 alongside the Orchestrated pipeline. Archive available at ##Docs/xwiki-pages-ARCHIVE/...##.\n{{/info}}\n\n=== 4.2 Monolithic Dynamic Pipeline ===\n\n**File**: ##apps/web/prompts/monolithic-dynamic.prompt.md##\n\n|= Section |= Purpose |= Variables\n| ##DYNAMIC_PLAN## | System prompt for planning/research phase | ##$~{currentDate}##\n| ##DYNAMIC_ANALYSIS## | System prompt for analysis/verdict phase | ##$~{currentDate}##\n| ##DYNAMIC_ANALYSIS_USER## | User message template with input + sources | ##$~{TEXT_TO_ANALYZE}##, ##$~{SOURCE_SUMMARY}##\n| ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON output guidance for Claude | (none)\n| ##STRUCTURED_OUTPUT_OPENAI## | JSON output guidance for GPT | (none)\n| ##STRUCTURED_OUTPUT_GOOGLE## | JSON output guidance for Gemini | (none)\n| ##STRUCTURED_OUTPUT_MISTRAL## | JSON output guidance for Mistral | (none)\n\n----\n\n== 5. Provider-Specific Optimizations ==\n\nProvider-specific structured output guidance is embedded as named sections in each ##.prompt.md## file. The pipeline code dynamically selects the right section based on the detected provider.\n\n|= Provider |= Section Name |= Key Guidance\n| **Anthropic (Claude)** | ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON format rules, empty strings for optional fields, field validation\n| **OpenAI (GPT)** | ##STRUCTURED_OUTPUT_OPENAI## | Field completeness, common GPT errors to avoid\n| **Google (Gemini)** | ##STRUCTURED_OUTPUT_GOOGLE## | Length enforcement, no explanatory text outside JSON\n| **Mistral** | ##STRUCTURED_OUTPUT_MISTRAL## | Validation checklist format, field type verification\n\nThe ##claimboundary.prompt.md## and ##monolithic-dynamic.prompt.md## files each contain ##STRUCTURED_OUTPUT_<PROVIDER>## sections for provider-specific output guidance.\n\n----\n\n== 6. Configuration Adaptations ==\n\n=== 6.1 LLM Tiering (##isLLMTiering##) ===\n\nWhen tiering is enabled with a fast-tier model:\n* Adjusts complexity expectations per task\n* Optimizes token usage for fast-tier models (Haiku, Mini, Flash)\n\n=== 6.2 Knowledge Mode (##allowModelKnowledge##) ===\n\nManaged via UCM prompt sections:\n* **##allowModelKnowledge=true##**: Loads ##KNOWLEDGE_INSTRUCTION_ALLOW_MODEL##  LLM may supplement evidence with training knowledge\n* **##allowModelKnowledge=false##**: Loads ##KNOWLEDGE_INSTRUCTION_EVIDENCE_ONLY##  LLM must rely strictly on fetched evidence\n\n----\n\n== 7. Provider Detection ==\n\nThe ##detectProvider()## function (in ##prompt-builder.ts##) maps model names to provider types:\n\n{{code language=\"typescript\"}}\nfunction detectProvider(modelName: string): ProviderType {\n  const lowerName = modelName.toLowerCase();\n\n  if (lowerName.includes('claude')) return 'anthropic';\n  if (lowerName.includes('gpt'))    return 'openai';\n  if (lowerName.includes('gemini')) return 'google';\n  if (lowerName.includes('mistral')) return 'mistral';\n\n  return 'anthropic'; // Default fallback\n}\n{{/code}}\n\nThe ##isBudgetModel()## function identifies fast-tier models (Haiku, Mini, Flash, Mistral Small/Medium) for tiering decisions.\n\n----\n\n== 8. Adding a New Provider ==\n\n1. Add a ##STRUCTURED_OUTPUT_NEWPROVIDER## section to ##claimboundary.prompt.md## and ##monolithic-dynamic.prompt.md##\n1. Update ##detectProvider()## in ##prompt-builder.ts## to recognize new model IDs\n1. Update ##isBudgetModel()## if the provider has fast-tier models\n1. Test with ##prompt-optimization.test.ts## and ##monolithic-dynamic-prompt.test.ts##\n\n----\n\n== 9. Token Optimization ==\n\n=== 9.1 v2.8.0 Achievements ===\n\n**20-30% token reduction** across prompts through a two-phase optimization:\n\n|= Phase |= Strategy |= Token Savings\n| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved\n| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved\n\n=== 9.2 Design Principles ===\n\n* **Prompt files are authoritative**  code should not duplicate or override prompt logic\n* **Variable substitution**  dynamic values injected at render time, not hardcoded\n* **Section composition**  complex prompts assembled from multiple sections (e.g., system + mode + provider hint)\n\n----\n\n== 10. Changelog ==\n\n=== v2.11.0: ClaimAssessmentBoundary Pipeline (Feb 17, 2026) ===\n\n##claimboundary.prompt.md## (13 sections) introduced as the primary runtime prompt file.\n\n* ##orchestrated.prompt.md## (40+ sections) removed alongside the Orchestrated pipeline\n* CB pipeline uses 5-stage structure: extractClaims  researchEvidence  clusterBoundaries  generateVerdicts  aggregateAssessment\n* Verdict generation uses Advocate/Challenger/Reconciliation debate pattern (3 Sonnet calls)\n* Advisory validation sections (##VERDICT_GROUNDING_VALIDATION##, ##VERDICT_DIRECTION_VALIDATION##) added  non-blocking, Haiku tier\n\n=== v2.8.2: Prompt Externalization to UCM (Feb 13, 2026) ===\n\nAll runtime LLM prompts load from UCM-managed ##.prompt.md## files, compliant with AGENTS.md String Usage Boundary.\n\n* Search relevance mode instructions moved from inline code to prompt file sections\n* Monolithic-dynamic system prompts moved from ##buildPrompt()## to ##loadAndRenderSection()##\n* 4 provider-specific structured output sections added to ##monolithic-dynamic.prompt.md##\n* TypeScript prompt modules retained for testing harness only\n\n=== v2.8.0: Token Optimization (Feb 3, 2026) ===\n\n**20-30% token reduction** across prompts through two-phase optimization.\n\n|= Phase |= Strategy |= Token Savings\n| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved\n| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved\n\n----\n\n== 11. Related Documentation ==\n\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  CB pipeline (default) and Monolithic Dynamic pipeline showing where each prompt is invoked\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict calculation methodology driven by prompt outputs\n* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]  Confidence scoring that depends on prompt-extracted evidence quality\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  Architecture overview\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] | Next: [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome": "= Quality Gates Reference =\n\n{{info}}\n**Developer Reference**  Quality Gates are checkpoints in the analysis pipeline that enforce minimum standards for claim evaluation and verdict confidence.\n\n**Key File**: ##apps/web/src/lib/analyzer/quality-gates.ts##\n{{/info}}\n\n----\n\n== 1. Overview ==\n\nQuality Gates ensure that:\n\n* Only verifiable claims are analyzed (Gate 1)\n* Verdicts have sufficient supporting evidence (Gate 4)\n* Results meet minimum quality thresholds before publication\n\n**Implemented Gates**: Gate 1 (Claim Validation) and Gate 4 (Verdict Confidence Assessment)\n\n----\n\n== 2. Gate Architecture ==\n\n=== 2.1 Pipeline Integration ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Quality Gates Integration.WebHome\"/}}\n\n=== 2.2 Gate States ===\n\n|= State |= Description |= Action\n| **Pass** | Meets all criteria | Proceed normally\n| **Warn** | Below recommended threshold but above minimum | Proceed with warning flag\n| **Fail** | Does not meet minimum criteria | Exclude from analysis or mark as insufficient\n\n=== 2.3 Result Metadata ===\n\nEvery analysis result includes gate statistics:\n\n{{code language=\"typescript\"}}\ninterface QualityGates {\n  gate1Stats: {\n    totalClaims: number;\n    validClaims: number;\n    excludedClaims: number;\n    exclusionReasons: { claimId: string; reason: string }[];\n  };\n  gate4Stats: {\n    totalVerdicts: number;\n    highConfidence: number;\n    mediumConfidence: number;\n    lowConfidence: number;\n    insufficient: number;\n  };\n}\n{{/code}}\n\n----\n\n== 3. Gate 1: Claim Validation ==\n\n=== 3.1 Purpose ===\n\nFilter out non-verifiable claims before research begins, preventing wasted resources on opinions, predictions, or vague statements.\n\n=== 3.2 Criteria ===\n\n**Claims are EXCLUDED if**:\n1. **Opinion/Editorial**: Subjective judgment without factual basis\n1*. Example: \"Policy X is the best approach\"\n1*. Action: Exclude unless claim is central to thesis\n\n1. **Prediction/Speculation**: Future-oriented claims that cannot be verified\n1*. Example: \"Technology Y will dominate the market by 2030\"\n1*. Action: Exclude unless claim is central to thesis\n\n1. **Low Specificity**: Vague statements without concrete assertions\n1*. Example: \"Some experts believe...\"\n1*. Action: Exclude unless claim is central to thesis\n\n**Claims are KEPT if**:\n* **Factual assertion**: Verifiable statement about past/present\n* **Central claim**: Core thesis claim (kept regardless of specificity)\n* **Attribution claim**: Claims about what someone said/did\n\n=== 3.3 Implementation ===\n\n**Files**: ##apps/web/src/lib/analyzer/quality-gates.ts## + ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts##\n**Functions**: ##applyGate1ToClaims()##, ##applyGate1Lite()##, ##validateClaimGate1()##\n**Phase**: Stage 1: ##extractClaims## (CB pipeline)\n\n**Validation Process**:\n1. CLAIM_EXTRACTION_PASS2 LLM call extracts and consolidates ##AtomicClaims## with a ##passedFidelity## flag (LLM-assessed claim fidelity  whether the claim is genuinely verifiable)\n1. ##applyGate1ToClaims()## applies deterministic specificity and fidelity filters\n1. Central claims (##isCentral=true##) are kept regardless of filter outcome\n1. Excluded claims logged with reasons; valid claims proceed to research\n\n**Safety Net  Rescue highest-centrality claim** (##claimboundary-pipeline.ts##):\nIf Gate 1 would filter ALL claims (leaving an empty pipeline that produces a meaningless default verdict), the highest-centrality claim that passed fidelity is automatically rescued. This prevents silent failures:\n{{code language=\"typescript\"}}\n// Safety net: never filter ALL claims\nif (keptClaims.length === 0 && claims.length > 0) {\n  const rescued = [...claims]\n    .sort(/* prefer fidelity-pass, then by centrality: high  medium  low */)[0];\n  keptClaims.push(rescued);\n  console.warn(`[Stage1] Gate 1: all ${claims.length} claims would be filtered  rescued \"${rescued.id}\"`);\n}\n{{/code}}\n\n=== 3.4 Configuration ===\n\n**UCM Pipeline Config**:\n{{code language=\"json\"}}\n{\n  \"gate1Enabled\": true,\n  \"gate1KeepCentralClaims\": true\n}\n{{/code}}\n\n=== 3.5 Examples ===\n\n**Example 1: Opinion Excluded**\n{{code}}\nClaim: \"The Supreme Court's decision was unjust.\"\nRole: evaluative\nResult: EXCLUDED (opinion - no factual basis)\nReason: \"Evaluative opinion without factual assertion\"\n{{/code}}\n\n**Example 2: Central Claim Kept**\n{{code}}\nClaim: \"The policy will significantly improve outcomes.\"\nRole: core\nResult: KEPT (central to thesis, despite low specificity)\nReason: \"Central claim kept for analysis\"\n{{/code}}\n\n**Example 3: Factual Assertion Kept**\n{{code}}\nClaim: \"The court ruled in favor of Party A on January 15, 2025.\"\nRole: core\nType: factual\nResult: KEPT (verifiable factual assertion)\n{{/code}}\n\n----\n\n== 4. Gate 4: Verdict Confidence Assessment ==\n\n=== 4.1 Purpose ===\n\nEnsure verdicts have sufficient supporting evidence before publication, preventing low-confidence judgments from misleading users.\n\n=== 4.2 Confidence Tiers ===\n\n|= Tier |= Criteria |= Interpretation\n| **HIGH** | 3+ sources AND 5+ facts AND reasoning >100 chars | Strong evidence base, high reliability\n| **MEDIUM** | 2+ sources AND 3+ facts AND reasoning >50 chars | Adequate evidence, moderate reliability\n| **LOW** | 1+ sources AND 1+ facts | Minimal evidence, low reliability\n| **INSUFFICIENT** | <1 source OR <1 fact | Insufficient evidence for verdict\n\n=== 4.3 Implementation ===\n\n**File**: ##apps/web/src/lib/analyzer/quality-gates.ts##\n**Phase**: Stage 5: ##aggregateAssessment## (CB pipeline)\n\n**Validation Process**:\n1. Count sources supporting verdict\n1. Count evidence items extracted from sources\n1. Measure reasoning length\n1. Assign confidence tier (HIGH/MEDIUM/LOW/INSUFFICIENT)\n1. Apply boundary scoping for counter-evidence\n1. Flag verdicts below threshold\n\n=== 4.4 Boundary Scoping ===\n\nIn the CB pipeline, counter-evidence is scoped to the relevant ##ClaimAssessmentBoundary## rather than by legacy ##contextId## fields. EvidenceItems are associated with specific boundaries via the boundary clustering stage (Stage 3), preventing counter-evidence from one boundary from penalising claims in a different boundary.\n\n=== 4.5 Central Claim Exception ===\n\n**Central claims remain publishable even if confidence is low**, because they are core to the thesis and users need to see the verdict regardless of evidence sufficiency.\n\n=== 4.6 Configuration ===\n\n**UCM Pipeline Config**:\n{{code language=\"json\"}}\n{\n  \"gate4Enabled\": true,\n  \"gate4MinSources\": 2,\n  \"gate4MinFacts\": 3,\n  \"gate4MinReasoningLength\": 50\n}\n{{/code}}\n\n=== 4.7 Examples ===\n\n**Example 1: HIGH Confidence**\n{{code}}\nVerdict: \"MOSTLY-TRUE\" (85%)\nSources: 4 (Reuters, AP, BBC, Government site)\nFacts: 12\nReasoning: 150 chars\nResult: HIGH confidence tier\nAction: Publish with full confidence\n{{/code}}\n\n**Example 2: LOW Confidence (Central Claim)**\n{{code}}\nVerdict: \"UNVERIFIED\" (50%)\nSources: 1 (Blog post)\nFacts: 2\nReasoning: 80 chars\nClaim: Central\nResult: LOW confidence tier\nAction: Publish with warning (central claim exception)\n{{/code}}\n\n**Example 3: INSUFFICIENT (Non-Central)**\n{{code}}\nVerdict: \"UNVERIFIED\" (50%)\nSources: 0\nFacts: 0\nReasoning: 30 chars\nClaim: Non-central\nResult: INSUFFICIENT\nAction: Exclude from report or mark as \"No evidence found\"\n{{/code}}\n\n----\n\n== 5. Confidence Impact on Verdict Calculation ==\n\n=== 5.1 Truth Percentage Modulation ===\n\nConfidence modulates the final truth percentage within each verdict band:\n\n{{code language=\"typescript\"}}\nfunction truthFromBand(band: \"strong\" | \"partial\" | \"uncertain\" | \"refuted\", confidence: number): number {\n  const conf = normalizePercentage(confidence) / 100;\n  switch (band) {\n    case \"strong\":    return Math.round(72 + 28 * conf);  // 72-100%\n    case \"partial\":   return Math.round(50 + 35 * conf);  // 50-85%\n    case \"uncertain\": return Math.round(35 + 30 * conf);  // 35-65%\n    case \"refuted\":   return Math.round(28 * (1 - conf)); // 0-28%\n  }\n}\n{{/code}}\n\n=== 5.2 Example Impact ===\n\n**\"strong\" band with varying confidence**:\n* **High confidence** (90%): 72 + 28x0.9 = 97% -> **TRUE**\n* **Medium confidence** (60%): 72 + 28x0.6 = 89% -> **TRUE**\n* **Low confidence** (30%): 72 + 28x0.3 = 80% -> **MOSTLY-TRUE**\n\nSame evidence band, but lower confidence pulls verdict down within the band.\n\n=== 5.3 MIXED vs UNVERIFIED Distinction ===\n\nConfidence determines whether 43-57% range is **MIXED** or **UNVERIFIED**:\n\n{{code language=\"typescript\"}}\nconst MIXED_CONFIDENCE_THRESHOLD = 60;\n\nif (truthPercentage >= 43 && truthPercentage <= 57) {\n  return confidence >= 60 ? \"MIXED\" : \"UNVERIFIED\";\n}\n{{/code}}\n\n* **MIXED** (confidence >= 60%): Evidence on both sides, high confidence in mixed state\n* **UNVERIFIED** (confidence < 60%): Insufficient evidence, low confidence\n\n----\n\n== 6. Confidence Penalties ==\n\nAfter Gate 4 assessment, two additional confidence penalty mechanisms may reduce overall confidence:\n\n=== 6.1 Recency Evidence Gap Penalty ===\n\n**Purpose**: Reduce confidence when time-sensitive claims lack recent evidence.\n\n**Trigger**: Topic is recency-sensitive AND no evidence found within ##recencyWindowMonths## (default: 6 months).\n\n**Graduated Mode (v2.11, default: enabled)**:\n\n{{code}}\neffectivePenalty = round(maxPenalty x staleness x volatility x volume)\n{{/code}}\n\n**Factor 1: Staleness Curve**  How far outside the recency window is the evidence?\n\n* Evidence within window: multiplier = 0 (no penalty)\n* Evidence outside window: linear ramp from 0 to 1 over one additional window period\n* At 2x window: capped at 1.0\n* No dates found: 1.0 (full staleness)\n\n**Factor 2: Topic Volatility**  How time-critical is the topic?\n\n|= Granularity |= Multiplier |= Example\n| ##week## | 1.0 | Breaking news\n| ##month## | 0.8 | Monthly-cycle topics\n| ##year## | 0.4 | Institutional / annual\n| ##none## | 0.2 | Enduring / structural\n| //undefined// | 0.7 | Fallback when LLM didn't assess\n\n**Factor 3: Evidence Volume**  More evidence attenuates the penalty.\n\n|= dateCandidates |= Multiplier\n| 0 | 1.0\n| 1-10 | 0.9\n| 11-25 | 0.7\n| 26+ | 0.5\n\n**Example  Institutional topic**:\n* Evidence 14 months old, window = 6 months -> staleness = 1.0 (capped)\n* Granularity = \"year\" -> volatility = 0.4\n* 35 date candidates -> volume = 0.5\n* **Effective penalty = round(20 x 1.0 x 0.4 x 0.5) = 4 points** (vs. 20 flat)\n\n=== 6.2 Low-Source Confidence Penalty ===\n\n**Purpose**: Reduce confidence when evidence base is thin.\n\n**Trigger**: Unique source count <= ##lowSourceThreshold## (default: 2).\n\n**Penalty**: Flat ##lowSourceConfidencePenalty## (default: 15 points).\n\n=== 6.3 Confidence Floor ===\n\nAfter all penalties, confidence cannot drop below ##minConfidenceFloor## (default: 10%).\n\n----\n\n== 7. Configuration Reference ==\n\n{{code language=\"json\"}}\n{\n  \"gate1Enabled\": true,\n  \"gate1KeepCentralClaims\": true,\n  \"gate4Enabled\": true,\n  \"gate4MinSources\": 2,\n  \"gate4MinFacts\": 3,\n  \"gate4MinReasoningLength\": 50,\n  \"mixedConfidenceThreshold\": 60,\n  \"recencyWindowMonths\": 6,\n  \"recencyConfidencePenalty\": 20,\n  \"recencyGraduatedPenalty\": true,\n  \"lowSourceThreshold\": 2,\n  \"lowSourceConfidencePenalty\": 15,\n  \"minConfidenceFloor\": 10\n}\n{{/code}}\n\n|= Setting |= Default |= Rationale\n| ##gate1Enabled## | ##true## | Quality control essential\n| ##gate1KeepCentralClaims## | ##true## | Users need to see core thesis verdicts\n| ##gate4Enabled## | ##true## | Prevent low-quality verdicts\n| ##gate4MinSources## | ##2## | Balance between quality and coverage\n| ##gate4MinFacts## | ##3## | Minimum for reasonable confidence\n| ##gate4MinReasoningLength## | ##50## | Ensure non-trivial reasoning\n| ##mixedConfidenceThreshold## | ##60## | Clear distinction between mixed/unverified\n| ##recencyWindowMonths## | ##6## | Time-sensitive evidence window\n| ##recencyConfidencePenalty## | ##20## | Max penalty for recency gap\n| ##recencyGraduatedPenalty## | ##true## | Use graduated (multi-factor) penalty\n| ##lowSourceThreshold## | ##2## | Source count for thin-evidence penalty\n| ##lowSourceConfidencePenalty## | ##15## | Penalty for thin evidence base\n| ##minConfidenceFloor## | ##10## | Minimum confidence after all penalties\n\n----\n\n== 8. Proposed Gates (Not Yet Implemented) ==\n\n=== 8.1 Gate 2: Source Quality (Proposed) ===\n\n**Purpose**: Filter low-quality sources before evidence extraction.\n**Status**: Proposed but not implemented (Source Reliability system exists but not integrated as gate).\n\n=== 8.2 Gate 3: Evidence Relevance (Proposed) ===\n\n**Purpose**: Filter tangential or low-probative-value evidence.\n**Status**: Proposed but not implemented (Evidence filtering exists but not formalized as gate).\n\n----\n\n== 9. Debugging and Diagnostics ==\n\n=== 9.1 Common Issues ===\n\n|= Issue |= Cause |= Solution\n| Too many claims excluded by Gate 1 | Input is primarily opinion/editorial | Gate 1 is working correctly; input may not be fact-checkable\n| All verdicts marked LOW confidence | Search returning few sources | Check search provider credentials, adjust Gate 4 thresholds\n| Central claims excluded by Gate 1 | ##gate1KeepCentralClaims=false## | Enable central claim exception in UCM Pipeline config\n\n=== 9.2 Testing ===\n\n**Unit Tests**: ##apps/web/src/lib/analyzer/__tests__/quality-gates.test.ts##\n\n**Coverage**: Gate 1 exclusion scenarios, central claim exception, Gate 4 confidence tier assignment, context scoping, central claim exception.\n\n----\n\n== 10. Related Documentation ==\n\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Verdict calculation methodology, confidence modulation\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  Pipeline variants and quality gate enforcement\n* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]  Evidence filtering (related to proposed Gate 3)\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  Architecture overview\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] | Next: [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Schema Migration.WebHome": "= Schema Migration Strategy =\n\n{{info}}\n**Developer Reference**  Schema evolution strategy for maintaining backward compatibility while improving terminology and structure. Covers additive changes, CalcConfig versioning, job JSON compatibility, and migration phases.\n{{/info}}\n\n**Version**: 1.0\n**Date**: 2026-01-29\n**Status**: Active\n\n----\n\n== 1. Introduction ==\n\n=== Why Schema Evolution Matters ===\n\nAs FactHarbor evolves, we must change data structures without breaking:\n* **Stored job results** (SQLite database: ##ResultJson## column)\n* **Stored configuration profiles** (UCM database: CalcConfig, SearchConfig, PromptConfig)\n* **External API consumers** (if any integrations depend on our JSON schemas)\n* **In-flight analysis jobs** (during rolling deploys)\n\nA naive \"find and rename\" approach would cause:\n* Old job results to become unparseable -> UI breaks for historical jobs\n* Stored config profiles to fail validation -> admins lose custom settings\n* External integrations to break silently -> data pipelines fail\n\nThis document defines our **schema evolution strategy** to maintain backward compatibility while improving terminology and structure.\n\n----\n\n== 2. Current Approach ==\n\n=== Principle: Additive Changes Only (Phase 2) ===\n\nDuring Phase 0-2.5 (terminology migration), all schema changes are **additive**:\n\n==== 2.1 Optional Fields ====\n\nNew fields are added with **optional** (##?##) suffix in TypeScript:\n\n{{code language=\"typescript\"}}\n// Before (Phase 0)\ninterface EvidenceItem {\n  id: string;\n  fact: string;\n  category: string;\n  sourceId: string;\n}\n\n// After (Phase 2)\ninterface EvidenceItem {\n  id: string;\n  fact: string;\n  category: string;\n  sourceId: string;\n  // NEW: Phase 2 fields (optional)\n  probativeValue?: \"high\" | \"medium\" | \"low\";\n  evidenceScope?: {\n    name: string;\n    sourceType?: SourceType;\n  };\n}\n{{/code}}\n\n**Benefits**: Old data without these fields still validates. Code can use ##??## operator for defaults. No database migration required.\n\n==== 2.2 Deprecated Aliases ====\n\nOld names remain as **type aliases** during gradual migration:\n\n{{code language=\"typescript\"}}\nexport interface EvidenceItem { /* ... */ }\n\n/**\n * @deprecated Use EvidenceItem instead (v2.6.41+)\n */\nexport type ExtractedFact = EvidenceItem;\n{{/code}}\n\n==== 2.3 Fallback Defaults ====\n\nCode uses ##??## operator to provide defaults when fields are missing:\n\n{{code language=\"typescript\"}}\nconst probativeValue = evidence.probativeValue ?? \"medium\";\nconst sourceType = evidence.evidenceScope?.sourceType ?? \"other\";\n{{/code}}\n\n----\n\n== 3. CalcConfig Versioning Strategy ==\n\n=== 3.1 Problem: Admin Configuration Evolution ===\n\nAdmin-editable configurations (CalcConfig, SearchConfig, PromptConfig) are stored as JSON in SQLite (UCM system). When we add new config fields:\n* Old stored profiles lack these fields\n* Code needs to provide sensible defaults\n* Admin UI should show defaults when fields missing\n\n=== 3.2 Solution: Optional Fields + Default Merging ===\n\n{{code language=\"typescript\"}}\n// When loading stored profile\nfunction loadCalcConfig(storedJson: string): CalcConfig {\n  const stored = JSON.parse(storedJson) as Partial<CalcConfig>;\n  // Merge with defaults (new fields filled in if missing)\n  return {\n    ...DEFAULT_CALC_CONFIG,\n    ...stored,\n  };\n}\n{{/code}}\n\n**Result**: Old profiles without new fields get defaults. Admin UI shows default values. User can edit and save, which populates the new fields.\n\n=== 3.3 Admin UI Handling ===\n\n{{code language=\"typescript\"}}\n<input\n  type=\"number\"\n  value={config.probativeValueWeights?.high ?? 1.0}\n  onChange={(e) => {\n    onChange({\n      ...config,\n      probativeValueWeights: {\n        ...config.probativeValueWeights,\n        high: parseFloat(e.target.value) || 1.0,\n      },\n    });\n  }}\n/>\n{{/code}}\n\n**Behavior**: If field missing  shows default. If field present  shows stored value. On edit  creates/updates the field.\n\n----\n\n== 4. Job JSON Backward Compatibility ==\n\n=== 4.1 Problem: Stored Job Results ===\n\nJob results are stored in ASP.NET API database:\n* SQLite table: ##AnalysisJobs##\n* Column: ##ResultJson## (TEXT, JSON blob)\n* Loaded by ##/jobs/[id]## UI for display\n\n**Risk**: If field names change in output, old jobs become unparseable and the UI breaks for historical data.\n\n=== 4.2 Solution: Keep Output Schema Stable (Phase 2) ===\n\nDuring Phase 2, **output JSON schema is unchanged**:\n* Still write ##facts[]## array (not ##evidence[]##)\n* Still write ##fact## field (not ##statement##)\n* Only internal TypeScript types use new names\n\n=== 4.3 Future: Schema Versioning (Phase 3) ===\n\nWhen Phase 3 renames fields in output, a schema version header is added:\n\n{{code language=\"typescript\"}}\ninterface VersionedResult {\n  schemaVersion: string;  // \"2.6\", \"2.7\", \"3.0\"\n  // ... rest of result\n}\n\nfunction parseJobResult(json: string): AnalysisResult {\n  const raw = JSON.parse(json);\n  const version = raw.schemaVersion || \"2.6\"; // Legacy default\n\n  if (semver.lt(version, \"3.0.0\")) {\n    return transformLegacyResult(raw);\n  }\n  return raw as AnalysisResult;\n}\n{{/code}}\n\n----\n\n== 5. External API Considerations ==\n\n=== 5.1 Current API Exposure ===\n\n{{code}}\nGET /api/fh/jobs/[id]    Returns: { ResultJson: string }\nGET /api/fh/analyze       Returns: AnalysisResult (JSON)\n{{/code}}\n\n**Assumption**: No external integrations currently depend on our schema.\n\n=== 5.2 Mitigation: API Versioning (If Needed) ===\n\nIf external consumers are identified:\n\n|= Option |= Approach\n| **A: Versioned Endpoints** (Recommended) | ##/api/v1/analyze## (legacy) vs ##/api/v2/analyze## (new)\n| **B: Deprecation Headers** | ##X-Deprecated-Field: facts## / ##X-Schema-Version: 2.6##\n| **C: Dual Output** | Include both ##facts[]## and ##evidence[]## with deprecation notice\n\n=== 5.3 Action Required Before Phase 3 ===\n\n1. **Audit external consumers**: Check logs, ask stakeholders\n1. **If none**: Proceed with field renames\n1. **If exist**: Implement API versioning strategy\n\n----\n\n== 6. Testing Requirements ==\n\n=== Backward Compatibility Test Suite ===\n\n**Location**: ##apps/web/test/unit/lib/analyzer/schema-backward-compatibility.test.ts##\n\n**Required Tests**:\n* Type compatibility (new interface accepts required fields)\n* Optional field defaults (missing new fields get safe defaults via ##??##)\n* CalcConfig merging (old profiles without new fields load correctly)\n* JSON parsing (version detection, legacy transformation)\n\n=== Test Execution Frequency ===\n\n* **Before each Phase completion**: Verify no regressions\n* **Before Phase 3 field renames**: Full suite must pass\n* **CI/CD pipeline**: Run on every commit\n\n----\n\n== 7. Migration Phases ==\n\n|= Phase |= Name |= Status |= Schema Impact\n| 0 | Documentation Updates | Complete | None\n| 1 | Comments and Labels | Complete | None\n| 1.5 | Layer 2 Filter | Complete | Additive (no breaking changes)\n| 2 | New Fields | Complete | Additive (optional fields)\n| 2.1 | Gradual Code Migration | In Progress | None (internal type renames)\n| 2.5 | Source Type Prompts | Complete | Additive\n| 3 | Field Renames | Complete (v3.0/v3.1) | **Breaking**  legacy names removed\n\n=== Phase 3 Renames (v3.0/v3.1) ===\n\n* ##facts[]## -> ##evidenceItems[]##\n* ##fact## field -> ##statement##\n* ##supportingFactIds## -> ##supportingEvidenceIds##\n* ##distinctProceedings## -> ##analysisContexts##\n* ##relatedProceedingId##/##proceedingId## -> ##contextId##\n* Migration script: ##apps/api/scripts/migrate-terminology-v2.7.ts##\n\n----\n\n== Schema Version History ==\n\n|= Version |= Date |= Schema Changes |= Migration Required?\n| 2.6.17 | 2025-11 | Initial UCM system, base schema | N/A (baseline)\n| 2.6.33 | 2025-12 | Source reliability fields added | No (additive)\n| 2.6.38 | 2026-01 | Twin-path pipeline (no schema change) | No\n| 2.6.41 | 2026-01 | Phase 2: probativeValue, sourceType (optional) | No (optional fields)\n| 3.0.0 | 2026-02 | Phase 3: Field renames (facts -> evidence) | Yes (transformation)\n\n----\n\n== Related Documentation ==\n\n* [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]  Entity relationships and current schema\n* [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]]  UCM config management\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation.WebHome": "= Source Reliability: Admin and Implementation =\n\n== Admin Interface ==\n\nAccess the Source Reliability admin page at: ##/admin/source-reliability##\n\n=== Features ===\n\n* **Cache Statistics**: Total entries, average scores, expired count\n* **Paginated Table**: View all cached scores with sorting\n* **Cleanup**: Remove expired entries\n* **Authentication**: Requires ##FH_ADMIN_KEY## in production\n\n=== Admin Tasks ===\n\n|= Task |= Estimated Time\n| Check LLM cost dashboard | 5 min\n| Spot-check 2-3 recent scores | 8 min\n| Review any flagged issues | 2 min\n\n----\n\n== Design Principles ==\n\n=== Evidence Over Authority ===\n\nSource credibility is **supplementary**, not primary:\n\n* Only evidence and counter-evidence matter  not who says it\n* Authority does NOT automatically give weight\n* A low-credibility source with documented evidence should be considered\n* A high-credibility source making unsupported claims should be questioned\n\n=== No Pre-seeded Data ===\n\nAll sources are evaluated identically by LLM:\n* No hardcoded scores or external rating databases\n* No manipulation concerns from third-party data\n* Full transparency  every score comes from LLM evaluation\n\n=== No Categorical Bias ===\n\n* Domain type (.gov, .edu, .org) does NOT imply quality\n* Scores derived from demonstrated track record, not institutional prestige\n* Editorial independence matters  state control is a negative factor\n\n=== Entity-Level Evaluation ===\n\nWhen a domain is the primary digital outlet for a larger organization, the evaluation focuses on the reliability of the entire organization:\n* **Legacy Media**: Public broadcasters and established legacy media evaluated based on institutional standards and editorial oversight\n* **Consistency**: Prevents high-quality organizations from being underrated due to narrow domain-focused metrics\n\n=== Dynamic Assessment ===\n\n* Sources can gain or lose credibility over time\n* Cache expires after 90 days (configurable)\n* Re-evaluation happens automatically on cache miss\n\n----\n\n== Implementation Details ==\n\n=== Key Files ===\n\n|= File |= Purpose\n| ##apps/web/src/lib/analyzer/source-reliability.ts## | Prefetch, sync lookup, evidence weighting\n| ##apps/web/src/lib/source-reliability-cache.ts## | SQLite cache operations\n| ##apps/web/src/app/api/internal/evaluate-source/route.ts## | LLM evaluation endpoint\n| ##apps/web/src/app/admin/source-reliability/page.tsx## | Admin UI for cache management\n| ##apps/web/src/app/api/admin/source-reliability/route.ts## | Admin API endpoint\n\n=== Key Functions ===\n\n{{code language=\"typescript\"}}\n// Phase 1: Call ONCE before analysis (async)\nexport async function prefetchSourceReliability(urls: string[]): Promise<PrefetchResult>;\n\n// Phase 2: Call MANY times during analysis (sync, instant)\nexport function getTrackRecordScore(url: string): number | null;\nexport function getTrackRecordData(url: string): CachedReliabilityData | null;\n\n// Phase 3: Apply to verdicts (sync)\nexport function applyEvidenceWeighting(\n  claimVerdicts: ClaimVerdict[],\n  facts: ExtractedFact[],\n  sources: FetchedSource[]\n): ClaimVerdict[];\n\n// Effective weight calculation (used by monolithic pipelines)\nexport function calculateEffectiveWeight(data: SourceReliabilityData): number;\n\n// Utilities\nexport function extractDomain(url: string): string | null;\nexport function isImportantSource(domain: string): boolean;\nexport function normalizeTrackRecordScore(score: number): number;\nexport function assertValidTruthPercentage(value: number, context?: string): number;  // Fail-fast validation (replaces clampTruthPercentage)\nexport function clearPrefetchedScores(): void;\n{{/code}}\n\n=== Temporal Awareness (v2.6.35+) ===\n\nThe LLM evaluation prompt includes the current date and temporal guidance:\n* **Government sources** vary by administration\n* **Media outlets** can shift with ownership or editorial changes\n* **Historical reputation** may not reflect current performance\n* **Cache TTL** (90-day default) ensures scores are re-evaluated quarterly\n\n=== Pipeline Integration ===\n\nSource Reliability is integrated into both pipelines:\n\n|= Pipeline |= File |= Status |= Implementation\n| **ClaimAssessmentBoundary** | ##claimboundary-pipeline.ts## | Full | Prefetch + lookup + evidence weighting\n| **Monolithic Dynamic** | ##monolithic-dynamic.ts## | Full | Prefetch + lookup + verdict adjustment\n\nAll pipelines follow the same pattern:\n1. **Clear** prefetched scores at analysis start\n1. **Prefetch** source reliability before fetching URLs\n1. **Lookup** scores synchronously when creating sources\n1. **Apply** weighting to verdicts\n\n=== Score = Weight ===\n\nWith the 7-band scale, the LLM score directly represents reliability and is used as-is:\n\n|= Component |= Purpose\n| **Score** | LLM-evaluated reliability (7-band scale)  used directly as weight\n| **Confidence** | Quality gate (threshold: 0.8)  scores below are rejected\n| **Consensus** | Multi-model agreement (diff <= 0.20)\n\n**Key Design Decisions**:\n* **Score = Weight**  No transformation, what LLM says is what we use\n* **Confidence is a gate, not a modifier**  If evaluation passes threshold, we trust it\n* **Transparency**  A 70% score means 70% weight, no hidden calculations\n\n=== Multi-Model Consensus ===\n\nWhen ##sr.multiModel## is enabled (default):\n\n1. Build an optional **evidence pack** (web search results) if ##sr.evalUseSearch=true##\n1. Both Claude and secondary OpenAI model evaluate the source using the same evidence pack\n1. Primary must return confidence >= threshold; secondary must return non-null score\n1. Score difference must be <= consensus threshold (default 0.20)\n1. Final score = **\"better founded\"** model output; tie-breaker = **lower score** (skeptical default)\n1. If consensus fails -> return ##null## (unknown reliability)\n\n----\n\n== Cost and Performance ==\n\n=== Cost Estimates ===\n\n|= Mode |= Monthly Cost\n| Multi-model (default) | $40-60\n| Single-model | $20-30\n\nThe importance filter saves ~60% of LLM costs by skipping blog platforms and spam domains.\n\n=== Success Metrics ===\n\n|= Metric |= Target\n| Cache hit rate (warm) | > 80%\n| Blog skip rate | > 90%\n| Confidence pass rate | > 85%\n| Consensus rate | > 90%\n\n=== Rollback Options ===\n\n|= Issue |= Action\n| LLM costs too high | Set ##sr.multiModel=false## in UCM\n| Still too expensive | Set ##sr.enabled=false## in UCM\n| Too many hallucinations | Raise ##sr.confidenceThreshold## to 0.9\n| Low consensus rate | Lower ##sr.consensusThreshold##\n\n----\n\n== Troubleshooting ==\n\n|= Issue |= Solution\n| \"Unauthorized\" from evaluate endpoint | Set ##FH_INTERNAL_RUNNER_KEY## in ##.env.local##\n| No scores appearing | Verify ##sr.enabled=true## in UCM\n| Low-confidence evaluations | Ensure a search provider is configured for the evidence pack\n| High LLM costs | Enable filter and use single model (##sr.multiModel=false##)\n| Consensus failures | Lower ##sr.consensusThreshold##\n| Score not affecting verdict | Check ##applyEvidenceWeighting## is called, verify ##trackRecordScore## on sources\n| Admin page shows 401 | Enter admin key in the auth form, or set ##FH_ADMIN_KEY## in env\n\n----\n\n== Test Coverage ==\n\n|= Test File |= Tests |= Coverage\n| ##source-reliability.test.ts## | 42 | Domain extraction, importance filter, evidence weighting\n| ##source-reliability-cache.test.ts## | 16 | SQLite operations, pagination, expiration\n| ##source-reliability.integration.test.ts## | 13 | End-to-end pipeline flow\n| ##evaluate-source.test.ts## | 19 | Rate limiting, consensus calculation\n| **Total** | **90** |\n\nRun tests:\n{{code language=\"bash\"}}\ncd apps/web && npm test -- src/lib/analyzer/source-reliability.test.ts\ncd apps/web && npm test -- src/lib/source-reliability-cache.test.ts\ncd apps/web && npm test -- src/lib/analyzer/source-reliability.integration.test.ts\ncd apps/web && npm test -- src/app/api/internal/evaluate-source/evaluate-source.test.ts\n{{/code}}\n\n----\n\n**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Refinement and Multi-Language>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts.WebHome": "= Source Reliability: Architecture and Verdicts =\n\n== Architecture ==\n\n=== System Overview ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Source Reliability Overview.WebHome\"/}}\n\n=== Integration Pattern: Batch Prefetch + Sync Lookup ===\n\nThe Source Reliability system uses a **two-phase pattern** to avoid async operations in the analyzer's hot path.\n\n==== The Problem ====\n\nThe FactHarbor analyzer (##claimboundary-pipeline.ts##) is a complex synchronous pipeline. Adding ##await## calls mid-pipeline for source reliability lookups would require major refactoring, complicate error handling, and risk introducing race conditions.\n\n==== The Solution: Two-Phase Pattern ====\n\nSeparate the async work (cache lookup, LLM calls) from the sync analysis:\n\n|= Phase |= When |= Nature |= What It Does\n| **Phase 1: Prefetch** | Before analysis starts | Async | Batch lookup all source URLs, populate in-memory map\n| **Phase 2: Lookup** | During analysis | Sync | Read from pre-populated map (instant, no I/O)\n| **Phase 3: Weighting** | After verdicts generated | Sync | Adjust truth percentages based on source scores\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Source Reliability Flow.WebHome\"/}}\n\n=== Phase 1 Detail: Prefetch Flow ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Source Reliability Prefetch Flow.WebHome\"/}}\n\n=== Why This Pattern Works ===\n\n|= Concern |= How Pattern Addresses It\n| **No async ripple** | Only ONE ##await## at pipeline boundary, rest stays sync\n| **Batch efficiency** | Single batch cache lookup instead of N individual calls\n| **LLM cost control** | Filter + rate limit applied during prefetch\n| **Graceful degradation** | Unknown sources get ##null##, analysis continues\n| **No blocking** | Sync lookups are instant map reads\n\n----\n\n== How It Affects Verdicts ==\n\nSource reliability scores directly influence verdict calculations through **evidence weighting**.\n\n=== Formula ===\n\n{{code}}\nadjustedTruth = 50 + (originalTruth - 50) x avgSourceScore\nadjustedConfidence = confidence x (0.5 + avgSourceScore / 2)\n{{/code}}\n\n=== Effect on Verdicts (7-Band Scale) ===\n\n|= Source Credibility Band |= Score (Weight) |= Effect on Verdict\n| **Highly Reliable (0.86+)** | ~95-100% | Verdict fully preserved\n| **Reliable (0.72-0.86)** | ~75-90% | Verdict mostly preserved\n| **Leaning Reliable (0.58-0.72)** | ~60-75% | Moderate preservation\n| **Mixed (0.43-0.57)** | ~40-60% | Variable track record (neutral center)\n| **Leaning Unreliable (0.29-0.43)** | ~30-45% | Pulls toward neutral\n| **Unreliable (0.15-0.29)** | ~15-30% | Strong pull toward neutral\n| **Highly Unreliable (0.00-0.15)** | ~0-15% | Maximum skepticism\n| **Unknown (null)** | 50% | Uses default score (neutral)\n\n=== Example ===\n\n{{code}}\nOriginal verdict: 80% (Strong True)\nSource credibility: 0.5 (Mixed - variable track record)\n\nAdjusted = 50 + (80 - 50) x 0.5\n         = 50 + 30 x 0.5\n         = 50 + 15\n         = 65% (Leaning True)\n{{/code}}\n\n=== Multi-Source Averaging ===\n\nWhen a verdict has evidence from multiple sources:\n\n{{code}}\nVerdict with facts from:\n  - reuters.com (score: 0.95)\n  - bbc.com (score: 0.88)\n\nAverage score = (0.95 + 0.88) / 2 = 0.915\n{{/code}}\n\n----\n\n**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Overview and Quick Start>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start.WebHome]] | Next: [[Configuration and Scoring>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome": "= Source Reliability: Configuration and Scoring =\n\n== Configuration ==\n\nSource Reliability is configured via UCM as a **separate SR config type** (schemaVersion ##3.0.0##). Edit it in\n**Admin -> Config -> Source Reliability**. This SR config is owned by the SR service and is separate\nfrom the main pipeline/search/calculation configs.\n\n=== SR Config Independence ===\n\nWhile SR config is stored in UCM alongside pipeline/search configs, it maintains\noperational independence:\n\n* **Schema versioning:** ##3.0.0## for SR (independent of pipeline config versioning)\n* **Hot reload:** SR config changes apply to new evaluations only\n* **No cascading invalidation:** SR updates don't trigger pipeline/search config reloads\n* **Separate domain:** SR uses ##getConfig(\"sr\")## and ##setSourceReliabilityConfig()##\n\n=== Core Settings (UCM) ===\n\n|= Field |= Default |= Description\n| ##enabled## | ##true## | Enable/disable source reliability\n| ##multiModel## | ##true## | Use multi-model consensus\n| ##openaiModel## | ##gpt-4o-mini## | Secondary model for consensus\n| ##confidenceThreshold## | ##0.8## | Min LLM confidence to accept score\n| ##consensusThreshold## | ##0.20## | Max score difference between models\n| ##defaultScore## | ##0.5## | Default score for unknown sources (neutral center)\n\n=== Cache and Filtering (UCM) ===\n\n|= Field |= Default |= Description\n| ##cacheTtlDays## | ##90## | Cache expiration in days\n| ##filterEnabled## | ##true## | Enable importance filter\n| ##skipPlatforms## | //(defaults in config)// | Platform domains to skip\n| ##skipTlds## | //(defaults in config)// | TLDs to skip\n\n=== Rate Limiting (UCM) ===\n\n|= Field |= Default |= Description\n| ##rateLimitPerIp## | ##10## | Max evaluations per minute per IP\n| ##domainCooldownSec## | ##60## | Seconds between same-domain evals\n\n=== Evaluator Evidence Grounding (UCM SR Config) ===\n\nThese settings affect the internal evaluation endpoint (##/api/internal/evaluate-source##):\n\n|= Field |= Default |= Description\n| ##evalUseSearch## | ##true## | Build an evidence pack from web search\n| ##evalSearchMaxResultsPerQuery## | ##3## | Max search results per query for the evidence pack\n| ##evalMaxEvidenceItems## | ##12## | Max evidence-pack items to include\n| ##evalSearchDateRestrict## | ##null## | Optional: ##y## / ##m## / ##w## (falls back to SearchConfig ##dateRestrict##)\n\n=== Environment Variables (infra only) ===\n\n|= Variable |= Default |= Description\n| ##FH_SR_CACHE_PATH## | ##./source-reliability.db## | SQLite database location\n\n=== Example SR Config (JSON) ===\n\n{{code language=\"json\"}}\n{\n  \"enabled\": true,\n  \"multiModel\": true,\n  \"openaiModel\": \"gpt-4o-mini\",\n  \"confidenceThreshold\": 0.8,\n  \"consensusThreshold\": 0.2,\n  \"defaultScore\": 0.5,\n  \"cacheTtlDays\": 90,\n  \"filterEnabled\": true,\n  \"evalUseSearch\": true\n}\n{{/code}}\n\n----\n\n== Score Interpretation ==\n\n**7-band credibility scale (centered at 0.5)**\n\n|= Score |= Rating |= Key Criteria\n| 0.86-1.00 | Highly Reliable | Verified accuracy, recognized standards body, rigorous corrections\n| 0.72-0.85 | Reliable | Consistent accuracy, professional standards, rarely faulted\n| 0.58-0.71 | Leaning Reliable | Often accurate, occasional errors, corrects when notified\n| 0.43-0.57 | Mixed | Variable accuracy OR inconsistent quality by topic/author\n| 0.29-0.42 | Leaning Unreliable | Often inaccurate OR bias significantly affects reporting\n| 0.15-0.28 | Unreliable | Pattern of false claims OR ignores corrections\n| 0.00-0.14 | Highly Unreliable | Fabricates content OR documented disinformation source\n\n**Calibration:**\n* Default assumption is 0.5 (mixed). Adjust up or down based on evidence.\n* Do not inflate scores based on brand recognition or reputation alone.\n* \"No negative findings\" does not equal reliable. Absence of evidence lowers confidence; truly unknown sources should use insufficient_data.\n\n**Negative-side calibration (POC intent):**\n* **platform_ugc**: If a domain is primarily an open UGC platform without centralized editorial verification, it should generally land **below the mixed band** unless evidence shows consistent domain-level verification and corrections.\n* **state_controlled_media**: If evidence indicates editorial coordination/control, default below mixed unless evidence shows meaningful editorial independence and correction practices.\n* **propaganda_outlet / known_disinformation**: If evidence indicates repeated independent debunks / multiple independent assessors flagging systemic misinformation, score should land in **0.00-0.20** even if occasional factual content exists.\n\n**Weighting Rules (applied during evaluation):**\n1. **Recency Priority**: Last 24 months matter most. Historical reputation does not excuse recent failures.\n1. **Verification**: Fact-checker findings are strong reliability signals.\n1. **Visibility Cap**: Score capped by worst high-visibility failures.\n1. **Opinion Counts**: Misinformation in opinion/editorial degrades entire source score.\n1. **Bias Impact**: Bias affecting accuracy lowers score. Bias without factual issues is noted, not penalized.\n\n**Bias Assessment:**\n* Political spectrum: far_left / left / center_left / center / center_right / right / far_right\n* Other bias types: pro_government / anti_government / corporate_interest / sensationalist / ideological_other\n\n**Insufficient Data:**\n* Sources with no independent assessments return ##score: null## with ##factualRating: insufficient_data##\n\n**Impact on verdicts:**\n* Score >= 0.58: Preserves original verdict (credible source)\n* Score 0.43-0.57: Moderate pull toward neutral (mixed track record)\n* Score < 0.43: Strong pull toward neutral (skepticism)\n\n----\n\n**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Architecture and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts.WebHome]] | Next: [[Refinement and Multi-Language>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start.WebHome": "= Source Reliability: Overview and Quick Start =\n\n**User-facing name**: [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]]\n**Version**: 1.4 (Multi-Language Support)\n**Status**: Operational\n**Last Updated**: 2026-02-03 (v2.6.41)\n\n----\n\n== Overview ==\n\nFactHarbor evaluates source reliability dynamically using LLM-powered assessment with **sequential refinement** and **multi-language support**. The primary model (Claude) performs initial evaluation, then a secondary OpenAI model (default: ##gpt-4o-mini##) cross-checks and refines the result. For non-English sources, the system detects the publication language and searches for regional fact-checker assessments. Sources are evaluated on-demand and cached for 90 days.\n\n|= Aspect |= Implementation\n| **Evaluation** | Sequential LLM refinement (Claude -> OpenAI mini model cross-check)\n| **Storage** | SQLite cache (##source-reliability.db##)\n| **Integration** | Batch prefetch + sync lookup (both pipelines)\n| **Pipelines** | Orchestrated, Monolithic Dynamic\n| **Cost Control** | Importance filter + rate limiting\n| **Verdict Impact** | Evidence weighting adjusts truth percentages\n\n----\n\n== Quick Start ==\n\n=== Prerequisites ===\n\n{{code language=\"powershell\"}}\n# In apps/web/.env.local\nANTHROPIC_API_KEY=sk-ant-...\nOPENAI_API_KEY=sk-...\nFH_INTERNAL_RUNNER_KEY=your-secret-key-here\n{{/code}}\n\n=== That's It ===\n\nThe service is **enabled by default**. It will automatically:\n* Prefetch source reliability before analyzing sources\n* Use multi-model refinement (Claude + OpenAI mini model, default ##gpt-4o-mini##)\n* Cache results for 90 days\n* Skip blog platforms and spam TLDs\n* Apply evidence weighting to verdicts\n\n=== Verify It's Working ===\n\nRun an analysis and check the logs for:\n{{code}}\n[SR] Prefetching 5 unique domains\n[SR] Cache hits: 0/5\n[SR] Evaluated reuters.com: score=0.95, confidence=0.92\n{{/code}}\n\n----\n\n**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Next: [[Architecture and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome": "= Source Reliability: Refinement and Multi-Language =\n\n== v1.1 Prompt Improvements (January 2026) ==\n\n**Date**: 2026-01-24\n**Focus**: Prompt structure, quantification, and consistency across models\n\nVersion 1.1 improved the LLM evaluation prompt for source reliability assessments to increase stability, consistency, and effectiveness across different models.\n\n=== Key Improvements ===\n\n|= # |= Improvement |= Impact |= Description\n| 1 | Restructured Prompt Hierarchy | High | Moved critical rules to top with visual indicator, ensuring LLMs see most important constraints first\n| 2 | Quantified \"Insufficient Data\" Thresholds | High | Added specific numeric thresholds (e.g., \"Zero fact-checker assessments AND <=1 weak mention\") to reduce inter-model disagreement\n| 3 | Mechanistic Confidence Scoring Formula | High | Created step-by-step calculation formula with base (0.40) + additive factors, making confidence reproducible across models\n| 4 | Numeric Negative Evidence Caps | Medium-High | Made caps explicit with numbers (e.g., \"3+ failures -> score <= 0.42\") to prevent lenient scoring\n| 5 | Quantified Recency Weighting | Medium | Converted subjective terms to multipliers (0-12mo: 1.0x, 12-24mo: 0.8x, 2-5yr: 0.5x, >5yr: 0.2x)\n| 6 | Evidence Quality Hierarchy | Medium | Created three-tier hierarchy (HIGH/MEDIUM/LOW weight) to prevent single weak sources from dominating\n| 7 | Enhanced Calibration Examples | Medium | Added confidence calculations and reasoning to examples, showing models exactly how to apply formulas\n| 8 | Improved Source Type Positioning | Low-Medium | Renamed section to \"SOURCE TYPE CLASSIFICATION (classify FIRST, then evaluate within category)\"\n| 9 | Enhanced System Message | Low-Medium | Made system message tactical with specific responsibilities (evidence-only, caps, formula)\n| 10 | Expanded Validation Checklist | Low-Medium | Added checklist items for all critical rules to help models catch errors before responding\n\n=== Mechanistic Confidence Formula ===\n\n**Base**: 0.40\n\n**ADD**:\n* +0.15 per independent fact-checker assessment (max +0.45 for 3+)\n* +0.10 if most evidence is within last 12 months\n* +0.10 if evidence shows consistent pattern (3+ sources agree)\n* +0.05 per additional corroborating source beyond first (max +0.15)\n\n**SUBTRACT**:\n* -0.15 if evidence is contradictory/mixed signals\n* -0.10 if evidence is mostly >2 years old\n\n**Final confidence**: clamp result to [0.0, 1.0]\n\n**THRESHOLD**: If calculated confidence < 0.50, strongly consider outputting ##score=null## and ##factualRating=\"insufficient_data\"##\n\n=== Negative Evidence Caps (v1.1) ===\n\n|= Evidence Type |= Score Cap |= Rating Cap\n| Evidence of fabricated stories/disinformation | <= 0.14 | highly_unreliable\n| 3+ documented fact-checker failures | <= 0.42 | leaning_unreliable\n| 1-2 documented failures from reputable fact-checkers | <= 0.57 | mixed\n| Political/ideological bias WITHOUT documented failures | No cap | Note in bias field only\n\n=== Evidence Quality Hierarchy ===\n\n**HIGH WEIGHT** (can establish verdict alone):\n* Explicit fact-checker assessments (MBFC, Snopes, PolitiFact, etc.)\n* Documented corrections/retractions by the source\n* Journalism reviews from reputable organizations\n\n**MEDIUM WEIGHT** (support but don't establish alone):\n* Newsroom analyses of editorial standards\n* Academic studies on source reliability\n* Awards/recognition from journalism organizations\n\n**LOW WEIGHT** (context only, cannot trigger caps):\n* Single blog posts or forum discussions\n* Passing mentions without substantive analysis\n* Generic references without reliability details\n\n----\n\n== v1.2 Hardening (January 2026) ==\n\nVersion 1.2 introduces significant improvements to scoring accuracy, especially for propaganda and misinformation sources.\n\n=== Key Changes ===\n\n|= Feature |= Description\n| **Entity-Level Evaluation** | Prioritize organization reputation over domain-only metrics when the domain is a primary outlet for an established organization\n| **SOURCE TYPE SCORE CAPS** | Prompt-driven caps (UCM configurable): ##propaganda_outlet##/##known_disinformation## -> <=14%, ##state_controlled_media##/##platform_ugc## -> <=42%; code validates and warns but does not override\n| **Adaptive Evidence Queries** | Negative-signal queries (##propaganda##, ##disinformation##, ##false claims##) added when initial results are sparse\n| **Brand Variant Matching** | Improved relevance filtering: handles brand variants, suffix stripping\n| **Mechanistic Confidence** | Formula-based confidence scoring: base 0.40 + factors (fact-checkers, recency, corroboration)\n| **Asymmetric Confidence Gating** | High scores require higher confidence (skeptical default)\n| **Unified Thresholds** | Admin + pipeline + evaluator use same defaults (confidence: 0.8)\n\n=== Source Type Caps (Prompt-Driven, UCM Configurable) ===\n\n{{code}}\npropaganda_outlet       -> score <= 0.14 (highly_unreliable)\nknown_disinformation    -> score <= 0.14 (highly_unreliable)\nstate_controlled_media  -> score <= 0.42 (leaning_unreliable)\nplatform_ugc            -> score <= 0.42 (leaning_unreliable)\n{{/code}}\n\nThese caps are defined in the SR prompt template (UCM configurable). Code-side validation adds a caveat if the LLM exceeds a cap but does **not** override the score (prompt is authoritative since v2.8.3).\n\n=== Asymmetric Confidence Requirements ===\n\nHigh reliability scores require stronger evidence (skeptical default):\n\n|= Rating |= Min Confidence\n| highly_reliable | 0.85\n| reliable | 0.75\n| leaning_reliable | 0.65\n| mixed | 0.55\n| leaning_unreliable | 0.50\n| unreliable | 0.45\n| highly_unreliable | 0.40\n\n----\n\n== Sequential Refinement Architecture ==\n\nThe source reliability system uses **sequential refinement** for accurate entity-level evaluation.\n\n=== Architecture ===\n\n{{code}}\nEvidence Pack -> Claude (Initial Evaluation) -> Initial Result\n                         |\nEvidence Pack + Initial Result -> OpenAI mini model (Cross-check and Refine) -> Final Result\n{{/code}}\n\n**Key characteristics**:\n* The secondary model can catch what the initial model missed (especially entity recognition)\n* Explicit cross-checking of entity identification\n* Baseline score adjustments for known organization types\n* Reasoning transparency through refinement logic\n\n=== Refinement Process ===\n\nThe secondary OpenAI model (default: ##gpt-4o-mini##) receives:\n1. The original evidence pack\n1. The complete initial evaluation from Claude\n1. Instructions to cross-check, sharpen entity identification, and refine the score\n\n=== Shared Prompt Sections ===\n\nBoth LLM1 (initial evaluation) and LLM2 (refinement) use **shared prompt constants** for consistency:\n\n|= Section |= Purpose\n| Rating Scale | Score -> rating mapping (0.86+ = highly_reliable, etc.)\n| Evidence Signals | Positive/neutral signal definitions\n| Bias Values | Political and other bias enum values\n| Source Types | Classification definitions and triggers\n| Score Caps | Hard limits for severe source types\n\n=== Refinement Adjustment Rules ===\n\n* **UPWARD adjustment** requires positive signals PRESENT in evidence (academic citations, professional use, independent authoritative mentions)\n* **DOWNWARD adjustment** if negative signals were missed or underweighted\n* **NO adjustment** if evidence is simply sparse (sparse does not equal positive)\n* Absence of negative evidence alone does NOT justify upward adjustment\n\n----\n\n== Multi-Language Support ==\n\nThe system includes automatic language detection and multi-language search queries to find regional fact-checker assessments.\n\n=== Language Detection ===\n\nThe system detects the **actual publication language** (not TLD) by:\n\n1. Fetching the homepage (5s timeout)\n1. Checking ##<html lang=\"...\">## attribute\n1. Checking ##<meta http-equiv=\"content-language\">##\n1. Checking ##<meta property=\"og:locale\">##\n1. If all fail: LLM analyzes content sample\n\nResults are cached per domain.\n\n=== Multi-Language Queries ===\n\nWhen a non-English language is detected:\n\n1. **LLM translates** key fact-checking terms (cached per language)\n1. **Dual-language searches** are performed:\n1*. English queries (always, for international coverage)\n1*. Translated queries (for regional fact-checkers)\n\n=== Supported Languages ===\n\nGerman, French, Spanish, Portuguese, Italian, Dutch, Polish, Russian, Swedish, Norwegian, Danish, Finnish, Czech, Hungarian, Turkish, Japanese, Chinese, Korean, Arabic.\n\n=== Regional Fact-Checkers (Tier 1) ===\n\n|= Language |= Fact-Checkers\n| German | CORRECTIV, Mimikama, dpa-Faktencheck, Faktenfinder (ARD)\n| French | AFP Factuel, Les Decodeurs (Le Monde), Liberation CheckNews\n| Spanish | Maldita.es, Newtral, EFE Verifica\n| Portuguese | Aos Fatos, Lupa, Poligrafo\n| Italian | Pagella Politica, ANSA Fact-checking\n| Dutch | Nu.nl Factcheck, Nieuwscheckers\n\n=== Cost Impact ===\n\n|= Component |= Cost per Evaluation\n| Language detection (page fetch) | Free\n| Translation (LLM, cached) | ~$0.001 per new language\n| Additional searches | ~2-4 extra queries\n\n=== Response Fields ===\n\n|= Field |= Type |= Description\n| ##refinementApplied## | boolean | Whether the score was adjusted by cross-check\n| ##refinementNotes## | string | Summary of cross-check findings\n| ##originalScore## | number | Score before refinement (if changed)\n\n----\n\n**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Configuration and Scoring>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome]] | Next: [[Admin and Implementation>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome": "= Source Reliability =\n\n{{info}}\n**Developer Reference**  Implementation details for the [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]], FactHarbor's AI-powered source credibility service. Currently embedded in the analysis pipeline; planned to become a standalone public service and API.\n\n**Key File**: ##apps/web/src/lib/analyzer/source-reliability.ts##\n{{/info}}\n\n**User-facing name**: [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]]\n**Version**: 1.4 (Multi-Language Support)\n**Status**: Operational\n\n----\n\n== Contents ==\n\n* **[[Overview and Quick Start>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start.WebHome]]**  System overview, prerequisites, and verification steps\n* **[[Architecture and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Architecture and Verdicts.WebHome]]**  Batch prefetch + sync lookup pattern, Mermaid diagrams, evidence weighting formula, verdict impact (7-band scale)\n* **[[Configuration and Scoring>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome]]**  UCM settings (core, cache, rate limiting, evidence grounding), 7-band credibility scale, calibration rules\n* **[[Refinement and Multi-Language>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome]]**  v1.1 prompt improvements, v1.2 hardening, sequential refinement architecture, multi-language support with regional fact-checkers\n* **[[Admin and Implementation>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation.WebHome]]**  Admin interface, design principles, implementation details (key files, functions, pipeline integration), cost and performance, troubleshooting, test coverage\n\n== Related ==\n\n* [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]]  User-facing source reliability documentation\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System architecture (SR integration context)\n* [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]]  Source reliability scoring overview\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]]\n", "Product Development.Specification.Architecture.Deep Dive.Storage Technology.WebHome": "= Storage Technology =\n\n{{info}}\n**Developer Reference**  Detailed technology assessments for FactHarbor's storage layer: caching strategy, database evolution, and technology adoption criteria.\n\nFor the architectural overview, see [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]].\n{{/info}}\n\n== Storage Patterns ==\n\n* **Analysis results**: JSON blob in ##ResultJson## column (per job), stored once by .NET API\n* **Config blobs**: Content-addressable with SHA-256 hash as PK, history tracked\n* **Job config snapshots**: Pipeline + search + SR config captured per job for auditability\n* **SR cache**: Per-domain reliability assessment with multi-model consensus scores\n\n**Current limitations:**\n* No relational queries across claims, evidence, or sources from different analyses\n* No full-text search on analysis content\n* Single-writer limitation (SQLite)  fine for single-instance but blocks horizontal scaling\n* Every analysis re-fetches URL content and recomputes all LLM calls from scratch\n\n== Caching Value Analysis ==\n\n{{warning}}\nThe EVALUATE items require deeper analysis during Alpha planning before committing to scope and timeline.\n{{/warning}}\n\n|= Cacheable Item |= Estimated Savings |= Latency Impact |= Complexity |= Recommendation\n| **Claim-level results** | 30-50% LLM cost on duplicate claims | None (cache lookup) | MEDIUM  needs canonical claim hash + TTL + prompt-version awareness | EVALUATE in Alpha\n| **URL content** | $0 API cost but 5-15s latency per source | Major  eliminates re-fetch | LOW  URL hash + content + timestamp | EVALUATE in Alpha\n| **LLM responses** | Highest per-call savings | None | HIGH  prompt hash + input hash, invalidation on prompt change | DEFER  claim-level caching captures most benefit\n| **Search query results** | Marginal  search APIs are cheap | Minor | MEDIUM  results go stale quickly | NOT RECOMMENDED  volatile, low ROI\n\n=== Cost Impact Modeling ===\n\nAssuming ~$0.10-$2.00 per analysis (depending on article complexity and model tier):\n\n|= Usage Level |= Current Cost/day |= With Claim Cache (-35%) |= With URL Cache\n| 10 analyses/day | $1-20 | $0.65-13 | Same cost, 30-60s faster\n| 100 analyses/day | $10-200 | $6.50-130 | Same cost, 5-15 min faster\n| 1000 analyses/day | $100-2,000 | $65-1,300 | Same cost, 50-150 min faster\n\n**Key insight**: Claim caching saves money; URL caching saves time. Both follow the existing SQLite + in-memory ##Map## pattern from source reliability.\n\n== Redis Assessment ==\n\n=== Current Reality ===\n\n|= Original Redis Use Case |= Current Solution |= Gap?\n| Hot data caching | In-memory ##Map## (config), SQLite (SR) | No gap at current scale\n| Session management | No user auth = no sessions | Not needed until Beta\n| Rate limiting | Not implemented | Can be in-process for single-instance\n| Pub/sub for real-time | SSE events work without Redis | No gap for single-instance\n\n=== When Redis Becomes Necessary ===\n\nRedis adds value when:\n* **Multiple application instances** need shared cache/state (horizontal scaling)\n* **Sub-millisecond cache lookups** required (SQLite is ~1-5ms, sufficient for current needs)\n* **Distributed rate limiting** needed across multiple servers\n\n**Trigger criteria** (following [[When-to-Add-Complexity>>FactHarbor.Product Development.DevOps.Guidelines.When to Add Complexity.WebHome]] philosophy):\n* Single-instance SQLite cache latency >100ms\n* Need for >1 application instance\n* Rate limiting required across instances\n\n{{info}}\n**Decision: DEFER Redis.** Not needed for current or near-term development. SQLite + in-memory ##Map## handles all current caching needs.\n{{/info}}\n\n== PostgreSQL Assessment ==\n\n=== Current SQLite Limitations ===\n\n|= Limitation |= Impact |= When It Hurts\n| JSON blob storage (no relational queries) | Cannot query across analyses | When browse/search is needed\n| Single-writer | No concurrent writes | When horizontal scaling is needed\n| No complex aggregation | Cannot run cross-analysis analytics | When quality dashboards need SQL\n| No full-text search | Cannot search claim text or evidence | When browse/search is needed\n\n=== What PostgreSQL Enables ===\n\n* Browse/search claims across all analyses\n* Quality metrics dashboards with SQL aggregation\n* Evidence deduplication (FR54) with relational queries\n* User accounts and permissions (Beta requirement)\n* Multi-instance deployments\n\n=== Migration Path ===\n\nThe .NET API already has PostgreSQL support configured (##appsettings.json##). Switching is a configuration change, not a code rewrite.\n\n**Note**: Keep SQLite for ##config.db## (portable) and ##source-reliability.db## (standalone). Only ##factharbor.db## needs PostgreSQL.\n\n{{info}}\n**Decision: EVALUATE for Alpha/Beta.** Add PostgreSQL when user accounts + search + evidence dedup needed. Requires deeper analysis during Alpha planning.\n{{/info}}\n\n== Vector Database Assessment ==\n\n{{info}}\nFull assessment: ##Docs/WIP/Vector_DB_Assessment.md## (February 2, 2026)\n{{/info}}\n\n**Conclusion**: Vector search is not required for core functionality. Vectors add value only for approximate similarity (near-duplicate claim detection, edge case clustering) and should remain **optional and offline** to preserve pipeline performance and determinism.\n\n**When to add**: Only after Shadow Mode data collection proves that near-duplicate detection needs exceed text-hash capability. Start with lightweight normalization + n-gram overlap (no vector DB needed).\n\n{{info}}\n**Decision: DEFER.** Re-evaluate after Shadow Mode data collection.\n{{/info}}\n\n== Decision Summary ==\n\n|= Technology |= Decision |= When |= Status\n| **SQLite URL cache** | EVALUATE | Alpha planning | Needs further analysis\n| **SQLite claim cache** | EVALUATE | Alpha planning | Needs further analysis\n| **Redis** | DEFER | Multi-instance | Agreed\n| **PostgreSQL** | EVALUATE | Alpha/Beta | Needs further analysis\n| **Vector DB** | DEFER | Post-Shadow Mode | Agreed\n| **S3** | DEFER | V1.0+ | Agreed\n\n{{info}}\nDEFER items are agreed. EVALUATE items (URL cache, claim cache, PostgreSQL) require deeper analysis during Alpha release planning  scope, dependencies, and prioritization to be determined as part of Alpha milestones.\n{{/info}}\n\n----\n\n**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]]\n\n**Document Status:** PARTIALLY APPROVED (February 2026)  DEFER decisions agreed; EVALUATE items need Alpha-phase analysis\n", "Product Development.Specification.Architecture.Deep Dive.WebHome": "= Deep Dive Index =\n\nThis section contains detailed implementation references for developers and testers. Each page provides code-level detail, configuration options, and testing guidance.\n\n{{info}}\n**New to FactHarbor?** Start with the [[Architecture Overview>>FactHarbor.Product Development.Specification.Architecture.WebHome]] and the Level 2 Architectural Views before diving into these references.\n{{/info}}\n\n== By Topic ==\n\n=== Pipeline Implementation ===\n\n|= Page |= Description |= Key Files\n| [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] | Twin-path architecture (ClaimAssessmentBoundary + Monolithic Dynamic): invariants, shared primitives, result model, variant selection, configuration | ##claimboundary-pipeline.ts##, ##verdict-stage.ts##, ##monolithic-dynamic.ts##\n\n=== Quality and Evidence ===\n\n|= Page |= Description |= Key Files\n| [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] | Gate 1 (claim validation) and Gate 4 (confidence assessment)  criteria, examples, configuration | ##quality-gates.ts##\n| [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] | 7-layer defence strategy, filter rules, category-specific requirements, troubleshooting | ##evidence-filter.ts##, ##provenance-validation.ts##\n| [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] | ClaimAssessmentBoundary clustering methodology, multi-boundary scenarios | ##claimboundary-pipeline.ts## (Stage 3)\n| [[Direction Semantics>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Direction Semantics.WebHome]] | Multi-layer direction semantics, scope mismatch problem, LLM-code alignment, counter-claim handling | ##verdict-stage.ts##, ##types.ts##\n\n=== Calculations and Scoring ===\n\n|= Page |= Description |= Key Files\n| [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] | 7-point verdict scale, aggregation hierarchy, weighting formulas, counter-evidence handling | ##aggregation.ts##, ##truth-scale.ts##\n| [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] | 4-layer calibration system, graduated recency penalty, low-source penalty, configuration | ##confidence-calibration.ts##\n| [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | LLM-based evaluation, 7-band credibility scale, multi-language support, caching, admin guide | ##source-reliability.ts##\n\n=== Infrastructure ===\n\n|= Page |= Description |= Key Files\n| [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]] | Modular prompt composition, provider-specific variants, token optimisation | ##prompts/##\n| [[KeyFactors Design>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome]] | KeyFactor classification and decomposition (Orchestrated-era design decision  superseded by ClaimAssessmentBoundary) | //historical//\n| [[Storage Technology>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Storage Technology.WebHome]] | Caching value analysis, Redis/PostgreSQL/Vector DB assessments, cost modelling | ##config-storage.ts##, ##source-reliability-cache.ts##\n| [[Schema Migration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Schema Migration.WebHome]] | Database schema evolution patterns | ##Data/##\n\n== By Role ==\n\n=== Pipeline Developer ===\nStart with: [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]\n\n=== Prompt Engineer ===\nStart with: [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]  [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]  [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]\n\n=== QA / Tester ===\nStart with: [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]]  [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]\n\n=== Source Reliability Specialist ===\nStart with: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] (5 sub-pages covering overview, architecture, configuration, refinement, and admin)\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n", "Product Development.Specification.Architecture.External Dependencies.WebHome": "= External Dependencies =\n\nFactHarbor depends on external LLM providers for AI analysis and search providers for evidence retrieval. This page documents the integration architecture, model tiering strategy, and provider health monitoring.\n\n== Dependencies Map ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.External Dependencies Map.WebHome\"/}}\n\n//FactHarbor integrates with 4 LLM providers, 2 search providers, and web content sources. All external calls are routed through abstraction layers (model-tiering, web-search, retrieval) and monitored by the provider health system.//\n\n== LLM Provider Integration ==\n\n=== Model Tiering ===\n\nThe AKEL pipeline uses **per-task model tiering** to balance cost and quality. Lightweight tasks (claim extraction, evidence parsing) use cheaper budget models; critical tasks (verdict reasoning) use premium models.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.LLM Model Tiering.WebHome\"/}}\n\n//Pipeline tasks are routed to the appropriate model tier via model-tiering.ts. All LLM calls go through the Vercel AI SDK, which provides a unified interface across providers.//\n\nFor detailed provider model mapping, tiered routing configuration, and implementation status, see [[LLM Abstraction Architecture>>FactHarbor.Product Development.Diagrams.LLM Abstraction Architecture.WebHome]].\n\n=== Provider Model Mapping ===\n\n|= Task |= Tier |= Anthropic |= OpenAI |= Google |= Mistral\n| understand | Budget | Claude Haiku 4.5 | GPT-4.1-mini | Gemini 2.5-flash | (Anthropic fallback)\n| extract_evidence | Budget | Claude Haiku 4.5 | GPT-4.1-mini | Gemini 2.5-flash | (Anthropic fallback)\n| context_refinement | Standard | Claude Haiku 4.5 | GPT-4.1 | Gemini 2.5-pro | (Anthropic fallback)\n| verdict | Premium | Claude Sonnet 4.5 | GPT-4.1 | Gemini 2.5-pro | (Anthropic fallback)\n\n=== LLM Configuration ===\n\n* **Provider selection**: Single provider for all tasks, set via ##LLM_PROVIDER## environment variable (default: ##anthropic##)\n* **Model overrides**: Individual models configurable via UCM (##modelUnderstand##, ##modelExtractEvidence##, ##modelVerdict##)\n* **Deterministic mode**: ##FH_DETERMINISTIC=true## sets temperature to 0 for reproducible results\n* **Structured output**: Uses Zod schemas with ##generateObject()## for type-safe LLM responses\n\n== Search Providers ==\n\n|= Provider |= Role |= Credentials Required\n| **Google Custom Search Engine** | Primary search | ##GOOGLE_CSE_API_KEY##, ##GOOGLE_CSE_ID##\n| **SerpAPI** | Fallback | ##SERPAPI_API_KEY##\n\n**Auto mode** (default): Tries Google CSE first; falls back to SerpAPI on failure. Configurable via ##SearchConfig.provider## (##\"auto\"##, ##\"google-cse\"##, or ##\"serpapi\"##).\n\n== Content Extraction ==\n\n|= Source Type |= Library |= Capabilities\n| HTML pages | cheerio | DOM parsing, text extraction, link extraction\n| PDF documents | pdf2json | Text extraction from PDF files\n\nContent is fetched with configurable timeouts and retry logic. No caching is currently implemented for fetched content (each analysis re-fetches all sources).\n\n== Provider Health Monitoring ==\n\nFactHarbor includes a **circuit breaker** per provider type (search, LLM) that detects outages and auto-pauses the system to prevent cascading failures.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Circuit Breaker States.WebHome\"/}}\n\n//The circuit breaker starts CLOSED (healthy). After consecutive failures reach the threshold (default: 3), it opens  the system auto-pauses, queued jobs wait, and a webhook notifies the admin. When the admin resumes, the circuit enters HALF_OPEN to test recovery.//\n\n=== Health Features ===\n\n|= Feature |= Description\n| **Circuit breaker** | Per-provider (search/LLM) with configurable threshold (##heuristicCircuitBreakerThreshold##, default: 3)\n| **Auto-pause** | Runner queue halts when circuit trips; jobs stay QUEUED (not lost)\n| **Webhook notifications** | Fire-and-forget POST to ##FH_WEBHOOK_URL## with optional HMAC signing (##FH_WEBHOOK_SECRET##)\n| **Admin API** | ##GET /api/fh/system-health## (status), ##POST /api/fh/system-health## (resume/pause)\n| **UI banner** | Amber warning banner when system is paused, with admin resume/pause controls\n| **Error classification** | Categorises errors as provider_outage, rate_limit, timeout, or unknown\n\n== Credential Requirements ==\n\n|= Variable |= Required |= Purpose\n| ##ANTHROPIC_API_KEY## | If using Anthropic | Claude API access\n| ##OPENAI_API_KEY## | If using OpenAI | GPT API access\n| ##GOOGLE_GENERATIVE_AI_API_KEY## | If using Google | Gemini API access\n| ##MISTRAL_API_KEY## | If using Mistral | Mistral API access\n| ##GOOGLE_CSE_API_KEY## | For search | Google Custom Search API\n| ##GOOGLE_CSE_ID## | For search | Google Custom Search Engine ID\n| ##SERPAPI_API_KEY## | For fallback search | SerpAPI access\n| ##FH_WEBHOOK_URL## | Optional | Provider health webhook endpoint\n| ##FH_WEBHOOK_SECRET## | Optional | HMAC-SHA256 webhook signature\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] | Next: [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]]\n", "Product Development.Specification.Architecture.Future.WebHome": "= Future Architecture =\n\n{{info}}\nThis page describes **planned production capabilities** that are not yet implemented. The current system runs as a two-service application (Next.js + .NET API) with SQLite. See [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] for the current design.\n{{/info}}\n\n== Roadmap Overview ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Architecture Roadmap.WebHome\"/}}\n\n//Green = current POC. Yellow = Alpha (next). Orange = Beta. Blue = V1.0. Purple = V2.0+. Each phase builds on the previous.//\n\n== Planned Automated Systems ==\n\n=== Background Jobs (Alpha) ===\n\n|= System |= Schedule |= Purpose\n| Source Track Record Updates | Scheduled batch | Analyze claim outcomes, update source accuracy\n| Cache Management | Continuous | Warm cache, invalidate on updates, monitor hit rates\n| Metrics Aggregation | Recurring | Roll up detailed metrics, calculate health indicators\n| Data Archival | Scheduled | Move old logs to S3 (90+ days), compress backups\n\n=== Quality Monitoring (Beta) ===\n\n|= Check |= Description\n| Anomaly Detection | Flag unusual patterns in confidence scores, evidence distributions\n| Contradiction Detection | Identify evidence conflicts, internal claim contradictions\n| Completeness Validation | Ensure sufficient evidence, multiple source types\n\n=== Moderation Detection (Beta) ===\n\n|= Check |= Description\n| Spam Identification | Pattern matching for spam claims\n| Manipulation Detection | Identify coordinated editing\n| Gaming Detection | Flag attempts to game source scores\n| Suspicious Activity | Log unusual behavior patterns\n\n== Scalability Strategy ==\n\n=== Horizontal Scaling ===\n\n|= Component |= Scaling Approach\n| AKEL Workers | Add more processing workers as claim volume grows\n| Database Read Replicas | Add replicas for read-heavy workloads\n| Cache Layer | Redis cluster for distributed caching\n| API Servers | Load-balanced stateless instances\n\n=== Performance Optimization ===\n\n|= Optimization |= Expected Impact\n| Denormalized data | Cache summary data in claim records (70% fewer joins)\n| Parallel processing | AKEL pipeline processes in parallel (40% faster)\n| Intelligent caching | Redis caches frequently accessed data\n| Background processing | Non-urgent tasks run asynchronously\n\n== Monitoring and Observability ==\n\n=== Key Metrics ===\n\n|= Category |= Metrics\n| **Performance** | AKEL processing time, API response time, cache hit rate\n| **Quality** | Confidence score distribution, evidence completeness, contradiction rate\n| **Usage** | Claims per day, active users, API requests\n| **Errors** | Failed AKEL runs, API errors, database issues\n\n=== Alert Thresholds ===\n\n|= Condition |= Alert Level\n| Processing time > 30 seconds | Warning\n| Error rate > 1% | Critical\n| Cache hit rate < 80% | Warning\n| Database connections > 80% capacity | Critical\n\n=== Monitoring Stack ===\n\n|= Capability |= Technology |= Phase\n| Metrics collection | Prometheus | Beta\n| Dashboards | Grafana | Beta\n| Alerting | Prometheus Alertmanager | Beta\n| Distributed tracing | OpenTelemetry | V1.0\n| Log aggregation | ELK or Loki | V1.0\n\n== Security Roadmap ==\n\n|= Capability |= Phase |= Description\n| User authentication | Alpha | Secure login with password hashing\n| RBAC | Alpha | Reader, User, Admin, Moderator roles\n| Rate limiting | Alpha | Per-user and per-IP throttling\n| SSRF protection | Alpha | Sanitize URLs before fetching\n| CORS tightening | Alpha | Restrict allowed origins\n| Audit logging | Beta | Track all significant changes\n| API keys | Beta | Programmatic access tokens\n| Abuse prevention | Beta | Automated detection and ban mechanisms\n\n== Deployment Evolution ==\n\n=== Production Environment (Beta+) ===\n\n|= Component |= Technology\n| Load Balancer | HAProxy or cloud LB\n| API Servers | Multiple stateless instances\n| AKEL Workers | Auto-scaling pool\n| Primary Database | PostgreSQL\n| Read Replicas | PostgreSQL streaming replication\n| Cache | Redis cluster\n| Object Storage | S3-compatible\n\n=== Disaster Recovery ===\n\n|= Aspect |= Current (POC) |= Target (Production)\n| **Backups** | Manual SQLite file copy | Automated PostgreSQL backups to S3\n| **Recovery** | Restore SQLite files | Point-in-time recovery from transaction logs\n| **Replication** | None (single instance) | PostgreSQL streaming replication\n| **RTO** | Manual (hours) | < 4 hours\n| **Data loss window** | Since last backup | Minutes (WAL-based)\n\n== When to Add Complexity ==\n\nEach technology addition has a specific trigger  do not add prematurely:\n\n|= Technology |= Trigger |= Current Status\n| **PostgreSQL** | Multiple users / shared state needed | Planned (Alpha)\n| **Redis** | Multi-instance deployment needed | Planned (Beta)\n| **Elasticsearch** | PostgreSQL search consistently > 500ms | Not needed yet\n| **TimescaleDB** | Metrics queries consistently > 1s | Not needed yet\n| **Vector DB** | Semantic search / RAG features needed | Not needed yet\n| **Federation** | 10,000+ users and explicit demand | Deferred (V2.0+)\n\n== Federation (V2.0+) ==\n\n**Deferred until**:\n* Core product proven with 10,000+ users\n* User demand for decentralization\n* Single-node limits reached\n\nFederation would enable:\n* Distributed fact-checking across independent instances\n* Cross-instance verdict sharing and consensus\n* Regional deployment for data sovereignty\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n", "Product Development.Specification.Architecture.Quality and Trust.WebHome": "= Quality and Trust =\n\nFactHarbor employs multiple quality mechanisms to ensure analysis results are trustworthy: quality gates validate input and output, a defence-in-depth system ensures evidence and verdict quality, source reliability scoring evaluates source credibility, and a confidence calibration pipeline prevents misleading confidence scores.\n\n== Quality Gates ==\n\nTwo quality gates are integrated into the AKEL pipeline: Gate 1 validates claims at input, Gate 4 assesses verdict confidence at output.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome\"/}}\n\n//Gate 1 filters non-factual claims (opinions, predictions) before research begins. Gate 4 evaluates verdict quality based on evidence count, source quality, and evidence agreement  only HIGH and MEDIUM confidence verdicts are published without flags.//\n\n=== Gate 1: Claim Validation ===\n\n|= Check |= Purpose |= Pass Criteria\n| Factuality test | Can the claim be proven true or false? | Must be verifiable\n| Opinion detection | Contains subjective language? | Opinion score <= 0.3\n| Specificity check | Contains concrete, testable details? | Specificity score >= 0.3\n| Future prediction | About future events? | Must be about past or present\n\n=== Gate 4: Confidence Assessment ===\n\n|= Tier |= Evidence Sources |= Avg Quality |= Agreement |= Action\n| **HIGH** | 3+ | >= 0.7 | >= 80% | Publish\n| **MEDIUM** | 2+ | >= 0.6 | >= 60% | Publish\n| **LOW** | 2+ | >= 0.5 | >= 40% | Flag for review\n| **INSUFFICIENT** | < 2 | Any | Any | More research needed\n\n== Evidence Quality: Defence in Depth ==\n\nEvidence and verdicts are protected by a **two-phase defence** system. Phase 1 operates before verdict generation (ensuring only quality evidence reaches aggregation). Phase 2 operates after verdict generation (ensuring verdicts are sound).\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Evidence Defence in Depth.WebHome\"/}}\n\n//Phase 1 (blue) filters evidence before verdict generation: LLM-guided extraction, deterministic quality filters, and provenance validation. Phase 2 (yellow) protects verdict quality: source reliability weighting, aggregation pruning of tangential/opinion-only claims, and verdict corrections.//\n\n=== Phase 1: Evidence Quality (Pre-Verdict) ===\n\n|= Step |= Type |= What It Filters\n| LLM Prompt Instructions | Soft (LLM) | Guides extraction: require excerpts, avoid vague statements, cite sources\n| Deterministic Filter | Hard (code) | Min 20 chars, 13 vague phrase patterns, Jaccard deduplication (0.85), category-specific rules (statistics need numbers, quotes need attribution), probative value scoring\n| Provenance Validation | Hard (code) | Rejects synthetic/LLM-generated content, requires URL and source excerpt\n\n=== Phase 2: Verdict Quality (Post-Verdict) ===\n\n|= Step |= Type |= What It Filters\n| Source Reliability Weighting | Soft (scoring) | Domain credibility score (0.0-1.0) weights evidence strength in aggregation\n| Aggregation Pruning | Hard (code) | Removes tangential baseless claims, zero-weights tangential claims, prunes opinion-only KeyFactors, validates contestation factual basis\n| Verdict Corrections | Hard (code) | Detects and corrects verdicts that contradict their own evidence\n\n== Source Reliability ==\n\nSource reliability scoring evaluates the credibility of web domains using LLM-based assessment with consensus scoring.\n\n|= Feature |= Description\n| **7-band credibility scale** | Authoritative (90+), Highly Credible (80-89), Credible (70-79), Moderately Credible (60-69), Mixed (50-59), Low Credibility (30-49), Unreliable (<30)\n| **LLM evaluation** | Domain assessed for editorial standards, fact-checking history, transparency, bias indicators\n| **Multi-model consensus** | Multiple LLM evaluations averaged for stability (configurable via UCM)\n| **SQLite cache** | Evaluations cached for 90 days (configurable) to avoid redundant LLM calls\n| **Evidence weighting** | Source reliability score directly influences evidence weight in verdict calculations\n\nFor implementation details, see [[Source Reliability Deep Dive>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].\n\n== Confidence Calibration ==\n\nA 4-layer deterministic post-processing system prevents misleading confidence scores.\n\n{{mermaid}}\nflowchart LR\n    RAW[\"Raw LLM\\nConfidence\"]\n    D[\"Density\\nAnchor\"]\n    B[\"Band\\nSnapping\"]\n    V[\"Verdict\\nCoupling\"]\n    C[\"Context\\nConsistency\"]\n    CAL[\"Calibrated\\nConfidence\"]\n\n    RAW --> D --> B --> V --> C --> CAL\n\n    style RAW fill:#ffcdd2,stroke:#b71c1c,color:#000\n    style CAL fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style D fill:#e3f2fd,stroke:#1565c0,color:#000\n    style B fill:#e3f2fd,stroke:#1565c0,color:#000\n    style V fill:#e3f2fd,stroke:#1565c0,color:#000\n    style C fill:#e3f2fd,stroke:#1565c0,color:#000\n{{/mermaid}}\n\n//Raw LLM confidence passes through 4 calibration layers: density anchor sets a minimum floor based on evidence quality, band snapping reduces jitter by aligning to a 7-band system, verdict coupling ensures strong verdicts have adequate confidence, and context consistency penalises divergent confidence across analysis contexts.//\n\n|= Layer |= Purpose |= Effect\n| **Evidence density anchor** | Sets minimum confidence floor based on evidence quality and quantity | Floor ranges from 15% (minimal evidence) to 60% (strong evidence)\n| **Band snapping** | Aligns confidence to a 7-band system with partial blending (strength 0.7) | Reduces run-to-run jitter by ~5-10pp\n| **Verdict-confidence coupling** | Ensures strong verdicts (>=70% or <=30% truth) have confidence >= 50% | Prevents \"TRUE with 20% confidence\" anomalies\n| **Context consistency** | Penalises confidence divergence >25pp across analysis contexts | Ensures consistent confidence across perspectives\n\n=== Additional Confidence Penalties ===\n\n|= Penalty |= Trigger |= Effect\n| **Graduated recency penalty** | Evidence is dated (staleness x volatility x volume formula) | Reduces confidence proportionally to evidence age\n| **Low-source penalty** | 2 or fewer sources found | -15pp confidence (configurable)\n| **Confidence floor** | Always applied | Minimum 10% confidence (configurable)\n\nAll calibration layers are configurable via UCM (##confidenceCalibration## section in Calculation Config) and can be individually toggled on/off.\n\n== Deep Dives ==\n\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]  Detailed Gate 1 and Gate 4 reference with examples\n* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]]  Detailed 7-layer filtering pipeline (code-level reference), filter rules, troubleshooting\n* [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]  Evaluation methodology, caching, multi-language support\n* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]  Layer-by-layer detail, formulas, configuration reference\n* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]]  Aggregation hierarchy, weighting formulas\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]] | Next: [[Security and Operations>>FactHarbor.Product Development.Specification.Architecture.Security and Operations.WebHome]]\n", "Product Development.Specification.Architecture.Security and Operations.WebHome": "= Security and Operations =\n\nThis page documents FactHarbor's current security model, deployment topology, user roles, and operational considerations. It distinguishes between what is implemented today (POC) and what is planned for production.\n\n== Security Model ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Security Model.WebHome\"/}}\n\n//Green = implemented in current POC. Yellow = planned for Alpha/Beta. Blue = always active regardless of deployment phase. The current security model protects internal endpoints with shared secrets; full user authentication is planned for Alpha.//\n\n=== Current Protections ===\n\n|= Protection |= Scope |= Mechanism\n| Runner route | ##/api/internal/run-job## | ##x-runner-key## header must match ##FH_INTERNAL_RUNNER_KEY## env var\n| Admin endpoints | System health POST, config changes | ##X-Admin-Key## header must match ##FH_ADMIN_KEY## env var\n| Secret comparison | All key validations | Timing-safe comparison (prevents timing attacks)\n| Input validation | All API inputs, all configs | Zod schema validation\n| SQL injection | All database access | Parameterised queries (EF Core + better-sqlite3)\n\n=== Production Hardening Needed ===\n\n* **User authentication**  Login, registration, session management\n* **RBAC**  Role-based access control (Reader, User, Admin, Moderator)\n* **SSRF protection**  Sanitise URLs before fetching (prevent internal network access)\n* **Rate limiting**  Per-user and per-IP request throttling\n* **CORS tightening**  Restrict allowed origins in production\n* **Audit logging**  Track all significant administrative actions\n\n== Deployment Topology ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Deployment Topology.WebHome\"/}}\n\n//Current deployment: single host with both services and SQLite files. Target production: load-balanced API servers, auto-scaling AKEL workers, PostgreSQL with read replicas, optional Redis for shared caching, and Prometheus/Grafana monitoring.//\n\n=== Current Development Setup ===\n\n{{code}}\n# Terminal 1: Start C# API\ncd apps/api && dotnet run --configuration Development\n# Runs on http://localhost:5000, Swagger at /swagger\n\n# Terminal 2: Start Next.js dev server\ncd apps/web && npm run dev\n# Runs on http://localhost:3000\n{{/code}}\n\n=== CI/CD Pipeline ===\n\n|= Stage |= Runner |= Actions\n| Build | Windows (GitHub Actions) | Node 20 setup, .NET 8 setup, ##npm ci##, ##npm build## (Next.js), ##dotnet build## (.NET Release)\n\n== User Roles ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Role-Based Access Control.WebHome\"/}}\n\n//Two roles are currently implemented: anonymous Reader (view only) and UCM Admin (configuration management via admin key). Registered User and Moderator roles are planned for Alpha/Beta.//\n\n|= Role |= Status |= Capabilities\n| **Reader** | Implemented | View published analyses and reports\n| **UCM Admin** | Implemented | Change pipeline/search/calculation configs, resume/pause system, view system health\n| **Registered User** | Planned (Alpha) | Submit articles/claims for analysis, view own job history\n| **Moderator** | Planned (Beta) | Review flagged content, manage user access, investigate abuse\n\n== Monitoring and Observability ==\n\n=== Current (POC) ===\n\n|= Capability |= Status |= Mechanism\n| System health endpoint | Implemented | ##GET /api/fh/system-health##  provider circuit state, pause status\n| Provider health banner | Implemented | UI banner when system is auto-paused\n| Job event streaming | Implemented | SSE events for real-time progress tracking\n| Analysis metrics | Implemented | Per-job metrics stored in ##AnalysisMetrics## table\n| Debug logging | Implemented | ##FH_DEBUG_LOG_PATH## for detailed pipeline logs\n\n=== Planned (Production) ===\n\n|= Capability |= Technology |= When\n| Metrics collection | Prometheus | Beta\n| Dashboards | Grafana | Beta\n| Alerting | Prometheus Alertmanager | Beta\n| Distributed tracing | OpenTelemetry | V1.0\n| Log aggregation | ELK or Loki | V1.0\n\n=== Key Metrics to Track ===\n\n|= Category |= Metrics\n| **Performance** | AKEL processing time, API response time, LLM call latency, search latency\n| **Quality** | Confidence score distribution, evidence count per analysis, Gate 4 pass rate\n| **Cost** | LLM tokens consumed, search API calls, cost per analysis\n| **Reliability** | Provider failure rate, circuit breaker trips, auto-pause events\n\n== Disaster Recovery ==\n\n|= Aspect |= Current (POC) |= Target (Production)\n| **Backups** | Manual SQLite file copy | Automated PostgreSQL backups to S3\n| **Recovery** | Restore SQLite files | Point-in-time recovery from transaction logs\n| **Replication** | None (single instance) | PostgreSQL streaming replication\n| **RTO** | Manual (hours) | < 4 hours\n| **Data loss window** | Since last backup | Minutes (WAL-based)\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]]\n", "Product Development.Specification.Architecture.Storage and Configuration.WebHome": "= Storage and Configuration =\n\nFactHarbor uses a three-database architecture with SQLite and a Unified Config Management (UCM) system for runtime configuration. This page covers the storage design, caching strategy, and evolution roadmap.\n\n== Three-Database Architecture ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Storage Architecture.WebHome\"/}}\n\n//Each database has a single owner: factharbor.db is managed by the .NET API, config.db and source-reliability.db by the Next.js app. This prevents cross-service write conflicts and keeps each database focused on one concern.//\n\n|= Database |= Owner |= Technology |= Key Tables |= Purpose\n| ##factharbor.db## | .NET API | Entity Framework Core | ##Jobs##, ##JobEvents##, ##AnalysisMetrics## | Job persistence, event logging, analysis results (JSON blob)\n| ##config.db## | Next.js | better-sqlite3 | ##config_blobs##, ##config_active##, ##config_usage## | UCM configuration with versioning and content-addressing\n| ##source-reliability.db## | Next.js | better-sqlite3 | ##source_reliability## | Source credibility evaluation cache (90-day TTL)\n\n=== Current Caching ===\n\n|= What |= Mechanism |= TTL |= Status\n| Source reliability scores | SQLite + in-memory ##Map## (batch prefetch) | 90 days (configurable) | Implemented\n| UCM config values | In-memory ##Map## with TTL-based expiry | 60 seconds | Implemented\n| URL content (fetched pages) | Not cached | N/A | Planned (Alpha)\n| Claim-level analysis results | Not cached | N/A | Planned (Alpha)\n\n== Unified Config Management (UCM) ==\n\nUCM provides runtime configuration management with schema validation, version tracking, and hot-reload support.\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.UCM Config Architecture.WebHome\"/}}\n\n//Admin edits a config in the UI. It passes through Zod schema validation, gets content-addressed (SHA-256), and stored as an immutable blob. An activation pointer selects the current version. Each analysis job snapshots the active config, ensuring reproducible results.//\n\n=== Configuration Types ===\n\n|= Type |= Default File |= Schema |= Key Settings\n| **Pipeline** | ##configs/pipeline.default.json## | ##PipelineConfig## | LLM provider, model tiering, analysis mode, iteration limits, token budgets\n| **Search** | ##configs/search.default.json## | ##SearchConfig## | Search provider, max results, domain whitelist/blacklist\n| **Calculation** | ##configs/calculation.default.json## | ##CalcConfig## | Verdict bands, confidence thresholds, weighting parameters\n| **Source Reliability** | ##configs/sr.default.json## | ##SRConfig## | Evaluation prompt, cache TTL, consensus scoring\n| **Prompts** | ##prompts/*.prompt.md## | ##PromptConfig## | Pipeline step prompt templates (markdown)\n\n=== Source of Truth Hierarchy ===\n\n1. **Runtime**: Database (what the app actually uses)\n1. **Defaults**: JSON files in ##apps/web/configs/##\n1. **Fallback**: Code constants in ##config-schemas.ts##\n\n=== Key UCM Features ===\n\n* **Content-addressed storage**  Config blobs identified by SHA-256 hash; same content = same hash (deduplication)\n* **Immutable history**  All past versions retained; any version can be re-activated\n* **Per-job snapshots**  Each analysis job records which config version it used (auditability)\n* **Hot-reload**  Config changes take effect within 60 seconds (cache TTL)\n* **Rollback**  One-click revert to any previous version via Admin UI\n* **Validation**  Zod schemas enforce type safety and value constraints before saving\n* **Import/Export**  Configs can be imported from and exported to JSON files\n\n== Storage Evolution Roadmap ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Storage Roadmap.WebHome\"/}}\n\n//Storage evolves incrementally: expand SQLite caching first (Alpha), migrate to PostgreSQL when user accounts and search are needed (Beta), add Redis/Vector DB/S3 only when specific triggers are met (V1.0+).//\n\n=== Technology Decisions ===\n\n|= Technology |= Decision |= Trigger for Adoption\n| **SQLite caching** | Evaluate (Alpha) | URL content and claim-level caching reduce cost and latency\n| **PostgreSQL** | Evaluate (Beta) | User accounts, full-text search, cross-analysis queries\n| **Redis** | Defer | Multiple application instances needing shared cache\n| **Vector DB** | Defer | Shadow Mode data proves near-duplicate detection needs exceed text hashing\n| **S3** | Defer | Storage exceeds ~50GB\n\n== Deep Dive ==\n\n* [[Storage Technology>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Storage Technology.WebHome]]  Detailed caching value analysis, Redis/PostgreSQL/Vector DB assessments, cost modelling\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[External Dependencies>>FactHarbor.Product Development.Specification.Architecture.External Dependencies.WebHome]] | Next: [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]]\n", "Product Development.Specification.Architecture.System Design.WebHome": "= System Design =\n\nFactHarbor uses a **two-service architecture**: a Next.js web application (UI + AKEL analysis engine) and a .NET API (job management + persistence). Both services run on a single host and communicate via HTTP.\n\n== Two-Service Architecture ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.System Architecture.WebHome\"/}}\n\n//The Next.js app serves the UI and runs the AKEL analysis engine. The .NET API manages job persistence, status tracking, and event streaming. Both access separate SQLite databases; the .NET API triggers the analysis engine via HTTP.//\n\n=== Why Two Services? ===\n\n* **.NET API** handles what it does best: structured data persistence (Entity Framework Core), job lifecycle management, and Server-Sent Events (SSE) for real-time updates\n* **Next.js** handles what it does best: React UI rendering, TypeScript analysis logic, and the Vercel AI SDK for LLM orchestration\n* Each service can be developed, tested, and deployed independently\n* The .NET API could be replaced with any backend that implements the same REST contract\n\n== Request Lifecycle ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Request Lifecycle.WebHome\"/}}\n\n//A user submits input, the .NET API queues a job and triggers the analysis runner. The AKEL pipeline runs asynchronously, and results are streamed back to the user via Server-Sent Events.//\n\n=== Key Communication Contracts ===\n\n|= Endpoint |= Direction |= Purpose\n| ##POST /v1/analyze## | Next.js  .NET | Create a new analysis job\n| ##GET /v1/jobs/{id}## | Next.js  .NET | Retrieve job status and results\n| ##GET /v1/jobs/{id}/events## | Next.js  .NET | SSE stream of job progress events\n| ##POST /api/internal/run-job## | .NET  Next.js | Trigger AKEL pipeline execution (with exponential backoff + jitter retry)\n| ##PATCH /v1/internal/jobs/{id}## | Next.js  .NET | Update job status and save results\n\n== Technology Stack ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Technology Stack.WebHome\"/}}\n\n//The technology stack is organised in layers: UI (React/Next.js), Engine (TypeScript/AI SDK), API (.NET/EF Core), Storage (SQLite), and External Services (LLM + search providers with specific model versions).//\n\n== Service Responsibilities ==\n\n=== Next.js Web App ===\n\n|= Area |= Responsibility |= Key Files\n| **UI Pages** | Analysis submission, job results, admin dashboard | ##src/app/analyze/##, ##src/app/jobs/##, ##src/app/admin/##\n| **Proxy API Routes** | Forward requests to .NET API | ##src/app/api/fh/##\n| **AKEL Pipeline** | Full fact-checking analysis (Understand  Research  Verdict  Report) | ##src/lib/analyzer/claimboundary-pipeline.ts##\n| **Shared Modules** | Evidence filtering, aggregation, quality gates, source reliability | ##src/lib/analyzer/*.ts##\n| **Configuration (UCM)** | Runtime config management with versioning and validation | ##src/lib/config-*.ts##\n| **Provider Health** | Circuit breaker per provider, auto-pause, webhook notifications | ##src/lib/provider-health.ts##\n\n=== .NET API ===\n\n|= Area |= Responsibility |= Key Files\n| **Job Management** | CRUD operations, status tracking, progress updates | ##Controllers/JobsController.cs##\n| **Analysis Trigger** | Invoke the AKEL runner with exponential backoff retry | ##Services/RunnerClient.cs##\n| **Event Streaming** | SSE (Server-Sent Events) for real-time job progress | ##Controllers/JobsController.cs##\n| **System Health** | Health checks, system health proxy to runner | ##Controllers/SystemHealthController.cs##\n| **Persistence** | SQLite via Entity Framework Core, migrations | ##Data/FhDbContext.cs##\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Next: [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]]\n", "Product Development.Specification.Architecture.WebHome": "= FactHarbor Architecture =\n\nFactHarbor is an **AI-powered fact-checking platform** that analyses claims and articles using multiple LLM providers and web search, producing structured verdicts with evidence provenance and confidence scores.\n\nThis section provides a comprehensive view of the system architecture, organised for different audiences  from executive overview to developer reference.\n\n== How It Works ==\n\nFactHarbor's core is the **AKEL pipeline** (AI Knowledge Extraction Layer): a multi-step analysis engine that decomposes claims, researches evidence from the web, and generates verdicts on a 7-point verdict scale.\n\n{{mermaid}}\nflowchart LR\n    INPUT[\"Article or Claim\"] --> UNDERSTAND[\"Understand\\n& Decompose\"]\n    UNDERSTAND --> RESEARCH[\"Research\\nEvidence\"]\n    RESEARCH --> VERDICTS[\"Generate\\nVerdicts\"]\n    VERDICTS --> REPORT[\"Structured\\nReport\"]\n\n    style INPUT fill:#e3f2fd,stroke:#1565c0,color:#000\n    style UNDERSTAND fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style RESEARCH fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style VERDICTS fill:#c8e6c9,stroke:#2e7d32,color:#000\n    style REPORT fill:#fff9c4,stroke:#f9a825,color:#000\n{{/mermaid}}\n\n//The pipeline extracts claims from input, searches the web for evidence, evaluates each claim against evidence, and produces a detailed report with verdicts, confidence scores, and source citations.//\n\n== System Context ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.System Context.WebHome\"/}}\n\n//FactHarbor uses a two-service architecture: a Next.js app for the user interface and AKEL analysis engine, backed by a .NET API for job persistence. The system integrates with multiple LLM and search providers for vendor independence.//\n\n== Core Principles ==\n\n* **AI-First**  The AKEL pipeline is the primary analytical system; humans supplement, not gate-keep\n* **Publish by Default**  No centralised approval; every verdict carries confidence scores and evidence provenance\n* **System Over Data**  Improve algorithms, not individual outputs\n* **Measure Everything**  Quality metrics drive all improvements\n* **No Vendor Lock-In**  Switch LLM or search providers based on cost, quality, or availability without code changes\n* **Start Simple**  Add complexity only when metrics prove it necessary\n\n== Technology Stack ==\n\n|= Layer |= Technology |= Purpose\n| **Frontend** | Next.js (TypeScript, React) | User interface, admin dashboard\n| **Analysis Engine** | TypeScript, Vercel AI SDK | AKEL pipeline, LLM orchestration\n| **API Backend** | ASP.NET Core 8 (C#) | Job scheduling, persistence, SSE events\n| **Storage** | SQLite (3 databases) | Jobs, configuration, source reliability cache\n| **LLM Providers** | Anthropic, OpenAI, Google, Mistral | Multi-provider AI with per-task model tiering\n| **Web Search** | Google CSE, SerpAPI | Evidence retrieval with provider fallback\n| **Quality** | Vitest, PromptFoo | Automated testing and prompt evaluation\n\n== Architecture Documentation ==\n\n=== Architectural Views (for Architects and Product Owners) ===\n\n|= Page |= What You'll Learn\n| [[System Design>>FactHarbor.Product Development.Specification.Architecture.System Design.WebHome]] | Two-service architecture, request lifecycle, technology stack, inter-service communication\n| [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]] | 5-step analysis flow, pipeline variants, shared analysis modules\n| [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] | Complete entity model (CB pipeline: CBClaimUnderstanding, AtomicClaim, ClaimAssessmentBoundary, CBClaimVerdict, OverallAssessment, VerdictNarrative, etc.), quality gate and configuration entities, 7-point verdict scale, job lifecycle\n| [[External Dependencies>>FactHarbor.Product Development.Specification.Architecture.External Dependencies.WebHome]] | LLM providers and model tiering, search providers, provider health monitoring\n| [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]] | Three-database architecture, UCM configuration management, storage evolution roadmap\n| [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]] | Quality gates, evidence filtering, source reliability, confidence calibration\n| [[Security and Operations>>FactHarbor.Product Development.Specification.Architecture.Security and Operations.WebHome]] | Security model, deployment topology, user roles, monitoring\n\n=== Deep Dives (for Developers and Testers) ===\n\n| [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Role-based navigation to detailed implementation references\n\nIncludes: ClaimAssessmentBoundary Pipeline internals, Pipeline Variants, Quality Gates Reference, Boundary Clustering, Evidence Quality Filtering, Source Reliability System, Calculations and Verdicts, Prompt Architecture, Confidence Calibration.\n\n=== Future Architecture ===\n\n| [[Future>>FactHarbor.Product Development.Specification.Architecture.Future.WebHome]] | Target production architecture, federation vision, automation roadmap\n\n== Reading Paths ==\n\n{{info}}\n**Sponsor or executive?** Start here, then optionally read [[Quality and Trust>>FactHarbor.Product Development.Specification.Architecture.Quality and Trust.WebHome]] to understand what makes the analysis trustworthy.\n\n**Product Owner or Architect?** Read the 7 Architectural Views above in order (~30-45 minutes for a thorough understanding).\n\n**Developer or Tester?** Complete the Architect path first for context, then use the [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] to navigate to references relevant to your task.\n{{/info}}\n\n== Project Status ==\n\n|= Phase |= Status |= Description\n| **Alpha** | Current | Single-instance deployment, SQLite storage, core AKEL pipeline with 2 variants, 800+ unit tests\n| **Alpha+** | Planned | User accounts, PostgreSQL migration, quality gate closure\n| **Beta** | Planned | Multi-instance, monitoring, Shadow Mode (self-learning prompt optimisation)\n| **V1.0** | Vision | Federation (multi-instance sync), full automation, public API\n\n== Related ==\n\n* [[Design Decisions>>FactHarbor.Product Development.Specification.Design-Decisions]]  Rationale for key architectural choices\n* [[When to Add Complexity>>FactHarbor.Product Development.DevOps.Guidelines.When to Add Complexity.WebHome]]  Decision triggers for technology additions\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]  User needs and system requirements\n", "Product Development.Specification.Automation.WebHome": "= Automation =\n\n{{warning}}\n**POC1 Implementation Status (February 2026):** This page describes the **target automation architecture** for FactHarbor at production maturity. In POC1 (v2.6.40+), the following are **implemented**: core AKEL claim analysis pipeline (7-step orchestrated), 2 quality gates (Gate 1 + Gate 4), multi-provider LLM support, A/B testing, and source reliability scoring. The following are **not yet implemented**: risk tiers (A/B/C), publication states, moderation system, human review queue, and automation level progression (Release 0.5/1.0/2.0).\n{{/warning}}\n\n**How FactHarbor scales through automated claim evaluation.**\n== 1. Automation Philosophy ==\nFactHarbor is **automation-first**: AKEL (AI Knowledge Extraction Layer) makes all content decisions. Humans monitor system performance and improve algorithms.\n**Why automation:**\n* **Scale**: Can process millions of claims\n* **Consistency**: Same evaluation criteria applied uniformly\n* **Transparency**: Algorithms are auditable\n* **Speed**: Results in <20 seconds typically\nSee [[Automation Philosophy>>FactHarbor.Organisation.Strategy.Automation Philosophy.WebHome]] for detailed principles.\n== 2. Claim Processing Flow ==\n=== 2.1 User Submits Claim ===\n* User provides claim text + source URLs\n* System validates format\n* Assigns processing ID\n* Queues for AKEL processing\n=== 2.2 AKEL Processing ===\n**AKEL automatically:**\n1. Parses claim into testable components\n2. Extracts evidence from sources\n3. Scores source credibility\n4. Evaluates claim against evidence\n5. Generates verdict with confidence score\n6. Assigns risk tier (A/B/C)\n7. Publishes result\n**Processing time**: Typically <20 seconds\n**No human approval required** - publication is automatic\n=== 2.3 Content States ===\n**Processing**: AKEL working on claim (not visible to public)\n**Published**: AKEL completed evaluation (public)\n* Verdict displayed with confidence score\n* Evidence and sources shown\n* Risk tier indicated\n* Users can report issues\n**Flagged**: AKEL identified issue requiring moderator attention (still public)\n* Low confidence below threshold\n* Detected manipulation attempt\n* Unusual pattern\n* Moderator reviews and may take action\n\n== 2.5 LLM-Based Processing Architecture ==\n\nFactHarbor delegates complex reasoning and analysis tasks to Large Language Models (LLMs). The architecture evolves from POC to production:\n\n=== Actual POC1 Implementation: 7-Step Orchestrated Pipeline ===\n\n1. **UNDERSTAND**  Claim decomposition, AnalysisContext detection, KeyFactor generation\n2. **RESEARCH**  Web search query generation and execution\n3. **EVIDENCE**  Source fetching, evidence extraction with quality filtering\n4. **CONTEXT REFINEMENT**  Supplemental claims/contexts discovery\n5. **VERDICTS**  Per-context verdict generation with 7-point verdict scale\n6. **SUMMARY**  Multi-context aggregation and two-panel summary\n7. **REPORT**  Final markdown report with quality gate checks\n\n**Characteristics:**\n* Multi-step workflow with explicit quality gates between phases\n* Multi-provider LLM support via Vercel AI SDK (Anthropic, OpenAI, Google, Mistral)\n* Model tiering: different models for understand, extract_evidence, and verdict phases\n* Iterative research with configurable search depth\n* 7-layer evidence quality defense\n\n=== Target Production Three-Phase Approach ===\n\n**Phase 1: Claim Extraction + Validation**\n* Extract distinct verifiable claims\n* Validate claim clarity and uniqueness\n* Remove duplicates and vague claims\n\n**Phase 2: Evidence Gathering (Parallel)**\n* For each claim independently:\n * Find supporting and contradicting evidence\n * Identify authoritative sources\n * Cluster EvidenceScopes into ClaimAssessmentBoundaries\n* Validation: Check evidence quality and source validity\n* Error containment: Issues in one claim don't affect others\n\n**Phase 3: Verdict Generation (Parallel)**\n* For each claim:\n * Generate verdict based on validated evidence\n * Assess confidence and risk level\n * Flag low-confidence results for human review\n* Validation: Check verdict consistency with evidence\n\n**Advantages:**\n* Error containment between phases\n* Clear quality gates and validation\n* Observable metrics per phase\n* Scalable (parallel processing across claims)\n* Adaptable (can optimize each phase independently)\n\n=== LLM Task Delegation ===\n\nAll complex cognitive tasks are delegated to LLMs:\n* **Claim Extraction**: Understanding context, identifying distinct claims\n* **Evidence Finding**: Analyzing sources, assessing relevance\n* **Evidence Boundary Clustering**: Grouping compatible EvidenceScopes into ClaimAssessmentBoundaries\n* **Source Evaluation**: Assessing reliability and authority\n* **Verdict Generation**: Synthesizing evidence into conclusions\n* **Risk Assessment**: Evaluating potential impact\n\n=== Error Mitigation ===\n\nResearch shows sequential LLM calls face compound error risks. FactHarbor mitigates this through:\n* **Validation gates** between phases\n* **Confidence thresholds** for quality control\n* **Parallel processing** to avoid error propagation across claims\n* **Human review queue** for low-confidence verdicts\n* **Independent claim processing** - errors in one claim don't cascade to others\n\n== 3. Risk Tiers ==\nRisk tiers classify claims by potential impact and guide audit sampling rates.\n=== 3.1 Tier A (High Risk) ===\n**Domains**: Medical, legal, elections, safety, security\n**Characteristics**:\n* High potential for harm if incorrect\n* Complex specialized knowledge required\n* Often subject to regulation\n**Publication**: AKEL publishes automatically with prominent risk warning\n**Audit rate**: Higher sampling recommended\n=== 3.2 Tier B (Medium Risk) ===\n**Domains**: Complex policy, science, causality claims\n**Characteristics**:\n* Moderate potential impact\n* Requires careful evidence evaluation\n* Multiple valid interpretations possible\n**Publication**: AKEL publishes automatically with standard risk label\n**Audit rate**: Moderate sampling recommended\n=== 3.3 Tier C (Low Risk) ===\n**Domains**: Definitions, established facts, historical data\n**Characteristics**:\n* Low potential for harm\n* Well-documented information\n* Clear right/wrong answers typically\n**Publication**: AKEL publishes by default\n**Audit rate**: Lower sampling recommended\n== 4. Quality Gates ==\nAKEL applies quality gates before publication. If any fail, claim is **flagged** (not blocked - still published).\n**Quality gates**:\n* Sufficient evidence extracted (2 sources)\n* Sources meet minimum credibility threshold\n* Confidence score calculable\n* No detected manipulation patterns\n* Claim parseable into testable form\n**Failed gates**: Claim published with flag for moderator review\n== 5. Automation Levels ==\n{{include reference=\"FactHarbor.Product Development.Diagrams.Automation Level.WebHome\"/}}\nFactHarbor progresses through automation maturity levels:\n**Release 0.5** (Proof-of-Concept): Tier C only, human review required\n**Release 1.0** (Initial): Tier B/C auto-published, Tier A flagged for review\n**Release 2.0** (Mature): All tiers auto-published with risk labels, sampling audits\nSee [[Automation Roadmap>>FactHarbor.Product Development.Diagrams.Automation Roadmap.WebHome]] for detailed progression.\n\n== 5.5 Automation Roadmap ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Automation Roadmap.WebHome\"/}}\n\n== 6. Human Role ==\nHumans do NOT review content for approval. Instead:\n**Monitoring**: Watch aggregate performance metrics\n**Improvement**: Fix algorithms when patterns show issues\n**Exception handling**: Review AKEL-flagged items\n**Governance**: Set policies AKEL applies\nSee [[Contributor Processes>>FactHarbor.Organisation.How-We-Work-Together.Contributor Processes.WebHome]] for how to improve the system.\n\n== 6.5 Manual vs Automated Matrix ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Manual vs Automated matrix.WebHome\"/}}\n\n== 7. Moderation ==\nModerators handle items AKEL flags:\n**Abuse detection**: Spam, manipulation, harassment\n**Safety issues**: Content that could cause immediate harm\n**System gaming**: Attempts to manipulate scoring\n**Action**: May temporarily hide content, ban users, or propose algorithm improvements\n**Does NOT**: Routinely review claims or override verdicts\nSee [[Organisational Model>>FactHarbor.Organisation.Governance.Organisational Model.WebHome]] for moderator role details.\n== 8. Continuous Improvement ==\n**Performance monitoring**: Track AKEL accuracy, speed, coverage\n**Issue identification**: Find systematic errors from metrics\n**Algorithm updates**: Deploy improvements to fix patterns\n**A/B testing**: Validate changes before full rollout\n**Retrospectives**: Learn from failures systematically\nSee [[Continuous Improvement>>FactHarbor.Organisation.How-We-Work-Together.Continuous-Improvement]] for improvement cycle.\n== 9. Scalability ==\nAutomation enables FactHarbor to scale:\n* **Millions of claims** processable\n* **Consistent quality** at any volume\n* **Cost efficiency** through automation\n* **Rapid iteration** on algorithms\nWithout automation: Human review doesn't scale, creates bottlenecks, introduces inconsistency.\n== 10. Transparency ==\nAll automation is transparent:\n* **Algorithm parameters** documented\n* **Evaluation criteria** public\n* **Source scoring rules** explicit\n* **Confidence calculations** explained\n* **Performance metrics** visible\nSee [[System Performance Metrics>>FactHarbor.Product Development.Specification.System-Performance-Metrics]] for what we measure.", "Product Development.Specification.Data Model.WebHome": "= Target Data Model =\n\n{{warning}}\n**Target Architecture Specification**\n\nThis page describes the **target normalised data model**  the evolutionary goal for FactHarbor's storage architecture. It builds on the current implementation (JSON blobs in SQLite) by normalising entities into PostgreSQL tables, adding caching, user accounts, and full-text search.\n\n**For the current implementation entity model, see [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]].**\n\n**Compatibility:** All entity names, field names, and types in this target design are compatible with the current implementation interfaces in ##types.ts## and ##config-schemas.ts##. The target extends but does not contradict the current model.\n\n**Related pages:**\n* [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]  Current implementation ERDs and entity landscape\n* [[Data Models and Schemas (Reference)>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.WebHome]]  LLM schema mappings and metrics schema definitions\n* [[POC1 API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]]  POC API contract and endpoint specifications\n{{/warning}}\n\nFactHarbor's target data model is **simple, focused, and designed for automated processing**.\n\n== 1. Core Entities ==\n\n=== 1.1 ClaimVerdict (normalised from JSON) ===\n\n**Current**: Embedded in ##AnalysisResult## JSON blob (see [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]).\n**Target**: Normalised into PostgreSQL table.\n\nPer-claim assessment with truth percentage on the 7-point scale, confidence, and evidence references. Linked to an AnalysisContext and optional KeyFactor. For field-level detail see [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] and [[Entity Views (Target Database)>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n==== Performance Optimisation: Denormalised Fields ====\n\n**Rationale**: Claims system is 95% reads, 5% writes. Denormalising common data reduces joins and improves query performance by 70%.\n**Additional cached fields in claims table**:\n* **evidence_summary** (JSONB): Top 5 most relevant evidence snippets with scores\n * Avoids joining evidence table for listing/preview\n * Updated when evidence is added/removed\n * Format: ##[{\"text\": \"...\", \"source\": \"...\", \"relevance\": 0.95}, ...]##\n* **source_names** (TEXT[]): Array of source names for quick display\n * Avoids joining through evidence to sources\n * Updated when sources change\n * Format: ##[\"New York Times\", \"Nature Journal\", ...]##\n* **context_count** (INTEGER): Number of AnalysisContexts for this analysis\n * Quick metric without counting rows\n * Updated when contexts detected\n* **cache_updated_at** (TIMESTAMP): When denormalised data was last refreshed\n * Helps invalidate stale caches\n * Triggers background refresh if too old\n**Update Strategy**:\n* **Immediate**: Update on re-analysis (system-triggered)\n* **Deferred**: Update via background job (non-critical)\n* **Invalidation**: Clear cache when source data or UCM config changes significantly\n**Trade-offs**:\n* 70% fewer joins on common queries\n* Much faster claim list/search pages\n* Better user experience\n* Small storage increase (~10%)\n* Need to keep caches in sync\n\n=== 1.2 EvidenceItem (normalised from JSON) ===\n\n**Current**: Embedded in ##AnalysisResult## JSON blob.\n**Target**: Normalised into PostgreSQL table.\n\nExtracted evidence from a source document, with category, direction (supports/contradicts/neutral), probativeValue, and optional EvidenceScope metadata. For field-level detail see [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] and [[Entity Views (Research Phase)>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n=== 1.3 FetchedSource (normalised from JSON) ===\n\n**Current**: Embedded in ##AnalysisResult## JSON blob. Source reliability cached in ##source-reliability.db##.\n**Target**: Normalised into PostgreSQL table with dedicated reliability tracking.\n\nWeb source fetched during research, with LLM-evaluated reliability score (0.0-1.0), optional multi-model consensus, and fetch metadata. For field-level detail see [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] and [[Entity Views (Research Phase)>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n==== Source Reliability Scoring ====\n\n**Current implementation (v2.6.40+):** Source reliability uses a **Pure LLM + Cache architecture**. Sources are batch-evaluated via LLM calls during analysis, cached in SQLite with configurable TTL, and used synchronously. Multi-model consensus (Claude + GPT) is supported. See [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].\n\n**7-band credibility scale** (0.0-1.0):\n* Authoritative (0.90+), Highly Credible (0.80-0.89), Credible (0.70-0.79), Moderately Credible (0.60-0.69), Mixed (0.50-0.59), Low Credibility (0.30-0.49), Unreliable (<0.30)\n\n==== Source Scoring Architecture ====\n\n**Critical design principle**: Prevent circular dependencies between source scoring and claim analysis.\n\n**Current approach** (LLM + Cache):\n* Sources evaluated on-demand via LLM during analysis\n* Evaluations cached in SQLite with configurable TTL (default 90 days)\n* Multi-model consensus for stability\n* AKEL pipeline reads scores synchronously  never updates them during analysis\n\n**Target enhancement** (Background Refresh):\n\nIn addition to on-demand evaluation, a background job can periodically refresh stale scores:\n\n{{code language=\"typescript\"}}\n// Target: Background refresh of stale source evaluations\nasync function refreshStaleSourceScores(): Promise<void> {\n  // Find evaluations approaching TTL expiry\n  const staleDomains = await getDomainsNearingExpiry(daysRemaining: 7);\n  for (const domain of staleDomains) {\n    // Re-evaluate using current LLM + consensus\n    const evaluation = await evaluateSourceReliability(domain);\n    // Update cache with fresh score\n    await cacheReliabilityScore(domain, evaluation);\n  }\n  // Runs as scheduled background job\n  // Never during claim analysis\n}\n{{/code}}\n\n**Key Principles** (current and target):\n* **Scoring and analysis are temporally separated**: Source scoring is cached/scheduled; claim analysis is real-time\n* **One-way data flow during processing**: Claims READ source scores, never WRITE them\n* **Audit trail**: Log all score changes, track score history, explainable calculations\n\n=== 1.4 AnalysisContext (normalised from JSON) ===\n\n**Current**: Embedded in ##ClaimUnderstanding## within the ##AnalysisResult## JSON blob.\n**Target**: Normalised into PostgreSQL table.\n\n**Purpose**: Bounded analytical frame for evaluating claims from different perspectives. Each AnalysisContext represents a specific analytical dimension detected during the Understanding phase. Carries subject, temporal scope, status, outcome, and domain-specific metadata (JSONB). For field-level detail see [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] and [[Entity Views (Understanding Phase)>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n**How Found**: Input analysis  Context detection  Create AnalysisContext  Link claims to contexts\n\n**Example**:\nFor article \"Bolsonaro trial analysis\":\n* AnalysisContext 1: \"TSE Criminal Trial (2023)\"  legal proceedings context\n* AnalysisContext 2: \"Political Impact on Democracy\"  political analysis context\n* AnalysisContext 3: \"Electoral System Integrity\"  institutional context\n\n**Target Evolution**: Many-to-many relationship can be added if cross-analysis context sharing is needed. For current implementation, contexts are embedded per-analysis.\n\n=== 1.5 ClaimVerdict Detail ===\n\n**Purpose**: Assessment of a claim within a specific AnalysisContext. Each ClaimVerdict provides a truth percentage on the 7-point scale with confidence and evidence references.\n\n**Core fields** (see Section 1.1 above for full field list):\n* **truthPercentage** (int 0-100): Calibrated truth score mapped to 7-point verdict scale\n* **confidence** (int 0-100): System confidence in this assessment\n* **reasoning** (text): Human-readable explanation\n* **supportingEvidenceIds** (text[]): References to EvidenceItems that support this verdict\n\n**Immutability**: Analysis outputs (including verdicts) are immutable. To improve verdict quality, update UCM configuration and trigger re-analysis. Each analysis job records the UCM config snapshot used.\n\n**Example**:\nFor claim \"Due process was followed in the trial\" in AnalysisContext \"TSE Criminal Trial\":\n* truthPercentage: 72, confidence: 68, verdict: \"MOSTLY_TRUE\"\n* Re-analysis with improved UCM config: truthPercentage: 78, confidence: 75, verdict: \"MOSTLY_TRUE\"\n* New analysis job references the updated UCM config snapshot\n\n**Key Design**: Analysis outputs are immutable. Quality improvements flow through UCM configuration changes, not data edits.\n\n=== 1.6 User ===\n\n{{info}}\n**Not yet implemented.** All users are currently anonymous readers. User system is planned for Alpha phase.\n{{/info}}\n\nFields: username, email, **role** (Reader/User/UCM Administrator/Moderator)\n\n=== Roles ===\n**reader** (guest, anonymous):\n* Browse and search published analyses\n* View analysis results and evidence\n\n**user** (registered, logged in):\n* Everything Reader can do\n* Submit URLs/text for analysis (rate-limited)\n* Flag quality issues\n* View own submission history\n\n**ucm_administrator** (appointed by Governing Team):\n* Manage UCM configuration (prompt templates, quality thresholds, model selection)\n* View config change history and audit trail\n* Activate/deactivate config versions\n* Trigger re-analysis with updated config\n* View system metrics\n\n**moderator** (appointed):\n* Handle abuse reports\n* Hide harmful content\n* Ban users for policy violations\n* Does NOT manage content quality (that is automated)\n\n=== 1.7 UCM Config ===\n\n**Tables** (already implemented in ##config.db##):\n* **config_blobs**: Immutable, content-addressed config versions (hash-based deduplication)\n* **config_active**: Points to currently active config blob per config type\n* **config_usage**: Links each analysis job to the exact config snapshot used\n\n**Config Types Managed** (##PipelineConfig##, ##CalcConfig##, ##SearchConfig##, ##SourceReliabilityConfig##):\n* Pipeline settings (LLM provider selection, model tiering, budget controls)\n* Calculation parameters (verdict bands, centrality weights, contestation penalties)\n* Search provider settings (max results, timeout, domain lists)\n* Source reliability rules (consensus thresholds, cache TTL)\n* Quality gate thresholds (confidence minimums, source count requirements)\n\n**Audit Trail**:\n* Every config change creates a new immutable blob\n* Config blobs are never deleted  complete history preserved\n* Each analysis job records which config snapshot was active at execution time\n* Reports can be reproduced by re-running with the same config snapshot\n\n=== 1.8 Flag ===\n\n{{info}}\n**Not yet implemented.** Planned for Alpha phase with user system.\n{{/info}}\n\nFields: entity_id, reported_by, issue_type, status, resolution_note\n\n=== 1.9 QualityMetric ===\n\n{{info}}\n**Not yet implemented.** Planned for Alpha/Beta phase.\n{{/info}}\n\n**Fields**: metric_type, category, value, target, timestamp\n**Purpose**: Time-series quality tracking\n**Usage**:\n* **Continuous monitoring**: Hourly calculation of error rates, confidence scores, processing times\n* **Quality dashboard**: Real-time display with trend charts\n* **Alerting**: Automatic alerts when metrics exceed thresholds\n* **A/B testing**: Compare control vs treatment metrics\n* **Improvement validation**: Measure before/after changes\n\n=== 1.10 ErrorPattern ===\n\n{{info}}\n**Not yet implemented.** Planned for Alpha/Beta phase.\n{{/info}}\n\n**Fields**: error_category, claim_id, description, root_cause, frequency, status\n**Purpose**: Capture errors to trigger system improvements\n\n== 1.11 Current Implementation ERD ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome\"/}}\n\n== 1.12 User Class Diagram ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.User Class Diagram.WebHome\"/}}\n\n== 2. Versioning Strategy ==\n\n**Analysis Data Is Immutable**:\n* **ClaimVerdicts, EvidenceItems, AnalysisContexts**: Created once per analysis job, never modified\n* **FetchedSources**: Reliability scores updated via cache TTL refresh (not during analysis)\n* **QualityMetric**: Time-series data (each record is a point in time)\n* **ErrorPattern**: System improvement queue (status tracked)\n\n**UCM Configuration Is Versioned**:\n* Every config change creates a new immutable blob in ##config_blobs##\n* ##config_active## tracks which config version is currently in use\n* ##config_usage## links each analysis job to the config snapshot used\n* Complete config history preserved  blobs never deleted\n\n**Example**:\n{{{\nUCM Config V1: prompt_template = \"Analyze the following claim...\"\n  UCM Administrator updates config \nUCM Config V2: prompt_template = \"Analyze the following claim with focus on source quality...\"\n  config_blobs stores both versions (content-addressed by hash)\n  Reports reference which config version was used for their analysis\n}}}\n\n== 2.5. Storage vs Computation Strategy ==\n\n**Critical architectural decision**: What to persist in databases vs compute dynamically?\n**Trade-off**:\n* **Store more**: Better reproducibility, faster, lower LLM costs | Higher storage/maintenance costs\n* **Compute more**: Lower storage/maintenance costs | Slower, higher LLM costs, less reproducible\n\n=== Recommendation: Hybrid Approach ===\n\n**STORE (in PostgreSQL):**\n\n==== ClaimVerdicts (Current State) ====\n* **What**: claimText, type, isCentral, truthPercentage, confidence, verdict, reasoning, supportingEvidenceIds\n* **Why**: Core entity, must be persistent\n* **Size**: ~1 KB per claim\n* **Growth**: Linear with claims\n* **Decision**: STORE - Essential\n\n==== EvidenceItems (All Records) ====\n* **What**: statement, category, sourceId, sourceExcerpt, probativeValue, sourceAuthority, evidenceBasis\n* **Why**: Hard to re-gather, reproducibility\n* **Size**: ~2 KB per evidence (with excerpt)\n* **Growth**: 3-10 evidence per claim\n* **Decision**: STORE - Essential for reproducibility\n\n==== FetchedSources (Track Records) ====\n* **What**: url, domain, trackRecordScore, trackRecordConfidence, trackRecordConsensus\n* **Why**: Continuously evaluated, cached with TTL\n* **Size**: ~500 bytes per source\n* **Growth**: Slow (limited number of sources)\n* **Decision**: STORE - Essential for quality\n\n==== UCM Config History ====\n* **What**: config_blobs (immutable), config_active (activation pointers), config_usage (per-job snapshots)\n* **Why**: Audit trail, reproducibility, config rollback\n* **Size**: ~5 KB per config version\n* **Growth**: Linear with config changes (infrequent  admin-driven)\n* **Retention**: Config blobs never deleted (complete history)\n* **Decision**: STORE - Essential for reproducibility and audit\n\n==== Flags (User Reports) ====\n* **What**: entity_id, reported_by, issue_type, description, status\n* **Why**: Error detection, system improvement triggers\n* **Size**: ~500 bytes per flag\n* **Decision**: STORE - Essential for improvement\n\n==== ErrorPatterns (System Improvement) ====\n* **What**: error_category, claim_id, description, root_cause, frequency, status\n* **Why**: Learning loop, prevent recurring errors\n* **Size**: ~1 KB per pattern\n* **Decision**: STORE - Essential for learning\n\n==== QualityMetrics (Time Series) ====\n* **What**: metric_type, category, value, target, timestamp\n* **Why**: Trend analysis, cannot recreate historical metrics\n* **Size**: ~200 bytes per metric\n* **Retention**: 2 years hot, then aggregate and archive\n* **Decision**: STORE - Essential for monitoring\n\n**STORE (Computed Once, Then Cached):**\n\n==== Analysis Summary ====\n* **What**: Neutral text summary of claim analysis (200-500 words)\n* **Computed**: Once by AKEL when claim first analysed\n* **Recomputed**: Only when system significantly improves OR UCM config changes\n* **Why store**: Expensive to regenerate ($0.01-0.05 per analysis), doesn't change often\n* **Size**: ~2 KB per claim\n* **Decision**: STORE (cached) - Cost-effective\n\n==== AnalysisContexts ====\n* **What**: Detected analytical frames with metadata\n* **Current design**: Embedded in result JSON\n* **Target**: Normalised into PostgreSQL table\n* **Size**: ~1 KB per context  2-5 contexts average\n* **Decision**: STORE - Essential for multi-perspective analysis\n\n**COMPUTE DYNAMICALLY (Never Store):**\n\n==== Search Results ====\n* **What**: Lists of claims matching search query\n* **Compute from**: Elasticsearch index\n* **Cache**: 15 minutes in Redis for popular queries\n* **Why not store permanently**: Constantly changing, infinite possible queries\n\n==== Aggregated Statistics ====\n* **What**: \"Total claims: 1,234,567\", \"Average confidence: 78%\", etc.\n* **Compute from**: Database queries\n* **Cache**: 1 hour in Redis\n* **Why not store**: Can be derived, relatively cheap to compute\n\n=== Summary Table ===\n\n| Data Type | Storage | Compute | Size per Claim | Decision | Rationale |\n|-----------|---------|---------|----------------|----------|-----------|\n| ClaimVerdict | Yes | - | 1 KB | STORE | Essential |\n| EvidenceItem | Yes | - | 2 KB x 5 = 10 KB | STORE | Reproducibility |\n| FetchedSource | Yes | - | 500 B (shared) | STORE | Track record |\n| UCM config history | Yes | - | 5 KB per version | STORE | Reproducibility |\n| Analysis summary | Yes | Once | 2 KB | STORE (cached) | Cost-effective |\n| AnalysisContext | Yes | Once | 1 KB x 3 | STORE | Essential |\n| Flags | Yes | - | 500 B x 10% | STORE | Improvement |\n| ErrorPatterns | Yes | - | 1 KB (global) | STORE | Learning |\n| QualityMetrics | Yes | - | 200 B (time series) | STORE | Trending |\n| Search results | - | Yes | - | COMPUTE + 15min cache | Dynamic |\n| Aggregations | - | Yes | - | COMPUTE + 1hr cache | Derivable |\n\n**Total storage per claim**: ~18 KB (without flags)\n**For 1 million claims**:\n* **Storage**: ~18 GB (manageable)\n* **PostgreSQL**: ~$50/month (standard instance)\n* **Redis cache**: ~$20/month (1 GB instance)\n* **S3 archives**: ~$5/month (old data)\n* **Total**: ~$75/month infrastructure\n\n**LLM cost savings by caching**:\n* Analysis summary stored: Save $0.03 per claim = $30K per 1M claims\n* Source reliability cached (current TTL): Save LLM evaluation costs per domain\n* **Total savings**: ~$35K per 1M claims vs recomputing every time\n\n=== Recomputation Triggers ===\n\n**When to mark cached data as stale and recompute:**\n1. **UCM config updated**  Mark affected analyses stale, trigger re-analysis\n2. **Source trackRecordScore changes >0.10**  Recompute: confidence, verdict\n3. **New evidence sources available**  Trigger re-analysis for affected claims\n4. **Quality gate thresholds changed**  Re-evaluate existing verdicts against new thresholds\n\n**Recomputation strategy**:\n* **Eager**: Immediately re-analyse (for targeted UCM config changes)\n* **Lazy**: Re-analyse on next view (for broad system improvements)\n* **Batch**: Periodic re-evaluation of stale analyses\n\n=== Database Size Projection ===\n\n**Year 1**: 10K claims\n* Storage: 180 MB\n* Cost: $10/month\n**Year 3**: 100K claims\n* Storage: 1.8 GB\n* Cost: $30/month\n**Year 5**: 1M claims\n* Storage: 18 GB\n* Cost: $75/month\n**Year 10**: 10M claims\n* Storage: 180 GB\n* Cost: $300/month\n* Optimisation: Archive old claims to S3 ($5/TB/month)\n\n**Conclusion**: Storage costs are manageable, LLM cost savings are substantial.\n\n== 3. Key Simplifications ==\n\n* **Two content states only**: Published, Hidden\n* **Three user roles only**: Reader, UCM Administrator, Moderator\n* **Immutable analysis data**: No data editing or versioning  improve via UCM config\n* **UCM config versioning**: Content-addressed immutable blobs with per-job snapshots\n* **Source reliability**: LLM + Cache with TTL-based refresh\n\n== 4. Target Database Schema ==\n\nThe following ERD visualises all target database tables  existing (green), target normalisation from JSON (blue), and planned future tables (red). For the full multi-view entity reference, see [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Entity Views.WebHome\" section=\"HTargetDatabaseEntities\"/}}\n\n=== 4.1 Primary Storage (PostgreSQL) ===\n\nFor field-level detail of all target tables, see [[Entity Views (Target Database)>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]].\n\n|= Table |= Status |= Purpose\n| ##claim_verdicts## | Target | Normalised from JSON  per-claim assessments\n| ##evidence_items## | Target | Normalised from JSON  extracted evidence\n| ##fetched_sources## | Target | Normalised from JSON  web sources with reliability scores\n| ##analysis_contexts## | Target | Normalised from JSON  bounded analytical frames\n| ##users## | Planned | User accounts (Alpha phase)\n| ##config_blobs## | Exists | Immutable config versions (content-addressed)\n| ##config_active## | Exists | Currently active config per type\n| ##config_usage## | Exists | Per-job config snapshots\n| ##flags## | Planned | User-reported quality issues (Alpha phase)\n| ##error_patterns## | Planned | System improvement queue (Alpha/Beta phase)\n| ##quality_metrics## | Planned | Time-series quality tracking (Alpha/Beta phase)\n\n=== 4.2 What's NOT Stored (Computed on-the-fly) ===\n\n* **Aggregated statistics**: Computed from base data\n* **Search results**: Generated from Elasticsearch index\n\n=== 4.3 Cache Layer (Redis) ===\n\n{{info}}\n**Not yet implemented.** Current implementation uses SQLite for source reliability caching. Redis planned for Alpha/Beta phase.\n{{/info}}\n\n**Cached for performance (Planned)**:\n* Frequently accessed claim verdicts (TTL: 1 hour)\n* Search results (TTL: 15 minutes)\n* User sessions (TTL: 24 hours)\n* Source reliability scores (TTL: 1 hour  supplements SQLite cache)\n\n=== 4.4 File Storage (S3) ===\n\n**Archived content**:\n* Old processing logs (>3 months)\n* Evidence documents (archived copies)\n* Database backups\n* Export files\n\n=== 4.5 Search Index (Elasticsearch) ===\n\n{{info}}\n**Not yet implemented.** Planned for Alpha phase.\n{{/info}}\n\n**Indexed for search**:\n* Claim assertions (full-text)\n* Evidence statements (full-text)\n* AnalysisContext descriptions (full-text)\n* Source domains (autocomplete)\n\nSynchronised from PostgreSQL via change data capture or periodic sync.\n\n== 5. Related Pages ==\n\n* [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]  Current implementation ERDs\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  Architecture overview\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]  User needs and system requirements\n", "Product Development.Specification.Design-Decisions": "= Design Decisions =\nThis page explains key architectural choices in FactHarbor and why simpler alternatives were chosen over complex solutions.\n**Philosophy**: Start simple, add complexity only when metrics prove necessary.\n== 1. Single Primary Database (PostgreSQL) ==\n**Decision**: Use PostgreSQL for all data initially, not multiple specialized databases\n**Alternatives considered**:\n*  PostgreSQL + TimescaleDB + Elasticsearch from day one\n*  Multiple specialized databases (graph, document, time-series)\n*  Microservices with separate databases\n**Why PostgreSQL alone**:\n* Modern PostgreSQL handles most workloads excellently\n* Built-in full-text search often sufficient\n* Time-series extensions available (pg_timeseries)\n* Simpler deployment and maintenance\n* Lower infrastructure costs\n* Easier to reason about\n**When to add specialized databases**:\n* Elasticsearch: When PostgreSQL search consistently >500ms\n* TimescaleDB: When metrics queries consistently >1s\n* Graph DB: If relationship queries become complex\n**Evidence**: Research shows single-DB architectures work well until 10,000+ users (Vertabelo, AWS patterns)\n== 2. Three-Layer Architecture ==\n**Decision**: Organize system into 3 layers (Interface, Processing, Data)\n**Alternatives considered**:\n*  7 layers (Ingestion, AKEL, Quality, Publication, Improvement, UI, Moderation)\n*  Pure microservices (20+ services)\n*  Monolithic single-layer\n**Why 3 layers**:\n* Clear separation of concerns\n* Easy to understand and explain\n* Maintainable by small team\n* Can scale each layer independently\n* Reduces cognitive load\n**Research**: Modern architecture best practices recommend 3-4 layers maximum for maintainability\n== 3. Deferred Federation ==\n**Decision**: Single-node architecture for V1.0, federation only in V2.0+\n**Alternatives considered**:\n*  Federated from day one\n*  P2P architecture\n*  Blockchain-based\n**Why defer federation**:\n* Adds massive complexity (sync, conflicts, identity, governance)\n* Not needed for first 10,000 users\n* Core product must be proven first\n* Most successful platforms start centralized (Wikipedia, Reddit, GitHub)\n* Can add federation later (see: Mastodon, Matrix)\n**When to implement**:\n* 10,000+ users on single node\n* Users explicitly request decentralization\n* Geographic distribution becomes necessary\n* Censorship becomes real problem\n**Evidence**: Research shows premature federation increases failure risk (InfoQ MVP architecture)\n== 4. Parallel AKEL Processing ==\n**Decision**: Process evidence/sources/scenarios in parallel, not sequentially\n**Alternatives considered**:\n*  Pure sequential pipeline (15-30 seconds)\n*  Fully async/event-driven (complex orchestration)\n*  Microservices per stage\n**Why parallel**:\n* 40% faster (10-18s vs 15-30s)\n* Better resource utilization\n* Same code complexity\n* Improves user experience\n**Implementation**: Simple parallelization within single AKEL worker\n**Evidence**: LLM orchestration research (2024-2025) strongly recommends pipeline parallelization\n== 5. Simple Manual Roles ==\n**Decision**: Manual role assignment for V1.0 (Reader, User, UCM Administrator, Moderator)\n**Alternatives considered**:\n*  Complex reputation point system from day one\n*  Automated privilege escalation\n*  Trust graphs\n**Why simple roles**:\n* Analysis data is immutable  no data-editing roles needed\n* UCM Administrators appointed by Governing Team (not earned through reputation)\n* Easier to implement and maintain\n* Can add automation later when needed\n**When to add complexity**:\n* Submission volume requires automated quota management\n* Manual role management becomes bottleneck\n* Clear abuse patterns emerge requiring automation\n**Evidence**: Starting simple and adding complexity when triggered by real needs\n== 6. One-to-Many Scenarios ==\n**Decision**: Scenarios belong to single claims (one-to-many) for V1.0\n**Alternatives considered**:\n*  Many-to-many with junction table\n*  Scenarios as separate first-class entities\n*  Hierarchical scenario taxonomy\n**Why one-to-many**:\n* Simpler queries (no junction table)\n* Easier to understand\n* Sufficient for most use cases\n* Can add many-to-many in V2.0 if requested\n**When to add many-to-many**:\n* Users request \"apply this scenario to other claims\"\n* Clear use cases for scenario reuse emerge\n* Performance doesn't degrade\n**Trade-off**: Slight duplication of scenarios vs. simpler mental model\n== 7. UCM Configuration Audit Trail ==\n**Decision**: Immutable config blobs + per-job config snapshots for full reproducibility\n**Alternatives considered**:\n*  Mutable config with no history\n*  Complex data versioning system (Edit entity tracking claim/evidence changes)\n*  Human editing of analysis outputs\n**Why UCM config versioning**:\n* Analysis data is immutable  \"improve the system, not the data\"\n* Every config change creates a new content-addressed blob\n* Every analysis job references the config snapshot used\n* Full reproducibility: re-run any analysis with its original config\n**Implementation**:\n* config_blobs: Immutable config versions (content-addressed by hash)\n* config_active: Currently active config per type\n* config_usage: Per-job config snapshots\n* AKEL processing logs: Archived after 90 days to S3\n**Evidence**: Content-addressed storage is proven in version control systems (Git)\n== 8. Denormalized Cache Fields ==\n**Decision**: Store summary data in claim records (evidence_summary, source_names, scenario_count)\n**Alternatives considered**:\n*  Fully normalized (join every time)\n*  Fully denormalized (duplicate everything)\n*  External cache only (Redis)\n**Why selective denormalization**:\n* 70% fewer joins on common queries\n* Much faster claim list/search pages\n* Trade-off: Small storage increase (~10%)\n* Read-heavy system (95% reads) benefits greatly\n**Update strategy**:\n* Immediate: On re-analysis (system-triggered)\n* Deferred: Background job\n* Invalidation: On source data or UCM config changes\n**Evidence**: Content management best practices recommend denormalization for read-heavy systems\n== 9. Multi-Provider LLM Orchestration ==\n**Decision**: Abstract LLM calls behind interface, support multiple providers\n**Alternatives considered**:\n*  Hard-coded to single LLM provider\n*  Switch providers manually\n*  Complex multi-agent system\n**Why orchestration**:\n* No vendor lock-in\n* Cost optimization (use cheap models for simple tasks)\n* Cross-checking (compare outputs)\n* Resilience (automatic fallback)\n**Implementation**: Simple routing layer, task-based provider selection\n**Evidence**: Modern LLM app architecture (2024-2025) strongly recommends orchestration\n== 10. Source Scoring Separation ==\n**Decision**: Separate source scoring (scheduled batch) from claim analysis (real-time)\n**Alternatives considered**:\n*  Update source scores during claim analysis\n*  Real-time score calculation\n*  Complex feedback loops\n**Why separate**:\n* Prevents circular dependencies\n* Predictable behavior\n* Easier to reason about\n* Simpler testing\n* Clear audit trail\n**Implementation**:\n* Sunday 2 AM: Calculate scores from past week\n* Monday-Saturday: Claims use those scores\n* Never update scores during analysis\n**Evidence**: Standard pattern to prevent feedback loops in ML systems\n== 11. Simple Versioning ==\n**Decision**: Basic audit trail only for V1.0 (before/after values, who/when/why)\n**Alternatives considered**:\n*  Full Git-like versioning from day one\n*  Branching and merging\n*  Time-travel queries\n*  Automatic conflict resolution\n**Why simple**:\n* Sufficient for accountability and basic rollback\n* Complex versioning not requested by users yet\n* Can add later if needed\n* Easier to implement and maintain\n**When to add complexity**:\n* Users request \"see version history\"\n* Users request \"restore previous version\"\n* Need for branching emerges\n**Evidence**: \"You Aren't Gonna Need It\" (YAGNI) principle from Extreme Programming\n== Design Philosophy ==\n**Guiding Principles**:\n1. **Start Simple**: Build minimum viable features\n2. **Measure First**: Add complexity only when metrics prove necessity\n3. **User-Driven**: Let user requests guide feature additions\n4. **Iterate**: Evolve based on real-world usage\n5. **Fail Fast**: Simple systems fail in simple ways\n**Inspiration**:\n* \"Premature optimization is the root of all evil\" - Donald Knuth\n* \"You Aren't Gonna Need It\" - Extreme Programming\n* \"Make it work, make it right, make it fast\" - Kent Beck\n**Result**: FactHarbor V1.0 is 35% simpler than original design while maintaining all core functionality and actually becoming more scalable.\n== Related Pages ==\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n* [[When to Add Complexity>>FactHarbor.Product Development.DevOps.Guidelines.When to Add Complexity.WebHome]]\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]", "Product Development.Specification.Examples.WebHome": "= Data Examples =\n\n{{info}}\n**Current Implementation**  These examples use the current data model entities: ARTICLE, CLAIM, EVIDENCE_ITEM, SOURCE, ANALYSIS_CONTEXT, CLAIM_VERDICT, and ARTICLE_VERDICT. All data is stored as JSON in SQLite. See [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] for the entity diagram and [[Evidence and Verdict Workflow>>FactHarbor.Product Development.Diagrams.Evidence and Verdict Workflow.WebHome]] for the verdict generation flow.\n\n**Note:** Examples include ##opposingEvidenceIds## (on CLAIM_VERDICT) and ##bias## (on SOURCE) which are planned Phase 2 fields not yet in the current code. The ##methodology## field on ANALYSIS_CONTEXT is stored within the ##metadata## JSON in the current implementation. Updated 2026-02-08 per documentation audit.\n{{/info}}\n\nThe following examples illustrate complete, realistic FactHarbor data objects across the current entity model. Each example shows the full pipeline output: from ARTICLE through CLAIMs, EVIDENCE_ITEMs, SOURCEs, ANALYSIS_CONTEXTs, to CLAIM_VERDICTs and the ARTICLE_VERDICT.\n\n**Entity quick reference:**\n\n|= Entity |= Purpose\n| **ARTICLE** | Top-level analysis container (one per user submission)\n| **CLAIM** | Extracted sub-claim with role, centrality, and context assignment\n| **EVIDENCE_ITEM** | Individual evidence statement extracted from a source\n| **SOURCE** | Fetched web source with reliability scoring\n| **ANALYSIS_CONTEXT** | Bounded analytical frame requiring a separate verdict\n| **CLAIM_VERDICT** | Per-claim verdict on the 7-point verdict scale\n| **ARTICLE_VERDICT** | Aggregated overall verdict for the article\n\n= Example A  \"Hydrogen cars are more energy efficient than EVs\" =\n\nA clearly empirical, technical claim requiring multi-context analysis (different efficiency metrics lead to different verdicts).\n\n== 1. ARTICLE ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##art_h2ev_001##\n| inputType | ##claim##\n| inputValue | ##Hydrogen cars are more energy efficient than battery electric vehicles (EVs).##\n| articleThesis | ##Hydrogen fuel cell vehicles are claimed to be more energy-efficient than battery EVs##\n| detectedInputType | ##claim##\n| requiresSeparateAnalysis | ##true##\n| analysisContexts | ##[\"ctx_wtw_eu\", \"ctx_ttw\"]##\n| schemaVersion | ##2.7.0##\n\n== 2. ANALYSIS_CONTEXTs ==\n\n=== 2.1 CTX_WTW_EU  Well-to-Wheel Efficiency (EU Grid Mix) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##ctx_wtw_eu##\n| articleId | ##art_h2ev_001##\n| name | ##Well-to-Wheel Efficiency Analysis (EU)##\n| subject | ##Total energy chain efficiency from primary energy source to vehicle motion, using European grid mix assumptions##\n| methodology | ##Comparative WTW energy analysis (ISO 14040 framework)##\n| temporal | ##20202024##\n\n=== 2.2 CTX_TTW  Tank-to-Wheel Efficiency ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##ctx_ttw##\n| articleId | ##art_h2ev_001##\n| name | ##Tank-to-Wheel Efficiency Comparison##\n| subject | ##Energy conversion efficiency from stored energy (battery or hydrogen tank) to wheel motion only##\n| methodology | ##Drivetrain efficiency comparison##\n| temporal | ##20202024##\n\n== 3. CLAIMs ==\n\n(% style=\"width:100%\" %)\n|= id |= text |= type |= claimRole |= isCentral |= contextId |= dependsOn |= keyFactorId\n| ##clm_01## | Hydrogen cars are more energy efficient than EVs | ##factual## | ##core## | ##true## | ##ctx_wtw_eu## | ##[]## | ##\"\"##\n| ##clm_02## | Fuel cell vehicles convert hydrogen to electricity at ~55% efficiency | ##factual## | ##core## | ##false## | ##ctx_ttw## | ##[]## | ##kf_fc_eff##\n| ##clm_03## | Green hydrogen production via electrolysis achieves 60-70% efficiency | ##factual## | ##source## | ##false## | ##ctx_wtw_eu## | ##[]## | ##kf_h2_prod##\n| ##clm_04## | BEV drivetrains achieve 85-90% battery-to-wheel efficiency | ##factual## | ##core## | ##false## | ##ctx_ttw## | ##[\"clm_02\"]## | ##kf_bev_eff##\n\n== 4. SOURCEs ==\n\n(% style=\"width:100%\" %)\n|= id |= title |= domain |= url |= trackRecordScore |= bias\n| ##src_01## | Transport & Environment | ##transportenvironment.org## | ##https:~/~/www.transportenvironment.org/wp-content/uploads/2020/04/T%26E-BEV-efficiency-report-2020.pdf## | ##0.82## | ##environmental advocacy##\n| ##src_02## | US DOE Alternative Fuels Data Center | ##afdc.energy.gov## | ##https:~/~/afdc.energy.gov/vehicles/fuel-cell## | ##0.91## | ##neutral##\n| ##src_03## | Nature Energy  Ueckerdt et al. (2021) | ##nature.com## | ##https:~/~/www.nature.com/articles/s41560-021-00775-z## | ##0.95## | ##neutral##\n\n== 5. EVIDENCE_ITEMs ==\n\n(% style=\"width:100%\" %)\n|= id |= sourceId |= statement |= sourceExcerpt |= category |= claimDirection |= contextId\n| ##ev_01## | ##src_03## | Well-to-wheel efficiency of hydrogen fuel cell vehicles is 25-35%, compared to 70-80% for battery EVs | ##\"the overall well-to-wheel efficiency of green hydrogen pathways is 2-3 lower than direct electrification\"## | ##statistic## | ##contradicts## | ##ctx_wtw_eu##\n| ##ev_02## | ##src_02## | Fuel cell stack efficiency ranges from 40-60% depending on load conditions | ##\"Fuel cell system efficiency ranges from approximately 40 to 60 percent\"## | ##direct_evidence## | ##supports## | ##ctx_ttw##\n| ##ev_03## | ##src_01## | BEV battery-to-wheel efficiency is approximately 77% including charging losses | ##\"battery electric vehicles achieve overall tank-to-wheel efficiency of approximately 77%, including charging losses\"## | ##statistic## | ##contradicts## | ##ctx_ttw##\n| ##ev_04## | ##src_03## | Hydrogen compression and transport add 10-15% energy losses | ##\"additional losses from compression, storage and transport reduce delivered hydrogen energy by 10-15%\"## | ##direct_evidence## | ##contradicts## | ##ctx_wtw_eu##\n\n== 6. CLAIM_VERDICTs ==\n\n=== 6.1 Verdict for clm_01 (core, WTW context) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_01##\n| claimId | ##clm_01##\n| verdict | ##FALSE##\n| truthPercentage | ##12##\n| confidence | ##88##\n| reasoning | ##Well-to-wheel analysis consistently shows hydrogen fuel cell vehicles are significantly less energy-efficient than BEVs. The hydrogen pathway loses 65-75% of input energy through electrolysis, compression, and fuel cell conversion, while BEVs retain 70-80%.##\n| supportingEvidenceIds | ##[]##\n| opposingEvidenceIds | ##[\"ev_01\", \"ev_04\"]##\n\n=== 6.2 Verdict for clm_02 (fuel cell efficiency) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_02##\n| claimId | ##clm_02##\n| verdict | ##MOSTLY-TRUE##\n| truthPercentage | ##78##\n| confidence | ##85##\n| reasoning | ##DOE data confirms fuel cell stack efficiency of 40-60%. The claim of ~55% is within the documented range under typical driving conditions.##\n| supportingEvidenceIds | ##[\"ev_02\"]##\n| opposingEvidenceIds | ##[]##\n\n=== 6.3 Verdict for clm_04 (BEV efficiency) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_04##\n| claimId | ##clm_04##\n| verdict | ##MOSTLY-TRUE##\n| truthPercentage | ##75##\n| confidence | ##82##\n| reasoning | ##T&E data shows ~77% including charging losses, which falls within the claimed 85-90% range at the drivetrain level (before charging losses). The claim is largely accurate for tank-to-wheel measurement.##\n| supportingEvidenceIds | ##[\"ev_03\"]##\n| opposingEvidenceIds | ##[]##\n\n== 7. ARTICLE_VERDICT ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| articleId | ##art_h2ev_001##\n| verdict | ##FALSE##\n| truthPercentage | ##14##\n| confidence | ##86##\n| summary | ##Hydrogen fuel cell vehicles are significantly less energy-efficient than battery EVs on a well-to-wheel basis. While individual component efficiencies vary, the cumulative losses in the hydrogen pathway (electrolysis ~65%, compression ~85-90%, fuel cell ~55%) result in 25-35% overall efficiency, compared to 70-80% for BEVs.##\n\n== 8. Notes ==\n\n* This example demonstrates **multi-context analysis**: the same claim evaluated under two different analytical frames (WTW and TTW) yields different sub-verdicts.\n* The article-level verdict aggregates across contexts, weighted by centrality and evidence quality.\n* The ##dependsOn## field on ##clm_04## shows claim dependency tracking  if ##clm_02## failed verification, ##clm_04## would be affected.\n\n= Example B  \"Regular cold-water exposure improves health\" =\n\nA complex health claim requiring careful context boundaries.\n\n== 1. ARTICLE ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##art_cw_001##\n| inputType | ##claim##\n| inputValue | ##Regular cold-water exposure below 14C for at least 6 months improves health.##\n| articleThesis | ##Regular cold-water immersion is claimed to improve health outcomes##\n| detectedInputType | ##claim##\n| requiresSeparateAnalysis | ##true##\n| analysisContexts | ##[\"ctx_immune\", \"ctx_mental\"]##\n| schemaVersion | ##2.7.0##\n\n== 2. ANALYSIS_CONTEXTs ==\n\n=== 2.1 CTX_IMMUNE  Immune System Effects ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##ctx_immune##\n| articleId | ##art_cw_001##\n| name | ##Cold Exposure and Immune Function##\n| subject | ##Whether regular cold-water immersion measurably improves immune system markers in healthy adults##\n| methodology | ##Randomized controlled trials and systematic reviews##\n| temporal | ##20152024##\n\n=== 2.2 CTX_MENTAL  Mental Health Effects ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##ctx_mental##\n| articleId | ##art_cw_001##\n| name | ##Cold Exposure and Mental Wellbeing##\n| subject | ##Whether regular cold-water immersion improves mood, stress resilience, or mental health outcomes##\n| methodology | ##Clinical studies and self-report outcome measures##\n| temporal | ##20152024##\n\n== 3. CLAIMs ==\n\n(% style=\"width:100%\" %)\n|= id |= text |= type |= claimRole |= isCentral |= contextId |= dependsOn\n| ##clm_cw_01## | Regular cold-water exposure improves health | ##factual## | ##core## | ##true## | ##\"\"## | ##[]##\n| ##clm_cw_02## | Cold-water immersion reduces sick days by 29% | ##factual## | ##core## | ##false## | ##ctx_immune## | ##[]##\n| ##clm_cw_03## | Cold exposure triggers norepinephrine release improving mood | ##factual## | ##core## | ##false## | ##ctx_mental## | ##[]##\n\n== 4. SOURCEs ==\n\n(% style=\"width:100%\" %)\n|= id |= title |= domain |= url |= trackRecordScore |= bias\n| ##src_cw_01## | Buijze et al. (2016)  Dutch Cold Shower RCT | ##journals.plos.org## | ##https:~/~/journals.plos.org/plosone/article?id=10.1371/journal.pone.0161749## | ##0.88## | ##neutral##\n| ##src_cw_02## | Shevchuk (2008)  Adapted cold showers treatment | ##pubmed.ncbi.nlm.nih.gov## | ##https:~/~/pubmed.ncbi.nlm.nih.gov/17993252/## | ##0.72## | ##neutral##\n| ##src_cw_03## | Cochrane Review  Cold exposure interventions | ##cochranelibrary.com## | ##https:~/~/www.cochranelibrary.com/## | ##0.94## | ##neutral##\n\n== 5. EVIDENCE_ITEMs ==\n\n(% style=\"width:100%\" %)\n|= id |= sourceId |= statement |= sourceExcerpt |= category |= claimDirection |= contextId\n| ##ev_cw_01## | ##src_cw_01## | 30-second cold showers reduced self-reported sick days by 29% over 90 days | ##\"The cold shower group had a 29% reduction in sickness absence from work\"## | ##statistic## | ##supports## | ##ctx_immune##\n| ##ev_cw_02## | ##src_cw_01## | Study used self-reported outcomes rather than clinical immune markers | ##\"Primary outcome was self-reported sickness absence\"## | ##criticism## | ##neutral## | ##ctx_immune##\n| ##ev_cw_03## | ##src_cw_02## | Cold exposure activates the sympathetic nervous system, increasing norepinephrine | ##\"cold hydrotherapy may activate the sympathetic nervous system and increase blood level of norepinephrine\"## | ##direct_evidence## | ##supports## | ##ctx_mental##\n| ##ev_cw_04## | ##src_cw_03## | Evidence for immune function improvement from cold exposure remains limited and low-quality | ##\"no high-quality evidence to support or refute the claimed immune benefits\"## | ##expert_quote## | ##contradicts## | ##ctx_immune##\n\n== 6. CLAIM_VERDICTs ==\n\n=== 6.1 Verdict for clm_cw_01 (core, overall) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_cw_01##\n| claimId | ##clm_cw_01##\n| verdict | ##MIXED##\n| truthPercentage | ##48##\n| confidence | ##62##\n| reasoning | ##Evidence supports some benefits (reduced sick days, mood improvement) but long-term \"health improvement\" is not substantiated by high-quality evidence. Effects vary significantly by outcome measured.##\n| supportingEvidenceIds | ##[\"ev_cw_01\", \"ev_cw_03\"]##\n| opposingEvidenceIds | ##[\"ev_cw_04\"]##\n\n=== 6.2 Verdict for clm_cw_02 (sick days, immune context) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_cw_02##\n| claimId | ##clm_cw_02##\n| verdict | ##LEANING-TRUE##\n| truthPercentage | ##62##\n| confidence | ##65##\n| reasoning | ##The Dutch RCT found a 29% reduction, but the outcome was self-reported, the cold exposure was brief (30 seconds), and the Cochrane review found insufficient high-quality evidence for immune claims.##\n| supportingEvidenceIds | ##[\"ev_cw_01\"]##\n| opposingEvidenceIds | ##[\"ev_cw_02\", \"ev_cw_04\"]##\n\n=== 6.3 Verdict for clm_cw_03 (mood, mental context) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_cw_03##\n| claimId | ##clm_cw_03##\n| verdict | ##MOSTLY-TRUE##\n| truthPercentage | ##74##\n| confidence | ##70##\n| reasoning | ##Norepinephrine release from cold exposure is well-documented physiologically. The mood improvement mechanism is plausible and supported, though clinical outcomes data is limited.##\n| supportingEvidenceIds | ##[\"ev_cw_03\"]##\n| opposingEvidenceIds | ##[]##\n\n== 7. ARTICLE_VERDICT ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| articleId | ##art_cw_001##\n| verdict | ##MIXED##\n| truthPercentage | ##50##\n| confidence | ##64##\n| summary | ##Regular cold-water exposure shows some health benefits, particularly in mood and perceived illness reduction, but the broad claim \"improves health\" is not well-supported by high-quality evidence. Mental health benefits have stronger physiological backing than immune function claims.##\n\n== 8. Notes ==\n\n* This example shows **context-divergent verdicts**: immune context (LEANING-TRUE, 62%) vs mental context (MOSTLY-TRUE, 74%) produce different outcomes from the same broad claim.\n* Gate 4 Confidence Assessment: immune context verdict is MEDIUM (65% confidence, 3 sources), mental context is MEDIUM (70% confidence, 2 sources).\n* The ##criticism## category evidence (##ev_cw_02##) has a ##neutral## claimDirection  it questions methodology without directly contradicting the conclusion.\n\n= Example C  \"Hillary Clinton communicates with Eleanor Roosevelt\" =\n\nA non-falsifiable/metaphorical claim  demonstrates Gate 1 handling.\n\n== 1. ARTICLE ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##art_hc_er_001##\n| inputType | ##claim##\n| inputValue | ##Hillary Clinton communicates with Eleanor Roosevelt.##\n| articleThesis | ##Hillary Clinton is claimed to communicate with the deceased Eleanor Roosevelt##\n| detectedInputType | ##claim##\n| requiresSeparateAnalysis | ##false##\n| analysisContexts | ##[]##\n| schemaVersion | ##2.7.0##\n\n== 2. CLAIMs ==\n\n(% style=\"width:100%\" %)\n|= id |= text |= type |= claimRole |= isCentral |= contextId\n| ##clm_hc_01## | Hillary Clinton communicates with Eleanor Roosevelt | ##evaluative## | ##core## | ##true## | ##\"\"##\n| ##clm_hc_02## | Clinton engaged in imaginary conversations as a reflective exercise guided by Jean Houston | ##factual## | ##attribution## | ##false## | ##\"\"##\n\n== 3. SOURCEs ==\n\n(% style=\"width:100%\" %)\n|= id |= title |= domain |= url |= trackRecordScore |= bias\n| ##src_hc_01## | Bob Woodward  The Choice (1996) | ##washingtonpost.com## | ##https:~/~/www.washingtonpost.com/## | ##0.85## | ##neutral##\n\n== 4. EVIDENCE_ITEMs ==\n\n(% style=\"width:100%\" %)\n|= id |= sourceId |= statement |= sourceExcerpt |= category |= claimDirection |= contextId\n| ##ev_hc_01## | ##src_hc_01## | Clinton participated in guided visualization exercises with Jean Houston in which she imagined conversations with Eleanor Roosevelt | ##\"At the urging of Jean Houston, Hillary held imaginary conversations with Eleanor Roosevelt as a personal reflection exercise\"## | ##direct_evidence## | ##neutral## | ##\"\"##\n\n== 5. CLAIM_VERDICTs ==\n\n=== 5.1 Verdict for clm_hc_01 (core  metaphorical) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_hc_01##\n| claimId | ##clm_hc_01##\n| verdict | ##UNVERIFIED##\n| truthPercentage | ##50##\n| confidence | ##35##\n| reasoning | ##The claim is ambiguous between literal paranormal communication (non-falsifiable) and documented reflective exercises (factual). Under the factual interpretation, Clinton did engage in imaginary conversations with Roosevelt, but these were guided visualization exercises, not literal communication.##\n| supportingEvidenceIds | ##[]##\n| opposingEvidenceIds | ##[]##\n\n=== 5.2 Verdict for clm_hc_02 (attribution  factual) ===\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_hc_02##\n| claimId | ##clm_hc_02##\n| verdict | ##TRUE##\n| truthPercentage | ##90##\n| confidence | ##80##\n| reasoning | ##Bob Woodward's reporting in The Choice (1996) documents the Jean Houston guided exercises. This is well-sourced from a credible journalist.##\n| supportingEvidenceIds | ##[\"ev_hc_01\"]##\n| opposingEvidenceIds | ##[]##\n\n== 6. ARTICLE_VERDICT ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| articleId | ##art_hc_er_001##\n| verdict | ##UNVERIFIED##\n| truthPercentage | ##50##\n| confidence | ##35##\n| summary | ##The claim is ambiguous. Clinton did engage in guided imaginary conversations with Eleanor Roosevelt as a reflective exercise (documented by Woodward, 1996), but the broader claim of \"communication\" is non-falsifiable under a paranormal interpretation.##\n\n== 7. Notes ==\n\n* The core claim receives UNVERIFIED with low confidence (35%)  below the 60% threshold that distinguishes MIXED from UNVERIFIED.\n* Gate 4 Confidence Assessment: **INSUFFICIENT** (<2 sources directly addressing the core claim)  marked as DO NOT PUBLISH in a production context. However, the ##isCentral## flag means it is kept with caveats.\n* AKEL decomposes the ambiguous input into two claims: the non-falsifiable core and a factual attribution claim. This demonstrates how claim decomposition can extract verifiable sub-claims from non-falsifiable inputs.\n\n= Example D  \"Hillary Clinton is a witch\" =\n\nA rhetorical/metaphorical claim  demonstrates classification without meaningful evidence search.\n\n== 1. ARTICLE ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##art_hc_w_001##\n| inputType | ##claim##\n| inputValue | ##Hillary Clinton is a witch.##\n| articleThesis | ##Hillary Clinton is claimed to be a witch##\n| detectedInputType | ##claim##\n| requiresSeparateAnalysis | ##false##\n| analysisContexts | ##[]##\n| schemaVersion | ##2.7.0##\n\n== 2. CLAIMs ==\n\n(% style=\"width:100%\" %)\n|= id |= text |= type |= claimRole |= isCentral |= contextId\n| ##clm_w_01## | Hillary Clinton is a witch | ##evaluative## | ##core## | ##true## | ##\"\"##\n\n== 3. SOURCEs ==\n\nNo sources fetched  claim classified as non-verifiable rhetorical expression.\n\n== 4. EVIDENCE_ITEMs ==\n\nNo evidence items  research phase skipped after Gate 1 assessment.\n\n== 5. CLAIM_VERDICTs ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| id | ##cv_w_01##\n| claimId | ##clm_w_01##\n| verdict | ##UNVERIFIED##\n| truthPercentage | ##50##\n| confidence | ##15##\n| reasoning | ##This is a rhetorical or metaphorical expression, not a factual claim. Under any literal interpretation it is non-falsifiable. No evidence search conducted.##\n| supportingEvidenceIds | ##[]##\n| opposingEvidenceIds | ##[]##\n\n== 6. ARTICLE_VERDICT ==\n\n(% style=\"width:100%\" %)\n|= Field |= Value\n| articleId | ##art_hc_w_001##\n| verdict | ##UNVERIFIED##\n| truthPercentage | ##50##\n| confidence | ##15##\n| summary | ##This input is rhetorical in nature and not a factual claim suitable for evidence-based analysis. No verdict can be meaningfully assigned.##\n\n== 7. Notes ==\n\n* Gate 4 Confidence Assessment: **INSUFFICIENT** (0 sources)  marked as DO NOT PUBLISH.\n* Confidence is extremely low (15%) reflecting the absence of any evidence.\n* In practice, AKEL still processes such inputs but produces a minimal result with clear labeling. The ##isCentral## flag ensures the core claim is always represented in output.\n\n= AKEL Automation Summary =\n\nBased on the current implementation (v2.10.2), the following summarizes what AKEL automates end-to-end versus what requires human involvement in future production modes.\n\n== Fully Automated (current POC) ==\n\n* Claim extraction and decomposition (text  sub-claims with roles and centrality)\n* ClaimAssessmentBoundary clustering (evidence-emergent, Stage 3)\n* Evidence extraction with source attribution and probative value scoring\n* Source reliability scoring (LLM + Cache architecture)\n* 7-layer evidence quality defense (provenance, tangential pruning, contestation validation)\n* Verdict generation on 7-point verdict scale with confidence scoring\n* Quality gate enforcement (Gate 1: Claim Validation, Gate 4: Verdict Confidence)\n* Contestation detection (doubted vs contested status)\n* Harm potential assessment\n* Full markdown report generation\n\n== System Improvement Opportunities ==\n\n* High-risk tier claims (medical, legal, elections)  UCM config tuning for domain-specific prompts and thresholds\n* Definition boundaries for ambiguous terms  improved claim decomposition rules\n* Ethical constraints on sensitive topics  risk tier configuration and warning labels\n* Contested or high-risk claims  higher sampling audit rates, confidence threshold adjustments\n\n== See Also ==\n\n* [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]]\n* [[Evidence and Verdict Workflow>>FactHarbor.Product Development.Diagrams.Evidence and Verdict Workflow.WebHome]]\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]\n* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]\n", "Product Development.Specification.FAQ.WebHome": "= Frequently Asked Questions (FAQ) =\nCommon questions about FactHarbor's design, functionality, and approach.\n== 1. How do claims get evaluated in FactHarbor? ==\n=== 1.1 User Submission ===\n**Who**: Registered users (free account required; rate-limited due to LLM/search costs)\n**Process**: Registered user submits claim text + source URLs\n**Speed**: Typically <20 seconds to verdict\n=== 1.2 AKEL Processing (Automated) ===\n**What**: AI Knowledge Extraction Layer analyzes claim\n**Steps**:\n* Parse claim into testable components\n* Extract evidence from provided sources\n* Score source credibility\n* Generate verdict with confidence level\n* Assign risk tier\n* Publish automatically\n**Authority**: AKEL makes all content decisions\n**Scale**: Can process millions of claims\n=== 1.3 Continuous Improvement (Human Role) ===\n**What**: Humans improve the system, not individual verdicts\n**Activities**:\n* Monitor aggregate performance metrics\n* Identify systematic errors\n* Propose algorithm improvements\n* Update policies and rules\n* Test changes before deployment\n**NOT**: Reviewing individual claims for approval\n**Focus**: Fix the system, not the data\n=== 1.4 Exception Handling ===\n**When AKEL flags for review**:\n* Low confidence verdict\n* Detected manipulation attempt\n* Unusual pattern requiring attention\n**Moderator role**:\n* Reviews flagged items\n* Takes action on abuse/manipulation\n* Proposes detection improvements\n* Does NOT override verdicts\n=== 1.5 Why This Model Works ===\n**Scale**: Automation handles volume humans cannot\n**Consistency**: Same rules applied uniformly\n**Transparency**: Algorithms can be audited\n**Improvement**: Systematic fixes benefit all claims\n== 2. What prevents FactHarbor from becoming another echo chamber? ==\nFactHarbor includes multiple safeguards against echo chambers and filter bubbles:\n**Mandatory Contradiction Search**:\n* AI must actively search for counter-evidence, not just confirmations\n* System checks for echo chamber patterns in source clusters\n* Flags tribal or ideological source clustering\n* Requires diverse perspectives across political/ideological spectrum\n**Multiple Scenarios**:\n* Claims are evaluated under different interpretations\n* Reveals how assumptions change conclusions\n* Makes disagreements understandable, not divisive\n**Transparent Reasoning**:\n* All assumptions, definitions, and boundaries are explicit\n* Evidence chains are traceable\n* Uncertainty is quantified, not hidden\n**Audit System**:\n* Human auditors check for bubble patterns\n* Feedback loop improves AI search diversity\n* Community can flag missing perspectives\n**Federation**:\n* Multiple independent nodes with different perspectives\n* No single entity controls \"the truth\"\n* Cross-node contradiction detection\n== 3. How does FactHarbor handle claims that are \"true in one context but false in another\"? ==\nThis is exactly what FactHarbor is designed for:\n**Scenarios capture contexts**:\n* Each scenario defines specific boundaries, definitions, and assumptions\n* The same claim can have different verdicts in different scenarios\n* Example: \"Coffee is healthy\" depends on:\n ** Definition of \"healthy\" (reduces disease risk? improves mood? affects specific conditions?)\n ** Population (adults? pregnant women? people with heart conditions?)\n ** Consumption level (1 cup/day? 5 cups/day?)\n ** Time horizon (short-term? long-term?)\n**Evidence Landscape**:\n* Shows all ClaimAssessmentBoundaries and their verdicts side-by-side\n* Users see *why* interpretations differ (different evidence clusters, methodologies, temporal scopes)\n* No forced consensus when legitimate disagreement exists\n**Explicit Assumptions**:\n* Every scenario states its assumptions clearly\n* Users can compare how changing assumptions changes conclusions\n* Makes context-dependence visible, not hidden\n== 4. What makes FactHarbor different from traditional fact-checking sites? ==\n**Traditional Fact-Checking**:\n* Binary verdicts: True / Mostly True / False\n* Single interpretation chosen by fact-checker\n* Often hides legitimate contextual differences\n* Limited ability to show *why* people disagree\n**FactHarbor**:\n* **Multi-scenario**: Shows multiple valid interpretations\n* **Likelihood-based**: Ranges with uncertainty, not binary labels\n* **Transparent assumptions**: Makes boundaries and definitions explicit\n* **Version history**: Shows how understanding evolves\n* **Contradiction search**: Actively seeks opposing evidence\n* **Federated**: No single authority controls truth\n== 5. How do you prevent manipulation or coordinated misinformation campaigns? ==\n**Quality Gates**:\n* Automated checks before AI-generated content publishes\n* Source quality verification\n* Mandatory contradiction search\n* Bubble detection for coordinated campaigns\n**Audit System**:\n* Stratified sampling catches manipulation patterns\n* UCM Administrators validate AI research quality via sampling audits\n* Failed audits trigger immediate review\n**Transparency**:\n* All reasoning chains are visible\n* Evidence sources are traceable\n* AKEL involvement clearly labeled\n* Version history preserved\n**Moderation**:\n* Moderators handle abuse, spam, coordinated manipulation\n* Content can be flagged by community\n* Audit trail maintained even if content hidden\n**Federation**:\n* Multiple nodes with independent governance\n* No single point of control\n* Cross-node contradiction detection\n* Trust model prevents malicious node influence\n== 6. What happens when new evidence contradicts an existing verdict? ==\nFactHarbor is designed for evolving knowledge:\n**Automatic Re-evaluation**:\n1. New evidence arrives\n2. System detects affected scenarios and verdicts\n3. AKEL proposes updated verdicts\n4. Contributors/experts validate\n5. New verdict version published\n6. Old versions remain accessible\n**Version History**:\n* Every verdict has complete history\n* Users can see \"as of date X, what did we know?\"\n* Timeline shows how understanding evolved\n**Transparent Updates**:\n* Reason for re-evaluation documented\n* New evidence clearly linked\n* Changes explained, not hidden\n**User Notifications**:\n* Users following claims are notified of updates\n* Can compare old vs new verdicts\n* Can see which evidence changed conclusions\n== 7. Who can submit claims to FactHarbor? ==\n**Registered users** (free account required):\n* Submit URLs/text for analysis (rate-limited  LLM and web search usage is not free)\n* Flag quality issues\n* View own submission history\n* System deduplicates and normalizes submitted content\n**Readers (guests)** can browse, search, and view all published analyses without login.\n**Workflow**:\n1. Registered user submits text or URL\n2. AKEL extracts claims\n3. Checks for existing duplicates\n4. Normalizes claim text\n5. Assigns risk tier (centrality, harm potential)\n6. Extracts AtomicClaims, researches evidence, clusters ClaimAssessmentBoundaries\n7. Runs quality gates (Gate 1: claim validation, Gate 4: verdict confidence)\n8. Publishes as AI-Generated if passes\n== 8. What are \"risk tiers\" and why do they matter? ==\nRisk tiers determine review requirements and publication workflow:\n**Tier A (High Risk)**:\n* **Domains**: Medical, legal, elections, safety, security, major financial\n* **Publication**: AI can publish with warnings, expert review required for \"AKEL-Generated\" status\n* **Audit rate**: Recommendation 30-50%\n* **Why**: Potential for significant harm if wrong\n**Tier B (Medium Risk)**:\n* **Domains**: Complex policy, science causality, contested issues\n* **Publication**: AI can publish immediately with clear labeling\n* **Audit rate**: Recommendation 10-20%\n* **Why**: Nuanced but lower immediate harm risk\n**Tier C (Low Risk)**:\n* **Domains**: Definitions, established facts, historical data\n* **Publication**: AI publication default\n* **Audit rate**: Recommendation 5-10%\n* **Why**: Well-established, low controversy\n**Assignment**:\n* AKEL suggests tier based on domain, keywords, impact\n* UCM Administrators can adjust risk tier rules via UCM config\n* Risk tiers reviewed based on audit outcomes\n== 9. How does federation work and why is it important? ==\n**Federation Model**:\n* Multiple independent FactHarbor nodes\n* Each node has own database, AKEL, governance\n* Nodes exchange claims, scenarios, evidence, verdicts\n* No central authority\n**Why Federation Matters**:\n* **Resilience**: No single point of failure or censorship\n* **Autonomy**: Communities govern themselves\n* **Scalability**: Add nodes to handle more users\n* **Specialization**: Domain-focused nodes (health, energy, etc.)\n* **Trust diversity**: Multiple perspectives, not single truth source\n**How Nodes Exchange Data**:\n1. Local node creates versions\n2. Builds signed bundle\n3. Pushes to trusted neighbor nodes\n4. Remote nodes validate signatures and lineage\n5. Accept or branch versions\n6. Local re-evaluation if needed\n**Trust Model**:\n* Trusted nodes  auto-import\n* Neutral nodes  import with review\n* Untrusted nodes  manual only\n== 10. Can experts disagree in FactHarbor? ==\n**Yes - and that's a feature, not a bug**:\n**Multiple Evidence Perspectives**:\n* AKEL clusters evidence into ClaimAssessmentBoundaries based on EvidenceScope compatibility\n* Each ClaimAssessmentBoundary may yield a different per-boundary verdict\n* Users see *why* assessments differ (different methodologies, temporal scopes, evidence weighting)\n**Multiple ClaimAssessmentBoundaries**:\n* Same claim, different evidence clusters (evidence-emergent, not predefined)\n* All boundary verdicts visible with methodology\n* No forced consensus when evidence genuinely conflicts\n**Transparency**:\n* ClaimAssessmentBoundary reasoning documented (via Advocate/Challenger/Reconciliation debate)\n* Evidence sources and methodology stated explicitly\n* Evidence chains traceable\n* Users can evaluate how different evidence clusters reach different conclusions\n**Federation**:\n* Different nodes can have different expert conclusions\n* Cross-node branching allowed\n* Users can see how conclusions vary across nodes\n== 11. What prevents AI from hallucinating or making up facts? ==\n**Multiple Safeguards**:\n**Quality Gate 4: Structural Integrity**:\n* Fact-checking against sources\n* No hallucinations allowed\n* Logic chain must be valid and traceable\n* References must be accessible and verifiable\n**Evidence Requirements**:\n* Primary sources required\n* Citations must be complete\n* Sources must be accessible\n* Reliability scored\n**Audit System**:\n* Human auditors check AI-generated content\n* Hallucinations caught and fed back into training\n* Patterns of errors trigger system improvements\n**Transparency**:\n* All reasoning chains visible\n* Sources linked\n* Users can verify claims against sources\n* AKEL outputs clearly labeled\n**Human Oversight**:\n* Tier A marked as highest risk\n* Audit sampling catches errors\n* Community can flag issues\n== 12. How does FactHarbor make money / is it sustainable? ==\n[ToDo: Business model and sustainability to be defined]\nPotential models under consideration:\n* Non-profit foundation with grants and donations\n* Institutional subscriptions (universities, research organizations, media)\n* API access for third-party integrations\n* Premium features for power users\n* Federated node hosting services\nCore principle: **Public benefit** mission takes priority over profit.\n== 13. Related Pages ==\n* [[Requirements (Roles)>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[AKEL (AI Knowledge Extraction Layer)>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]\n* [[Automation>>FactHarbor.Product Development.Specification.Automation.WebHome]]\n* [[Federation & Decentralization>>FactHarbor.Product Development.Specification.Federation & Decentralization.WebHome]]\n* [[Mission & Purpose>>FactHarbor.Organisation.Strategy.Core Problems FactHarbor Solves.WebHome]]\n== 20. Glossary / Key Terms ==\n\n=== POC (Proof of Concept) ===\nFeature-complete at v2.10.2. Core AKEL pipeline operational with quality gates, multi-provider LLM, and source reliability. Pending baseline quality test to declare complete.\n\n=== Alpha ===\nQuality measurement + optimization phase. Baseline testing, caching, quality improvements based on empirical data.\n\n=== Beta ===\nUser testing and production readiness. External testers, PostgreSQL migration, user accounts, corrections system.\n\n=== V1.0 ===\nPublic launch with IFCN compliance, ClaimReview schema, security audit, and public quality metrics.", "Product Development.Specification.Federation & Decentralization.WebHome": "= Federation & Decentralization =\n**Status**:  **Planned for V2.0+** (not in V1.0 scope)\nFactHarbor is designed to eventually support federation, but this feature is **deferred until core product is proven and user demand exists**.\n== V1.0 Scope (Current) ==\n**Single-node deployment**:\n* Full-featured standalone instance\n* Read replicas for scaling if needed\n* All features available without federation\n* Focus on core product quality\n**Why defer federation**:\n* Adds massive complexity (sync, conflicts, identity, governance)\n* Not needed for first 10,000 users\n* Core product must be proven first\n* Build when users explicitly request it\n== V2.0+ Federation (Future Vision) ==\n**Implement federation when**:\n*  10,000+ users on single node\n*  Users explicitly request decentralization\n*  Core product is stable and mature\n*  Geographic distribution becomes necessary\n=== Federation Features (Future) ===\n**Independent FactHarbor Instances**:\n* Each node operates autonomously\n* Local governance and moderation\n* Regional data sovereignty\n* Domain specialization possible\n**Claim Synchronization**:\n* Claims propagate between trusted nodes\n* Conflict resolution protocols\n* Distributed consensus on verdicts\n* Maintain source attribution\n**Federated Identity**:\n* Users recognized across nodes\n* Reputation portable (with consent)\n* Single sign-on capabilities\n* Privacy-preserving authentication\n**Decentralized Governance**:\n* Each node sets own policies\n* Collaborative standards development\n* Inter-node arbitration for disputes\n* Federation-level oversight (optional)\n=== Technical Architecture (Future) ===\n**Sync Protocol**:\n* ActivityPub-inspired messaging\n* Claim deltas and updates\n* Source verification across nodes\n* Bandwidth-efficient synchronization\n**Identity Management**:\n* Decentralized identifiers (DIDs)\n* Verifiable credentials\n* Public key infrastructure\n* Privacy-preserving authentication\n**Data Sovereignty**:\n* Each node controls own data\n* Cross-node queries (opt-in)\n* GDPR compliance per jurisdiction\n* Right to be forgotten respected\n=== Benefits of Federation (Future) ===\n**Resilience**:\n* No single point of failure\n* Censorship resistance\n* Geographic redundancy\n* Community ownership\n**Autonomy**:\n* Local governance\n* Regional moderation policies\n* Cultural sensitivity\n* Domain specialization\n**Scalability**:\n* Horizontal growth through new nodes\n* Regional distribution reduces latency\n* Load distribution across federation\n* Independent scaling per node\n**Trust**:\n* Decentralized verification\n* Multiple independent sources\n* Transparent provenance\n* Community validation\n=== Challenges to Address (Future) ===\n**Technical Complexity**:\n* Synchronization protocols\n* Conflict resolution\n* Network reliability\n* Performance at scale\n**Governance Complexity**:\n* Inter-node standards\n* Dispute resolution\n* Quality consistency\n* Abuse handling\n**User Experience**:\n* Node discovery\n* Identity management\n* Seamless cross-node interaction\n* Performance transparency\n== Why Not V1.0? ==\n**Reality check**:\n* Most successful platforms start centralized (Wikipedia, Reddit, GitHub)\n* Federation can be added later (see: Mastodon, Matrix)\n* Core product quality matters more than architecture philosophy\n* Users don't care about federation until they need it\n**Build federation when**:\n* Users say \"I want to run my own instance\"\n* Censorship becomes a real problem\n* Geographic distribution is required\n* Community governance demands it\nUntil then: **Focus on making the core product excellent** \n== Related Pages ==\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] - Current V1.0 architecture\n* [[Design Decisions>>FactHarbor.Product Development.Specification.Design-Decisions]] - Why we defer complexity\n* [[When to Add Complexity>>FactHarbor.Product Development.DevOps.Guidelines.When to Add Complexity.WebHome]] - Federation triggers\n== Historical Note ==\nEarlier specification versions included detailed federation specifications. These have been moved to future vision documents. The federation architecture is well-designed and ready to implement when the time comes  not before V2.0+.\n{{include reference=\"FactHarbor.Product Development.Diagrams.Federation Architecture.WebHome\"/}}", "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude.WebHome": "= FactHarbor Analysebericht =\n\n== Chemtrails-Verschwrungstheorie: Wissenschaftliche Bewertung ==\n\n**Analysedatum**: 15. Dezember 2025\n**Quelle**: Chemtrails-Verschwrungstheorie (Behauptung ber versprhte Chemikalien)\n**FactHarbor Version**: 0.9.18 POC\n**Analysetyp**: Umfassend\n**Sprache**: Deutsch\n\n----\n\n== **WICHTIGER HINWEIS** ==\n\nDiese Analyse untersucht eine**Verschwrungstheorie ohne wissenschaftliche Grundlage**. Das Ergebnis vorweg:**Es gibt keine wissenschaftlichen Beweise fr \"Chemtrails\"**. Die Analyse dokumentiert dennoch ausfhrlich die Behauptungen und die berwltigende wissenschaftliche Evidenz dagegen, um Transparenz zu schaffen und Fehlinformationen entgegenzuwirken.\n\n----\n\n== Zusammenfassung ==\n\nDie Chemtrails-Verschwrungstheorie behauptet, dass Kondensstreifen von Flugzeugen in Wirklichkeit gezielt versprhte Chemikalien zur Bevlkerungskontrolle, Wettermanipulation oder Vergiftung seien. Die wissenschaftliche Evidenz ist**eindeutig und berwltigend**:\n\n* **99% Konfidenz**, dass Chemtrails NICHT existieren\n* **98,7% der Atmosphrenwissenschaftler**fanden keine Hinweise (Caldeira-Studie 2016)\n* **Alle wissenschaftlichen Institutionen**(Umweltbundesamt, DLR, NASA, WHO, etc.) widersprechen der Theorie\n* **Vollstndige Erklrung**durch normale Physik und Chemie mglich\n* **Risikoklasse A**(Fehlinformation mit Gesundheitsrisiko)\n\n----\n\n== Analysebersicht ==\n\n|=Kennzahl|=Anzahl/Status\n|**Identifizierte Behauptungen**|6\n|**Generierte Szenarien**|12 (2 pro Behauptung)\n|**Evidenzquellen**|15\n|**Urteile erstellt**|12\n|**Widerspruchssuche**| Abgeschlossen (Pro-Chemtrail-Quellen geprft)\n|**Qualittsgates**| Alle bestanden (12/12)\n|**Risikoklasse A Behauptungen**|6 (Hoch - Fehlinformation/Gesundheit)\n|**Wissenschaftlicher Konsens**|98,7% GEGEN Existenz\n\n----\n\n== Behauptungen-Analyse ==\n\n=== **BEHAUPTUNG 1**: Ungewhnliche Kondensstreifen sind Chemtrails ===\n\n>//\"Kondensstreifen, die ungewhnlich geformt sind oder sich besonders lange in der Luft halten, sind in Wirklichkeit Chemtrails\"//\n\n**Domne**: Naturwissenschaft & Physik\n**Risikoklasse**: **A**(Hoch - Wissenschaftliche Fehlinformation)\n**Behauptungstyp**: Wrtlich, Empirisch, Verschwrungstheorie\n\n==== Urteil:**Falsch (hchste Sicherheit)** ====\n\n**Konfidenz**: 99% (Bereich: 97% - 99,9%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **Caldeira-Studie 2016**: 76 von 77 Atmosphrenwissenschaftlern (98,7%) fanden KEINE Hinweise\n* **Umweltbundesamt**: \"Keinerlei wissenschaftliche Belege\"\n* **DLR**: \"Phnomene nicht bekannt\" trotz jahrzehntelanger Luftverkehrsforschung\n* **WHO**: Keine Kenntnisse zur Existenz\n* **US Air Force, NASA, Greenpeace**: Alle widersprechen\n* **Vollstndige physikalische Erklrung**: Kondensstreifen durch Wasserdampf-Kondensation\n\n**Wissenschaftliche Erklrung fr \"ungewhnliche\" Kondensstreifen**:\n\n**~1. Langlebigkeit der Streifen**:\n\n{{{Trockene Luft  Kondensstreifen lsen sich schnell auf (Minuten)\nFeuchte Luft  Kondensstreifen bleiben lange (Stunden bis Tage)\nSehr feuchte Luft  Ausbreitung zu Zirruswolken (Contrail-Cirrus)\n}}}\n\n**Faktoren**:\n\n* **Relative Luftfeuchtigkeit**: Bei >60% Feuchte bleiben Streifen lnger\n* **Temperatur**: Unter -40C optimale Kondensation\n* **Luftdruck**: Hhenabhngige Eigenschaften\n* **Triebwerks-Effizienz**: Moderne Triebwerke  kltere Abgase  mehr Wasserdampf\n\n**2. Unterschiedliche Formen**:\n\n* **Gerade Linien**: Normaler Flug in konstanter Hhe\n* **Kurven/Kreise**: Warteschleifen, Umleitung\n* **Unterbrechungen**: Wechsel zwischen Luftschichten mit unterschiedlicher Feuchte\n* **Gittermuster**: Kreuzende Flugrouten (Luftverkehrskorridor)\n* **Breite Streifen**: Wind verteilt Eiskristalle\n* **\"Seltsame\" Muster**: Atmosphrische Windscherung\n\n**3. Nur bei manchen Flugzeugen**:\n\n* **Flughhe**: Unter 8 km meist keine Kondensstreifen (zu warm)\n* **Triebwerkstyp**: Moderne Turbinen  mehr Kondensstreifen\n* **Atmosphrische Bedingungen**: Knnen sich auf 100m Hhendifferenz ndern\n\n**Begrndung fr \"FALSCH\"**:\n\n1. (((\n**Caldeira-Studie 2016 (peer-reviewed)**:\n\n* 77 weltweit fhrende Atmosphrenwissenschaftler und Geochemiker befragt\n* 76 (98,7%) fanden KEINE Hinweise auf \"Secret Large Scale Atmospheric Program\" (SLAP)\n* Alle vorgelegten \"Beweise\" (Fotos, Messungen) wurden als normal erklrt\n* **Zitat Caldeira**: \"Kondensstreifen lassen sich problemlos mit konventioneller Chemie und Physik erklren\"\n)))\n1. (((\n**Umweltbundesamt (offiziell, 2011)**:\n\n>\"Fr ein Einbringen von Aluminium- oder Bariumverbindungen in die Atmosphre und die Bildung so genannter Chemtrails gibt es keinerlei wissenschaftliche Belege.\"\n)))\n1. (((\n**Deutsches Zentrum fr Luft- und Raumfahrt (DLR)**:\n\n* Untersucht seit Jahrzehnten Luftverkehrsemissionen\n* \"Falls es Chemtrails gbe, mssten beim DLR Informationen vorliegen\"\n* Messungen enthalten**keinerlei Hinweise**\n)))\n1. (((\n**Physikalische Unmglichkeit der Theorie**:\n\n* **Logistische Unmglichkeit**: Sprhen in 10-13 km Hhe  Substanzen verdnnen sich massiv, erreichen Boden nicht konzentriert\n* **Fehlende chemische Beweise**: Keine anormalen Konzentrationen von Aluminium, Barium oder anderen Chemikalien gefunden\n* **Tausende Mitwisser**: Piloten, Flughafenmitarbeiter, Tanklaster-Fahrer, Wissenschaftler  Geheimhaltung praktisch unmglich\n)))\n1. (((\n**Zunahme der Kondensstreifen erklrbar**:\n\n* Flugverkehr seit 1980er Jahren verfnffacht (allein in Deutschland)\n* ber 2 Millionen Starts/Landungen pro Jahr in Deutschland\n* Moderne Triebwerke: Effizientere Verbrennung  mehr Wasserdampf  mehr Kondensstreifen\n)))\n\n**Gegenargumente bercksichtigt**:\n\n*  \"Frher gab es nicht so viele Streifen\" **Erklrt durch massiv gestiegenen Flugverkehr**\n*  \"Manche Streifen bleiben lnger\" **Erklrt durch Luftfeuchtigkeit und Wetterbedingungen**\n*  \"Kreuz-und-Quer-Muster\" **Erklrt durch Flugrouten (Luftkorridore kreuz sich)**\n*  \"Nur manche Flugzeuge\" **Erklrt durch Flughhe und Triebwerkstyp**\n\n**Evidenzquellen**:\n\n* Caldeira et al. 2016, Environmental Research Letters (Hchste Zuverlssigkeit, Peer-reviewed)\n* Umweltbundesamt Deutschland (Hchste Zuverlssigkeit, offiziell)\n* DLR - Deutsches Zentrum fr Luft- und Raumfahrt (Hchste Zuverlssigkeit)\n* WHO (Hchste Zuverlssigkeit)\n* Quarks (WDR) (Hohe Zuverlssigkeit, wissenschaftlich)\n\n----\n\n=== **BEHAUPTUNG 2**: Versprhung im Auftrag der \"Elite\" ===\n\n>//\"Versprht werden diese Gifte laut Theorie im Auftrag der 'Elite'  also beispielsweise Geschftsleute, Angela Merkel oder die Queen\"//\n\n**Domne**: Verschwrungstheorie & Politik\n**Risikoklasse**: **A**(Hoch - Verschwrungsmythos)\n**Behauptungstyp**: Spekulativ, Verschwrungstheorie\n\n==== Urteil:**Falsch (keine Beweise)** ====\n\n**Konfidenz**: 99% (Bereich: 97% - 99,9%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **Null Beweise**fr geheimes Sprhprogramm\n* **Keine Dokumente**, keine Whistleblower, keine Papierspur\n* **Logistische Unmglichkeit**: Tausende Mitwisser ntig, Geheimhaltung unmglich\n* **Verteidigungsministerien widersprechen**: Deutschland, USA, UK\n* **US Air Force offizielles Statement**: \"Keine Wettermanipulation, nicht geplant\"\n* **Greenpeace 2004**: \"Spekulation als Verschwrungstheorie bewertet\"\n\n**Begrndung fr \"FALSCH\"**:\n\n1. (((\n**Fehlende Beweise trotz intensiver Suche**:\n\n* Keine Dokumente ber Sprhprogramme\n* Keine glaubwrdigen Whistleblower\n* Keine Rechnungen, Vertrge, Budgetposten\n* Keine Bestellungen fr Chemikalien in bentigten Mengen\n)))\n1. (((\n**Logistische Unmglichkeit**:\n\n{{{Fr globales Sprhprogramm ntig:\n- Tausende Flugzeuge mit Spezialausrstung\n- Zehntausende Tonnen Chemikalien pro Monat\n- Tausende eingeweihte Piloten, Techniker, Wissenschaftler\n- Hunderte Produktionssttten, Logistikunternehmen\n- Koordination ber Dutzende Lnder und Regierungen\n\n Geheimhaltung praktisch UNMGLICH\n}}}\n)))\n1. (((\n**\"Elite\" wrde sich selbst schaden**:\n\n* Chemikalien wrden auch \"Elite\" und deren Familien treffen\n* Keine Mglichkeit, sich zu schtzen (Chemikalien in Atmosphre)\n* Widersprchliche \"Motive\" (Bevlkerungsreduktion vs. Klimaschutz)\n)))\n1. (((\n**Offizielle Statements aller relevanten Institutionen**:\n\n* **Deutsches Verteidigungsministerium**: Keine entsprechenden Projekte\n* **US Air Force**: \"Wir manipulieren das Wetter nicht und planen es nicht\"\n* **European HQ US Air Force**: Keine Programme\n* **NATO**: Keine Besttigung\n* **Greenpeace**: Bewertung als Verschwrungstheorie\n)))\n1. (((\n**Verschwrungstheoretische Muster**:\n\n* **Vage \"Elite\"**: Keine konkreten Namen, keine berprfbaren Akteure\n* **Allmchtige Verschwrer**: Gleichzeitig perfekt geheim UND ffentlich sichtbar\n* **Fehlende Falsifizierbarkeit**: Jeder Gegenbeweis wird als \"Teil der Verschwrung\" gedeutet\n* **Zirkelschluss**: \"Keine Beweise = Beweis fr Vertuschung\"\n)))\n\n**Psychologische Mechanismen**:\n\n* **Kontrollillusion**: Komplexe Welt durch einfache Verschwrung \"erklrt\"\n* **Ohnmachtsgefhle**: \"Elite\" als Sndenbock fr Probleme\n* **Selektive Wahrnehmung**: Besttigungsfehler (confirmation bias)\n* **Misstrauen gegenber Autoritten**: Verstndlich, aber fhrt zu Irrationalitt\n\n**Evidenzquellen**:\n\n* Bundeszentrale fr politische Bildung (Hchste Zuverlssigkeit)\n* Wikipedia Chemtrails (Hohe Zuverlssigkeit, gut belegt)\n* US Air Force Statements (Hchste Zuverlssigkeit, offiziell)\n* Greenpeace Magazin 2004 (Hohe Zuverlssigkeit)\n* Spektrum der Wissenschaft Podcast (Hohe Zuverlssigkeit)\n\n----\n\n=== **BEHAUPTUNG 3**: Zweck ist Klimawandel-Bekmpfung ===\n\n>//\"Die versprhten Gifte sollten, so die Theorie weiter, eigentlich den Klimawandel bremsen, htten aber ungewollte Folgen\"//\n\n**Domne**: Wissenschaft & Geoengineering\n**Risikoklasse**: **A**(Hoch - Vermischung von Fakten und Fiktion)\n**Behauptungstyp**: Spekulativ, Missverstndnis\n\n==== Urteil:**Falsch (Verwechslung mit Geoengineering-Diskussion)** ====\n\n**Konfidenz**: 95% (Bereich: 90% - 98%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **Geoengineering ist DISKUSSION, nicht Realitt**: Wissenschaftliche Debatte ber potenzielle Zukunftstechnologien\n* **KEINE Implementierung**: Geoengineering wird NICHT durchgefhrt\n* **Umweltbundesamt**: \"Kontraproduktiv\" - Kondensstreifen/Zirruswolken**erwrmen**das Klima\n* **Klimapolitik-Fokus**: Vermeidung von Contrail-Cirrus (nicht Erzeugung!)\n* **Verwechslung**: Chemtrail-Anhnger verwechseln wissenschaftliche Diskussion mit Realitt\n\n**Wichtige Klarstellung ber Geoengineering**:\n\n**Was Geoengineering IST**:\n\n* Wissenschaftliche**Diskussion**ber mgliche zuknftige Technologien\n* **Theoretische**Anstze zur Klimabeeinflussung\n* **Keine aktuelle Implementierung**(Stand 2025)\n* Intensive Debatte ber Risiken, Ethik, Nebenwirkungen\n* Mgliche Methoden (nur theoretisch):\n** Sulfat-Aerosole in Stratosphre (Solar Radiation Management)\n** Aufhellung von Wolken ber Ozeanen\n** CO2-Entfernung aus Atmosphre (Carbon Capture)\n\n**Was Geoengineering NICHT ist**:\n\n*  KEIN geheimes Programm\n*  KEINE aktuelle Durchfhrung\n*  KEINE \"Chemtrails\" (vllig unterschiedliche Konzepte)\n*  KEINE Vergiftung der Bevlkerung\n\n**Begrndung fr \"FALSCH\"**:\n\n1. (((\n**Kondensstreifen ERWRMEN das Klima (nicht khlen)**:\n\n* **Umweltbundesamt**: \"Kondensstreifen und Zirren erwrmen insgesamt das Klima\"\n* **Effekt**: Treibhauswirkung durch Contrail-Cirrus\n* **Klimapolitik**: Vermeidung von Kondensstreifen wird diskutiert (nicht Erzeugung!)\n* **Zitat UBA**: \"Es wre kontraproduktiv, mit Hilfe zustzlicher Zirren der Klimaerwrmung entgegenzuwirken\"\n)))\n1. (((\n**Geoengineering ist wissenschaftliche Diskussion**:\n\n* Forscher wie Ken Caldeira diskutieren**theoretisch**ber mgliche Zukunftstechnologien\n* **Zitat Caldeira**: \"Als Diskussion ber Geoengineering begann, wurde das von Chemtrail-Anhngern missverstanden\"\n* Keine praktische Durchfhrung (wrde internationale Vertrge, ffentliche Debatten erfordern)\n)))\n1. (((\n**Verwechslung durch Chemtrail-Anhnger**:\n\n* **Missverstndnis**: Wissenschaftliche Papers ber Geoengineering als \"Beweis\" fr Chemtrails interpretiert\n* **Tatsache**: Diskussion ber theoretische Zukunftstechnologien  geheime Implementierung\n* **\"Weather as Force Multiplier: Owning the Weather 2025\"**: US Air Force Strategiepapier aus 1990ern -**reine Spekulation**, keine tatschlichen Plne\n)))\n1. (((\n**Wissenschaftler gegen Geoengineering-Implementierung**:\n\n* Viele Forscher warnen vor Risiken\n* Caldeira selbst: \"Gefhrlich und vielleicht tricht\"\n* Komplexes Klimasystem: Unbekannte Nebenwirkungen\n* Ethische und juristische Probleme\n)))\n\n**Warum die Verwechslung entstand**:\n\n{{{1990er: US Air Force Strategiepapier ber theoretische Wettermanipulation\n2000er: Wissenschaftler diskutieren Geoengineering als letztes Mittel\n Chemtrail-Anhnger interpretieren: \"Beweis fr geheimes Programm!\"\nRealitt: Nur wissenschaftliche Diskussion, keine Implementierung\n}}}\n\n**Evidenzquellen**:\n\n* Umweltbundesamt Chemtrails-Bericht (Hchste Zuverlssigkeit, offiziell)\n* Ken Caldeira Statements (Hohe Zuverlssigkeit, Primrquelle)\n* Klimaschutz-Portal Luftfahrt (Hohe Zuverlssigkeit)\n* Vice Interview mit Caldeira (Mittlere Zuverlssigkeit)\n\n----\n\n=== **BEHAUPTUNG 4**: Gedankenkontrolle mglich ===\n\n>//\"Teils heit es auch, sie seien in der Lage, die Gedanken der Menschen zu steuern\"//\n\n**Domne**: Pseudowissenschaft\n**Risikoklasse**: **A**(Hoch - Gefhrliche Fehlinformation)\n**Behauptungstyp**: Spekulativ, Pseudowissenschaftlich\n\n==== Urteil:**Falsch (wissenschaftlich unmglich)** ====\n\n**Konfidenz**: 99,9% (Bereich: 99% - 100%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **Physikalisch unmglich**: Keine Chemikalie kann aus 10 km Hhe Gedanken kontrollieren\n* **Neurowissenschaftlich absurd**: Gedankenkontrolle durch Aerosole existiert nicht\n* **Null wissenschaftliche Basis**: Keine Forschung, keine Technologie, keine Mechanismen\n* **Science-Fiction**: Basiert auf Aluhut-Mythos aus Science-Fiction-Geschichte\n\n**Begrndung fr \"FALSCH\"**:\n\n1. (((\n**Wissenschaftliche Unmglichkeit**:\n\n* **Keine Chemikalie bekannt**, die aus atmosphrischer Verteilung Gedanken kontrolliert\n* **Neurowissenschaft**: Gehirn ist komplex geschtzt (Blut-Hirn-Schranke)\n* **Spezifische Wirkung unmglich**: Chemikalien aus 10 km Hhe verdnnen sich massiv\n* **Kein Wirkmechanismus**: Wie sollte das funktionieren?\n)))\n1. (((\n**Verdnnung macht es unmglich**:\n\n{{{Sprhhhe: 10-13 km\nAtmosphrisches Volumen: Gigantisch\nVerdnnungsfaktor: Millionenfach\nKonzentration am Boden: Unmessbar niedrig\n\n KEINE wirksame Dosis mglich\n}}}\n)))\n1. (((\n**\"Aluhut\"-Ursprung**:\n\n* **Bundeszentrale fr politische Bildung**: Mythos geht auf Science-Fiction-Geschichte zurck\n* In Geschichte: Protagonist trgt Aluhut zur Abwehr von Telepathie\n* **Ironie**: Symbol fr Verschwrungstheoretiker geworden\n)))\n1. (((\n**Wenn Gedankenkontrolle mglich wre**:\n\n* Warum gibt es dann Widerstand gegen Regierungen?\n* Warum demonstrieren Chemtrail-Anhnger ffentlich?\n* Warum funktioniert die angebliche Kontrolle nicht bei ihnen?\n* **Logischer Widerspruch**: Theorie widerlegt sich selbst\n)))\n\n**Psychologische Erklrung der Behauptung**:\n\n* **Ohnmachtsgefhle**: Bedrfnis, unerklrliche Phnomene (eigene Gedanken, Gesellschaft) zu erklren\n* **Externalisation**: Eigene Probleme ueren Krften zuschreiben\n* **Paranoia**: Gefhl, verfolgt oder manipuliert zu werden\n* **Kontrollillusion**: Paradox - Glaube an Kontrolle hilft, Kontrollverlust zu kompensieren\n\n**Evidenzquellen**:\n\n* Bundeszentrale fr politische Bildung (Hchste Zuverlssigkeit)\n* Neurowissenschaftliche Grundlagen (Hchste Zuverlssigkeit, etabliert)\n* Physik der Atmosphre (Hchste Zuverlssigkeit, etabliert)\n\n----\n\n=== **BEHAUPTUNG 5**: Langsame Vergiftung der Bevlkerung ===\n\n>//\"Oder dass sie die Menschen langsam vergiften\"//\n\n**Domne**: Gesundheit & Toxikologie\n**Risikoklasse**: **A**(Hoch - Gesundheits-Fehlinformation)\n**Behauptungstyp**: Spekulativ, Gesundheitsgefhrdend\n\n==== Urteil:**Falsch (keine Evidenz, widerlegte These)** ====\n\n**Konfidenz**: 98% (Bereich: 95% - 99%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **Keine erhhten Giftstoff-Konzentrationen**: Messungen zeigen normale Werte\n* **Keine epidemiologischen Aufflligkeiten**: Keine Krankheitshufung unter Flugschneisen\n* **Caldeira-Studie**: 76 von 77 Geochemikern fanden keine anormalen Schwermetall-Konzentrationen\n* **Analysierte \"Beweise\" widerlegt**: Bariumwerte erklrbar durch Feuerwerke, Industrie\n* **Reale Gesundheitsrisiken ignoriert**: Fokus auf fiktive Chemtrails statt reale Luftverschmutzung\n\n**Begrndung fr \"FALSCH\"**:\n\n1. (((\n**Caldeira-Studie 2016 - Materialproben analysiert**:\n\n* Chemtrail-Anhnger legten \"Beweise\" vor: Laboranalysen von Tmpelwasser, Schnee, Luftfiltern\n* **76 von 77 Wissenschaftlern**: Gehalte an Schwermetallen durch naheliegende Ursachen erklrbar\n* **Barium**: Erhhte Werte erklrbar durch Feuerwerke, bengalische Fackeln, Industrieemissionen\n* **Aluminium**: Natrliche Bodenbestandteile, Baustaub\n* **KEINE anormalen Konzentrationen**gefunden\n)))\n1. (((\n**Keine epidemiologischen Aufflligkeiten**:\n\n* Wenn Chemtrails vergiften wrden: Krankheitshufung unter Flugschneisen\n* **Realitt**: Keine statistisch signifikanten Unterschiede\n* Gesundheitsdaten widersprechen der These\n* Menschen unter Hauptflugrouten nicht krnker als anderswo\n)))\n1. (((\n**Logische Widersprche**:\n\n{{{Wenn globale Vergiftung:\n- Warum steigt Lebenserwartung weltweit?\n- Warum keine eindeutigen Vergiftungssymptome?\n- Warum keine Toten direkt nach \"Sprhtagen\"?\n- Warum betrifft es \"Elite\" nicht?\n}}}\n)))\n1. (((\n**Gefhrliche Fehlinformation**:\n\n* **Vice Interview**: Frau lsst Kinder nicht mehr drauen spielen aus Angst vor Chemtrails\n* **Reale Gefahren ignoriert**: Feinstaub, Stickoxide, Ozon (nachgewiesene Gesundheitsrisiken)\n* **Fehlgeleitete Angst**: Energie fr fiktive Bedrohung statt reale Umweltprobleme\n)))\n1. (((\n**Was WIRKLICH die Gesundheit gefhrdet**:\n\n* **Luftverschmutzung**: Nachgewiesene Gesundheitsrisiken (Feinstaub, NO)\n* **Klimawandel**: Hitzewellen, Extremwetter, Krankheitsausbreitung\n* **Echte Umweltgifte**: Schwermetalle aus Industrie, Pestizide, Mikroplastik\n* **Kondensstreifen-Klimawirkung**: Treibhauseffekt (aber keine Vergiftung!)\n)))\n\n**Evidenzquellen**:\n\n* Caldeira et al. 2016 (Hchste Zuverlssigkeit, Peer-reviewed)\n* Scinexx Wissenschaftsportal (Hohe Zuverlssigkeit)\n* Vice Interview mit Caldeira (Mittlere Zuverlssigkeit, Primrquelle)\n* Epidemiologische Daten (Hchste Zuverlssigkeit, etabliert)\n\n----\n\n=== **BEHAUPTUNG 6**: USA besonders verdchtig ===\n\n>//\"Besonders oft werden auch die USA verdchtigt\"//\n\n**Domne**: Geopolitik & Verschwrungstheorie\n**Risikoklasse**: **A**(Hoch - Anti-amerikanische Propaganda)\n**Behauptungstyp**: Spekulativ, Verschwrungstheorie\n\n==== Urteil:**Unbegrndet (typisches Verschwrungsmuster)** ====\n\n**Konfidenz**: 97% (Bereich: 93% - 99%)\n\n**Hauptevidenz GEGEN die Behauptung**:\n\n* **US Air Force offizielle Stellungnahme**: \"Wir manipulieren das Wetter nicht und planen es nicht\"\n* **US Militr**: Kein Budget, keine Projekte, keine Programme\n* **NASA**: Widerspricht Chemtrail-Theorie\n* **\"Weather as Force Multiplier 2025\"**: Nur Strategiepapier, keine tatschlichen Plne oder Fhigkeiten\n* **Weltweites Phnomen**: Kondensstreifen berall, nicht nur ber USA\n\n**Begrndung fr \"UNBEGRNDET\"**:\n\n1. (((\n**US Air Force offizielle Position**:\n\n* **Informationspapier zu Kondensstreifen**verffentlicht\n* **Klarstellung**: \"Wettermanipulationen werden nicht vorgenommen\"\n* **Keine Plne**: \"Wir planen auch nicht, damit zu beginnen\"\n* **Hoax-Bewertung**: Air Force sieht Chemtrails als Hoax (Falschmeldung)\n)))\n1. (((\n**\"Weather as Force Multiplier\" - Missverstndnis**:\n\n* **1996**: US Air Force Strategiepapier ber theoretische Zukunftsszenarien (Zeitraum: bis 2025)\n* **Inhalt**: Spekulation ber mgliche militrische Nutzung von Wetterbeeinflussung\n* **Realitt**: Keine tatschlichen Plne, keine Fhigkeiten, keine Implementierung\n* **Air Force Klarstellung**: Papier war spekulativ, nicht operativ\n)))\n1. (((\n**Verschwrungstheoretisches Muster**:\n\n* **Anti-Amerikanismus**: USA oft als Sndenbock in Verschwrungstheorien\n* **Mchtige Feindbilder**: \"USA als Weltverschwrer\"\n* **Selektive Wahrnehmung**: Kondensstreifen weltweit, aber nur USA verdchtigt\n* **Politische Instrumentalisierung**: Verschwrungstheorien oft mit anti-westlicher Propaganda verknpft\n)))\n1. (((\n**Kondensstreifen sind weltweites Phnomen**:\n\n{{{Kondensstreifen gibt es ber:\n- Europa (Deutschland, Frankreich, UK, etc.)\n- Asien (China, Japan, Indien, etc.)\n- Australien\n- Sdamerika\n- Afrika\n- Antarktis (Forschungsflge)\n\n KEIN speziell amerikanisches Phnomen\n}}}\n)))\n1. (((\n**Warum USA verdchtigt werden**:\n\n* **Militrische Strke**: USA haben grtes Militr  Verdacht naheliegend\n* **Historische Geheimprojekte**: MKUltra, Tuskegee-Experiment  Misstrauen begrndet\n* **Globale Prsenz**: US-Militr weltweit stationiert\n* **Kulturelle Faktoren**: Hollywood, Verschwrungstheorien in US-Popkultur\n* **Aber**: Historische Geheimprojekte  Beweis fr Chemtrails\n)))\n\n**Wichtige Klarstellung**:\n\n* **Berechtigtes Misstrauen**gegenber Regierungen ist demokratisch wichtig\n* **Historische Menschenversuche**(MKUltra, Tuskegee) waren real und verwerflich\n* **ABER**: Das macht Chemtrails nicht real\n* **ABER**: Man sollte echte Verbrechen nicht mit Fiktion gleichsetzen\n\n**Evidenzquellen**:\n\n* US Air Force Statements (Hchste Zuverlssigkeit, offiziell)\n* Wikipedia Chemtrails (Hohe Zuverlssigkeit, gut dokumentiert)\n* Bundeszentrale fr politische Bildung (Hchste Zuverlssigkeit)\n\n----\n\n== Evidenzbersicht ==\n\n=== Evidenz-Qualittsverteilung ===\n\n|=Zuverlssigkeit|=Anzahl|=Prozent\n|**Hchste**|10|67%\n|**Hoch**|4|27%\n|**Mittel**|1|6%\n|**Niedrig**|0|0%\n\n=== Evidenz nach Position ===\n\n|=Position|=Anzahl|=Anmerkungen\n|**GEGEN Chemtrails**(Wissenschaftlich)|15|Umweltbundesamt, DLR, Caldeira, WHO, etc.\n|**FR Chemtrails**(Verschwrungstheorien)|0|Keine wissenschaftlich belastbaren Quellen\n|**Neutral**(Erklrt Phnomen)|0|Alle wissenschaftlichen Quellen widerlegen\n\n=== Schlssel-Evidenzquellen ===\n\n==== 1.**Caldeira et al. 2016**(Hchste Zuverlssigkeit) ====\n\n* **Typ**: Peer-reviewed Studie in Environmental Research Letters\n* **Methode**: Umfrage unter 77 fhrenden Atmosphrenwissenschaftlern und Geochemikern\n* **Ergebnis**:**76 von 77 (98,7%)**fanden KEINE Hinweise auf \"Secret Large Scale Atmospheric Program\"\n* **Analysierte \"Beweise\"**: Fotos und Materialproben von Chemtrail-Websites  alle normal erklrt\n* **Bedeutung**: Erste und einzige Peer-reviewed Studie zum Thema\n* **Finanzierung**: Universitt Kalifornien, Carnegie Institution, Near Zero (teilweise Bill Gates)\n* **Wichtig**: Bill Gates hatte KEINE Rolle bei Studienauswahl (laut Caldeira)\n\n==== 2.**Umweltbundesamt Deutschland**(Hchste Zuverlssigkeit) ====\n\n* **Typ**: Offizielle Stellungnahme\n* **Jahr**: 2011, mehrfach aktualisiert\n* **Klarstellung**: \"Keinerlei wissenschaftliche Belege fr Chemtrails\"\n* **Expertise**: DLR untersucht seit Jahrzehnten Luftverkehrsemissionen\n* **Messungen**: Keine Hinweise auf chemische Manipula tion\n* **Kontext**: Kondensstreifen ERWRMEN Klima (kontraproduktiv fr Klimaschutz)\n\n==== 3.**Deutsches Zentrum fr Luft- und Raumfahrt (DLR)**(Hchste Zuverlssigkeit) ====\n\n* **Typ**: Forschungseinrichtung\n* **Expertise**: Jahrzehntelange Forschung zu Luftverkehrsemissionen\n* **Institut fr Physik der Atmosphre**: Spezialisiert auf Kondensstreifen\n* **Messungen**: Gas- und partikelfrmige Emissionen von Verkehrsflugzeugen\n* **Fazit**: \"Falls es Chemtrails gbe, mssten beim DLR Informationen vorliegen\"\n* **Realitt**: Keinerlei Hinweise\n\n==== 4.**WHO - Weltgesundheitsorganisation**(Hchste Zuverlssigkeit) ====\n\n* **Position**: Keine Kenntnisse zur Existenz von Chemtrails\n* **Bedeutung**: WHO wrde Gesundheitsrisiken dokumentieren\n\n==== 5.**US Air Force**(Hchste Zuverlssigkeit) ====\n\n* **Offizielle Stellungnahme**: Informationspapier zu Kondensstreifen\n* **Position**: \"Wir manipulieren das Wetter nicht und planen es nicht\"\n* **Hoax-Bewertung**: Chemtrails als Falschmeldung eingestuft\n\n==== 6.**Greenpeace Magazin 2004**(Hohe Zuverlssigkeit) ====\n\n* **Position**: Bewertung als Verschwrungstheorie\n* **Anschluss**: An Ergebnisse des Umweltbundesamtes\n* **Bedeutung**: Auch kritische Umweltorganisationen widersprechen\n\n==== 7.**Deutsche Flugsicherung und Deutscher Wetterdienst**(Hchste Zuverlssigkeit) ====\n\n* **Anfragen**: Keine Hinweise auf auffllige Flugbewegungen oder Kondensstreifen\n\n==== 8.**Quarks (WDR)**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Wissenschaftsjournalismus\n* **Artikel**: Umfassende Erklrung von Kondensstreifen\n* **Klarstellung**: Physikalische Erklrung fr alle \"verdchtigen\" Phnomene\n\n==== 9.**Spektrum der Wissenschaft**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Wissenschaftliches Magazin\n* **Podcast**: Multidimensionale Betrachtung aus wissenschaftlicher und gesellschaftlicher Perspektive\n* **Experten**: Flugzeug-Enthusiasten, ehemalige Verschwrungsglubige, Fachleute\n\n==== 10.**Bundeszentrale fr politische Bildung**(Hchste Zuverlssigkeit) ====\n\n* **Typ**: Staatliche Bildungseinrichtung\n* **Analyse**: Chemtrails als Verschwrungstheorie\n* **Warnung**: Verbindungen zu Rechtsextremismus und Antisemitismus\n\n==== 11.**Mimikama (Faktenchecker)**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Unabhngige Faktenchecker\n* **Position**: Entschiedene Widerlegung\n* **Argumentation**: Satirisch-kritisch, aber faktisch korrekt\n\n==== 12.**Klimaschutz-Portal Luftfahrt**(Hohe Zuverlssigkeit) ====\n\n* **Fokus**: Geoengineering-Diskussion\n* **Klarstellung**: Geoengineering ist Diskussion, nicht Realitt\n* **Ausschluss**: Manipulation zur Bevlkerungsbeeinflussung definitiv ausgeschlossen\n\n----\n\n== Widerspruchssuche ==\n\n**Pflicht-Widerspruchssuche abgeschlossen**\n\n=== Suchstrategie: ===\n\n1. Gesucht nach Pro-Chemtrail-Quellen\n1. Verschwrungstheoretische Websites geprft\n1. Angebliche \"Beweise\" analysiert\n1. Gegenargumente zu wissenschaftlicher Position gesucht\n1. Psychologische und soziologische Faktoren untersucht\n\n=== \"Argumente\" der Chemtrail-Anhnger: ===\n\n==== **Pro-Chemtrail \"Argumente\" und wissenschaftliche Widerlegung**: ====\n\n1. (((\n**\"Frher gab es nicht so viele Streifen\"**:\n\n* **Widerlegt**: Flugverkehr seit 1980er Jahren verfnffacht\n* **Daten**: ber 2 Millionen Starts/Landungen pro Jahr allein in Deutschland\n* **Technologie**: Moderne Triebwerke erzeugen mehr Kondensstreifen\n)))\n1. (((\n**\"Manche Streifen bleiben sehr lange\"**:\n\n* **Erklrt**: Abhngig von Luftfeuchtigkeit (ber 60%  lange Streifen)\n* **Contrail-Cirrus**: Kondensstreifen knnen zu echten Zirruswolken werden\n* **Physik**: Normale adiabatische Zustandsnderung\n)))\n1. (((\n**\"Nur bei manchen Flugzeugen sichtbar\"**:\n\n* **Erklrt**: Flughhe entscheidend (unter 8 km meist keine Streifen)\n* **Technologie**: Moderne Turbinen  effizientere Verbrennung  mehr Streifen\n* **Atmosphre**: Bedingungen knnen sich auf 100m Hhendifferenz ndern\n)))\n1. (((\n**\"Kreuz und Quer, keine geradlinigen Flugrouten\"**:\n\n* **Erklrt**: Luftkorridore kreuzen sich ber Ballungsrumen\n* **Warteschleifen**: Flugzeuge warten auf Landegenehmigung\n* **Wind**: Verteilt ltere Kondensstreifen, sieht aus wie \"Muster\"\n)))\n1. (((\n**\"Fotos vom Flugzeug-Inneren mit Tanks\"**:\n\n* **Erklrt**: Belastungstests (Wassertanks simulieren Passagiere)\n* **Lschflugzeuge**: Wassertanks fr Waldbrandbekmpfung\n* **Aerodynamik-Experimente**: Ballast-Tanks fr Tests\n)))\n1. (((\n**\"Erhhte Bariumwerte gemessen\"**:\n\n* **Caldeira-Studie**: 76 von 77 Wissenschaftlern erklrten Werte als normal\n* **Quellen**: Feuerwerke, bengalische Fackeln, Industrieemissionen\n* **Keine anormalen Konzentrationen**: Im erwartbaren Bereich\n)))\n1. (((\n**\"Geoengineering wird diskutiert\"**:\n\n* **Fakt**: Wissenschaftliche Diskussion existiert\n* **ABER**: Diskussion  Implementierung\n* **ABER**: Geoengineering  Chemtrails\n* **Realitt**: KEINE praktische Durchfhrung\n)))\n\n=== Echo-Kammern vermieden: ===\n\n Chemtrail-Behauptungen aktiv recherchiert\n Verschwrungstheoretische \"Argumente\" gesammelt\n Wissenschaftliche Widerlegungen dokumentiert\n Psychologische Mechanismen verstanden\n Soziologische Faktoren bercksichtigt\n Historischen Kontext einbezogen (MKUltra, etc.)\n\n=== Warum findet man KEINE wissenschaftlichen Pro-Chemtrail-Quellen? ===\n\n**Antwort**: Weil die Theorie wissenschaftlich unhaltbar ist. Keine Peer-reviewed Studie, kein Forscher, keine Institution untersttzt die Theorie.\n\n**Was man findet**:\n\n* Blogs, YouTube-Videos, Social Media Posts\n* Missinterpretationen wissenschaftlicher Papers\n* Aus dem Kontext gerissene Zitate\n* Fotos mit Fehlinterpretationen\n* Anekdoten ohne Beweiskraft\n\n**Was man NICHT findet**:\n\n* Peer-reviewed Studien\n* Universitre Forschung\n* Chemische Analyseergebnisse von akkreditierten Laboren\n* Offizielle Dokumente ber Sprhprogramme\n* Glaubwrdige Whistleblower mit verifizierbaren Fakten\n\n----\n\n== Risikobewertung ==\n\n=== Existenzrisiko von Chemtrails: **NULL** ===\n\n* **Wissenschaftlicher Konsens**: 98,7% der Experten widersprechen\n* **Physikalische Unmglichkeit**: Alle Behauptungen widerlegt\n* **Logistische Unmglichkeit**: Geheimhaltung praktisch unmglich\n* **Null Beweise**: Trotz intensiver Suche keine Belege\n\n=== Gesundheitsrisiko durch Chemtrails: **NULL** ===\n\n* **Keine toxikologischen Befunde**: Normale Schadstoffkonzentrationen\n* **Keine epidemiologischen Aufflligkeiten**: Keine Krankheitshufungen\n* **Messungen widersprechen**: Keine anormalen Werte\n\n=== Risiko durch die VERSCHWRUNGSTHEORIE: **HOCH** ===\n\n1. (((\n**Gesundheitsgefhrdung durch Angst**:\n\n* Menschen lassen Kinder nicht mehr raus (Vice-Beispiel)\n* Stress und Angst durch fiktive Bedrohung\n* Ignorieren realer Gesundheitsrisiken\n)))\n1. (((\n**Ablenkung von echten Problemen**:\n\n* **Reale Luftverschmutzung**ignoriert (Feinstaub, NO)\n* **Klimawandel**verharmlost\n* **Echte Umweltgifte**bersehen\n)))\n1. (((\n**Demokratiegefhrdung**:\n\n* **Misstrauen gegenber Wissenschaft**geschrt\n* **Misstrauen gegenber Institutionen**verstrkt\n* **Verschwrungsdenken**normalisiert\n)))\n1. (((\n**Verbindung zu Extremismus**:\n\n* **Bundeszentrale fr politische Bildung warnt**: Verbindungen zu Rechtsextremismus\n* **Antisemitische Muster**: \"Elite\", \"Weltverschwrung\"\n* **Radikalisierungspotenzial**: Einstieg in extremistisches Denken\n)))\n1. (((\n**Politische Instrumentalisierung**:\n\n* **USA 2025**: Louisiana verbietet \"Chemtrails\" (trotz Nicht-Existenz)\n* **Politiker nutzen Theorie**: Robert F. Kennedy Jr. untersttzt ffentlich\n* **Gefhrlicher Przedenzfall**: Gesetze gegen fiktive Bedrohungen\n)))\n\n----\n\n== Qualittssicherung ==\n\n=== Qualittsgate-Ergebnisse ===\n\n|=Qualittsgate|=Status|=Details\n|**Quellenqualitt**|**PASS**(12/12)|67% hchste Zuverlssigkeit\n|**Widerspruchssuche**|**PASS**(12/12)|Pro-Chemtrail-Argumente aktiv gesucht & widerlegt\n|**Unsicherheitsquantifizierung**|**PASS**(12/12)|Alle Unsicherheiten explizit (minimal bei diesem Thema)\n|**Strukturelle Integritt**|**PASS**(12/12)|Begrndungsketten vollstndig nachvollziehbar\n\n=== Konfidenz-Scores nach Urteil ===\n\n|=Urteil|=Behauptung|=Konfidenz|=Qualitt\n|FALSCH|Kondensstreifen sind Chemtrails|99%| Hchste\n|FALSCH|Versprhung durch \"Elite\"|99%| Hchste\n|FALSCH|Zweck Klimaschutz|95%| Sehr hoch\n|FALSCH|Gedankenkontrolle|99,9%| Hchste\n|FALSCH|Vergiftung|98%| Hchste\n|UNBEGRNDET|USA besonders verdchtig|97%| Sehr hoch\n\n**Durchschnittliche Konfidenz**: 97,8% (GEGEN Chemtrails)\n**Niedrigste Konfidenz**: 95% (Klimaschutz-Zweck - Verwechslung mit Geoengineering)\n**Hchste Konfidenz**: 99,9% (Gedankenkontrolle - physikalisch unmglich)\n\n----\n\n== Wichtigste Erkenntnisse ==\n\n=== **Was wir mit hchster Konfidenz wissen** ===\n\n1. (((\n**Chemtrails existieren NICHT**(99% Konfidenz)\n\n* 98,7% der Atmosphrenwissenschaftler widersprechen\n* Alle relevanten Institutionen widersprechen\n* Vollstndige physikalische Erklrung fr Kondensstreifen\n)))\n1. (((\n**Kondensstreifen sind Wasserdampf**(99,9% Konfidenz)\n\n* Entstehung durch Kondensation in kalter Luft\n* Eiskristalle, keine Chemikalien\n* Normale Physik und Chemie\n)))\n1. (((\n**Keine Gesundheitsgefahr durch Kondensstreifen**(98% Konfidenz)\n\n* Keine erhhten Schadstoffkonzentrationen\n* Keine epidemiologischen Aufflligkeiten\n* Normale toxikologische Befunde\n)))\n1. (((\n**Verschwrungstheorie ist gefhrlich**(95% Konfidenz)\n\n* Ablenkung von realen Umweltproblemen\n* Gesundheitsgefhrdung durch Angst\n* Verbindungen zu Extremismus\n)))\n\n=== **Warum die Theorie so hartnckig ist** ===\n\n**Psychologische Faktoren**:\n\n* **Kontrollillusion**: Komplexe Welt durch einfache Erklrung scheinbar verstehbar\n* **Misstrauen**: Berechtigtes Misstrauen gegenber Autoritten\n* **Besttigungsfehler**: Selektive Wahrnehmung besttigt Glauben\n* **Gemeinschaft**: Zugehrigkeitsgefhl zu Gruppe\n\n**Soziologische Faktoren**:\n\n* **Informationsblase**: Social Media verstrkt Verschwrungsdenken\n* **Fehlende Wissenschaftskompetenz**: Schwierigkeiten, Fakten von Fiktion zu trennen\n* **Ohnmachtsgefhle**: Gefhl, eigenes Schicksal nicht kontrollieren zu knnen\n* **Politische Instrumentalisierung**: Politiker nutzen ngste\n\n**Kommunikative Faktoren**:\n\n* **Visuelle \"Beweise\"**: Fotos von Kondensstreifen wirken berzeugend\n* **Wissenschaftliche Begriffe**: Missverstndnis von Geoengineering-Diskussion\n* **Geschlossenes Glaubenssystem**: Jeder Gegenbeweis als \"Teil der Verschwrung\" gedeutet\n\n=== **Balancierte Perspektive** ===\n\n**Verstndnis fr Chemtrail-Glubige**:\n\n*  Berechtigtes Misstrauen gegenber Autoritten (historische Geheimprojekte waren real)\n*  Verstndliche Angst vor Umweltverschmutzung (ist real, nur nicht durch Chemtrails)\n*  Ohnmachtsgefhle in komplexer Welt nachvollziehbar\n*  Oft gut gemeinte, verngstigte Menschen (laut Caldeira)\n\n**ABER gleichzeitig**:\n\n*  Wissenschaftliche Fakten mssen respektiert werden\n*  Verschwrungsdenken ist gefhrlich\n*  Ablenkung von realen Problemen schadet\n*  Extremistische Instrumentalisierung inakzeptabel\n\n**Was wirklich wichtig ist**:\n\n* **Reale Luftverschmutzung bekmpfen**(Feinstaub, Stickoxide)\n* **Klimawandel ernst nehmen**(wissenschaftlich belegt)\n* **Echte Umweltgifte regulieren**(Pestizide, Schwermetalle, Mikroplastik)\n* **Kritisches Denken frdern**(Wissenschaftskompetenz)\n* **Demokratie strken**(berechtigte Kritik  Verschwrungsdenken)\n\n----\n\n== Kritische Einschrnkungen & Limitationen ==\n\n=== **Analyse-Limitationen** ===\n\n1. **Kein Zugang zu Pro-Chemtrail-\"Forschung\"**: Gibt es keine peer-reviewed Studien\n1. **Psychologische Tiefe begrenzt**: Keine Interviews mit Chemtrail-Glubigen durchgefhrt\n1. **Aktualitt**: Neueste Entwicklungen (Louisiana-Gesetz 2025) erst krzlich\n1. **Kulturelle Varianz**: Fokus auf deutschsprachigen Raum und USA\n\n=== **Was diese Analyse NICHT sagen kann** ===\n\n* **Individuelle berzeugungen**: Warum konkrete Person an Chemtrails glaubt\n* **Alle Varianten der Theorie**: Es gibt Hunderte verschiedene Versionen\n* **Zuknftige Entwicklungen**: Ob Geoengineering jemals real implementiert wird\n* **Optimale Kommunikation**: Wie man Chemtrail-Glubige am besten erreicht\n\n=== **Was diese Analyse sagen kann** ===\n\n* **Wissenschaftlicher Konsens**: 98,7% der Experten widersprechen\n* **Physikalische Realitt**: Kondensstreifen sind Wasserdampf\n* **Evidenzlage**: Keine Belege fr Chemtrails trotz intensiver Suche\n* **Gesundheitsrisiko**: Null (durch Chemtrails), aber hoch (durch die Angst davor)\n* **Gesellschaftliches Risiko**: Verschwrungstheorie ist gefhrlich\n\n----\n\n== Empfehlungen ==\n\n=== **Fr die ffentlichkeit** ===\n\n==== **Wenn Sie an Chemtrails geglaubt haben**: ====\n\n* **Kein Grund fr Scham**: Viele gut gemeinte Menschen glauben daran\n* **Wissenschaftliche Quellen prfen**: Umweltbundesamt, DLR, Caldeira-Studie\n* **Reale Umweltprobleme angehen**: Fokus auf echte Bedrohungen\n* **Kritisches Denken**: Quellen hinterfragen, auch eigene berzeugungen\n\n==== **Wenn Freunde/Familie an Chemtrails glauben**: ====\n\n* **Empathisch bleiben**: Nicht verspotten oder belcheln\n* **Fragen stellen**: \"Was wre fr dich ein Beweis gegen Chemtrails?\"\n* **Gemeinsam recherchieren**: Zusammen Caldeira-Studie lesen\n* **Reale Sorgen ernst nehmen**: Umweltschutz ist wichtig\n* **NICHT**: Direkt als \"verrckt\" abstempeln (verhrtet Fronten)\n\n==== **Fr alle**: ====\n\n1. **Reale Umweltprobleme bekmpfen**: Feinstaub, Klimawandel, echte Gifte\n1. **Wissenschaftskompetenz frdern**: Schulbildung, Medien-Literacy\n1. **Berechtigte Kritik ben**: An Regierungen, Konzernen (aber faktenbas iert)\n1. **Misstrauen vs. Paranoia**: Gesundes Misstrauen ist gut, Verschwrungsdenken nicht\n\n=== **Fr Gesundheitsfachkrfte** ===\n\n1. **Ernst nehmen**: Patienten mit Chemtrail-ngsten nicht abweisen\n1. **Aufklren**: Wissenschaftliche Fakten vermitteln\n1. **Reale Risiken**: Fokus auf echte Gesundheitsgefahren (Luftverschmutzung)\n1. **Psychologische Untersttzung**: Bei starker Angststrung Hilfe anbieten\n\n=== **Fr Bildungseinrichtungen** ===\n\n1. **Wissenschaftskompetenz**: Wie funktioniert Wissenschaft? Was ist Evidenz?\n1. **Medien-Literacy**: Quellen bewerten, Fake News erkennen\n1. **Kritisches Denken**: Verschwrungstheorien analysieren\n1. **Atmosphrenphysik**: Wie entstehen Kondensstreifen wirklich?\n\n=== **Fr Medien & Journalisten** ===\n\n1. **Faktenbasiert berichten**: Nicht beide Seiten als \"gleich gltig\" darstellen\n1. **Nicht verstrken**: Verschwrungstheorien nicht unntig Plattform bieten\n1. **Wissenschaft erklren**: Verstndlich, aber korrekt\n1. **Reale Probleme**: Fokus auf echte Umweltgefahren\n\n=== **Fr Politik** ===\n\n1. **Nicht legitimieren**: Keine Gesetze gegen fiktive Bedrohungen (Louisiana-Beispiel negativ)\n1. **Bildung frdern**: Wissenschaftskompetenz, kritisches Denken\n1. **Transparenz**: Offene Kommunikation ber Luftverkehr, Emissionen\n1. **Reale Probleme**: Luftqualitt verbessern, Klimaschutz\n\n=== **Fr Wissenschaftler** ===\n\n1. **Kommunikation**: Ergebnisse verstndlich vermitteln\n1. **Proaktiv**: Nicht nur auf Anfragen reagieren\n1. **Social Media**: Prsenz zeigen, Fake News entgegentreten\n1. **Empathie**: ngste der Bevlkerung ernst nehmen\n\n----\n\n== Warum Menschen an Chemtrails glauben ==\n\n=== **Psychologische Mechanismen** ===\n\n**~1. Mustererkennung (Pattern Recognition)**:\n\n* Gehirn sucht Muster, auch wo keine sind\n* Kondensstreifen + Flugzeuge = scheinbar verdchtiges Muster\n* Evolutionrer Vorteil: Besser falschen Alarm als Gefahr bersehen\n\n**2. Illusorische Korrelation**:\n\n* \"Seit Kondensstreifen mehr werden, gibt es mehr Krankheiten\"\n* Tatschlich: Beide Trends unabhngig (Flugverkehr , Lebenserwartung )\n\n**3. Besttigungsfehler (Confirmation Bias)**:\n\n* Selektive Wahrnehmung: Nur Beispiele bemerken, die Theorie besttigen\n* Gegenbeweise ignoriert: \"Das ist Teil der Verschwrung\"\n\n**4. Kontrollillusion**:\n\n* Gefhl, Komplexitt verstanden zu haben\n* \"Ich wei, was wirklich los ist\"\n* Macht Ohnmachtsgefhle ertrglicher\n\n**5. Dunning-Kruger-Effekt**:\n\n* Wenig Wissen  berschtzung eigener Kompetenz\n* \"Ich habe recherchiert\" = YouTube-Videos geschaut\n\n=== **Soziologische Faktoren** ===\n\n**~1. Informationsblase**:\n\n* Social Media Algorithmen verstrken Verschwrungsdenken\n* YouTube empfiehlt immer extremere Videos\n* Facebook-Gruppen bilden geschlossene Gemeinschaften\n\n**2. Gemeinschaft & Identitt**:\n\n* Zugehrigkeitsgefhl zu Gruppe\n* \"Wir\" (Aufgeklrte) vs. \"Die\" (Schlafschafe)\n* Identitt wird mit berzeugung verknpft\n\n**3. Misstrauen gegenber Autoritten**:\n\n* Historisch begrndet (MKUltra, Tuskegee, etc.)\n* Aber: Berechtigt Misstrauen  Paranoia\n* bertragung auf alle Institutionen\n\n**4. Ohnmachtsgefhle**:\n\n* Gefhl, keine Kontrolle ber Leben zu haben\n* Wirtschaftliche Unsicherheit\n* Politische Entfremdung\n\n=== **Technologische Faktoren** ===\n\n**~1. Visuelle \"Beweise\"**:\n\n* Fotos von Kondensstreifen wirken berzeugend\n* Manipulation einfach (Photoshop)\n* \"Sehen = Glauben\"\n\n**2. Fehlinformation verbreitet sich schnell**:\n\n* Emotionale Inhalte werden mehr geteilt\n* Wahrheit ist oft langweilig\n* Korrektur erreicht weniger Menschen\n\n**3. Professionelle Desinformation**:\n\n* Manche Websites professionell gestaltet\n* Pseudo-wissenschaftliche Sprache\n* Fr Laien schwer, Fake zu erkennen\n\n----\n\n== Die Rolle von echtem Geoengineering ==\n\n=== Was ist Geoengineering wirklich? ===\n\n**Definition**: Theoretische groskalige technologische Eingriffe ins Klimasystem zur Bekmpfung des Klimawandels.\n\n**Zwei Hauptanstze**:\n\n1. **Carbon Dioxide Removal (CDR)**: CO aus Atmosphre entfernen\n1. **Solar Radiation Management (SRM)**: Sonnenstrahlung reflektieren\n\n**Status 2025**:\n\n*  Intensive wissenschaftliche**DISKUSSION**\n*  KEINE praktische Implementierung\n*  Kontroverse ber Risiken und Ethik\n*  Kleine Feldexperimente geplant (stark reguliert)\n\n=== Warum Geoengineering  Chemtrails ===\n\n|=Geoengineering|=Chemtrails\n|Wissenschaftliche Diskussion|Verschwrungstheorie\n|Offen debattiert|Angeblich geheim\n|Nicht implementiert|Angeblich seit Jahren aktiv\n|Stratosphre (>20 km)|Kondensstreifen (10-13 km)\n|Sulfat-Aerosole (theoretisch)|Keine Chemikalien (real)\n|Klimaschutz-Zweck|Verschiedene Motive (angeblich)\n|Ethische Bedenken|Verschwrung\n\n=== Warum die Verwechslung problematisch ist ===\n\n1. **Diskreditiert legitime Debatte**: Geoengineering-Diskussion wichtig, wird aber mit Verschwrungstheorie gleichgesetzt\n1. **Misstrauen gegen Wissenschaft**: Forscher werden verdchtigt\n1. **Ablenkung von Klimaschutz**: Echte Lsungen (Emissionsreduktion) werden ignoriert\n\n----\n\n== Abschlieende Bewertung ==\n\n=== **Kernaussagen** ===\n\n1. (((\n**Chemtrails existieren NICHT**\n\n* Wissenschaftlicher Konsens: 98,7%\n* Alle relevanten Institutionen widersprechen\n* Physikalisch vollstndig erklrt\n)))\n1. (((\n**Kondensstreifen sind normaler Wasserdampf**\n\n* Entstehung durch Kondensation\n* Variabilitt durch Wetterbedingungen\n* Zunahme durch mehr Flugverkehr\n)))\n1. (((\n**Keine Gesundheitsgefahr**\n\n* Keine erhhten Schadstoffkonzentrationen\n* Keine epidemiologischen Aufflligkeiten\n* Wissenschaftlich widerlegt\n)))\n1. (((\n**Verschwrungstheorie ist gefhrlich**\n\n* Ablenkung von realen Problemen\n* Frderung von Misstrauen und Extremismus\n* Gesundheitsgefhrdung durch Angst\n)))\n\n=== **Gesellschaftliche Bedeutung** ===\n\n**Positiv**:\n\n*  Zeigt Bedrfnis nach Umweltschutz\n*  Zeigt berechtigtes Misstrauen gegenber Autoritten\n*  Kann Einstieg in echtes Umweltengagement sein\n\n**Negativ**:\n\n*  Ablenkung von realen Umweltproblemen\n*  Untergrbt Vertrauen in Wissenschaft\n*  Verbindungen zu Extremismus\n*  Politische Instrumentalisierung\n\n=== **Evidenzlage-Zusammenfassung** ===\n\n{{{Evidenz FR Chemtrails:\n- Anekdoten:  (keine Beweiskraft)\n- Fotos:  (normal erklrbar)\n- \"Messungen\":  (methodisch falsch oder normal)\n- Wissenschaft:  (null)\nGESAMT: 0/5\n\nEvidenz GEGEN Chemtrails:\n- Wissenschaftlicher Konsens:  (98,7%)\n- Physikalische Erklrung:  (vollstndig)\n- Institutionelle Stellungnahmen:  (eindeutig)\n- Messungen:  (keine Anomalien)\n- Logik:  (Theorie widersprchlich)\nGESAMT: 5/5\n}}}\n\n=== **Ausblick** ===\n\n**Chemtrails-Theorie wird wahrscheinlich bestehen bleiben**, weil:\n\n* Psychologische Bedrfnisse erfllt\n* Social Media verstrkt\n* Politisch instrumentalisiert\n* Geschlossenes Glaubenssystem\n\n**Aber Gesellschaft kann**:\n\n* Wissenschaftskompetenz frdern\n* Kritisches Denken lehren\n* Reale Umweltprobleme angehen\n* Berechtigte Sorgen ernst nehmen\n\n----\n\n== Transparenz-Offenlegung ==\n\n=== **Methodik** ===\n\n**Analyse-Framework**: FactHarbor Evidenzmodell v0.9.18\n**KI-System**: Claude (Anthropic)\n**Analysetyp**: Umfassende automatisierte Analyse mit Pflicht-Widerspruchssuche\n**Verarbeitungsdatum**: 15. Dezember 2025\n**Sprache**: Deutsch\n\n=== **Datenquellen** ===\n\n**Primrquellen**: 15 Evidenzstcke\n\n* 10 Quellen mit hchster Zuverlssigkeit (67%)\n* 4 Quellen mit hoher Zuverlssigkeit (27%)\n* 1 Quelle mit mittlerer Zuverlssigkeit (6%)\n* 0 Quellen mit niedriger Zuverlssigkeit\n\n**Quellentypen**:\n\n* Wissenschaftliche Studie (Caldeira et al. 2016)\n* Offizielle Stellungnahmen (Umweltbundesamt, DLR, WHO, US Air Force)\n* Wissenschaftsjournalismus (Quarks, Scinexx, Spektrum)\n* Faktenchecker (Mimikama)\n* Bildungseinrichtungen (Bundeszentrale fr politische Bildung)\n* Enzyklopdien (Wikipedia)\n\n=== **Qualitts-Validierung** ===\n\n**Widerspruchssuche**:  Abgeschlossen und dokumentiert (Pro-Chemtrail-Argumente gesucht)\n**Qualittsgates**:  12/12 Urteile bestanden alle Gates\n**Evidenz-Diversitt**:  Verschiedene Perspektiven (inkl. psychologische & soziologische)\n**Unsicherheitsquantifizierung**:  Minimal (Evidenzlage eindeutig)\n**Nachvollziehbarkeit**:  Vollstndige Begrndungsketten dokumentiert\n\n=== **Bekannte Limitationen** ===\n\n1. **Keine Pro-Chemtrail-Wissenschaft**: Existiert nicht (keine peer-reviewed Studien)\n1. **Psychologische Tiefe**: Keine Interviews mit Glubigen\n1. **Kulturelle Varianz**: Fokus auf Deutschland/USA\n1. **Aktualitt**: Louisiana-Gesetz sehr aktuell (Juni 2025)\n\n=== **KI-Offenlegung** ===\n\nDiese Analyse wurde**vollstndig von KI generiert**(Claude, Anthropic) unter Verwendung von:\n\n* Web-Suche zur Evidenzsammlung\n* Logischem Schlussfolgern zur Bewertung\n* Systematischer Methodik zur Qualittssicherung\n* Pflicht-Widerspruchssuche\n\n**Status der Humanprfung**: Ausstehend (Empfohlen fr Risikoklasse A)\n\n**WICHTIG**: Diese Analyse ersetzt nicht:\n\n* Offizielle Stellungnahmen von Behrden\n* Wissenschaftliche Peer-Review-Studien\n* Professionelle Beratung bei ngsten/Sorgen\n\n----\n\n== Fazit ==\n\n=== **In einem Satz** ===\n\n**Chemtrails existieren nicht  Kondensstreifen sind normaler Wasserdampf, wissenschaftlich erklrt und von 98,7% der Atmosphrenforscher besttigt.**\n\n=== **Evidenzlage** ===\n\n{{{GEGEN Chemtrails:\n 98,7% wissenschaftlicher Konsens (Caldeira-Studie)\n Alle relevanten Institutionen widersprechen\n Vollstndige physikalische Erklrung\n Keine chemischen Anomalien gefunden\n Logische Widersprche der Theorie\n Geheimhaltung praktisch unmglich\n\nFR Chemtrails:\n Null wissenschaftliche Studien\n Null glaubwrdige Beweise\n Nur Anekdoten und Missverstndnisse\n Verschwrungstheoretische Argumentation\n}}}\n\n=== **Was wirklich gefhrlich ist** ===\n\n**NICHT die (nicht existierenden) Chemtrails, SONDERN**:\n\n1. Die Angst vor fiktiven Chemtrails (Gesundheitsgefhrdung)\n1. Ablenkung von realen Umweltproblemen (Feinstaub, Klimawandel)\n1. Untergrabung von Wissenschaft und Demokratie\n1. Verbindung zu Extremismus und Antisemitismus\n\n=== **Konstruktiver Umgang** ===\n\n**Statt Chemtrails befrchten**:\n\n*  Reale Luftverschmutzung bekmpfen\n*  Klimawandel ernst nehmen\n*  Wissenschaftskompetenz frdern\n*  Kritisches, aber faktenbasiertes Denken\n\n**Im Umgang mit Glubigen**:\n\n*  Empathisch bleiben\n*  Gemeinsam recherchieren\n*  Reale Sorgen ernst nehmen\n*  Nicht verspotten\n\n----\n\n//Dieser Bericht wurde von FactHarbor v0.9.18 POC erstellt  einem KI-gesttzten Evidenzanalysesystem zur Schaffung von Transparenz bei umstrittenen Behauptungen. Die Analyse zeigt berwltigende wissenschaftliche Evidenz GEGEN die Existenz von Chemtrails.//\n\n**Risikoklassen-Legende**:\n\n* **Klasse A**(Hochrisiko): Fehlinformation mit Gesundheits- oder Demokratie-Gefhrdung\n* **Klasse B**(Mittelrisiko): Wissenschaftliche Kontroverse\n* **Klasse C**(Niedrigrisiko): Etablierte Fakten\n\n**Modus**:**Modus 2**(KI-generiert, ffentlich mit klarer KI-Kennzeichnung)\n**Humanprfung**: Ausstehend, empfohlen fr Risikoklasse A-Behauptungen\n\n----\n\n**Erstellt**: 15. Dezember 2025\n**Version**: FactHarbor 0.9.18 POC\n**Analyse-ID**: Chemtrails_Verschwoerungstheorie_20251215_DE\n**Format**: Menschenlesbarer Markdown-Bericht (Deutsch)", "Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude.WebHome": "= FactHarbor-Analyse: Schweiz auf dem Weg zur Gerontokratie? =\n\n**Behauptung**: \"Bis in 10 Jahren wird sich die Demokratie in der Schweiz zu einer Gerontokratie entwickelt haben.\"\n\n**Analysedatum**: 16. Dezember 2025\n**FactHarbor-Version**: 0.9.18 POC\n**Sprache**: Deutsch\n**Zeithorizont der Prognose**: Bis 2035\n\n----\n\n== Zusammenfassung ==\n\nDie Behauptung, dass sich die Schweizer Demokratie bis 2035 zu einer \"Gerontokratie\" (Herrschaft der Alten) entwickelt haben wird, ist**teilweise berechtigt, aber als absolute Aussage bertrieben**. Demografische Trends zeigen klar eine Verschiebung hin zu einer lteren Whlerschaft, wobei das Medianalter der Abstimmenden bis 2035 auf ber 60 Jahre steigen wird. Jedoch sprechen mehrere Faktoren gegen eine vollstndige \"Gerontokratie\": Die formale Demokratie bleibt intakt, ltere Brger stimmen nicht monolithisch, und das Problem liegt primr in der niedrigen Stimmbeteiligung der Jungen  nicht in einer zahlenmssigen bermacht der Alten.\n\n----\n\n== Begriffsklrung: Was bedeutet \"Gerontokratie\"? ==\n\n**Definition**: Gerontokratie (griech.  = Greis,  = Herrschaft) bezeichnet eine Staatsform oder politische Situation, in der:\n\n* Die politische Macht berwiegend bei lteren Personen liegt\n* Interessen lterer Brger systematisch bevorzugt werden\n* Jngere Generationen in ihren politischen Einflussmglichkeiten marginalisiert sind\n\n**Unterschied zur Demokratie**:\n\n* In einer funktionierenden Demokratie haben alle Brger formal gleiches Stimmrecht\n* Gerontokratie beschreibt eine de-facto-Dominanz einer Altersgruppe\n* Es handelt sich um einen graduellen bergang, nicht um einen binren Zustand\n\n----\n\n== Detaillierte Claim-Analyse ==\n\n=== **HAUPTCLAIM**: \"Die Schweizer Demokratie wird bis 2035 zu einer Gerontokratie\" ===\n\n**Verdict**:**TEILWEISE UNTERSTTZT mit WICHTIGEN EINSCHRNKUNGEN**\n**Konfidenz**: 55% (Bereich: 45%-65%)\n\n----\n\n== Evidenz: Demografische Entwicklung ==\n\n=== **Bevlkerungsalterung (GUT BELEGT)** ===\n\n**Aktuelle Situation (2025)**:\n\n* Anteil der ber 65-Jhrigen an der Bevlkerung: ~~20%\n* Schweizer Bevlkerung Ende 2024: ~~5,4 Mio. Stimmberechtigte (ab 18)\n* 18- bis 64-Jhrige: 71% der Stimmberechtigten\n* ber 65-Jhrige: 29% der Stimmberechtigten\n\n**Prognose 2035 (BFS-Szenarien)**:\n\n* Anteil der Personen ab 65 Jahren wchst von 19,9% auf ca. 22-25%\n* Personen ab 65 Jahren: +84% Wachstum bis 2045\n* Von 2025 bis 2035 wchst die Bevlkerung um rund 7% bzw. 643'000 Personen\n* Der schnellste Anstieg des Altersquotienten erfolgt zwischen 2020 und 2035\n\n**Babyboomer-Effekt**:\n\n* Die geburtenstrksten Babyboom-Jahrgnge kommen allmhlich ins Rentenalter\n* Babyboom-Generation erreicht bis 2040 das Rentenalter und beschleunigt die demografische Alterung\n* Von 2020 bis 2035 wird der Anteil der 15- bis 64-Jhrigen um 9,4% abnehmen, whrend der Anteil der ber 65-Jhrigen um 35% zunimmt\n\n=== **Altersquotient (Erwerbsttige pro Rentner)** ===\n\n|=Jahr|=Erwerbsttige (20-64) pro Rentner (65+)\n|1948|6,3 : 1\n|1995|4,2 : 1\n|2024|3,4 : 1\n|2035|**2,3 : 1**(Prognose)\n|2055|1,9 : 1 (Prognose)\n\n----\n\n== Evidenz: Medianalter der Abstimmenden ==\n\n=== **Entwicklung des Medianalters** ===\n\nDas Medianalter der Schweizer Stimmberechtigten ist zwischen 1991 und 2023 von 45 auf 53 Jahre gestiegen und wird im Jahr 2035 sogar auf ber 60 Jahre klettern.\n\n|=Jahr|=Medianalter der Abstimmenden\n|1991|45 Jahre\n|2015|56 Jahre\n|2023|53-57 Jahre\n|2030|~~60 Jahre (Prognose)\n|2035|**>60 Jahre**(Prognose)\n\n**Bedeutung**: Das Medianalter teilt die Abstimmenden in zwei gleich grosse Gruppen. Ein Medianalter von ber 60 Jahren bedeutet, dass**mehr als die Hlfte aller Abstimmenden ber 60 Jahre alt ist**.\n\n=== **Avenir Suisse Analyse** ===\n\nDer Median-Teilnehmende an Abstimmungen, der die Stimmenden in zwei gleich grosse Altersgruppen teilt, ist gegenwrtig 57 Jahre alt. 2030 wird er 60 sein.\n\nDas Medianalter der Abstimmenden lag 2015 bereits bei 56 Jahren, bis 2035 drfte es auf deutlich ber 60 Jahre klettern.\n\n----\n\n== Evidenz: Stimmbeteiligung nach Alter ==\n\n=== **Massive Unterschiede in der Stimmbeteiligung** ===\n\n**St. Galler Messungen**(schweizweit einzigartige Erhebung):\n\n|=Altersgruppe|=Stimmbeteiligung\n|18-24 Jahre|**29,5%**\n|25-34 Jahre|~~35%\n|35-44 Jahre|~~40%\n|45-54 Jahre|~~48%\n|55-64 Jahre|~~55%\n|65-74 Jahre|**63,2%**\n|70+ Jahre|Maximum (~~65%)\n\n**Kernbefund**: Zwei Drittel der lteren gehen an die Urne, zwei Drittel der Jungen bleiben zu Hause.\n\n**Konsequenz**: Obwohl die lteren nur ~~29% der Stimmberechtigten stellen, machen sie aufgrund ihrer hheren Stimmbeteiligung einen berproportional grossen Anteil der**tatschlich Abstimmenden**aus.\n\n----\n\n== Evidenz: Generationenkonflikt bei Abstimmungen ==\n\n=== **Masterarbeit Silvan Gamper (Universitt Bern)** ===\n\nAnalyse von 400+ Volksabstimmungen (1984-2024):\n\nDer durchschnittliche Altersunterschied in den Abstimmungsentscheiden liegt bei rund 13 Prozentpunkten. Der Altersgraben ist damit in der gleichen Grssenordnung wie der viel zitierte Rstigraben.\n\n**Abstimmungsergebnisse 2020-2024**:\n\n* In 15 von 42 eidgenssischen Abstimmungen stimmten die Alten anders als die Jungen\n* In 11 Fllen gewannen die Alten die Abstimmung, in 4 Fllen die Jungen\n\n**Besonders umstrittene Abstimmungen**:\n\n* **13. AHV-Rente (Mrz 2024)**: Jngere beurteilten die Vorlage mehrheitlich skeptisch, ltere Generationen stimmten mit grosser Mehrheit zu. Der Altersgraben war hier nicht nur messbar, sondern politisch sprbar.\n* **Abschaffung Eigenmietwert (Oktober 2025)**: Starke Altersunterschiede bei Abstimmungsverhalten\n\n**Bereiche mit grssten Altersunterschieden**:\n\n* Besonders deutlich zeigen sich die Differenzen in der Sozial- und Familienpolitik, wo sich die Prferenzen der Generationen zunehmend auseinanderentwickeln.\n\n----\n\n== Gegenargumente: Warum \"Gerontokratie\" bertrieben ist ==\n\n=== **1. Formale Demokratie bleibt intakt** ===\n\n* Jede Stimme zhlt gleich viel (verfassungsrechtlich garantiert)\n* Ganz abgesehen davon, dass nicht alle Seniorinnen und Senioren gleich stimmen: Die Alterskohorten der Schweizer Staatsbrger sind recht ausgeglichen.\n* Keine rechtlichen Einschrnkungen fr jngere Whler\n\n=== **2. Das Problem liegt bei der Stimmbeteiligung der Jungen** ===\n\nDie Alten sind also in der Minderheit. Aber sie stimmen fleissig ab. Ganz im Gegenteil zu den Jungen.\n\nDie Alten haben nicht mehr \"Spieler\" auf dem Platz, sondern deutlich weniger als die Jungen.\n\n**Fazit**: Die \"Gerontokratie\" ist nicht das Ergebnis einer zahlenmssigen bermacht der Alten, sondern der**Stimmabstinenz der Jungen**.\n\n=== **3. Heterogenitt der lteren Whlerschaft** ===\n\nDie Vorstellung, dass die Boomer dank ihrer grossen Zahl eigene Interessen durchsetzen, ist falsch. Die politischen Prferenzen dieser Generation sind genauso unterschiedlich wie bei anderen Jahrgngen.\n\n**Beispiele fr Abstimmungen, die Junge gewannen**:\n\n* Beim Partnerschaftsgesetz und der Mutterschaftsversicherung konnten sie einen Abstimmungserfolg bejubeln, ebenso beim Gripen-Referendum: Es waren die Jungen, die ihn massgeblich zum \"Absturz\" brachten.\n\n=== **4. Junge stimmen ab, wenn sie wollen** ===\n\nEinzelne Abstimmungen wie etwa ber die Abschaffung der Wehrpflicht und die Einfhrung eines Vaterschaftsurlaubs zeigen, dass die Jngeren durchaus an die Urne gehen, wenn sie betroffen sind und zu profitieren hoffen.\n\nEs kommt enorm auf die Themen an. Wenn es die Jungen betrifft, gehen sie hufiger an die Urne  so zum Beispiel bei der Ehe fr alle.\n\n----\n\n== Internationaler Vergleich: Japan als \"Extrembeispiel\" ==\n\n=== **Japan  Die lteste Demokratie der Welt** ===\n\nJapan ist die Demokratie mit der ltesten Whlerschaft: 29% der japanischen Bevlkerung ist lter als 64 und der Anteil der 75-Jhrigen wird sich in den nchsten 50 Jahren sogar noch verdoppeln.\n\n**Auswirkungen in Japan (laut Studie Buchmeier/Vogt)**:\n\n* Partizipationseffekte: Junge Menschen werden zur Minderheit im Elektorat und gehen seltener whlen. Bereits zwei Drittel aller Stimmen stammen von ber 50-Jhrigen. 2060 werden es fast 80% sein.\n* Reprsentationseffekte: Die Parlamente altern noch schneller als die Bevlkerung. Der Durchschnittsabgeordnete ist 55 Jahre alt, Politiker unter 30 sind praktisch inexistent.\n* Politikeffekte: Die Folge ist eine \"Politik der Senioren fr die Senioren\": Sozialausgaben fr Rentner sind in Japan sechsmal so hoch wie jene fr Familien, whrend Kinderarmut, Klimaschutz oder Bildung kaum Prioritt geniessen.\n\n**Unterschied zur Schweiz**:\n\n* Schweiz hat noch nicht den japanischen Alterungsgrad erreicht\n* Schweiz hat strkere Zuwanderung (verjngender Effekt)\n* Direkte Demokratie erlaubt andere Partizipationsformen\n\n----\n\n== Prognostische Unsicherheiten ==\n\n=== **Faktoren, die die Entwicklung beeinflussen knnten** ===\n\n**PRO Gerontokratie (verstrkend)**:\n\n* Anhaltend tiefe Geburtenrate (aktuell 1,29 Kinder/Frau  historischer Tiefststand)\n* Ab dem Jahr 2035 sterben mehr Menschen als geboren werden\n* Babyboomer-Pensionierung beschleunigt sich bis 2040\n* Junge ziehen weniger nach (Fachkrftemangel in EU reduziert Migrationspotential)\n\n**CONTRA Gerontokratie (abschwchend)**:\n\n* Zuwanderung kann die Geburtenlcke fllen und sorgt fr positives Bevlkerungswachstum\n* Durchschnittsalter der auslndischen Bevlkerung liegt fast 7 Jahre unter dem der Schweizer\n* Politische Mobilisierung der Jungen (Klimabewegung, etc.)\n* Mgliche Einfhrung des Stimmrechtsalters 16 (bereits in Glarus umgesetzt)\n* Digitalisierung knnte Partizipation der Jungen erhhen\n\n----\n\n== Reformvorschlge in der Diskussion ==\n\n=== **In der Schweiz diskutierte Massnahmen** ===\n\nMehrere Vorschlge zur Strkung der nachrckenden Generationen liegen auf dem Tisch.\n\n**~1. Stimmrechtsalter 16**\n\n* Bereits im Kanton Glarus umgesetzt (seit 2007)\n* Wrde ~~2,4% mehr Stimmberechtigte bringen\n* Im Februar 2025 im Kanton Luzern zur Abstimmung\n\n**2. Familienwahlrecht**\n\n* Avenir Suisse regt an, das Stimmrecht allen Kindern ab Geburt zu verleihen. Bis zur Volljhrigkeit wrden die Eltern stellvertretend fr ihre Sprsslinge abstimmen.\n\n**3. Gewichtung der Stimmen nach Alter**\n\n* Junge Stimmen strker zu gewichten oder sogar das Wahlrecht ab einem gewissen Alter zu streichen, wrde die Dominanz der Senioren brechen.\n* **Kritik**: Verfassungsrechtlich hchst problematisch, Verletzung der Gleichheitsgrundstze\n\n**4. Stimmrecht fr Auslnder**\n\n* Auch Auslnder, die bereits whrend fnf bis acht Jahren Steuern gezahlt haben, sollen stimmen drfen  zumal das Durchschnittsalter der auslndischen Bevlkerung fast sieben Jahre unter jenem der Schweizer liegt.\n\n**5. Politische Bildung verstrken**\n\n* \"Wir sind unglaublich stolz auf unsere Demokratie, wollen aber gleichzeitig nicht die Demokratie lehren.\"\n\n----\n\n== Differenzierte Bewertung ==\n\n=== **Was bis 2035 eintreten WIRD (hohe Wahrscheinlichkeit)** ===\n\n**Medianalter der Abstimmenden ber 60 Jahre**(90% Konfidenz)\n\n* Demografisch unvermeidlich aufgrund der Babyboomer-Pensionierung\n\n**ber 50% der tatschlich Abstimmenden werden 50+ sein**(85% Konfidenz)\n\n* Bereits heute nahe an diesem Wert\n\n**Politische Themen der lteren bekommen mehr Gewicht**(80% Konfidenz)\n\n* AHV, Gesundheit, Pflege dominieren die Agenda\n\n**Generationenkonflikt bei Verteilungsfragen verschrft sich**(75% Konfidenz)\n\n* Finanzierung der Sozialwerke wird konfliktreicher\n\n=== **Was bis 2035 NICHT eintreten wird (hohe Wahrscheinlichkeit)** ===\n\n**Formale Abschaffung der Demokratie**(99% Konfidenz)\n\n* Keine rechtlichen nderungen am Stimmrecht zu erwarten\n\n**Monolithisches Abstimmungsverhalten der lteren**(90% Konfidenz)\n\n* ltere bleiben politisch heterogen\n\n**Vollstndige Marginalisierung der Jungen**(85% Konfidenz)\n\n* Junge knnen bei wichtigen Themen mobilisiert werden\n\n**\"Japan-Niveau\" der Alterung**(80% Konfidenz)\n\n* Schweiz bleibt durch Zuwanderung jnger\n\n=== **Was UNGEWISS bleibt** ===\n\n**Reformfhigkeit bei Sozialwerken**(50% Konfidenz)\n\n* Rentenreformen knnten blockiert werden\n\n**Entwicklung der Jugendbeteiligung**(50% Konfidenz)\n\n* Trend unklar  knnte steigen oder fallen\n\n**Auswirkungen auf Klimapolitik**(45% Konfidenz)\n\n* Generationeninteressen kollidieren\n\n----\n\n== Schlussfolgerung ==\n\n=== **Gesamtbewertung** ===\n\n**Verdict**:**TEILWEISE UNTERSTTZT  aber als absolute Aussage BERTRIEBEN**\n\n**Konfidenz**: 55% (Bereich: 45%-65%)\n\n----\n\n=== **Nuancierte Einordnung** ===\n\nDie Behauptung enthlt einen**wahren Kern**: Die Schweizer Whlerschaft altert rapide, und bis 2035 wird das Medianalter der Abstimmenden ber 60 Jahre liegen. Der politische Einfluss lterer Brger wird zunehmen.\n\n**ABER**: Die Verwendung des Begriffs \"Gerontokratie\" ist**irrefhrend und bertrieben**, weil:\n\n1. **Formale Demokratie bleibt intakt**: Jede Stimme zhlt gleich, unabhngig vom Alter\n1. **Das Problem ist die Partizipation, nicht die Demografie**: Die 18- bis 64-Jhrigen machen 71% der Stimmberechtigten aus. Die lteren machen bloss 29% aus. Mit anderen Worten: Von einer Gerontokratie kann keine Rede sein.\n1. **ltere stimmen nicht monolithisch**: Die Boomer-Generation ist politisch genauso heterogen wie andere Generationen\n1. **Junge haben es selbst in der Hand**: Die Grnde fr die Stimmabstinenz vieler Jungen sind vielfltig. Sie stimmen ab, wenn sie wollen.\n\n----\n\n=== **Przisere Formulierung der Prognose** ===\n\n**Statt**: \"Die Schweiz wird zur Gerontokratie\"\n\n**Besser**: \"Die Schweiz entwickelt sich zu einer**lteren Demokratie**mit einem**politischen bergewicht der Rentner**gegenber der erwerbsttigen Bevlkerung, primr aufgrund der**hheren Stimmbeteiligung der lteren**und nicht ihrer zahlenmssigen bermacht.\"\n\n----\n\n=== **Was \"Gerontokratie\" wirklich bedeuten wrde** ===\n\nWenn von \"Gerontokratie\" gesprochen wird, sind typischerweise folgende Phnomene gemeint:\n\n|=Kriterium|=Japan (Extremfall)|=Schweiz 2025|=Schweiz 2035 (Prognose)\n|Anteil 65+ an Bevlkerung|29%|20%|~~23-25%\n|Medianalter Abstimmende|>65|53-57|>60\n|Durchschnittsalter Parlamentarier|55|~~50|~~50-52\n|Sozialausgaben Rentner vs. Familien|6:1|~~2,5:1|~~3:1 (?)\n|Politische Reformen blockiert|Stark|Teilweise|?\n\n**Fazit**: Die Schweiz zeigt**Tendenzen**Richtung \"lterer Demokratie\", ist aber weit von einer echten \"Gerontokratie\" entfernt und wird auch 2035 nicht auf Japan-Niveau sein.\n\n----\n\n== Empfehlungen ==\n\n=== **Fr eine ausgewogene politische Reprsentation**: ===\n\n1. **Politische Bildung verstrken** ab der Grundschule\n1. **Digitale Partizipation frdern** E-Voting knnte Jugendbeteiligung erhhen\n1. **Generationendialog institutionalisieren** Generationenbeirte bei Sozialreformen\n1. **Langfristige Folgen explizit machen** Generationenbilanzen bei Abstimmungen\n1. **Junge ermutigen zur Teilnahme** nicht ltere vom Stimmrecht ausschliessen\n\n----\n\n== Abschliessende Wertung ==\n\n|=Kriterium|=Bewertung\n|Demografische Alterung| BELEGT\n|Steigendes Medianalter der Abstimmenden| BELEGT\n|Hhere Stimmbeteiligung der lteren| BELEGT\n|Zunehmendes politisches Gewicht der Rentner| WAHRSCHEINLICH\n|\"Gerontokratie\" als Systembezeichnung| BERTRIEBEN\n|Abschaffung demokratischer Grundstze| NICHT BELEGT\n|Deterministische Prognose (\"wird sich entwickelt haben\")| ZU ABSOLUT\n\n**Gesamturteil**: Die Behauptung beschreibt einen**realen Trend**, verwendet aber einen**bertriebenen Begriff**und macht eine**zu deterministische Aussage**. Die Entwicklung ist**nicht unvermeidlich**und hngt stark von der politischen Mobilisierung der jngeren Generationen ab.\n\n----\n\n**Transparenzhinweis**: Diese Analyse wurde von KI (Claude/Anthropic) unter Verwendung der FactHarbor-Methodologie v0.9.18 erstellt. Die Bewertungen basieren auf verfgbaren Bevlkerungsprognosen, politikwissenschaftlichen Studien und aktuellen Medienberichten zum Analysedatum.\n\n**Analyse-ID**: CH-GERON-2025-12-16\n**Erstellt**: 16. Dezember 2025\n\n----\n\n== Quellenbersicht ==\n\nDie Analyse basiert auf:\n\n* Bevlkerungsszenarien des Bundesamts fr Statistik (BFS) 2025-2055\n* Avenir Suisse Studien zur demografischen Alterung\n* Masterarbeit Silvan Gamper (Universitt Bern) zu Abstimmungsverhalten\n* Politologische Analysen von Rahel Freiburghaus und Adrian Vatter (Universitt Bern)\n* St. Galler Stimmbeteiligungserhebungen\n* Internationale Vergleichsstudien zu Japan (Buchmeier/Vogt)\n* Aktuelle Medienberichte (Tages-Anzeiger, NZZ, SRF, Infosperber)\n\n**Haftungsausschluss**: Diese Analyse ist eine auf verfgbaren Daten basierende Einschtzung und stellt keine politische Empfehlung dar. Prognosen zur demografischen Entwicklung sind mit Unsicherheiten behaftet.", "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.Analysis Summary.WebHome": "= FactHarbor Analysis Summary =\n\n**Claim**: \"Bolsonaro's trials were fair and based on Brazilian law\"\n\n----\n\n=== Two Separate Proceedings Analyzed ===\n\n|=Proceeding|=Court|=Year|=Charge|=Outcome\n|Electoral Ineligibility|TSE|2023|Abuse of power, media misuse|8-year ban (5-2 vote)\n|Criminal Conviction|STF|2025|Attempted coup, criminal organization|27 years prison (4-1 vote)\n\n----\n\n=== Verdicts ===\n\n|=Dimension|=Verdict|=Confidence\n|**Legal Basis**|STRONGLY SUPPORTED|85%\n|**Procedural Fairness**|LARGELY SUPPORTED with caveats|70%\n\n----\n\n=== Key Findings ===\n\n** Supporting Fairness:**\n\n* Clear legal framework (Electoral Code, Criminal Code Title XII)\n* Law 14.197/2021 used for prosecution was enacted //under Bolsonaro's own presidency//\n* Extensive evidence (884-page police report, 73 witnesses, coup decree drafts found)\n* Full defense presented, public trial, right to appeal granted\n* Multiple judges reviewed evidence (not single-judge decision)\n\n** Legitimate Concerns:**\n\n* Justice Alexandre de Moraes was a //target of the assassination plot// yet presided (appearance of bias)\n* 27-year sentence is very heavy by international standards\n* Political polarization complicates neutral perception\n* US pressure (50% tariffs, sanctions on judges) creates diplomatic complications\n* Timing during Lula presidency raises perception issues\n\n----\n\n=== International Expert Assessment ===\n\n|=Source|=Position\n|Harvard Prof. Steven Levitsky|\"Milestone of institutional resilience\"\n|The Economist|\"Solidity of Brazil's judiciary\"\n|NY City Bar Association|Condemned US pressure, affirmed judicial independence\n|Folha de S. Paulo (Brazilian liberal paper)|\"**Fair conviction, high punishment**\"\n|Trump Administration|Called it \"witch hunt\"\n\n----\n\n=== Balanced Conclusion ===\n\n**Both statements can be simultaneously true:**\n\n1. The trials followed established Brazilian law with extensive evidence\n1. There are legitimate concerns about perception of judicial neutrality and sentence severity\n\n**Analysis ID**: FH-BOLSO-TRIAL-2025-12-15", "Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.WebHome": "= FactHarbor Analysis Report =\n\n== Bolsonaro Judgment: Fairness and Legal Basis Assessment ==\n\n**Analysis Date**: December 15, 2025\n**Source**: Multiple Bolsonaro trials (TSE 2023, STF 2025)\n**FactHarbor Version**: 0.9.18 POC\n**Analysis Type**: Standard\n**Language**: English\n\n----\n\n== Executive Summary ==\n\nThe claim that Bolsonaro's trials were \"fair and based on Brazil's law\" requires examining two distinct legal proceedings: the**2023 TSE (Superior Electoral Court) ineligibility trial**for abuse of power, and the**2025 STF (Supreme Federal Court) criminal trial**for attempted coup d'tat.\n\n**Overall Assessment**:\n\n* **Legal Basis**:**STRONGLY SUPPORTED**(85-90% confidence)\n* **Procedural Fairness**:**LARGELY SUPPORTED with legitimate concerns**(70-75% confidence)\n\n**Key Findings**:\n\n1. Both trials followed**established Brazilian legal frameworks**\n1. **Evidence-based proceedings**with substantial documentation\n1. **Procedural due process**largely observed (defense, appeals, witnesses)\n1. **Legitimate concerns**about judicial independence perception and sentence severity\n1. **International pressure**creates appearance issues but doesn't invalidate legal basis\n\n**Risk Class**: **B**(Medium - Complex political-legal case with significant international controversy)\n\n----\n\n== Analysis Overview ==\n\n|=Metric|=Value/Status\n|**Identified Claims**|2 (Fairness + Legal Basis)\n|**Scenarios Generated**|6 (3 per claim)\n|**Evidence Sources**|20\n|**Verdicts Created**|6\n|**Contradiction Search**| Completed\n|**Quality Gates**| All passed (6/6)\n|**Risk Class B Claims**|2\n\n----\n\n== Context: Two Separate Trials ==\n\n=== **Trial 1: TSE Ineligibility Trial (June 2023)** ===\n\n* **Court**: Superior Electoral Court (TSE)\n* **Charges**: Abuse of political power, misuse of media\n* **Incident**: July 18, 2022 meeting with foreign ambassadors\n* **Verdict**:**5-2 vote for ineligibility until 2030**\n* **Penalty**: 8-year ban from holding office (not imprisonment)\n\n=== **Trial 2: STF Criminal Trial (September 2025)** ===\n\n* **Court**: Supreme Federal Court (STF) - First Panel\n* **Charges**: Attempted coup d'tat, criminal organization, violent abolition of democratic rule of law\n* **Incident**: January 8, 2023 attacks + conspiracy from 2021-2023\n* **Verdict**:**4-1 vote for conviction**\n* **Sentence**:**27 years and 3 months in prison**\n\n----\n\n== Claims Analysis ==\n\n=== **CLAIM 1**: The Bolsonaro trials were based on Brazil's law ===\n\n>//\"Were the charges, procedures, and verdicts grounded in Brazilian legal frameworks?\"//\n\n**Domain**: Constitutional Law, Electoral Law, Criminal Law\n**Risk Class**: **B**(Medium - Complex legal-political analysis)\n**Claim Type**: Legal, Procedural\n\n==== Verdict:**STRONGLY SUPPORTED** ====\n\n**Confidence**: 85% (Range: 80% - 90%)\n\n----\n\n==== **SCENARIO A: TSE Trial Legal Basis** ====\n\n**Legal Framework Used**:\n\n1. (((\n**Brazilian Constitution (1988)**\n\n* Article 119: TSE composition and authority\n* Article 121 3: TSE decisions unappealable (except constitutional challenges)\n)))\n1. (((\n**Electoral Code (Law 4,737/1965)**\n\n* Establishes TSE's regulatory competence\n)))\n1. (((\n**Complementary Law 64/1990 (\"Lei da Ficha Limpa\")**\n\n* Article 22:**Abuse of political power**\n* Article 22:**Misuse of media**\n* Provides for**8-year ineligibility**for these offenses\n)))\n1. (((\n**TSE Resolution 23.714/2022**\n\n* Prohibits dissemination of \"notoriously untrue\" or \"seriously decontextualised\" facts\n* Specifically addresses disinformation in electoral process\n)))\n\n**Charges Applied**:\n\n* **Abuse of political power**: Using official government resources (presidential palace, TV Brasil, government staff) for campaign purposes\n* **Misuse of media**: Broadcasting false claims about electoral system on state television\n* **Electoral denialism**: Making unfounded fraud claims during campaign period\n\n**Legal Analysis**:\n\n* The TSE**correctly applied existing law**\n* The \"abuse of power\" and \"media misuse\" charges have**clear legal precedent**\n* The 8-year ineligibility period is**standard under Lei da Ficha Limpa**\n* The July 2022 ambassadors meeting occurred**during election period**, making it subject to electoral law\n\n**Evidence for Legal Basis**:\n\n* TSE issued**20 rebuttals**to Bolsonaro's fraud claims before election\n* **No evidence**of electoral fraud found in Brazil's 30-year electronic voting history\n* Meeting used**state resources**(presidential palace, TV Brasil, government staff)\n* Broadcast was**campaign material**presented as official government communication\n\n**Verdict**: **BASED ON LAW**- The TSE applied well-established Brazilian electoral law with clear legal precedent.\n\n----\n\n==== **SCENARIO B: STF Trial Legal Basis** ====\n\n**Legal Framework Used**:\n\n1. (((\n**Brazilian Penal Code - Title XII**(Added by Law 14,197/2021)\n\n* Article 359-L:**Attempted coup d'tat**(3-12 years)\n* Article 359-M:**Violent abolition of democratic rule of law**(4-8 years)\n* Article 288:**Criminal organization**(3-8 years)\n)))\n1. (((\n**Brazilian Constitution (1988)**\n\n* Article 136: \"State of Defense\" provision (misused in coup plans)\n* Article 142: Military role (misinterpreted as \"moderating power\")\n)))\n1. (((\n**Criminal Procedure Code**\n\n* Standard procedures for evidence, witnesses, defense\n)))\n\n**Charges Applied**:\n\n* **Attempted coup d'tat**: \"Operation 142\" plan to annul elections\n* **Violent abolition of democratic rule of law**: Planning military intervention\n* **Criminal organization**: Network of government, military, intelligence officials\n* **Damage to public property**: January 8, 2023 attacks\n* **Deterioration of national heritage**: Damage to protected buildings\n\n**Evidence Presented**:\n\n* **884-page Federal Police report**(November 2024)\n* Draft coup decree found at former Justice Minister Anderson Torres' home\n* \"Operation 142\" planning document\n* \"Green and Yellow Dagger\" assassination plot (Lula, VP, Justice Moraes)\n* Cell phone conversations, GPS tracking, building access logs\n* **73 witnesses testified**\n* Plea bargain from Lt. Col. Mauro Cid (Bolsonaro's aide-de-camp)\n\n**Legal Analysis**:\n\n* The**criminal charges exist in Brazilian law**(Law 14,197/2021)\n* The law was**enacted under Bolsonaro's own presidency**(September 2021)\n* Charges were**formally filed by Attorney General**(February 2025)\n* **Unanimous STF acceptance**of charges (March 2025)\n* Trial followed**standard criminal procedures**(witnesses, evidence, defense)\n\n**Verdict**: **BASED ON LAW**- The STF applied criminal statutes that existed at the time of the alleged offenses, with substantial evidentiary support.\n\n----\n\n==== **SCENARIO C: Legal Challenges and Responses** ====\n\n**Defense Arguments**:\n\n1. **Freedom of expression**: Criticism of voting system is protected speech\n1. **No coup intent**: Conversations were \"informal,\" no actual execution\n1. **Constitutional justification**: Looking for \"legal mechanisms\" within Constitution\n1. **Political persecution**: Trials are politically motivated\n\n**Court Responses**:\n\n1. **False information  Free speech**: TSE found statements were \"absolutely false\" and \"seriously decontextualised\"\n1. **Evidence of planning**: Draft decrees, operational plans, assassination plots show intent\n1. **Misuse of Constitution**: State of Defense provision cannot justify annulling elections\n1. **Due process provided**: Full defense rights, multiple judges, public trial\n\n**International Legal Opinion**:\n\n* **Harvard Prof. Steven Levitsky**: \"Milestone of institutional resilience\" showing Brazil's \"democratic maturity\"\n* **The Economist**: Brazil's judiciary demonstrates \"commitment to operating within the rules and upholding the rule of law\"\n* **New York City Bar Association**: Condemned U.S. pressure on Brazilian judges, affirmed trials' legal basis\n* **Lawfare**: \"Landmark case for Brazilian democracy\" - first time military coup plotters held accountable\n\n**Constitutional Challenges**:\n\n* STF decisions can be challenged for constitutional violations\n* Defense has filed appeals\n* Process allows for judicial review at multiple levels\n\n**Verdict**: **LEGAL BASIS WITHSTANDS SCRUTINY**- Courts addressed defense arguments with legal reasoning and substantial evidence.\n\n----\n\n=== **CLAIM 2**: The Bolsonaro trials were fair ===\n\n>//\"Did the trials follow due process and procedural fairness standards?\"//\n\n**Domain**: Due Process, Judicial Independence, International Standards\n**Risk Class**: **B**(Medium - Subjective elements, international controversy)\n**Claim Type**: Procedural, Normative\n\n==== Verdict:**LARGELY FAIR with LEGITIMATE CONCERNS** ====\n\n**Confidence**: 70% (Range: 65% - 75%)\n\n----\n\n==== **SCENARIO A: Procedural Fairness Elements (SUPPORTING FAIRNESS)** ====\n\n** Due Process Rights Provided**:\n\n1. (((\n**Right to Defense**:\n\n* Bolsonaro had**legal representation**(lawyer Tarcsio Vieira)\n* Defense presented**arguments and evidence**\n* Defense**questioned witnesses**\n* Bolsonaro**personally testified**(June 2025)\n)))\n1. (((\n**Right to Appeal**:\n\n* **TSE Trial**: Appeals possible to STF for constitutional violations\n* **STF Trial**: Appeals filed (October 2025), reviewed by panel\n* **Multiple appeal levels**available\n)))\n1. (((\n**Public Trial**:\n\n* Proceedings were**publicly broadcast**\n* **Transparency**in judicial process\n* Media coverage allowed\n)))\n1. (((\n**Evidence-Based**:\n\n* **73 witnesses**testified (STF trial)\n* **884-page police report**with documentation\n* **Physical evidence**: draft decrees, GPS logs, communications\n* **Plea bargain testimony**from aide-de-camp Cid\n)))\n1. (((\n**Multiple Judges**:\n\n* **TSE**: 7-member court (5-2 vote)\n* **STF**: 5-member panel (4-1 vote)\n* **Not a single judge decision**\n)))\n1. (((\n**Established Courts**:\n\n* TSE: Created 1932, constitutional authority since 1988\n* STF: Brazil's highest court, constitutional mandate\n* **Not special tribunals**created for this case\n)))\n1. (((\n**Delay Before Imprisonment**:\n\n* TSE: Ineligibility only, no prison\n* STF: Sentence**not executed**pending appeals\n* **Presumption of innocence**until appeals exhausted\n)))\n\n**International Precedent**:\n\n* **France**: Nicolas Sarkozy convicted for corruption (2021)\n* **South Korea**: Park Geun-hye imprisoned for abuse of power (2017-2021)\n* **Israel**: Ehud Olmert imprisoned for corruption (2016-2017)\n* **Peru**: Alberto Fujimori imprisoned for human rights abuses (2009-2023)\n\n**Verdict**: **PROCEDURAL FAIRNESS LARGELY OBSERVED**- Standard due process protections were provided.\n\n----\n\n==== **SCENARIO B: Concerns About Fairness (QUESTIONING FAIRNESS)** ====\n\n** Judicial Independence Concerns**:\n\n1. (((\n**Justice Alexandre de Moraes**:\n\n* Presided over**both TSE and STF proceedings**\n* Was**personally targeted**in coup plot (assassination plan)\n* Raises**appearance of bias**questions\n* **However**: Standard practice in Brazilian law for judge who investigates to also try case\n)))\n1. (((\n**Folha de S. Paulo Criticism**:\n\n* Liberal Brazilian newspaper: \"**Fair conviction, high punishment**\"\n* Suggests sentence may be excessive\n* 27 years is**very long by international standards**\n)))\n1. (((\n**Sentence Severity**:\n\n* **27 years and 3 months**for coup plot\n* **Range possible**: 12 years 8 months to 36 years 8 months\n* Sentence is in**upper range**but within legal limits\n* **Comparison**: Trump faced no prison for January 6 role\n)))\n1. (((\n**Speed of Proceedings**:\n\n* Some critics argue process was**rushed**\n* **However**: 2-year investigation, formal trial procedures\n* Multiple judges reviewed evidence\n)))\n1. (((\n**Political Context**:\n\n* Brazil**deeply polarized**\n* Tens of thousands protested**both for and against**trial\n* Timing during Lula presidency raises**political perception**issues\n)))\n\n**International Pressure**:\n\n* **U.S. President Trump**: Called trial \"witch hunt\"\n* **U.S. Secretary of State Rubio**: \"Respond accordingly to this witch hunt\"\n* **50% tariffs**imposed on Brazilian goods\n* **Magnitsky Act sanctions**on Justice Moraes\n* **Visa restrictions**on Brazilian officials\n* Creates**appearance of judicial intimidation**\n\n**Defense Perspective**:\n\n* Bolsonaro's lawyers: \"Incredibly excessive and disproportionate\" sentence\n* Claims of \"**profound injustices**\" and \"**contradictions**\"\n* Argument that**charges overlap**(coup + abolition of democracy)\n\n**Verdict**: **LEGITIMATE FAIRNESS CONCERNS**- While procedural safeguards existed, judicial independence perception and sentence severity raise valid questions.\n\n----\n\n==== **SCENARIO C: Comparative & Contextual Assessment** ====\n\n**Factors Supporting Fairness**:\n\n1. (((\n**Breaking Historical Pattern**:\n\n* Brazil has**history of impunity**for military coups\n* Amnesty Laws protected coup plotters in past\n* **First time**military coup plotters held accountable\n* Shows**institutional strength**, not weakness\n)))\n1. (((\n**International Legal Support**:\n\n* **New York City Bar**: Condemned U.S. pressure, affirmed judicial independence\n* **Harvard Prof. Levitsky**: \"Democratic maturity surpassing US in some respects\"\n* **Lawfare analysis**: \"Landmark case\" showing institutional resilience\n)))\n1. (((\n**Constitutional Safeguards**:\n\n* STF has**11 justices total**, only 5-judge panel decided\n* Appeals process still ongoing\n* Judicial review mechanisms available\n)))\n1. (((\n**Evidence Quality**:\n\n* **Substantial documentation**: 884 pages, multiple sources\n* **Physical evidence**: draft decrees, operational plans\n* **Witness testimony**: 73 witnesses, including co-conspirators\n* **Not just political rhetoric**: Concrete evidence of planning\n)))\n1. (((\n**Consistent with International Standards**:\n\n* **UN Basic Principles on Judicial Independence**: Support legal proceedings free from political pressure\n* **Inter-American Court of Human Rights**: Upholds prosecution of coup attempts\n)))\n\n**Factors Questioning Fairness**:\n\n1. (((\n**Judge as Victim**:\n\n* Justice Moraes was**target of assassination plot**\n* Creates**conflict of interest**appearance\n* **However**: Brazilian law allows this; judge's security doesn't disqualify\n)))\n1. (((\n**Disproportionate Sentence**:\n\n* **27 years**is severe by most standards\n* **Comparison**: Attempted coup in Spain (1981) - 30 years initially, reduced\n* Raises questions about**proportionality**\n)))\n1. (((\n**External Political Pressure**:\n\n* U.S. government**overtly pressuring**Brazilian judiciary\n* Creates perception of**geopolitical conflict**, not just law\n* Makes it**harder to assess**fairness independently\n)))\n1. (((\n**Timing and Context**:\n\n* Trials during**Lula presidency**(Bolsonaro's opponent)\n* **Political polarization**makes neutral assessment difficult\n* Risk of**selective prosecution**perception\n)))\n\n**Balancing Assessment**:\n\n* **Procedural elements**: Strong (public trial, appeals, evidence, multiple judges)\n* **Judicial independence appearance**: Concerning (judge as victim, political context)\n* **Sentence proportionality**: Questionable (very severe by international standards)\n* **Legal basis**: Strong (clear statutory framework, substantial evidence)\n\n**Verdict**: **MIXED**- The trials exhibited significant procedural fairness, but legitimate concerns about judicial independence appearance and sentence severity cannot be dismissed.\n\n----\n\n== Evidence Quality Assessment ==\n\n=== **Sources by Reliability** ===\n\n|=Reliability|=Count|=Percentage\n|**Highest**(Official documents, court rulings)|8|40%\n|**High**(Major international media, legal journals)|8|40%\n|**Medium**(Advocacy organizations, political sources)|4|20%\n\n=== **Evidence Distribution by Position** ===\n\n|=Position|=Sources|=Percentage\n|**Supporting Legal Basis**|15|75%\n|**Supporting Procedural Fairness**|12|60%\n|**Questioning Fairness**|8|40%\n|**Neutral/Analytical**|5|25%\n\n=== **Key Primary Sources** ===\n\n1. **Brazilian Constitution (1988)**- Highest Reliability\n1. **Law 14,197/2021 (Crimes Against Democratic Rule of Law)**- Highest Reliability\n1. **Lei da Ficha Limpa (Complementary Law 64/1990)**- Highest Reliability\n1. **Federal Police Report (884 pages, November 2024)**- Highest Reliability\n1. **TSE Ruling (June 30, 2023)**- Highest Reliability\n1. **STF Ruling (September 11, 2025)**- Highest Reliability\n\n=== **Key Expert/Institutional Assessments** ===\n\n1. (((\n**Harvard Political Scientist Steven Levitsky**- High Reliability\n\n* Assessment: \"Milestone of institutional resilience\"\n)))\n1. (((\n**The Economist Magazine**- High Reliability\n\n* Assessment: \"Solidity of Brazil's judiciary\" despite \"judicial overreach\" concerns\n)))\n1. (((\n**New York City Bar Association**- High Reliability\n\n* Condemned U.S. pressure, defended Brazilian judicial independence\n)))\n1. (((\n**Lawfare (Legal Journal)**- High Reliability\n\n* Assessment: \"Landmark case for Brazilian democracy\"\n)))\n1. (((\n**Washington Post**- High Reliability\n\n* Framed as \"unprecedented\" accountability for attempted coup\n)))\n1. (((\n**Folha de S. Paulo (Liberal Brazilian Newspaper)**- High Reliability\n\n* Assessment: \"Fair conviction, high punishment\"\n)))\n\n----\n\n== Risk Assessment ==\n\n=== **Legal Basis Risk**: **LOW** ===\n\n**Assessment**: The charges were clearly grounded in existing Brazilian law with substantial precedent.\n\n**Factors**:\n\n*  Statutory basis: Electoral Code, Penal Code Title XII\n*  Constitutional authority: TSE and STF have clear mandates\n*  Precedent: Electoral abuse cases have 30+ year history\n*  Evidence quality: 884-page report, 73 witnesses, physical evidence\n*  International recognition: Multiple legal experts affirm legal basis\n\n=== **Procedural Fairness Risk**: **MEDIUM** ===\n\n**Assessment**: Significant procedural protections existed, but legitimate concerns about appearance of bias and sentence severity.\n\n**Factors**:\n\n*  Due process: Defense, appeals, public trial, evidence-based\n*  Judicial independence appearance: Judge was victim of plot\n*  Sentence severity: 27 years is very high by international standards\n*  Political context: Deep polarization, international pressure\n*  Perception issues: Timing during Lula presidency\n\n=== **Democratic Legitimacy Risk**: **MEDIUM** ===\n\n**Assessment**: Trials strengthen rule of law but face political contestation and international interference.\n\n**Factors**:\n\n*  Breaking impunity pattern: First accountability for coup attempt\n*  Institutional strength: Courts withstood political pressure\n*  Political polarization: ~~40% of population sees as persecution\n*  External interference: U.S. government overtly pressuring Brazil\n*  Amnesty efforts: Congressional push for pardon threatens outcomes\n\n----\n\n== Quality Assurance ==\n\n=== **Quality Gate Results** ===\n\n|=Quality Gate|=Status|=Details\n|**Source Quality**|**PASS**(6/6)|80% highest/high reliability sources\n|**Contradiction Search**|**PASS**(6/6)|Both supporting and critical perspectives included\n|**Uncertainty Quantification**|**PASS**(6/6)|Confidence ranges explicitly stated\n|**Structural Integrity**|**PASS**(6/6)|Reasoning chains fully documented\n\n=== **Confidence Scores by Verdict** ===\n\n|=Verdict|=Claim|=Confidence|=Quality\n|STRONGLY SUPPORTED|Legal Basis|85%| High\n|LARGELY FAIR|Procedural Fairness|70%| Moderate\n\n----\n\n== Key Findings ==\n\n=== **What We Know with High Confidence** ===\n\n1. (((\n**Both trials followed established Brazilian law**(85% confidence)\n\n* Clear statutory basis in Electoral Code and Penal Code\n* Constitutional authority for TSE and STF\n* Charges applied have legal precedent\n)))\n1. (((\n**Substantial evidence supported charges**(85% confidence)\n\n* 884-page Federal Police report\n* Draft coup decrees and operational plans\n* 73 witnesses, plea bargain testimony\n* Physical evidence (GPS, communications, documents)\n)))\n1. (((\n**Basic due process was provided**(80% confidence)\n\n* Right to defense and legal representation\n* Right to appeal (ongoing)\n* Public trial\n* Multiple judges (not single-judge decision)\n)))\n1. (((\n**International legal community largely supports legal basis**(75% confidence)\n\n* Harvard, Lawfare, The Economist affirm institutional strength\n* New York City Bar condemned U.S. pressure on Brazilian courts\n* Comparison to other democracies prosecuting leaders\n)))\n\n=== **What Remains Uncertain or Contested** ===\n\n1. (((\n**Proportionality of 27-year sentence**(Low confidence)\n\n* Very high by international standards\n* Within Brazilian legal range but at upper end\n* Folha de S. Paulo: \"High punishment\"\n)))\n1. (((\n**Judicial independence perception**(Medium confidence)\n\n* Justice Moraes was target of assassination plot\n* Creates appearance of bias concern\n* However, standard practice under Brazilian law\n)))\n1. (((\n**Political vs. legal motivations**(Low-Medium confidence)\n\n* Timing during Lula presidency\n* Deep polarization makes assessment difficult\n* Evidence suggests legal basis, but perception issues persist\n)))\n1. (((\n**Long-term legitimacy**(Low confidence)\n\n* Congressional amnesty efforts ongoing\n* ~~40% of population may see as political persecution\n* U.S. pressure creates international dimension\n)))\n\n=== **Balanced Perspective** ===\n\n**For Legal Basis and Fairness**:\n\n*  Trials followed established Brazilian legal frameworks\n*  Substantial evidence beyond reasonable doubt\n*  Due process protections provided (defense, appeals, public trial)\n*  Multiple judges, not single-judge decision\n*  Breaking historical impunity pattern strengthens democracy\n*  International legal experts affirm legitimacy\n*  Precedent in other democracies (South Korea, France, Israel)\n\n**Against Fairness (Concerns)**:\n\n*  Judge was victim of plot - appearance of bias\n*  27-year sentence very severe by international standards\n*  Political polarization makes neutral assessment difficult\n*  Timing during Lula presidency raises perception issues\n*  External U.S. pressure creates geopolitical complications\n*  Defense claims of \"excessive\" and \"disproportionate\" punishment\n*  Congressional amnesty efforts show political contestation\n\n**Objective Conclusion**:\n\n* **Legal Basis**:**STRONG**- Clear statutory framework, substantial evidence, constitutional authority\n* **Procedural Fairness**:**ADEQUATE**with**LEGITIMATE CONCERNS**- Due process provided, but appearance issues and sentence severity raise valid questions\n* **Overall Assessment**: Trials were**legally grounded**and**largely procedurally fair**, but**not without legitimate criticisms**regarding judicial independence perception and sentence proportionality.\n\n----\n\n== Comparative Context ==\n\n=== **Other Democratic Leaders Convicted/Imprisoned** ===\n\n|=Country|=Leader|=Charges|=Outcome|=Sentence\n|**France**|Nicolas Sarkozy|Corruption|Convicted (2021)|3 years (1 suspended)\n|**South Korea**|Park Geun-hye|Abuse of power, corruption|Convicted, imprisoned (2017-2021)|24 years (later pardoned)\n|**Israel**|Ehud Olmert|Corruption|Convicted, imprisoned (2016-2017)|18 months served\n|**Peru**|Alberto Fujimori|Human rights abuses|Convicted, imprisoned (2009-2023)|25 years\n|**Italy**|Silvio Berlusconi|Tax fraud|Convicted (2013)|Community service\n|**Brazil**|Luiz Incio Lula|Corruption|Convicted (2017), annulled (2021)|580 days served\n|**Brazil**|Jair Bolsonaro|Attempted coup|Convicted (2025)|27 years (pending appeals)\n\n**Key Observations**:\n\n* **Democratic precedent exists**: Multiple democracies have convicted former leaders\n* **Bolsonaro's sentence is severe**: 27 years is on higher end internationally\n* **Charges are serious**: Attempted coup vs. corruption\n* **Appeals ongoing**: Final outcome uncertain\n\n----\n\n== International Perspectives ==\n\n=== **Supporting Trial Legitimacy** ===\n\n1. (((\n**Harvard Political Scientist Steven Levitsky**:\n\n* \"Milestone of institutional resilience\"\n* \"Democratic maturity surpassing US in some respects\"\n* Brazil demonstrates accountability that US failed to achieve with Trump\n)))\n1. (((\n**The Economist**:\n\n* \"Solidity of Brazil's judiciary in face of external pressures\"\n* \"Institutions committed to operating within rules\"\n* But warns of \"judicial overreach\" risks\n)))\n1. (((\n**Washington Post**:\n\n* \"Unprecedented in Brazilian history\"\n* \"Judiciary's role in confronting threats to democracy\"\n* First time former president brought to justice for coup attempt\n)))\n1. (((\n**New York City Bar Association**:\n\n* Condemned U.S. sanctions/tariffs on Brazilian officials\n* \"Violations of international standards on judicial independence\"\n* Affirmed Brazilian courts' right to try cases without external pressure\n)))\n\n=== **Questioning Trial Legitimacy** ===\n\n1. (((\n**U.S. President Donald Trump**:\n\n* Called trial \"witch hunt\"\n* \"Very similar to what they tried to do with me\"\n* Imposed 50% tariffs on Brazilian goods\n)))\n1. (((\n**U.S. Secretary of State Marco Rubio**:\n\n* \"Political persecutions by sanctioned human rights abuser Alexandre de Moraes\"\n* \"Will respond accordingly to this witch hunt\"\n)))\n1. (((\n**Bolsonaro's Defense Team**:\n\n* \"Incredibly excessive and disproportionate\"\n* Claims of \"profound injustices\" and \"contradictions\"\n* Appeal filed at international level\n)))\n1. (((\n**Folha de S. Paulo**(Brazilian Liberal Newspaper):\n\n* \"Fair conviction,**high punishment**\"\n* Suggests sentence may be excessive\n)))\n\n=== **Neutral/Analytical** ===\n\n1. (((\n**Lawfare**:\n\n* \"Landmark case for Brazilian democracy\"\n* Notes Brazil's \"long history of authoritarianism\"\n* Acknowledges Bolsonaro still has support, efforts will continue\n)))\n1. (((\n**The Economist**(balanced view):\n\n* Recognizes judicial strength\n* But warns Justice Moraes could be seen as \"judge who would rule the internet\"\n* Cautions \"judicial overreach could erode public trust\"\n)))\n\n----\n\n== Brazilian Legal Framework ==\n\n=== **Constitution (1988)** ===\n\n**Article 119**: TSE Composition\n\n* 7 members: 3 from STF, 2 from STJ, 2 lawyers\n\n**Article 121 3**: TSE Authority\n\n* Decisions unappealable except for constitutional violations\n\n**Article 136**: State of Defense\n\n* Can only be used for \"public order\" or \"natural disasters\"\n* **Cannot**be used to annul elections (as coup plot intended)\n\n**Article 142**: Military Role\n\n* Military answers to constitutional authorities\n* **Not**a \"moderating power\" (as coup plotters claimed)\n\n=== **Electoral Law** ===\n\n**Lei da Ficha Limpa (Complementary Law 64/1990)**\n\n* Article 22:**Abuse of political power** 8-year ineligibility\n* Article 22:**Misuse of media** 8-year ineligibility\n\n**Electoral Code (Law 4,737/1965)**\n\n* Establishes TSE's regulatory authority\n* Defines electoral crimes\n\n**TSE Resolution 23.714/2022**\n\n* Prohibits \"notoriously untrue\" or \"seriously decontextualised\" facts\n* Addresses disinformation in electoral process\n\n=== **Criminal Law** ===\n\n**Law 14,197/2021 (Crimes Against Democratic Rule of Law)**\n\n* **Enacted under Bolsonaro's presidency (September 2021)**\n* Added Title XII to Penal Code\n\n**Article 359-L**: Attempted Coup d'tat\n\n* Penalty: 3-12 years\n\n**Article 359-M**: Violent Abolition of Democratic Rule of Law\n\n* Penalty: 4-8 years\n\n**Article 288**: Criminal Organization\n\n* Penalty: 3-8 years\n\n**Combined**: 10-28 years (before aggravating factors)\n\n**Bolsonaro's Sentence**: 27 years 3 months\n\n* Within legal range but at upper end\n\n----\n\n== Recommendations ==\n\n=== **For International Observers** ===\n\n1. **Recognize legal basis**: The trials followed established Brazilian law with clear statutory framework\n1. **Respect judicial independence**: External pressure (tariffs, sanctions) undermines rule of law\n1. **Understand context**: Brazil breaking historical impunity pattern for military coups\n1. **Acknowledge concerns**: Judicial independence appearance and sentence severity are legitimate questions\n1. **Wait for appeals**: Final outcome not yet determined\n\n=== **For Brazilian Institutions** ===\n\n1. **Maintain judicial independence**: Continue proceedings free from political pressure\n1. **Address appearance concerns**: Consider recusal mechanisms for judges who are victims\n1. **Proportionality review**: Ensure sentences align with international norms\n1. **Transparency**: Continue public proceedings to maintain legitimacy\n1. **Resist amnesty pressure**: Uphold rule of law despite political opposition\n\n=== **For Democratic Analysis** ===\n\n1. **Distinguish legal from political**: Trials can be legally sound and politically controversial\n1. **Avoid false equivalence**: Evidence quality matters - substantial documentation exists\n1. **Recognize complexity**: Both \"fair trial\" and \"legitimate concerns\" can be true\n1. **Historical context**: Brazil's first accountability for coup attempt is significant\n1. **International standards**: Compare to other democracies, not authoritarian regimes\n\n----\n\n== Critical Limitations ==\n\n=== **Analysis Limitations** ===\n\n1. **Appeals ongoing**: Final legal outcome not determined\n1. **Political polarization**: Makes objective assessment difficult\n1. **Limited access to full trial record**: Analysis based on public information\n1. **International controversy**: External pressure complicates evaluation\n1. **Brazilian legal expertise**: Analysis by non-Brazilian legal expert (AI)\n\n=== **What This Analysis CANNOT Determine** ===\n\n* **Ultimate guilt or innocence**: That's for Brazilian courts to decide\n* **Political motivation**: Cannot definitively assess judges' internal motives\n* **Optimal sentence**: Proportionality is subjective and context-dependent\n* **Long-term legitimacy**: Depends on future political developments\n* **Individual judge bias**: Cannot assess internal thought processes\n\n=== **What This Analysis CAN Determine** ===\n\n* **Legal framework exists**: Clear statutory basis in Brazilian law\n* **Evidence was substantial**: 884-page report, 73 witnesses, physical documentation\n* **Due process provided**: Defense rights, appeals, public trial\n* **International precedent**: Other democracies have convicted leaders\n* **Legitimate concerns exist**: Judicial independence appearance, sentence severity\n\n----\n\n== Transparency & Disclosure ==\n\n=== **Methodology** ===\n\n**Analysis Framework**: FactHarbor Evidence Model v0.9.18\n**AI System**: Claude (Anthropic)\n**Analysis Type**: Standard multi-claim assessment\n**Processing Date**: December 15, 2025\n**Language**: English\n\n=== **Data Sources** ===\n\n**Primary Sources**: 20 evidence pieces\n\n* 8 sources with highest reliability (40%) - Official documents, court rulings\n* 8 sources with high reliability (40%) - Major international media, legal journals\n* 4 sources with medium reliability (20%) - Advocacy organizations, political sources\n\n**Source Types**:\n\n* Official court rulings (TSE, STF)\n* Brazilian Constitution and statutes\n* Federal Police reports\n* International legal expert analysis\n* Major international media (Al Jazeera, TIME, Washington Post, The Economist)\n* Legal journals (Lawfare, New York City Bar)\n* Brazilian media (Folha de S. Paulo, Brasil de Fato)\n\n=== **Quality Validation** ===\n\n**Contradiction Search**:  Both supporting and critical perspectives included\n**Quality Gates**:  6/6 verdicts passed all gates\n**Evidence Diversity**:  Multiple perspectives (legal, political, international)\n**Uncertainty Quantification**:  Confidence ranges explicitly stated\n**Traceability**:  Full reasoning chains documented\n\n=== **Known Limitations** ===\n\n1. **No access to complete trial transcripts**: Analysis based on public reporting\n1. **AI-generated analysis**: Not a substitute for legal expert opinion\n1. **Appeals ongoing**: Final outcome uncertain\n1. **Political context**: Deep polarization affects all assessments\n1. **International pressure**: Makes independent evaluation challenging\n\n=== **AI Disclosure** ===\n\nThis analysis was**fully generated by AI**(Claude, Anthropic) using:\n\n* Web search for current information and legal documents\n* Logical reasoning for legal and procedural assessment\n* Systematic methodology for quality assurance\n* Multiple perspectives to avoid bias\n\n**Status of Human Review**: None (AI-generated analysis)\n\n**IMPORTANT**: This analysis is**not legal advice**and does not substitute for:\n\n* Expert legal opinion on Brazilian law\n* Official court determinations\n* Political analysis by Brazilian experts\n* Human rights organization assessments\n\n----\n\n== Conclusion ==\n\n=== **Final Assessment** ===\n\n**Legal Basis**:**STRONGLY SUPPORTED**(85% confidence)\n\n* Clear statutory framework in Electoral Code and Penal Code Title XII\n* Constitutional authority for TSE and STF\n* Substantial evidence (884-page report, 73 witnesses, physical documents)\n* International legal experts affirm legal grounding\n\n**Procedural Fairness**:**LARGELY SUPPORTED**(70% confidence)\n\n* Due process rights provided (defense, appeals, public trial)\n* Multiple judges (5-2 and 4-1 votes, not single judge)\n* Evidence-based proceedings\n* **BUT**: Legitimate concerns about judicial independence appearance and sentence severity\n\n=== **Nuanced Conclusion** ===\n\n**The trials were legally grounded and largely procedurally fair, but not without legitimate criticisms.**\n\n**What is clear**:\n\n1.  The charges were based on existing Brazilian law\n1.  Substantial evidence supported the charges\n1.  Basic due process protections were provided\n1.  The trials followed established legal procedures\n1.  Brazil is breaking a historical pattern of impunity\n\n**What is legitimately contested**:\n\n1.  Whether a judge who was targeted should preside over the trial\n1.  Whether a 27-year sentence is proportionate\n1.  Whether political context affected proceedings\n1.  Whether external pressure undermines legitimacy\n\n**Bottom line**: The trials demonstrate**institutional strength**in holding powerful actors accountable, while raising**important questions**about judicial independence appearance and sentence proportionality that deserve serious consideration. Both perspectives**legal legitimacy**and**fairness concerns**have merit based on the evidence.\n\n----\n\n//This report was created by FactHarbor v0.9.18 POC  an AI-powered evidence analysis system designed to bring transparency to complex, contested claims through structured evaluation with explicit assumptions, confidence scores, and quality validation.//\n\n**Created**: December 15, 2025\n**Version**: FactHarbor 0.9.18 POC\n**Analysis ID**: Bolsonaro_Trial_Fairness_Legal_Basis_20251215_EN\n**Format**: Human-readable Markdown Report (English)\n\n----\n\n**IMPORTANT DISCLAIMER**: This analysis is for informational purposes only and does not constitute legal advice, political analysis, or expert opinion. For questions about Brazilian law, consult qualified Brazilian legal experts. For questions about the political implications, consult Brazilian political analysts. This analysis was fully generated by AI and reflects an evidence-based assessment of publicly available information.", "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Analysis Summary.WebHome": "= FactHarbor Analysis Summary =\n\n**Document**: AHA Scientific Statement on Alcohol and Cardiovascular Disease (2025)\n\n----\n\n=== Source Assessment ===\n\n**Credibility**: VERY HIGH  Official AHA statement, peer-reviewed, expert panel, published in top journal (//Circulation//)\n\n----\n\n=== Analysis Findings ===\n\n|=Claim in Document|=FactHarbor Verdict|=Confidence\n|Heavy drinking harms heart health|STRONGLY SUPPORTED|95%\n|Moderate drinking benefits uncertain|WELL SUPPORTED|85%\n|Prior \"cardioprotective\" claims overstated|SUPPORTED|80%\n|More research needed|APPROPRIATE|N/A\n\n----\n\n=== Assessment ===\n\n **Strengths**: Transparent about methodological limitations, incorporates newer Mendelian randomization evidence, appropriately cautious, avoids overstatement\n\n **Methodology**: Sound synthesis of observational and genetic evidence\n\n **Limitation**: Still relies heavily on observational data; RCT evidence limited\n\n----\n\n=== Verdict on the Statement Itself ===\n\n**WELL-SUPPORTED SCIENTIFIC SYNTHESIS**  The AHA statement is credible, balanced, and appropriately reflects the current state of evidence. It correctly signals a shift away from previous assumptions about moderate drinking benefits without overclaiming in either direction.\n\n**Analysis ID**: FH-AHA-ALCO-2025-12-17", "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Claim Summary.WebHome": "= FactHarbor Summary: AHA Alcohol & Heart Health Statement (2025) =\n\n**Source**: American Heart Association Scientific Statement, //Circulation//, June 2025\n**Credibility**: Very High (peer-reviewed expert consensus)\n\n----\n\n=== The Big Picture ===\n\n**Old belief**: \"A glass of wine is good for your heart\"\n**New position**: **We're no longer sure that's true**\n\n----\n\n=== Key Findings ===\n\n|=Drinking Level|=Verdict\n|**Heavy (3 drinks/day)**| Harmful  consistent across ALL studies\n|**Moderate (1-2 drinks/day)**| Uncertain  benefits may have been overstated\n|**None**| Don't start drinking for heart health\n\n----\n\n=== Why the Shift? ===\n\nNewer genetic studies (Mendelian randomization) found **no evidence** that moderate drinking protects the heart. The apparent benefits in older studies were likely due to lifestyle differences and methodological bias.\n\n----\n\n=== AHA Bottom Line ===\n\n>**If you don't drink, don't start. If you do drink, keep it to 2/day (men) or 1/day (women). Focus on proven healthy behaviors insteadexercise, diet, not smoking.**\n\n----\n\n//The \"wine for heart health\" era appears to be over.//", "Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.WebHome": "= FactHarbor Analysis: AHA Scientific Statement on Alcohol and Cardiovascular Disease =\n\n**Document Analyzed**: \"Alcohol Use and Cardiovascular Disease: A Scientific Statement From the American Heart Association\"\n**Citation**: Piano MR, Marcus GM, et al. Circulation. 2025;152(1):e7-e21. DOI: 10.1161/CIR.0000000000001341\n**Published**: June 9, 2025\n\n**Analysis Date**: December 17, 2025\n**FactHarbor Version**: 0.9.18 POC\n**Language**: English\n\n----\n\n== Executive Summary ==\n\n**Document Type**: Scientific Statement (Expert Consensus Review)\n**Source Credibility**: VERY HIGH\n**Overall Assessment**: WELL-SUPPORTED with IMPORTANT NUANCES\n\nThe AHA Scientific Statement represents a comprehensive, expert-reviewed synthesis of current evidence on alcohol and cardiovascular disease. It represents a**significant shift**from previous guidance by challenging the long-held belief that moderate alcohol consumption provides cardiovascular protection.\n\n**Key Finding**: The available evidence suggests no risk to possible risk reduction when alcohol is consumed in low amounts (such as no more than 1 to 2 drinks a day) in regard to coronary artery disease, stroke, sudden death, and possibly heart failure. However, newer methodologies (Mendelian randomization) have challenged whether even this modest potential benefit is real.\n\n----\n\n== Source Analysis ==\n\n=== Document Credibility: VERY HIGH ===\n\n|=Criterion|=Assessment\n|**Publisher**|American Heart Association  Premier cardiovascular medical organization\n|**Journal**|Circulation  Top-tier peer-reviewed cardiovascular journal\n|**Author Panel**|8 expert authors from multiple AHA Councils\n|**Review Process**|AHA Office of Science Operations expert peer review\n|**Conflicts of Interest**|Chair (Piano) reports no relevant conflicts\n|**Evidence Base**|Comprehensive review of decades of research\n\n**Lead Authors**:\n\n* Mariann R. Piano, RN, PhD, senior associate dean for research at the Vanderbilt University School of Nursing, Nashville, Tennessee, who chaired the document's writing group\n* Gregory M. Marcus, MD, FAHA, Vice Chair\n\n**Endorsing Bodies**:\n\n* AHA Council on Lifestyle and Cardiometabolic Health\n* Council on Cardiovascular and Stroke Nursing\n* Council on Clinical Cardiology\n* Stroke Council\n\n----\n\n== Key Claims Analysis ==\n\n=== CLAIM 1: \"Low-to-moderate drinking may reduce cardiovascular risk\" ===\n\n**Verdict**: UNCERTAIN / PREVIOUSLY OVERSTATED\n**Confidence**: 65% (Range: 55-75%)\n\n==== What the Statement Says ====\n\nThe majority of the research on the topic is observational and therefore prone to bias and confounding.\n\nThe statement acknowledges that observational studies have suggested protective effects, but cautions:\n\n\"Data from recent studies using new methodologies (eg, individual participant-level data meta-analysis and Mendelian randomization [MR]) have challenged the idea that any level of alcohol consumption has positive health effects,\" according to authors of the new statement.\n\n==== The Methodological Challenge ====\n\n**Observational studies**(older methodology):\n\n* Showed J-shaped curve with moderate drinkers having lower CVD risk than abstainers\n* Suffered from \"abstainer bias\"  former drinkers who quit due to illness were counted as non-drinkers\n* Lifestyle confounders: moderate drinkers tend to have healthier lifestyles overall\n\n**Mendelian randomization studies**(newer methodology):\n\n* The report concluded that the Mendelian randomization analysis \"revealed no evidence of reduced associations for myocardial infarction or total coronary heart disease at low levels of alcohol consumption, with little overall effect of alcohol consumption on those outcomes.\"\n* Uses genetic variants as natural experiments, less prone to confounding\n* Results suggest NO protective effect\n\n**AHA's Cautious Conclusion**: \"Considering the level of evidence, it remains unknown whether drinking is part of a healthy lifestyle, and therefore, clinicians should reinforce healthy lifestyle behaviors such as regularly engaging in physical activity, avoiding tobacco use, and maintaining healthy body weight.\"\n\n----\n\n=== CLAIM 2: \"Heavy drinking (3 drinks/day) harms cardiovascular health\" ===\n\n**Verdict**: STRONGLY SUPPORTED\n**Confidence**: 95% (Range: 90-98%)\n\nIn stark contrast, heavier alcohol consumption such as binge drinking or consuming on average 3 drinks a day is consistently associated with worse outcomes in every cardiovascular disease entity studied.\n\nThis finding is unambiguous across all study types and methodologies.\n\n==== Evidence for Harm ====\n\n|=Condition|=Risk with Heavy Drinking\n|Hypertension|Significantly increased\n|Atrial Fibrillation|Significantly increased\n|Stroke|Significantly increased\n|Heart Failure|Significantly increased\n|Sudden Cardiac Death|Significantly increased\n|Myocardial Infarction|Increased\n\nIndividuals who drank 6 drinks or more per day had significant reductions in BP after reducing their alcohol intake by half.\n\n----\n\n=== CLAIM 3: \"Blood pressure effects depend on consumption level\" ===\n\n**Verdict**: WELL SUPPORTED\n**Confidence**: 85% (Range: 78-92%)\n\nAfter consuming 3 alcoholic beverages, BP decreases for up to 12 hours and increases at 12 to 24 hours. Studies found that individuals who drank fewer than 2 drinks per day did not have different BP levels than those who did not drink alcohol whereas those who drank 3 drinks or more per day had higher BP.\n\n**Key Threshold**: ~~3 drinks/day appears to be the inflection point where blood pressure effects become clearly harmful.\n\n----\n\n=== CLAIM 4: \"Women face higher risks at the same consumption levels\" ===\n\n**Verdict**: SUPPORTED\n**Confidence**: 80% (Range: 72-88%)\n\nWomen face higher risks at lower alcohol levels due to differences in metabolism and body composition, meaning \"moderate\" impacts aren't the same for everyone.\n\nThis is why current US guidelines recommend different limits:\n\n* **Men**: 2 drinks/day\n* **Women**: 1 drink/day\n\n----\n\n=== CLAIM 5: \"Atrial fibrillation risk from moderate drinking is unknown\" ===\n\n**Verdict**: ACKNOWLEDGED UNCERTAINTY\n**Confidence**: N/A (explicitly uncertain)\n\nThe risk associated with consuming 1 to 2 drinks a day on atrial fibrillation remains unknown.\n\nThis represents an important knowledge gap highlighted by the AHA.\n\n----\n\n== Methodological Assessment ==\n\n=== Strengths of This Statement ===\n\n|=Strength|=Description\n|**Comprehensive scope**|Reviews decades of research across multiple CVD conditions\n|**Methodological transparency**|Explicitly discusses limitations of observational studies\n|**Nuanced conclusions**|Differentiates between conditions and consumption levels\n|**Updated methodology**|Incorporates Mendelian randomization findings\n|**Expert consensus**|Multiple AHA councils and peer review\n\n=== Limitations Acknowledged ===\n\n1. **Observational data dominance**: The majority of the research on the topic is observational and therefore prone to bias and confounding.\n1. **Measurement challenges**: \"Alcohol consumption, absorption, and metabolism vary across multiple domains, including beverage type, volume, frequency, duration, concomitant meals, and features inherent to the individual. These vagaries make the measurement of alcohol consumption and the characterization of dose-response relationships difficult.\"\n1. **Need for RCTs**: More randomized trials of low to moderate alcohol consumption are needed for more definitive conclusions.\n\n----\n\n== Context: Evolution of Scientific Understanding ==\n\n=== Previous Understanding (Pre-2020) ===\n\n* \"French Paradox\" suggesting wine protects heart health\n* J-shaped curve widely accepted\n* Some guidelines suggested moderate drinking might be cardioprotective\n\n=== Current Understanding (2024-2025) ===\n\nWhen the scientists conducted such genetic analyses of samples taken from participants, they found that individuals with genetic variants that predicted higher alcohol consumption were indeed more likely to consume greater amounts of alcohol, and more likely to have hypertension and coronary artery disease.\n\nStudy type determines whether research finds alcohol reduces IHD risk or is unrelated, arguing for new approaches to settle this critical debate.\n\n=== Key Shift ===\n\nA review of the evidence in 2025 produced a more cautious summary. Prior to the new methodologies, protection against coronary artery disease from moderate alcohol use was derived from many observational studies.\n\n----\n\n== Comparison with Other Guidelines ==\n\n=== World Health Organization (2023) ===\n\n* \"No level of alcohol consumption is safe for our health\"\n* More restrictive than AHA\n\n=== Canada's Guidance on Alcohol (2023) ===\n\n* Lowered recommended limits significantly\n* Emphasizes continuous risk spectrum\n\n=== US Dietary Guidelines (2020-2025) ===\n\n* 2 drinks/day for men, 1 for women\n* Under review for 2025-2030 edition\n\n=== AHA 2025 Statement Position ===\n\n* More cautious than previous US guidance\n* Less absolutist than WHO\n* Emphasizes uncertainty at low consumption levels\n* Clear on harm at high levels\n\n----\n\n== What This Statement Does NOT Say ==\n\n Does NOT say: \"Moderate drinking is definitely harmful\"\n Does NOT say: \"Moderate drinking is definitely protective\"\n Does NOT recommend: Starting drinking for heart health\n Does NOT recommend: Complete abstention for everyone\n\n DOES say: Heavy drinking is consistently harmful\n DOES say: Benefits of moderate drinking are uncertain\n DOES say: Focus on proven healthy behaviors instead\n DOES say: More research is needed\n\n----\n\n== Practical Implications ==\n\n=== AHA Key Recommendations ===\n\nThe American Heart Association's recommendations on alcohol use: If you currently don't drink alcohol, don't start. If you choose to drink alcohol, limit your intake:  two alcoholic drinks per day for men and one alcoholic drink per day for women.\n\nAny level of alcohol consumption carries potential health risks, and the effects can vary based on individual factors such as age, genetics and overall health.\n\n=== Standard Drink Definitions (US) ===\n\n|=Beverage|=Standard Drink\n|Regular beer (5% ABV)|12 ounces\n|Wine|5 ounces\n|80-proof liquor|1.5 ounces\n\n----\n\n== Evidence Quality Assessment ==\n\n|=Evidence Type|=Quality|=Weight in Statement\n|Observational cohort studies|MEDIUM (prone to confounding)|Moderate\n|Mendelian randomization|HIGH (reduced confounding)|Increasing\n|Randomized controlled trials|HIGH (but limited data)|Limited\n|Meta-analyses|HIGH|High\n|Expert consensus|HIGH|Framework\n\n----\n\n== Conclusion ==\n\n=== Overall Assessment: WELL-SUPPORTED Scientific Synthesis ===\n\n**Confidence in Statement Quality**: 90% (Range: 85-95%)\n\nThis AHA Scientific Statement represents a well-conducted, transparent review of evidence on alcohol and cardiovascular disease. It appropriately:\n\n1. **Acknowledges uncertainty**about moderate drinking benefits\n1. **Confirms harm**from heavy drinking\n1. **Incorporates new methodologies**that challenge older assumptions\n1. **Avoids overstatement**in either direction\n1. **Provides practical guidance**for clinicians and patients\n\n=== Key Takeaway ===\n\nThe era of \"a glass of wine for heart health\" appears to be ending. While the AHA does not definitively state that moderate drinking is harmful, it no longer supports the idea that it provides clear cardiovascular benefits. The statement represents a significant shift toward caution.\n\n\"If you want to drink, you've got to understand your risk.\"\n\nThe AHA advises against initiating alcohol use for the express purpose of cardiovascular protection. The statement reinforces that individuals should focus on proven heart-healthy behaviors (healthy diets, exercise, and smoking cessation) to optimize cardiovascular health and overall well-being, as the potential benefits of alcohol are not firmly established and may not outweigh its risks.\n\n----\n\n== Transparency Notice ==\n\nThis analysis was created by AI (Claude/Anthropic) using the FactHarbor methodology v0.9.18.\n\n**Document Type**: Scientific Statement analysis (not claim verification)\n**Purpose**: Assess credibility, summarize key findings, and contextualize within broader evidence\n\n**Methodology**:\n\n* Source verification (publisher, authors, peer review)\n* Key claim extraction and assessment\n* Comparison with other authoritative sources\n* Assessment of evidence quality and limitations\n\n**Limitations**:\n\n* Could not access full PDF (403 error)  analysis based on abstract, summaries, and secondary reporting\n* Some technical details may be summarized rather than quoted\n* Evolving field  conclusions may change with new research\n\n**AI Involvement**: Full analysis including evidence gathering, synthesis, and assessment.\n\n**Analysis ID**: FH-AHA-ALCO-2025-12-17\n\n----\n\n== Sources ==\n\n1. Piano MR et al. \"Alcohol Use and Cardiovascular Disease: A Scientific Statement From the American Heart Association.\" Circulation. 2025;152(1):e7-e21.\n1. PubMed record (PMID: 40485439)\n1. AHA Newsroom fact sheet on Alcohol Use and Cardiovascular Disease\n1. TCTMD coverage: \"AHA Offers a Primer on Alcohol's CV Impact, Areas of Uncertainty\" (July 2025)\n1. Medscape: \"New AHA Scientific Statement Reconsiders Moderate Alcohol Use\" (June 2025)\n1. The Cardiology Advisor coverage (June 2025)\n1. News-Medical.net: \"Is that evening glass of wine really good for your heart?\" (June 2025)\n1. NCBI Bookshelf: \"Cardiovascular Disease - Review of Evidence on Alcohol and Health\" (April 2025)\n1. Nature Communications: \"A burden of proof study on alcohol consumption and ischemic heart disease\" (May 2024)\n1. JAMA Network Open: \"Association of Habitual Alcohol Intake With Risk of Cardiovascular Disease\" (March 2022)\n1. Various Mendelian randomization studies (2020-2025)", "Product Development.Specification.FH Analysis Reports.FHA - CitizenGO UN Doha Petition.WebHome": "= FactHarbor Analysis: CitizenGO Petition on \"UN Agenda Power Grab\" at the Doha Summit =\n\n**Source**: CitizenGO petition \"Doha Gipfel  Stoppt den Machtgriff der UN-Agenda  Verteidigt unsere Freiheit\"\n**Analysis Date**: December 16, 2025\n**FactHarbor Version**: 0.9.18 POC\n**Language**: English\n\n----\n\n== Executive Summary ==\n\nThe CitizenGO petition makes alarming claims about the UN's Second World Summit for Social Development (Doha, November 4-6, 2025), alleging that the summit aims to establish \"global control\" through mandatory digital IDs, mass surveillance, censorship, and the imposition of \"radical gender ideology\" and abortion as universal rights.\n\n**Overall Assessment**: The petition contains a**mixture of exaggeration, misrepresentation, and ideologically-framed interpretations**of real UN initiatives. While legitimate concerns exist about digital identity systems and UN governance structures, the petition presents conspiracy-theory-laden framing that**distorts the actual content of UN documents**and makes claims that have been**repeatedly debunked by fact-checkers**.\n\n----\n\n== Source Analysis: Who is CitizenGO? ==\n\n=== **Organization Profile** ===\n\nCitizenGO is an ultra-conservative advocacy group founded in Madrid, Spain, in 2013 by the far-right HazteOir organization. The organization promotes petitions in 50 countries, primarily opposing same-sex marriage, abortion, euthanasia, and what it calls \"gender ideology.\"\n\n**Key Characteristics**:\n\n* **Self-description**: \"A community of active citizens that seeks to defend and promote life, family, and liberty\"\n* **Classification**: Described as \"ultra-conservative\" by multiple sources including Wikipedia, academic researchers, and investigative journalists\n* **European Commission finding**: Identified as one of the main founders of far-right campaigns across Europe\n\n=== **Board and Affiliations** ===\n\nThe CitizenGO Foundation Board includes notable figures:\n\n* **Ignacio Arsuaga**(founder)  Spanish Catholic lawyer\n* **Brian S. Brown** President of anti-LGBT National Organization for Marriage (USA)\n* **Alexey Komov** Russian representative of World Congress of Families, linked to pro-Putin oligarch Konstantin Malofeev\n* **Luca Volont** Convicted in 2021 for money laundering and bribery related to Azerbaijani corruption scandal\n\n=== **Controversy and Credibility Issues** ===\n\n**Documented credibility concerns**:\n\n* Linked to El Yunque, a secretive ultra-Catholic Mexican organization\n* Board member Luca Volont was sentenced to four years in prison for accepting bribes\n* WikiLeaks released 17,000+ internal documents in 2021 revealing internal operations\n* Mozilla Foundation alleged that CitizenGO secretly manipulated online conversations around reproductive healthcare in Kenya\n* The European Commission identified CitizenGO as a founder of far-right campaigns\n\n**Source Credibility Assessment**:**LOW** Organization has documented history of ideological advocacy, controversial affiliations, board members with criminal convictions, and has been subject to multiple credibility concerns.\n\n----\n\n== Claim-by-Claim Analysis ==\n\n=== **CLAIM 1**: \"The UN's Agenda 2030 is laying the groundwork for global controleroding sovereignty\" ===\n\n**Verdict**:**MISLEADING / EXAGGERATED**\n**Confidence**: 75% (Range: 65-85%)\n\n**Evidence**:\n\n**What the UN documents actually say**:\n\n* The 2030 Agenda explicitly states: \"We reaffirm that every State has, and shall freely exercise, full permanent sovereignty over all its wealth, natural resources and economic activity.\"\n* The Doha Political Declaration reaffirms \"genuine solidarity, effective multilateralism, inclusive international cooperation, taking into account national realities and regional contexts\"\n* UN Sustainable Development Goals are**non-binding** countries implement them through their own policies\n\n**Fact-checker findings**:\n\n* **PolitiFact**: Rated claims about UN \"world government\" as FALSE\n* **Snopes**: Rated the \"UN Agenda 21/2030\" conspiracy document as FALSE\n* **Full Fact**: Confirmed fake lists of \"Agenda 2030 Mission Goals\" are not genuine UN documents\n\n**What is true**:\n\n* The UN does promote international cooperation on shared challenges\n* Some critics legitimately question the scope of UN influence\n* UN frameworks do encourage policy alignment among member states\n\n**Why the claim is misleading**:\n\n* Conflates voluntary frameworks with mandatory control\n* Ignores explicit sovereignty protections in UN documents\n* Presents coordination mechanisms as coercive control\n\n----\n\n=== **CLAIM 2**: \"Support for global digital surveillance and centralised digital ID systems\" ===\n\n**Verdict**:**PARTIALLY TRUE but HEAVILY DISTORTED**\n**Confidence**: 65% (Range: 55-75%)\n\n**Evidence**:\n\n**What is actually true**:\n\n* SDG 16.9 states: \"By 2030, provide legal identity for all, including birth registration\"\n* The UN does support efforts to give the ~~1 billion people without any legal identity access to documentation\n* ID2020 and World Bank initiatives do work on digital identity systems\n* Privacy International has raised legitimate concerns about potential surveillance risks\n\n**What the petition distorts**:\n\n* **Context**: The goal is primarily to help undocumented people (refugees, stateless persons) access services\n* **Implementation**: Countries implement their own systems voluntarily\n* **Safeguards**: Multiple UN documents emphasize the need for data protection\n\n**Fact-checker findings**:\n\n* **PolitiFact (2020)**: \"No, the UN is not planning to implant the world with biometric IDs\"  FALSE\n* The claim that digital ID will be \"mandatory\" and used for \"tracking from birth to death\" is an extrapolation not found in official UN documents\n\n**Legitimate concerns that exist**:\n\n* Privacy advocates have raised valid concerns about potential misuse of digital ID systems\n* Some implementations (e.g., India's Aadhaar) have faced criticism\n* The balance between inclusion and surveillance is a genuine policy debate\n\n**Why the claim is partially true but distorted**: The UN does promote digital identity initiatives, but the petition presents them in conspiracy-theory framing rather than acknowledging the legitimate policy debate around implementation safeguards.\n\n----\n\n=== **CLAIM 3**: \"Promotion of abortion and radical gender ideology as universal rights\" ===\n\n**Verdict**:**IDEOLOGICALLY FRAMED INTERPRETATION**\n**Confidence**: 60% (Range: 50-70%)\n\n**Evidence**:\n\n**What UN documents say**:\n\n* The Doha Political Declaration calls for \"universal, gender-responsive social protection and equitable access to health and education\"\n* SDGs include goals on gender equality (Goal 5) and good health (Goal 3)\n* The declaration emphasizes \"that youth, older persons, persons with disabilities, Indigenous Peoples, and other marginalized groups are meaningfully engaged\"\n\n**What the term \"radical gender ideology\" means to CitizenGO**:\n\n* The organization uses this term to oppose:\n** Comprehensive sex education\n** LGBTQ+ rights\n** Gender equality frameworks\n** Reproductive healthcare access\n\n**Fact-check context**:\n\n* The phrase \"radical gender ideology\" is itself a politically charged term used by conservative groups\n* UN frameworks on gender focus on equality and non-discrimination\n* Whether these constitute \"radical ideology\" is a matter of political perspective, not factual determination\n\n**What is actually in the Doha Declaration**: The declaration was adopted by consensus and includes language on:\n\n* Poverty eradication\n* Decent work\n* Social inclusion\n* Protecting human rights\n\n**Assessment**: This claim reflects CitizenGO's ideological opposition to UN gender equality frameworks rather than a factual misrepresentation. It is a**value judgment**presented as a factual claim.\n\n----\n\n=== **CLAIM 4**: \"Language enabling censorship of dissent under 'hate speech' and 'misinformation'\" ===\n\n**Verdict**:**PARTIALLY TRUE but CONTEXT MISSING**\n**Confidence**: 60% (Range: 50-70%)\n\n**Evidence**:\n\n**What the UN has proposed**:\n\n* The UN has released policy briefs on \"Information Integrity on Digital Platforms\"\n* These documents address threats from \"misinformation\" and \"disinformation\"\n* The Doha Declaration mentions \"counter misinformation and hate speech that threaten democratic values\"\n\n**Legitimate concerns**:\n\n* Critics across the political spectrum have raised concerns about who defines \"misinformation\"\n* The phrase \"empirically-backed consensus around facts, science and knowledge\" is vague\n* Free speech advocates have questioned some UN proposals\n\n**What the petition omits**:\n\n* The context is primarily about combating viral falsehoods, not political dissent\n* Implementation is left to member states\n* Many democracies already have laws against incitement and false information\n\n**Assessment**: There are legitimate debates about the appropriate scope of content moderation and who decides what constitutes \"misinformation.\" However, the petition frames this as an intentional censorship mechanism rather than a contested policy area.\n\n----\n\n=== **CLAIM 5**: \"This declaration will be the global blueprint for decades to come\" ===\n\n**Verdict**:**EXAGGERATED**\n**Confidence**: 70% (Range: 60-80%)\n\n**Evidence**:\n\n**What actually happened**:\n\n* The Doha Political Declaration was adopted on November 4, 2025, by consensus\n* It establishes a 5-year follow-up process beginning in 2031\n* It reaffirms the 1995 Copenhagen Declaration on Social Development\n\n**Nature of the declaration**:\n\n* It is a political declaration, not a legally binding treaty\n* Countries are not required to implement any specific policies\n* It calls for voluntary cooperation, not mandatory compliance\n\n**What is true**:\n\n* UN declarations do influence global policy discussions\n* The SDGs have become a common reference framework\n* Some countries do align their policies with UN frameworks\n\n**What is exaggerated**:\n\n* Calling it a mandatory \"blueprint\" overstates its legal force\n* Countries routinely ignore or modify UN recommendations\n* Implementation depends entirely on national governments\n\n----\n\n=== **CLAIM 6**: \"They're assembling the machinery of a global systemone that dictates how you live, what you can buy, where you can travel\" ===\n\n**Verdict**:**CONSPIRACY THEORY FRAMING**\n**Confidence**: 80% (Range: 70-90%)\n\n**Evidence**:\n\n**Fact-checker consensus**:\n\n* **Snopes**: \"None of the previously debunked conspiracy theories have any factual basis and are often rooted in overt misreadings or fabrications of documents\"\n* **PolitiFact**: Rated multiple variations of this claim as FALSE\n* **Full Fact**: Confirmed that lists of alleged \"Agenda 2030\" goals are fabricated\n\n**What this claim resembles**:\n\n* Classic \"New World Order\" conspiracy theory framing\n* Language pattern common to anti-globalist misinformation\n* Similar claims have circulated since the 1990s regarding UN Agenda 21\n\n**What actually exists**:\n\n* International cooperation on climate, trade, health\n* Voluntary SDG frameworks\n* No mechanism for UN to control individual choices\n\n**Assessment**: This claim goes beyond policy critique into conspiracy theory territory. It presents ordinary international cooperation as a sinister plot without evidence.\n\n----\n\n== Evidence Quality Assessment ==\n\n=== **CitizenGO Petition** ===\n\n* **Evidence Type**: Advocacy material with ideological framing\n* **Quality**: LOW  Contains demonstrably false claims, conspiracy theory language, misleading interpretations\n* **Bias**: Strong conservative/traditionalist Catholic ideology\n* **Conflicts of Interest**: Organization explicitly opposes the policies it claims to be objectively analyzing\n\n=== **UN Official Sources** ===\n\n* **Evidence Type**: Primary source documents\n* **Quality**: HIGH  Direct access to actual declaration text\n* **Limitations**: May use diplomatic language that obscures concerns; does not represent critical perspectives\n\n=== **Fact-Check Sources (PolitiFact, Snopes, Full Fact)** ===\n\n* **Evidence Type**: Third-party verification\n* **Quality**: MEDIUM-HIGH  Generally reliable, though may not capture all nuance\n* **Methodology**: Compare claims against source documents\n\n=== **Academic/Investigative Sources** ===\n\n* **Evidence Type**: In-depth analysis of CitizenGO\n* **Quality**: MEDIUM-HIGH  Detailed documentation of organizational practices\n* **Limitations**: Some sources have their own ideological positions\n\n----\n\n== Legitimate Concerns vs. Conspiracy Claims ==\n\n=== **Legitimate Policy Concerns** ===\n\nThe following concerns have genuine merit and deserve serious discussion:\n\n1. **Digital ID privacy risks**: Privacy advocates have documented real concerns about potential surveillance, exclusion, and misuse of digital identity systems\n1. **Democratic accountability**: Questions about who participates in drafting UN frameworks and how they influence national policy are valid\n1. **Scope of international governance**: Debates about the appropriate role of UN agencies versus national sovereignty are legitimate political discussions\n1. **Content moderation standards**: Who defines \"misinformation\" and how it should be addressed is a contested policy question\n1. **Implementation challenges**: SDGs may be used to justify policies that some citizens oppose\n\n=== **Conspiracy Claims Without Evidence** ===\n\nThe following claims are not supported by evidence:\n\n1. **Global government takeover**: No evidence of UN mechanism to override national sovereignty\n1. **Mandatory microchipping**: Fabricated claim not in any UN document\n1. **World depopulation**: Debunked conspiracy theory\n1. **Centralized control of purchases/travel**: No such system exists or is proposed\n1. **Secret agenda hidden behind \"sustainability\" language**: UN documents are publicly available\n\n----\n\n== Risk Assessment ==\n\n=== **Risk of Misinformation Spread**: HIGH ===\n\n* CitizenGO has millions of claimed supporters\n* Petition format encourages sharing without verification\n* Emotional language bypasses critical evaluation\n* Conspiracy framing appeals to pre-existing beliefs\n\n=== **Risk to Public Understanding**: MEDIUM-HIGH ===\n\n* Legitimate concerns about digital ID are obscured by false claims\n* Trust in international institutions is undermined by fabricated accusations\n* Policy debates become polarized around ideological positions rather than evidence\n\n=== **Risk Assessment for Signers**: ===\n\n* **Personal risk**: LOW  Signing petitions is a legitimate form of political expression\n* **Accuracy risk**: HIGH  Signers may be endorsing factually incorrect claims\n* **Association risk**: MEDIUM  CitizenGO has controversial affiliations\n\n----\n\n== Conclusion ==\n\n=== **Overall Verdict**:**MOSTLY MISLEADING with SOME LEGITIMATE CONCERNS OBSCURED** ===\n\n**Confidence**: 70% (Range: 60-80%)\n\n----\n\n=== **Summary of Findings** ===\n\n|=Claim|=Verdict|=Confidence\n|UN eroding sovereignty|MISLEADING|75%\n|Global digital surveillance|PARTIALLY TRUE / DISTORTED|65%\n|Mandatory abortion/gender ideology|IDEOLOGICAL FRAMING|60%\n|Censorship under \"misinformation\"|PARTIALLY TRUE / CONTEXT MISSING|60%\n|Binding global blueprint|EXAGGERATED|70%\n|Global control system|CONSPIRACY THEORY|80%\n\n----\n\n=== **Key Takeaways** ===\n\n**What is TRUE**:\n\n* The UN held the Second World Summit for Social Development in Doha (Nov 4-6, 2025)\n* The Doha Political Declaration was adopted\n* UN initiatives do promote digital identity systems\n* UN frameworks include language on gender equality and information integrity\n* Legitimate debates exist about scope and implementation\n\n**What is FALSE or MISLEADING**:\n\n* Claims of mandatory \"global control\" over citizens\n* Assertions that the declaration is legally binding\n* Conspiracy framing about secret agendas\n* Fabricated \"Agenda 2030 Mission Goals\" lists\n* Claims of forced microchipping, depopulation, etc.\n\n**What is IDEOLOGICALLY FRAMED**:\n\n* Characterization of gender equality as \"radical ideology\"\n* Framing of content moderation as \"censorship of dissent\"\n* Presentation of international cooperation as \"power grab\"\n\n----\n\n=== **Recommendations for Readers** ===\n\n1. **Read primary sources**: The actual Doha Political Declaration is publicly available at social.desa.un.org\n1. **Check fact-checkers**: Claims about UN Agenda 2030 have been extensively fact-checked\n1. **Consider source credibility**: CitizenGO has documented credibility issues and ideological motivations\n1. **Distinguish concerns from conspiracy**: Legitimate policy debates exist, but conspiracy framing distorts them\n1. **Evaluate language**: Emotional, alarmist language often signals advocacy rather than factual reporting\n\n----\n\n== Transparency Notice ==\n\nThis analysis was created by AI (Claude/Anthropic) using the FactHarbor methodology v0.9.18. The assessment is based on:\n\n* Official UN documents and press releases\n* Multiple independent fact-checking organizations\n* Academic and investigative journalism on CitizenGO\n* Critical analysis of claim language and framing\n\n**Limitations**:\n\n* Could not access full text of the specific CitizenGO petition URL (page loading issue)\n* Analysis based on CitizenGO's publicly stated positions and similar petitions\n* UN documents may contain language open to multiple interpretations\n\n**Analysis ID**: CG-DOHA-UN-2025-12-16\n**Created**: December 16, 2025\n\n----\n\n== Sources ==\n\n=== Primary Sources ===\n\n* UN Second World Summit for Social Development official website\n* Doha Political Declaration (A/RES/80/5)\n* UN press releases on the Doha Summit\n\n=== Fact-Checking Sources ===\n\n* PolitiFact (multiple fact-checks on Agenda 2030 claims)\n* Snopes (UN Agenda 21/2030 conspiracy claims)\n* Full Fact (Fake UN Agenda 2030 list)\n\n=== Investigative/Academic Sources ===\n\n* The Bureau of Investigative Journalism (CitizenGO investigation)\n* Political Research Associates (CitizenGO profile)\n* FOIA Research (CitizenGO documentation)\n* openDemocracy (CitizenGO reporting)\n* WikiLeaks HazteOir/CitizenGO files (2021)\n\n=== Reference Sources ===\n\n* Wikipedia (CitizenGO)\n* RationalWiki (CitizenGO)\n* Privacy International (digital identity concerns)\n\n----\n\n**Disclaimer**: This analysis represents an evidence-based assessment of the claims in the cited petition. It does not constitute a political endorsement or opposition to any policy position. Readers are encouraged to consult primary sources and form their own conclusions.", "Product Development.Specification.FH Analysis Reports.FHA - F-35 Remote Control and 'Kill Switch' Claims.WebHome": "= FactHarbor Analysis: F-35 Remote Control and \"Kill Switch\" Claims =\n\n**Claim Analyzed**: \"The US could remotely control F-35 fighter jets or switch them off at the touch of a button\"\n\n**Analysis Date**: December 17, 2025\n**FactHarbor Version**: 0.9.18 POC\n**Language**: English\n\n----\n\n== Executive Summary ==\n\n**Overall Verdict**: MOSTLY FALSE with LEGITIMATE UNDERLYING CONCERNS\n**Confidence**: 80% (Range: 70-88%)\n\nThe claim that the US could \"remotely control\" or instantly \"switch off\" F-35 fighter jets is **not supported by evidence**. Both the Pentagon and defense officials from multiple allied nations have explicitly denied the existence of any remote \"kill switch.\" However, this denial obscures a more nuanced reality: while the US cannot flip a switch to disable F-35s mid-flight, it **does** maintain significant leverage over allied F-35 operations through control of software updates, spare parts, maintenance support, and critical Mission Data Files.\n\nThe claim conflates two distinct issues:\n\n1. **Immediate remote control** (FALSE) - No evidence exists\n1. **Long-term operational dependency** (TRUE) - Well-documented\n\n----\n\n== Source Context ==\n\nThis claim gained significant traction in early 2025 following:\n\n* President Trump's pause on military aid to Ukraine (March 2025)\n* Growing tensions between the US and NATO allies\n* Media reports in European outlets (Germany, Switzerland, Belgium, UK)\n* Social media speculation about alleged \"backdoors\" in F-35 code\n\nThe claim has been circulating in defense forums and mainstream media, prompting official responses from:\n\n* The Pentagon's F-35 Joint Program Office (JPO)\n* Lockheed Martin\n* Defense ministries of Belgium, Switzerland, Czech Republic, and others\n\n----\n\n== Claim Decomposition ==\n\nThe original claim contains two distinct sub-claims:\n\n|=Sub-Claim|=Assessment\n|US could \"remotely control\" F-35s|**FALSE** (90% confidence)\n|US could \"switch them off at the touch of a button\"|**FALSE** (85% confidence)\n|US could degrade F-35 effectiveness through support withdrawal|**TRUE** (90% confidence)\n\n----\n\n== Detailed Evidence Analysis ==\n\n=== CLAIM 1: \"Remotely control F-35 fighter jets\" ===\n\n**Verdict**: FALSE\n**Confidence**: 90% (Range: 85-95%)\n\n==== Official Denials ====\n\n**Pentagon F-35 Joint Program Office (March 18, 2025)**:\n\n>\"There is no kill switch. The program operates under well-established agreements that ensure all F-35 operators have the necessary capabilities to sustain and operate their aircraft effectively.\"\n\n**Belgian Chief of Defence Gen. Frederik Vansina**:\n\n>\"We have no indication that this is possible. The F-35 is not a remote-controlled aircraft. The program relies on worldwide logistical support, with spare parts circulating between user countries.\"\n\n**Swiss Department of Federal Defense (DDPS)**:\n\n>\"A 'remote control' or 'blocking' of the F-35A fighter jets, for example through external interventions in the electronics, is not possible.\"\n\n**Czech Republic Ministry of Defense**:\n\n>\"No, the aircraft itself cannot be remotely interfered with.\"\n\n**Stacie Pettyjohn, Director of Defense Program, Center for a New American Security**:\n\n>\"[It] isn't an electronic kill switch. The US also cannot remotely take control of the F-35.\"\n\n==== Technical Assessment ====\n\nThe F-35's communication systems (ALIS/ODIN, MADL) are designed for:\n\n* Logistics and maintenance tracking\n* Mission planning data transfer\n* Sensor data sharing among coalition partners\n\nThese systems are **not designed for remote flight control**. The aircraft's flight systems operate independently of external networks during missions.\n\n**Evidence Quality**: HIGH - Multiple official government sources across allied nations confirm the same finding.\n\n----\n\n=== CLAIM 2: \"Switch them off at the touch of a button\" ===\n\n**Verdict**: FALSE (as literally stated)\n**Confidence**: 85% (Range: 78-92%)\n\n==== What the Evidence Shows ====\n\nNo mechanism exists for the US to instantaneously disable an F-35 in flight or on the ground through remote command. The \"kill switch\" as popularly imagineda button that immediately renders the aircraft inoperabledoes not exist.\n\n**Lockheed Martin Statement**:\n\n>\"The F-35 is designed to enhance interoperability among allied nations, protecting their sovereignty and ensuring they can operate effectively together to achieve common defense goals.\"\n\n**UK Confirmation**: The UK confirms its F-35 fleet operates independently, including integration of non-US weapons.\n\n**Evidence Quality**: HIGH - Consistent denials from manufacturer and multiple operators.\n\n----\n\n=== CLAIM 3 (Implicit): US Control Over F-35 Operations ===\n\n**Verdict**: TRUE - The US maintains significant leverage\n**Confidence**: 90% (Range: 85-95%)\n\nWhile no \"kill switch\" exists, the US **does** control critical elements that allied F-35 operators depend upon:\n\n==== 1. Software and Mission Data Files (MDF) ====\n\nThe F-35 is a \"software-defined weapon system\" with over 8 million lines of code. Critical to its combat effectiveness are **Mission Data Files (MDF)**the \"electronic battle manual\" that enables:\n\n* Threat identification (radars, SAMs)\n* Optimal flight path calculation (the \"blue line\")\n* Electronic warfare countermeasures\n* Sensor fusion and target correlation\n\n**Bill Sweetman, Defense Analyst**:\n\n>\"The Mission Data File (MDF) is the electronic battle manual for the F-35... It provides known target characteristics for the fusion engine that IDs targets with minimal emissions.\"\n\nMDF updates are managed by a 90-person team at the AustCanUK Reprogramming Laboratory (ACURL) at Eglin AFB, Florida. Without current MDFs, F-35s would be significantly more vulnerable to enemy air defenses.\n\n==== 2. ALIS/ODIN Logistics System ====\n\nThe **Autonomic Logistics Information System (ALIS)** and its successor **Operational Data Integrated Network (ODIN)** manage:\n\n* Spare parts ordering and tracking\n* Maintenance scheduling\n* Mission planning\n* Technical data storage\n* Software distribution\n\nData flows through US-based servers run by Lockheed Martin in Fort Worth, Texas.\n\n==== 3. Spare Parts and Maintenance ====\n\nThe F-35's global supply chain involves:\n\n* 1,450 US suppliers\n* 80 suppliers in 11 other countries\n* Specialized maintenance requiring US-trained contractors\n\n**The War Zone analysis**:\n\n>\"Just cutting off support to the jets would accomplish the same result, albeit maybe not instantly, but soon enough. Without access to American-controlled maintenance and logistics chains, as well as computer networks, any F-35 fleet would quickly start to become unusable.\"\n\n==== 4. Historical Precedent: Turkey ====\n\nWhen Turkey purchased Russian S-400 air defense systems despite US objections, the US:\n\n* Removed Turkey from the F-35 program\n* Blocked delivery of aircraft Turkey had ordered and partially paid for\n* Cut off access to maintenance and upgrades\n\nThis demonstrates the US **can and has** used logistics leverage against allies.\n\n----\n\n== The Israel Exception ==\n\n**Key Finding**: Israel is the **only** F-35 operator with significant operational independence.\n\nIsrael's F-35I \"Adir\" variant features:\n\n* Israeli-developed mission software independent of ALIS\n* Ability to install domestically-developed software\n* Independent depot-level maintenance capability\n* Local Mission Data File production\n* Integration with Israeli C4I systems (Iron Dome, David's Sling, Arrow)\n* Domestic production of some components (wings, helmet displays)\n\n**Israeli Air Force officer (2016)**:\n\n>\"The ingenious, automated ALIS system that Lockheed Martin has built will be very efficient and cost-effective. But the only downfall is that it was built for countries that don't have missiles falling on them.\"\n\nIsrael negotiated this independence specifically because it anticipated the need for rapid, autonomous operations during conflictwithout waiting for US support or approval.\n\n**No other F-35 operator has been granted comparable autonomy.**\n\n----\n\n== Evidence Quality Assessment ==\n\n|=Source Type|=Quality|=Key Sources\n|Official Government Statements|HIGH|Pentagon JPO, Swiss DDPS, Belgian MoD, Czech MoD\n|Defense Industry Analysis|HIGH|The War Zone, Breaking Defense, Flight Global, The Aviationist\n|Manufacturer Statements|MEDIUM-HIGH|Lockheed Martin (potential bias)\n|Expert Commentary|HIGH|CNAS analysts, Bill Sweetman, defense journalists\n|Social Media Claims|LOW|Unsubstantiated rumors, speculation\n\n----\n\n== Alternative Perspectives ==\n\n=== Why the Claim Persists ===\n\n1. **Genuine anxiety about US reliability**: Trump administration's unpredictable policies toward allies have fueled concerns about long-term dependencies.\n1. **Software opacity**: The F-35's 8+ million lines of code are not accessible to most operators, creating uncertainty about what capabilities might exist.\n1. **Historical precedents**: US has used export controls and support withdrawal against allies (Turkey, Israel before special agreement).\n1. **Conflation of issues**: The real vulnerabilities (software dependency, MDF updates, spare parts) get conflated with fictional instant-disable capabilities.\n\n=== Legitimate Concerns ===\n\nDefense analysts and some allied officials have raised valid points:\n\n**Joachim Schranzhofer, Hensoldt Communications Head**:\n\n>Allegations of a \"kill switch\" may be \"more than just a rumor\" in the sense that the US could \"effectively ground any foreign-operated F-35s by blocking access to key software updates.\"\n\n**Wolfgang Ischinger, former Munich Security Conference head**:\n\n>Contract termination should be considered if the US were to limit the F-35's capabilities.\n\n----\n\n== Comparative Analysis ==\n\n|=Country|=F-35 Status|=Independence Level\n|United States|Operator|Full control\n|Israel|F-35I \"Adir\"|High (negotiated autonomy)\n|UK|Operator|Medium (some weapons integration)\n|Italy|Operator, assembly line|Medium\n|Other NATO|Operators|Standard (dependent on US support)\n|Turkey|Removed from program|N/A (blocked after S-400 purchase)\n\n----\n\n== Political Context ==\n\nThe \"kill switch\" narrative emerged during heightened US-European tensions in early 2025:\n\n* **Portugal**: Announced reconsideration of F-35 purchase\n* **Canada**: Reviewing $14.5 billion F-35 contract, seeking alternatives\n* **Germany**: Facing domestic pressure to cancel 35 F-35A order\n* **France**: President Macron urging European allies to consider Rafale as alternative\n\nThese political decisions appear driven more by broader concerns about US reliability under the Trump administration than by specific \"kill switch\" fears.\n\n----\n\n== Conclusion ==\n\n=== Final Verdict: MOSTLY FALSE ===\n\n**Confidence**: 80% (Range: 70-88%)\n\nThe claim that \"the US could remotely control F-35 fighter jets or switch them off at the touch of a button\" is **false as literally stated**. No evidence supports the existence of an instant remote-disable capability.\n\nHowever, the claim touches on **real underlying vulnerabilities**:\n\n|=What's FALSE|=What's TRUE\n|Instant remote \"kill switch\"|US controls critical software updates\n|Remote piloting capability|US controls Mission Data Files essential for combat\n|Ability to disable mid-flight|US controls spare parts and maintenance chains\n|\"Touch of a button\" shutdown|Long-term withholding of support would degrade effectiveness\n\n=== More Accurate Framing ===\n\nInstead of: //\"The US can switch off F-35s at the touch of a button\"//\n\nMore accurate: //\"The US cannot instantly disable F-35s, but maintains significant long-term leverage through control of software updates, mission data, spare parts, and maintenance support. Without US cooperation, allied F-35 fleets would gradually become less effective and eventually difficult to operatethough this would take weeks to months, not seconds.\"//\n\n=== Key Takeaways ===\n\n1. **No kill switch exists** in the sense of instant remote disable\n1. **Significant US control does exist** over long-term operational capability\n1. **Israel is uniquely independent** among F-35 operators\n1. **The claim reflects real anxieties** about US reliability as an ally\n1. **Alternative framings matter**: The difference between \"can switch off instantly\" (false) and \"can hobble over time\" (true) is significant for policy discussions\n\n----\n\n== Transparency Notice ==\n\nThis analysis was created by AI (Claude/Anthropic) using the FactHarbor methodology v0.9.18.\n\n**Methodology**: Claims were decomposed into testable sub-claims. Evidence was gathered through web search of official sources, defense publications, and expert commentary. Contradiction search was conducted to find evidence supporting the \"kill switch\" claim. Confidence levels reflect evidence quality and source consensus.\n\n**Limitations**:\n\n* Could not access classified information about F-35 systems\n* Cannot independently verify technical claims about software architecture\n* Analysis relies on publicly available sources and official statements\n* Potential for undisclosed capabilities exists but is not supported by evidence\n\n**AI Involvement**: Full analysis including evidence gathering, synthesis, and verdict computation.\n\n**Analysis ID**: FH-F35-KS-2025-12-17\n\n----\n\n== Sources ==\n\n1. The Aviationist - \"The F-35 'Kill Switch': Separating Myth from Reality\" (March 11, 2025)\n1. Breaking Defense - \"No, there's no 'kill switch': Pentagon tries to reassure international F-35 partners\" (March 2025)\n1. The War Zone - \"You Don't Need A Kill Switch To Hobble Exported F-35s\" (March 11, 2025)\n1. Flight Global - \"'There is no kill switch': Pentagon denies F-35 rumours\" (March 19, 2025)\n1. The Defense Post - \"Pentagon Dismisses Claims of Remote 'Kill Switch' in Exported F-35s\" (March 20, 2025)\n1. Interesting Engineering - \"No kill switch present in Lockheed's F-35s exported to US allies\" (March 12, 2025)\n1. F35.com (Lockheed Martin) - \"The F-35 Advantage: Interoperability and Allied Deterrence\"\n1. The Week (India) - \"The F-35 'kill switch' myth\" (March 14, 2025)\n1. Swiss Department of Federal Defense - Official statement on F-35A (March 2025)\n1. Belgian Ministry of Defense - Statement by Gen. Frederik Vansina\n1. Czech Republic Ministry of Defense - Official Q&A\n1. INFODAS - \"Solving data sovereignty and classified information protection issues with the F-35 ALIS/ODIN system\"\n1. The War Zone - \"Israel's F-35I Combat Experience Is Providing Lessons For Future Pacific Fight\" (December 2023)\n1. National Security Journal - \"An Expert Reveals the Real F-35 Fighter 'Kill Switch'\" (August 2025)\n1. Bulgarian Military - \"Israel awaits eight upgraded F-35Is with improved software\" (February 2025)\n1. DVIDSHUB - \"F-35 Joint Strike Fighter Demonstrates Enhanced Interoperability with Initial Coalition Mission Data\"\n1. Military Aerospace Electronics - ODIN/ALIS technical documentation", "Product Development.Specification.FH Analysis Reports.FHA Claude - Hagebuttenpulver Arthrose DE.WebHome": "= FactHarbor Analysebericht =\n\n== Hagebuttenpulver bei Arthrose: Wirksamkeit und Evidenzlage ==\n\n**Analysedatum**: 15. Dezember 2025\n**Quelle**: Focus.de Artikel ber orthopdische Einschtzung\n**FactHarbor Version**: 0.9.18 POC\n**Analysetyp**: Umfassend\n**Sprache**: Deutsch\n\n----\n\n== Zusammenfassung ==\n\nHagebuttenpulver wird zunehmend als natrliches Mittel gegen Arthrosebeschwerden beworben. Die wissenschaftliche Evidenz ist**gemischt**: Es gibt mehrere positive Studien, die eine schmerzlindernde Wirkung nahelegen, aber auch kritische Studien und methodische Einschrnkungen. Unsere Analyse zeigt:\n\n* **65% Konfidenz**, dass Hagebuttenpulver bei 2/3 der Patienten Schmerzen lindert\n* **58% Konfidenz**, dass die Wirkung ber Placebo hinausgeht\n* **80% Konfidenz**, dass standardisierte Extrakte besser untersucht sind als normales Pulver\n* **Risikoklasse A**(Gesundheitsempfehlung - erfordert Expertenprfung)\n\n----\n\n== Analysebersicht ==\n\n|=Kennzahl|=Anzahl/Status\n|**Identifizierte Behauptungen**|5\n|**Generierte Szenarien**|10 (2 pro Behauptung)\n|**Evidenzquellen**|12\n|**Urteile erstellt**|10\n|**Widerspruchssuche**| Abgeschlossen\n|**Qualittsgates**| Alle bestanden (10/10)\n|**Risikoklasse A Behauptungen**|3 (Hoch - Gesundheit)\n|**Risikoklasse B Behauptungen**|2 (Mittel - Wissenschaft)\n\n----\n\n== Behauptungen-Analyse ==\n\n=== **BEHAUPTUNG 1**: Hagebuttenpulver hilft in 2/3 der Flle ===\n\n>//\"In zwei Dritteln der Flle hat Hagebuttenpulver geholfen, Schmerzen deutlich zu lindern und auch die Beweglichkeit zu verbessern\"//\n\n**Domne**: Gesundheit & Medizin\n**Risikoklasse**: **A**(Hoch - Gesundheitsempfehlung)\n**Behauptungstyp**: Wrtlich, Empirisch\n\n==== Urteil:**Wahrscheinlich Wahr (mit Einschrnkungen)** ====\n\n**Konfidenz**: 65% (Bereich: 52% - 75%)\n\n**Hauptevidenz**:\n\n*  Warholm-Studie (2003): 64,6% der Probanden berichteten ber Schmerzreduktion\n*  Chrubasik-Studie (Rckenschmerzen): 60% erfllten Responder-Kriterium\n*  Mehrere kleinere Studien zeigen hnliche Erfolgsraten\n*  Aber: Methodische Mngel, Herstellerfinanzierung, kleine Teilnehmerzahlen\n*  Medizin-Transparent: \"Widersprchliche Ergebnisse\" aus 3 Studien\n\n**Begrndung**:\n\n1. **Positive Studienlage**: Mehrere randomisiert-kontrollierte Doppelblindstudien zeigen Erfolgsraten von 60-65%\n1. **Konsistentes Muster**: Verschiedene Studien kommen zu hnlichen Zahlen (60-65% Responder)\n1. **Aber Einschrnkungen**:\n1*. Herstellerfinanzierung in mehreren Studien\n1*. Kleine Teilnehmerzahlen (100-112 Personen)\n1*. Fehlende Vergleiche mit Standardmedikamenten\n1*. Eine Studie fand keinen Unterschied zu Placebo\n1. **Cochrane Review**: Bescheinigt nur \"geringe Evidenz\" aufgrund miger Studienqualitt\n\n**Gegenargumente bercksichtigt**:\n\n*  Interessenkonflikte durch Herstellerfinanzierung\n*  Methodische Schwchen (kleine Gruppen, kurze Dauer)\n*  Publikationsbias mglich (negative Studien werden seltener verffentlicht)\n*  \"Responder\" = subjektive Schmerzeinschtzung, nicht objektive Messungen\n\n**Evidenzquellen**:\n\n* Warholm et al. 2003 (Mittlere Zuverlssigkeit, Herstellerfinanzierung)\n* Rein et al. 2004 (Mittlere Zuverlssigkeit, 112 Probanden)\n* Chrubasik, Uni Freiburg (Mittlere Zuverlssigkeit)\n* Medizin-Transparent Analyse (Hohe Zuverlssigkeit, kritisch)\n\n----\n\n=== **BEHAUPTUNG 2**: 5g tglich ber 4-6 Wochen ntig ===\n\n>//\"Eine Menge von 5g tglich, mindestens 4-6 Wochen eingenommen werden muss, sich die volle Wirksamkeit aber erst nach 3-4 Monaten zeigt\"//\n\n**Domne**: Gesundheit & Medizin\n**Risikoklasse**: **A**(Hoch - Dosierungsempfehlung)\n**Behauptungstyp**: Wrtlich, Empirisch\n\n==== Urteil:**Weitgehend Wahr** ====\n\n**Konfidenz**: 78% (Bereich: 68% - 86%)\n\n**Hauptevidenz**:\n\n*  Alle untersuchten Studien verwendeten 5g tglich als Dosierung\n*  Warholm-Studie: 4 Monate Einnahme\n*  Rein-Studie: 3 Monate Einnahme mit Crossover\n*  Chrubasik: 5-10g tglich ber 1 Jahr\n*  Erste Effekte nach 4 Wochen berichtet\n*  Volle Wirkung nach 3-4 Monaten konsistent beschrieben\n\n**Begrndung**:\n\n**Dosierung (5g tglich)**:\n\n* Alle wissenschaftlichen Studien verwendeten diese Dosierung\n* Meist aufgeteilt in 2  2,5g pro Tag\n* Keine Studien mit niedrigeren oder hheren Dosen fr Vergleich\n\n**Zeitrahmen**:\n\n{{{Woche 1-4:    Erste Effekte mglich (einige Patienten)\nWoche 4-6:    Deutlichere Verbesserungen\nWoche 8-12:   Volle Wirksamkeit erreicht\nMonat 3-4:    Maximale Effekte, Carry-over-Effekt beginnt\n}}}\n\n**Unsicherheitsfaktoren**:\n\n* Keine Dosis-Wirkungs-Studien durchgefhrt\n* Optimale Dosierung unbekannt\n* Individuelle Variation nicht untersucht\n* Langzeitwirkung (>1 Jahr) unzureichend erforscht\n\n**Evidenzquellen**:\n\n* Warholm et al. 2003 (Hohe Zuverlssigkeit fr Dosierung)\n* Rein et al. 2004 (Hohe Zuverlssigkeit fr Zeitrahmen)\n* Pharmazeutische Zeitung Review (Hohe Zuverlssigkeit)\n\n----\n\n=== **BEHAUPTUNG 3**: Galaktolipide sind der Wirkstoff ===\n\n>//\"Hagebuttenpulver enthlt entzndungshemmende Galaktolipide und Antioxidantien\"//\n\n**Domne**: Wissenschaft & Biochemie\n**Risikoklasse**: **B**(Mittel - Wissenschaftliche Behauptung)\n**Behauptungstyp**: Wrtlich, Empirisch\n\n==== Urteil:**Wahrscheinlich Wahr** ====\n\n**Konfidenz**: 72% (Bereich: 60% - 82%)\n\n**Hauptevidenz**:\n\n*  Larsen et al. (2003): Galaktolipid isoliert, hemmt Leukozyten-Migration in vitro\n*  Dnische Forschung: Aktiver Inhaltsstoff identifiziert und isoliert\n*  Senkt C-reaktives Protein (CRP) in vivo\n*  Antioxidative und membranstabilisierende Wirkung nachgewiesen\n*  Aber: Auch andere Lebensmittel enthalten Galaktolipide\n*  Wahrscheinlich Wirkstoffkombination, nicht nur Galaktolipide\n\n**Begrndung**:\n\n**Was ist bewiesen**:\n\n1. **Galaktolipide sind vorhanden**: Aus Zuckeranteil + Fettsuren bestehend\n1. **In-vitro-Wirkung nachgewiesen**: Hemmt Migration weier Blutkrperchen\n1. **In-vivo-Wirkung gezeigt**: Senkt Entzndungsmarker (CRP)\n1. **Mechanismus plausibel**: Antioxidativ, membranstabilisierend, anti-inflammatorisch\n\n**Was ist unsicher**:\n\n1. **Monokausale Erklrung fragwrdig**: Andere Inhaltstoffe (Vitamin C, Pektine, etc.) tragen wahrscheinlich bei\n1. **Andere galaktolipidreiche Lebensmittel**: Petersilie, Lauch, Erbsen, Spinat, Krbis - auch wirksam?\n1. **Komplexe Wirkstoffkombination**: Hagebutte wirkt vermutlich durch Gesamtkomposition\n\n**Zitat Zentrum der Gesundheit**:\n\n>\"Davon auszugehen ist, dass das Hagebuttenpulver vielmehr aufgrund seiner Kombination an unterschiedlichen Wirkstoffen so gute Ergebnisse im Hinblick auf die Gelenkgesundheit erzielt.\"\n\n**Evidenzquellen**:\n\n* Larsen et al. 2003 (Hohe Zuverlssigkeit, Peer-reviewed)\n* JAMA Online 2006 (Hohe Zuverlssigkeit)\n* Pharmazeutische Zeitung (Hohe Zuverlssigkeit)\n* Zentrum der Gesundheit (Mittlere Zuverlssigkeit)\n\n----\n\n=== **BEHAUPTUNG 4**: Evidenz ist noch nicht ausreichend ===\n\n>//\"Es gibt vielversprechende Studien, aber noch ist es fr eine eindeutige und uneingeschrnkte Empfehlung zu frh\"//\n\n**Domne**: Wissenschaft & Evidenz\n**Risikoklasse**: **A**(Hoch - Medizinische Empfehlung)\n**Behauptungstyp**: Bewertend, Meta-Aussage\n\n==== Urteil~*~*: ~*~*Weitgehend Wahr ====\n\n**Konfidenz**: 82% (Bereich: 75% - 88%)\n\n**Hauptevidenz**:\n\n*  Medizin-Transparent: \"Richtig berzeugen konnte uns die Studienlage nicht\"\n*  Cochrane Review (Cameron 2014): \"Geringe Evidenz\" aufgrund miger Studienqualitt\n*  Ernhrungsmedizin Blog: \"Evidenz nur gering\", \"methodische Mngel\"\n*  Gute Pillen Schlechte Pillen: \"Keine zuverlssigen Belege\"\n*  Systematische bersicht Rossnagel: Eingeschrnkte Aussagekraft\n*  Aber: Mehrere positive Studien existieren\n\n**Begrndung**:\n\n**Warum Evidenz noch unzureichend**:\n\n1. (((\n**Methodische Schwchen**:\n\n* Kleine Teilnehmerzahlen (100-112 Personen pro Studie)\n* Kurze Studiendauer (3-4 Monate meist)\n* Keine Langzeitdaten (>1 Jahr)\n* Fehlende Vergleiche mit Standardmedikamenten\n)))\n1. (((\n**Interessenkonflikte**:\n\n* Herstellerfinanzierung in mehreren Studien\n* Teilweise identische Autoren\n* Publikationsbias mglich\n)))\n1. (((\n**Widersprchliche Ergebnisse**:\n\n* Cameron 2014: \"Widersprchliche Aussagen\"\n* Eine Studie fand keinen Unterschied zu Placebo\n* Mangelnde Reproduzierbarkeit\n)))\n1. (((\n**Fehlende Informationen**:\n\n* Keine knorpelschtzende Wirkung nachgewiesen\n* Kein Nachweis, dass Arthrose-Fortschreiten gebremst wird\n* Optimale Dosierung unklar\n* bertragbarkeit von standardisiertem Extrakt auf normales Pulver fraglich\n)))\n\n**Expertenmeinungen**:\n\n**Medizin-Transparent (kritisch-neutral)**:\n\n>\"Schon vorab: Richtig berzeugen konnte uns die Studienlage nicht. [...] Die Studien hatten mehrere methodische Schwchen, sodass die Ergebnisse wenig verlsslich sind.\"\n\n**Cochrane Review**:\n\n>\"Geringe Evidenz aufgrund miger Studienqualitt\"\n\n**Ernhrungsmedizin Blog**:\n\n>\"Die Evidenz ist wegen der migen Studienqualitt nur gering. [...] Es spricht nichts gegen die versuchsweise Anwendung, aber Weihrauch-Kapseln wren vermutlich sinnvoller.\"\n\n**Neue Orthopdie (aus Focus-Artikel)**:\n\n>\"Fakt ist: Es gibt vielversprechende Studien, aber noch ist es fr eine eindeutige und uneingeschrnkte Empfehlung zu frh.\"\n\n**Evidenzquellen**:\n\n* Medizin-Transparent (Hohe Zuverlssigkeit, unabhngig)\n* Cochrane Review Cameron 2014 (Hchste Zuverlssigkeit)\n* Ernhrungsmedizin Blog (Hohe Zuverlssigkeit)\n* Gute Pillen Schlechte Pillen (Hohe Zuverlssigkeit, Verbraucherschutz)\n\n----\n\n=== **BEHAUPTUNG 5**: Gut vertrglich mit wenigen Nebenwirkungen ===\n\n>//\"Hagebuttenpulver gilt als gut vertrglich. Bekannte Nebenwirkungen sind Verdauungsprobleme bzw. eine entwssernde Wirkung\"//\n\n**Domne**: Gesundheit & Sicherheit\n**Risikoklasse**: **B**(Mittel - Sicherheitsinformation)\n**Behauptungstyp**: Wrtlich, Empirisch\n\n==== Urteil:**Wahrscheinlich Wahr** ====\n\n**Konfidenz**: 75% (Bereich: 65% - 83%)\n\n**Hauptevidenz**:\n\n*  Medizin-Transparent: \"Kein deutlicher Unterschied zu Placebo bei Nebenwirkungen\"\n*  Utopia: \"Bis jetzt sind keine Nebenwirkungen bekannt\"\n*  Studien: Wenige ernste Nebenwirkungen berichtet\n*  Allergische Reaktionen gelegentlich (selten)\n*  Gastrointestinale Beschwerden meist durch Flssigkeitsanpassung behebbar\n*  Aber: Kurze Studiendauer, kleine Gruppen = Langzeitsicherheit unklar\n\n**Begrndung**:\n\n**Positive Sicherheitsprofil**:\n\n1. **Studien zeigen Vertrglichkeit**: Kein deutlicher Unterschied zu Placebo\n1. **Naturprodukt**: Lange Verwendungsgeschichte\n1. **Mild und schonend**: Geringere Nebenwirkungen als synthetische Schmerzmittel\n1. **Fr Schwangere geeignet**: Gilt als sicher (laut Utopia)\n\n**Bekannte Nebenwirkungen**(mild):\n\n* Verdauungsprobleme (durch hohen Ballaststoffgehalt)\n* Entwssernde/harntreibende Wirkung (mehr Flssigkeit ntig)\n* Allergische Reaktionen (bei Allergie gegen Rosengewchse)\n\n**Wechselwirkungen**(zu beachten):\n\n* Medikamente mit Vitamin-C-Interaktion\n* Eisenprparate (Vitamin C erhht Absorption)\n* Blutverdnnende Medikamente (Vorsicht)\n\n**Unsicherheitsfaktoren**:\n\n* **Langzeitsicherheit unbekannt**: Studien meist 3-6 Monate\n* **Kleine Teilnehmerzahlen**: Seltene Nebenwirkungen werden nicht erfasst\n* **Fehlende systematische Erfassung**: Nebenwirkungen oft nicht detailliert dokumentiert\n* **Qualittsunterschiede**: Verschiedene Produkte, keine standardisierte Qualitt\n\n**Medizin-Transparent Einschtzung**:\n\n>\"Ob Hagebuttenpulver tatschlich gut vertrglich ist, lsst sich auch wegen der geringen Zahl an Teilnehmerinnen und Teilnehmern und aufgrund der relativ kurzen Studiendauer ohnehin nicht sicher sagen.\"\n\n**Evidenzquellen**:\n\n* Medizin-Transparent (Hohe Zuverlssigkeit)\n* Utopia (Mittlere Zuverlssigkeit)\n* Studienberichte Warholm/Rein (Mittlere Zuverlssigkeit)\n* Nrnberger Versicherung (Mittlere Zuverlssigkeit)\n\n----\n\n== Evidenzbersicht ==\n\n=== Evidenz-Qualittsverteilung ===\n\n|=Zuverlssigkeit|=Anzahl|=Prozent\n|**Hoch**|6|50%\n|**Mittel**|6|50%\n|**Niedrig**|0|0%\n\n=== Evidenz nach Position ===\n\n|=Position|=Anzahl|=Anmerkungen\n|**Untersttzend**(Positive Studien)|5|Warholm, Rein, Chrubasik, PZ, ZdG\n|**Kritisch**(Methodische Bedenken)|5|Medizin-Transparent, Cochrane, GPsP, Ernmed\n|**Neutral/Gemischt**|2|bersichtsarbeiten, Meta-Analysen\n\n=== Schlssel-Evidenzquellen ===\n\n==== 1.**Warholm et al. 2003**(Mittlere Zuverlssigkeit) ====\n\n* **Typ**: Randomisierte kontrollierte Doppelblindstudie\n* **Teilnehmer**: 100 Patienten mit Knie-/Hftarthrose\n* **Ergebnis**: 64,6% Schmerzreduktion, signifikante Verbesserung vs. Placebo\n* **Limitation**: Herstellerfinanzierung, kleine Gruppe\n\n==== 2.**Rein et al. 2004**(Mittlere Zuverlssigkeit) ====\n\n* **Typ**: RCT Doppelblindstudie mit Crossover\n* **Teilnehmer**: 112 Osteoarthritis-Patienten\n* **Ergebnis**: Signifikante Besserung bei Schmerzen und Steifigkeit\n* **Besonders**: 44% Reduktion von Schmerzmittelgebrauch\n* **Limitation**: Herstellerfinanzierung\n\n==== 3.**Chrubasik, Uni Freiburg**(Mittlere Zuverlssigkeit) ====\n\n* **Typ**: Langzeit-Beobachtungsstudie\n* **Teilnehmer**: 112 Patienten mit chronischen Rckenschmerzen\n* **Dauer**: Bis zu 1 Jahr\n* **Ergebnis**: 60% erfllten Responder-Kriterium\n* **Limitation**: Kein Placebo-Arm, Beobachtungsstudie\n\n==== 4.**Cameron et al. 2014 (Cochrane Review)**(Hchste Zuverlssigkeit) ====\n\n* **Typ**: Systematische bersichtsarbeit\n* **Studien**: 3 RCTs mit 306 Teilnehmern\n* **Fazit**: \"Widersprchliche Ergebnisse\", \"geringe Evidenz\"\n* **Qualitt**: Hchster Evidenzstandard, aber kritische Bewertung\n\n==== 5.**Medizin-Transparent (2022)**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Unabhngige kritische Bewertung\n* **Position**: Skeptisch\n* **Hauptkritik**: Methodische Mngel, Herstellerfinanzierung, widersprchliche Ergebnisse\n* **Fazit**: \"Richtig berzeugen konnte uns die Studienlage nicht\"\n\n==== 6.**Larsen et al. 2003 (J Nat Prod)**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Biochemische Grundlagenforschung\n* **Ergebnis**: Galaktolipid isoliert, hemmt Leukozyten-Migration in vitro\n* **Bedeutung**: Wirkmechanismus identifiziert\n* **Limitation**: In-vitro-Daten, nicht direkt auf Klinik bertragbar\n\n==== 7.**Pharmazeutische Zeitung (2007)**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Fachlicher Review\n* **Position**: Vorsichtig positiv\n* **Fazit**: \"Spektrum der Therapieanstze erweitert\", \"interessante Perspektiven\"\n* **Limitation**: lterer Review, vor neueren kritischen Analysen\n\n==== 8.**Ernhrungsmedizin Blog**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Kritische fachliche Einordnung\n* **Position**: Zurckhaltend\n* **Fazit**: \"Evidenz gering\", \"unkritischer Verzehr\", \"Weihrauch vermutlich sinnvoller\"\n* **Strke**: Balancierte wissenschaftliche Perspektive\n\n==== 9.**Gute Pillen Schlechte Pillen**(Hohe Zuverlssigkeit) ====\n\n* **Typ**: Verbraucherschutz-orientiert\n* **Position**: Sehr skeptisch\n* **Kritik**: Herstellerfinanzierung, identische Autoren, mangelnde Verlsslichkeit\n* **Fazit**: \"Keine zuverlssigen Belege\"\n\n----\n\n== Widerspruchssuche ==\n\n**Pflicht-Widerspruchssuche abgeschlossen**\n\n=== Suchstrategie: ===\n\n1. Gesucht nach negativen/kritischen Studien\n1. Unabhngige Bewertungen geprft (Cochrane, Medizin-Transparent)\n1. Methodische Kritik recherchiert\n1. Interessenkonflikte identifiziert\n1. Alternative Interpretationen bercksichtigt\n\n=== Gefundene Gegenargumente: ===\n\n==== **Kritische Position**: ====\n\n1. **Methodische Schwchen**: Kleine Studien, kurze Dauer, Herstellerfinanzierung\n1. **Widersprchliche Ergebnisse**: Nicht alle Studien zeigen Wirkung\n1. **Fehlender Vergleich**: Keine Studien vs. Standardmedikamente\n1. **bertragbarkeit fraglich**: Standardisierter Extrakt  normales Pulver\n1. **Keine strukturelle Wirkung**: Nur Symptomlinderung, kein Knorpelschutz\n\n==== **Neutrale Kontextinformationen**: ====\n\n1. **Andere galaktolipidreiche Lebensmittel**knnten hnlich wirken\n1. **Weihrauch-Extrakt**hat bessere Studienlage\n1. **Placebo-Effekt**bei Schmerzstudien ausgeprgt\n1. **Selbstselektionsbias**bei Anwendungsberichten\n\n=== Echo-Kammern vermieden: ===\n\n Sowohl positive als auch kritische Quellen analysiert\n Unabhngige Bewertungen einbezogen (Medizin-Transparent, Cochrane)\n Herstellerfinanzierte vs. unabhngige Studien unterschieden\n Verschiedene Expertenmeinungen bercksichtigt\n Methodische Limitationen explizit genannt\n\n=== Diverse Perspektiven inkludiert: ===\n\n* **Positive Forschung**(Warholm, Rein, Chrubasik)\n* **Kritische Wissenschaft**(Medizin-Transparent, Cochrane)\n* **Verbraucherschutz**(Gute Pillen Schlechte Pillen)\n* **Fachmedizin**(Pharmazeutische Zeitung, Ernhrungsmedizin)\n* **Biochemische Grundlagen**(Larsen, Galaktolipid-Forschung)\n\n----\n\n== Risikobewertung ==\n\n=== Gesundheitswirkung: **MITTEL-NIEDRIG** ===\n\n* **Wahrscheinliche Wirkung**: Schmerzlinderung bei 60-65% der Anwender (mild bis moderat)\n* **Evidenzqualitt**: Gering bis moderat (methodische Schwchen)\n* **Vergleich zu Alternativen**: Schwcher als NSAIDs, mglicherweise hnlich wie Placebo\n* **Mechanismus**: Plausibel (Galaktolipide, anti-inflammatorisch), aber nicht eindeutig bewiesen\n\n=== Sicherheitsrisiko~*~*: **NIEDRIG** ===\n\n* **Nebenwirkungen**: Minimal, meist gut vertrglich\n* **Schwere Risiken**: Keine bekannt\n* **Kontraindikationen**: Rosengewchs-Allergie, Vorsicht bei bestimmten Medikamenten\n* **Langzeitsicherheit**: Unklar, aber natrliches Lebensmittel mit Tradition\n\n=== Empfehlungsrisiko: **MITTEL** ===\n\n* **Fr leichte Arthrose**: Versuch vertretbar, geringe Kosten, niedriges Risiko\n* **Fr schwere Arthrose**: Unzureichend, nur ergnzend zu Standardtherapie\n* **Statt Standardmedikation**: NICHT empfohlen (unzureichende Evidenz)\n* **rztliche Beratung**: Empfohlen, besonders bei Medikamenteneinnahme\n\n=== Evidenzqualitt-Risiko: **HOCH** ===\n\n* **Hauptproblem**: Methodische Schwchen, Herstellerfinanzierung\n* **Vertrauenswrdigkeit**: Eingeschrnkt durch Interessenkonflikte\n* **Reproduzierbarkeit**: Unzureichend (widersprchliche Ergebnisse)\n* **bertragbarkeit**: Fraglich (Extrakt vs. Pulver)\n\n----\n\n== Qualittssicherung ==\n\n=== Qualittsgate-Ergebnisse ===\n\n|=Qualittsgate|=Status|=Details\n|**Quellenqualitt**|**PASS**(10/10)|Alle Quellen auf Zuverlssigkeit geprft\n|**Widerspruchssuche**|**PASS**(10/10)|Gegenargumente aktiv gesucht und einbezogen\n|**Unsicherheitsquantifizierung**|**PASS**(10/10)|Alle Unsicherheiten explizit identifiziert\n|**Strukturelle Integritt**|**PASS**(10/10)|Begrndungsketten vollstndig nachvollziehbar\n\n=== Konfidenz-Scores nach Urteil ===\n\n|=Urteil|=Behauptung|=Konfidenz|=Qualitt\n|URTEIL_001|2/3 Erfolgsrate (positiv)|65%| Moderat\n|URTEIL_002|2/3 Erfolgsrate (kritisch)|58%| Moderat\n|URTEIL_003|5g Dosierung|78%| Hoch\n|URTEIL_004|Zeitrahmen 4-6 Wochen|75%| Hoch\n|URTEIL_005|Galaktolipide (Wirkmechanismus)|72%| Gut\n|URTEIL_006|Galaktolipide (alleinige Ursache)|35%| Niedrig\n|URTEIL_007|Evidenz unzureichend|82%| Hoch\n|URTEIL_008|Versuch vertretbar|77%| Hoch\n|URTEIL_009|Gut vertrglich|75%| Hoch\n|URTEIL_010|Langzeitsicherheit unklar|68%| Moderat\n\n**Durchschnittliche Konfidenz**: 68,5%\n**Niedrigste Konfidenz**: 35% (Galaktolipide als alleinige Ursache)\n**Hchste Konfidenz**: 82% (Evidenz noch unzureichend)\n\n----\n\n== Wichtigste Erkenntnisse ==\n\n=== **Was wir mit hoher Konfidenz wissen** ===\n\n1. (((\n**Standardisierte Dosierung**: 5g tglich, 4-6 Wochen mindestens (78% Konfidenz)\n\n* Konsistent in allen Studien verwendet\n* Zeitrahmen gut dokumentiert\n)))\n1. (((\n**Evidenz ist begrenzt**: Fr uneingeschrnkte Empfehlung zu frh (82% Konfidenz)\n\n* Methodische Schwchen besttigt\n* Unabhngige Quellen stimmen berein\n)))\n1. (((\n**Wahrscheinlich gut vertrglich**: Wenige und milde Nebenwirkungen (75% Konfidenz)\n\n* Sicherheitsprofil in Studien okay\n* Kein Vergleich zu Placebo bei NW\n)))\n\n=== **Was unklar bleibt** ===\n\n1. **Tatschliche Wirkung ber Placebo hinaus**(58% Konfidenz)\n1. **Wirkmechanismus im Detail**(Galaktolipide allein? Kombination?)\n1. **bertragbarkeit**: Standardisierter Extrakt  normales Pulver?\n1. **Optimale Dosierung**(keine Dosis-Wirkungs-Studien)\n1. **Langzeitwirkung**(>1 Jahr unerforscht)\n1. **Knorpelschutz**: Keine strukturelle Wirkung nachgewiesen\n\n=== **Balancierte Perspektive** ===\n\n**Fr Hagebuttenpulver spricht**:\n\n* Mehrere positive Studien mit hnlichen Ergebnissen\n* Plausibler Wirkmechanismus (Galaktolipide)\n* Gut vertrglich, niedriges Risiko\n* Kostengnstig, natrliches Mittel\n* Anekdotische Erfolgsberichte\n\n**Gegen uneingeschrnkte Empfehlung spricht**:\n\n* Methodische Schwchen der Studien\n* Herstellerfinanzierung, Interessenkonflikte\n* Widersprchliche Ergebnisse\n* Keine strukturelle Wirkung (nur Symptome)\n* Fehlender Vergleich zu Standardmedikamenten\n* bertragbarkeit Extrakt  Pulver fraglich\n\n**Gesamteinschtzung**: Die**Evidenz ist vielversprechend aber nicht eindeutig**. Ein Versuch ist bei leichter bis mittlerer Arthrose vertretbar, sollte aber nicht Standardmedikation ersetzen und unter rztlicher Begleitung erfolgen.\n\n----\n\n== Kritische Einschrnkungen & Limitationen ==\n\n=== **Analyse-Limitationen** ===\n\n1. **Kein Zugriff auf Volltexte**: Primrstudien nur ber Sekundrquellen analysiert\n1. **Sprachbarriere**: Einige Studien mglicherweise in Dnisch/Englisch nicht vollstndig erfasst\n1. **Publikationsbias**: Negative Studien seltener verffentlicht\n1. **Aktualitt**: Neueste Forschung (2024-2025) mglicherweise nicht vollstndig bercksichtigt\n1. **Produktvariabilitt**: Unterschiede zwischen Herstellern nicht analysierbar\n\n=== **Was diese Analyse NICHT sagen kann** ===\n\n* **Individuelle Wirksamkeit**: Ob es bei Ihnen persnlich wirkt\n* **Vergleich zu Alternativen**: Direkte Wirksamkeit vs. Ibuprofen, Diclofenac, etc.\n* **Kosteneffektivitt**: Ob es das Geld wert ist\n* **Optimale Produktauswahl**: Welches Produkt/welcher Hersteller am besten\n* **Langfristige Auswirkungen**: Was passiert nach Jahren der Einnahme\n\n=== **Was diese Analyse sagen kann** ===\n\n* **Evidenzqualitt**: Wie gut die wissenschaftliche Grundlage ist\n* **Wahrscheinlichkeit der Wirkung**: Basierend auf vorliegenden Studien\n* **Sicherheitsprofil**: Bekannte Nebenwirkungen und Risiken\n* **Unsicherheitsfaktoren**: Was noch unklar ist\n* **Empfehlungsgrad**: Ob ein Versuch vertretbar erscheint\n\n----\n\n== Empfehlungen ==\n\n=== **Fr Patienten mit Arthrose** ===\n\n==== **Leichte bis mittlere Arthrose**: ====\n\n**Versuch vertretbar**\n\n* Geringe Kosten, niedriges Risiko\n* 60-65% Chance auf Schmerzlinderung (laut Studien)\n* Dosierung: 5g tglich, mindestens 3 Monate\n* Vorab rztliche Beratung empfohlen\n\n==== **Schwere Arthrose**: ====\n\n**Nur ergnzend**\n\n* NICHT als Ersatz fr Standardmedikation\n* Hchstens als Add-on-Therapie\n* Eng rztlich begleitet\n* Realistische Erwartungen\n\n==== **Generelle Ratschlge**: ====\n\n1. **Standardisiertes Produkt whlen**: Studien verwendeten spezifische Extrakte\n1. **Geduld haben**: Wirkung erst nach 4-12 Wochen\n1. **Realistisch bleiben**: Nur Symptomlinderung, kein Wundermittel\n1. **Dokumentieren**: Schmerz-Tagebuch fhren (objektive Bewertung)\n1. **Arzt informieren**: Besonders bei Medikamenteneinnahme\n\n=== **Fr rzte** ===\n\n1. **Aufklrung ber Evidenzlage**: Ehrlich ber begrenzte Studienlage informieren\n1. **Versuch bei leichter Arthrose**: Kann empfohlen werden (informed consent)\n1. **Nicht statt NSAIDs**: Keine Alternative zu etablierten Therapien\n1. **Monitoring**: Wirksamkeit nach 3 Monaten evaluieren\n1. **Wechselwirkungen beachten**: Vitamin C, Eisen, Blutverdnner\n\n=== **Fr Forscher** ===\n\n1. **Grere Studien ntig**: >500 Teilnehmer, mehrere Zentren\n1. **Lngere Studiendauer**: Mindestens 12 Monate\n1. **Direktvergleiche**: vs. NSAIDs, andere Phytotherapeutika\n1. **Unabhngige Finanzierung**: Vermeidung von Interessenkonflikten\n1. **Strukturelle Endpunkte**: Knorpeldicke (MRT), nicht nur Schmerz\n1. **Extrakt vs. Pulver**: bertragbarkeit klren\n1. **Dosis-Wirkungs-Studien**: Optimale Dosierung ermitteln\n\n----\n\n== Transparenz-Offenlegung ==\n\n=== **Methodik** ===\n\n**Analyse-Framework**: FactHarbor Evidenzmodell v0.9.18\n**KI-System**: Claude (Anthropic)\n**Analysetyp**: Umfassende automatisierte Analyse mit Pflicht-Widerspruchssuche\n**Verarbeitungsdatum**: 15. Dezember 2025\n**Sprache**: Deutsch\n\n=== **Datenquellen** ===\n\n**Primrquellen**: 12 Evidenzstcke\n\n* 6 Quellen mit hoher Zuverlssigkeit\n* 6 Quellen mit mittlerer Zuverlssigkeit\n* 0 Quellen mit niedriger Zuverlssigkeit\n\n**Quellentypen**:\n\n* Wissenschaftliche Studien (Warholm, Rein, Chrubasik, Larsen)\n* Systematische Reviews (Cochrane, Rossnagel)\n* Unabhngige Bewertungen (Medizin-Transparent, GPsP)\n* Fachpublikationen (Pharmazeutische Zeitung, Ernhrungsmedizin)\n* Verbraucherinformationen (Utopia, Zentrum der Gesundheit)\n* Anwendungsberichte (Jean Ptz Produkte)\n\n=== **Qualitts-Validierung** ===\n\n**Widerspruchssuche**:  Abgeschlossen und dokumentiert\n**Qualittsgates**:  10/10 Urteile bestanden alle Gates\n**Evidenz-Diversitt**:  Verschiedene Perspektiven einbezogen\n**Unsicherheitsquantifizierung**:  Alle Unsicherheiten explizit\n**Nachvollziehbarkeit**:  Vollstndige Begrndungsketten dokumentiert\n\n=== **Bekannte Limitationen** ===\n\n1. **Zugriff**: Analyse basiert ausschlielich auf ffentlich verfgbaren Informationen\n1. **Primrstudien**: Kein direkter Zugriff auf Volltexte, nur Sekundrberichte\n1. **Aktualitt**: Neueste Studien (letzte 6 Monate) mglicherweise nicht erfasst\n1. **Sprache**: Fokus auf deutsch- und englischsprachige Quellen\n1. **Produktvariabilitt**: Keine Analyse spezifischer Herstellerprodukte\n\n=== **KI-Offenlegung** ===\n\nDiese Analyse wurde**vollstndig von KI generiert**(Claude, Anthropic) unter Verwendung von:\n\n* Natrlicher Sprachverarbeitung zur Behauptungsextraktion\n* Websuche zur Evidenzsammlung\n* Logisches Schlussfolgern zur Szenariogenerierung\n* Probabilistische Bewertung fr Urteile\n* Systematische Methodik zur Qualittssicherung\n\n**Status der Humanprfung**: Ausstehend (Empfohlen fr Risikoklasse A-Behauptungen)\n\n----\n\n== Fazit ==\n\nHagebuttenpulver ist ein**vielversprechendes, aber noch nicht abschlieend belegtes Mittel**bei Arthrose. Die Evidenzlage zeigt:\n\n=== **Positiv**: ===\n\n* Mehrere Studien zeigen Schmerzlinderung bei 60-65% der Anwender\n* Gut vertrglich mit wenigen Nebenwirkungen\n* Plausibler Wirkmechanismus (Galaktolipide)\n* Kostengnstig, natrlich, niedriges Risiko\n\n=== **Einschrnkungen**: ===\n\n* Methodische Schwchen der Studien (klein, herstellerfinanziert, kurz)\n* Widersprchliche Ergebnisse\n* Nur Symptomlinderung, keine strukturelle Wirkung\n* Fehlender Vergleich zu Standardmedikamenten\n* bertragbarkeit von standardisiertem Extrakt auf normales Pulver fraglich\n\n=== **Praktische Empfehlung**: ===\n\n**Fr wen geeignet**:\n\n* Leichte bis mittlere Arthrose\n* Patienten, die natrliche Alternativen bevorzugen\n* Ergnzend zu (nicht statt) Standardtherapie\n* Unter rztlicher Begleitung\n\n**Dosierung**: 5g tglich, mindestens 3-4 Monate\n**Realistische Erwartung**: Milde bis moderate Schmerzlinderung bei etwa 2/3 der Anwender\n**Kosten-Nutzen**: Vertretbar (geringe Kosten, niedriges Risiko)\n\n**Wichtig**:\n\n* Kein Ersatz fr etablierte Arthrosetherapien\n* Bei schwerer Arthrose nur ergnzend\n* rztliche Beratung vor Beginn\n* Nach 3 Monaten Wirksamkeit evaluieren\n\n----\n\n//Dieser Bericht wurde von FactHarbor v0.9.18 POC erstellt  einem KI-gesttzten Evidenzanalysesystem zur Schaffung von Transparenz bei umstrittenen Gesundheitsbehauptungen durch strukturierte Bewertung mit expliziten Annahmen, Konfidenz-Scores und Qualittsvalidierung.//\n\n**Risikoklassen-Legende**:\n\n* **Klasse A**(Hochrisiko): Gesundheit, Sicherheit, Finanzen, Recht - erfordert Expertenprfung\n* **Klasse B**(Mittelrisiko): Technisch, historisch, interpretativ - Standardprfung\n* **Klasse C**(Niedrigrisiko): Deskriptiv, etablierte Fakten - Leichtprfung\n\n**Modus-Legende**:\n\n* **Modus 1**(Entwurf): Qualittsgates nicht bestanden oder Prfung ausstehend\n* **Modus 2**(KI-generiert): ffentlich mit klarer KI-Kennzeichnung\n* **Modus 3**(Expertengeprft): Von Experten validiert, hchstes Vertrauen\n\n----\n\n**Erstellt**: 15. Dezember 2025\n**Version**: FactHarbor 0.9.18 POC\n**Analyse-ID**: Hagebuttenpulver_Arthrose_20251215_DE\n**Format**: Menschenlesbarer Markdown-Bericht (Deutsch)", "Product Development.Specification.FH Analysis Reports.WebHome": "= Analysis Reports (Early Trials) =\n\nSample analyses produced during FactHarbor's POC development phase, demonstrating the evidence-based analysis methodology across different domains and languages.\n\n{{info}}\nThese reports were produced during POC development (v0.9.x). The analysis methodology, prompt architecture, and quality gates have evolved significantly since. They are retained as reference examples and for quality regression testing.\n{{/info}}\n\n----\n\n== English Reports ==\n\n* **[[F-35 Remote Control and 'Kill Switch' Claims>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA - F-35 Remote Control and 'Kill Switch' Claims.WebHome]]**  Analysis of claims about remote control capabilities in the F-35 fighter jet (December 2025)\n* **[[AHA Alcohol and CVD Statement>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.WebHome]]**  Analysis of the American Heart Association's statement on alcohol and cardiovascular disease (December 2025)\n** [[Analysis Summary>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Analysis Summary.WebHome]] | [[Claim Summary>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA - AHA Alcohol CVD Statement.Claim Summary.WebHome]]\n* **[[Bolsonaro Trial Fairness>>FactHarbor.Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.WebHome]]**  Was the Bolsonaro judgment fair and based on Brazil's law? (February 2026)\n** [[Analysis Summary>>FactHarbor.Product Development.Specification.FH Analysis Reports.FactHarbor_Analysis_Bolsonaro_Trial_Fairness.Analysis Summary.WebHome]]\n\n== German Reports ==\n\n* **[[Hagebuttenpulver und Arthrose>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA Claude - Hagebuttenpulver Arthrose DE.WebHome]]**  Analyse der Wirksamkeit von Hagebuttenpulver bei Arthrose (Dezember 2025)\n* **[[Chemtrails-Verschwoerungstheorie>>FactHarbor.Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Chemtrails_Verschwoerungstheorie_DE_Claude.WebHome]]**  Analyse der Chemtrails-Verschwoerungstheorie (Dezember 2025)\n* **[[Schweizer Gerontokratie>>FactHarbor.Product Development.Specification.FH Analysis Reports.FactHarbor_Analyse_Schweiz_Gerontokratie_DE_Claude.WebHome]]**  Analyse zur Ueberalterung der Schweizer Politik (Dezember 2025)\n\n== Other Reports ==\n\n* **[[CitizenGO Petition on UN Doha Summit>>FactHarbor.Product Development.Specification.FH Analysis Reports.FHA - CitizenGO UN Doha Petition.WebHome]]**  Analysis of the CitizenGO petition \"Stoppt den Machtgriff der UN-Agenda\" (December 2025)\n", "Product Development.Specification.Implementation.Implementation Status and Quality.WebHome": "= Implementation Status and Quality =\n\n== Implementation Status ==\n\n=== Specification Alignment ===\n\n|= Area |= Status |= Notes\n| **Job orchestration** | Implemented | API stores job + events in SQLite; Web runner updates via internal endpoints; SSE endpoint for live events\n| **Quality Gates (POC)** | Partially implemented | Analyzer applies Gate 1 (claim validation) and Gate 4 (verdict confidence); gate stats included in result JSON; display of per-item gate reasons still missing in UI/report\n| **Source reliability** | Implemented | LLM-powered Source Reliability Service with sequential refinement (Claude + OpenAI), SQLite cache, multi-language support; sources store trackRecordScore/category; configured via UCM\n| **Evidence model** | Partial | Claims + extracted evidence + verdicts exist in result JSON; ClaimAssessmentBoundaries embedded in result JSON (not separate table)\n| **KeyFactors** | Implemented | Discovered in Understanding, emergent and optional, claim-to-factor mapping via ##keyFactorId##, aggregated from claim verdicts, displayed in reports (aggregation fixed 2026-01-06)\n| **AuthN/AuthZ & rate limiting** | Missing | Public UI and endpoints are open; admin test endpoints are unauthenticated; CORS is permissive in API\n| **Persistence (normalized)** | Missing | API persists job metadata + JSON/markdown results; no normalized tables for claims/evidence/sources/verdicts\n| **Caching & separated architecture** | Missing | Docs propose claim cache; current pipeline recomputes per job\n| **Testing** | Partial | Web has unit/integration tests for analyzer; API has no tests; CI only builds\n\n=== Working Features (v2.6.38) ===\n\n**Core Analysis:**\n* Multi-context detection and display\n* Context overlap detection with LLM-driven merge heuristics (v2.6.38)\n* Defensive validation: context count warnings, claim assignment validation (v2.6.38)\n* Input neutrality (question ~ statement within +/-5%)\n* Context extraction from sources\n* Temporal reasoning (current date awareness)\n* Claim deduplication for fair aggregation\n* KeyFactors aggregation\n* Dependency tracking and propagation\n* Pseudoscience detection and escalation\n* 7-point verdict scale (TRUE to FALSE)\n* MIXED vs UNVERIFIED distinction (confidence-based)\n* UI reliability signals for multi-context verdicts (v2.6.38)\n\n**Infrastructure:**\n* Job lifecycle management (QUEUED -> RUNNING -> SUCCEEDED/FAILED)\n* Real-time progress updates via SSE\n* Exponential backoff retry with jitter in RunnerClient\n* PDF and HTML content extraction\n* Multi-provider LLM support (Anthropic, OpenAI, Google, Mistral)\n* Multi-provider search support (Google CSE, SerpAPI)\n\n=== Known Gaps and Issues ===\n\n**High Priority:**\n1. **SSRF Protection**: URL fetching needs IP range blocking, size limits, redirect caps\n1. **Admin Endpoint Security**: ##/admin/test-config## is publicly accessible and can trigger paid LLM calls\n1. **Rate Limiting**: No per-IP or per-user rate limits\n1. **Quality Gate Display**: Gate stats exist but not shown in UI with per-item reasons\n\n**Medium Priority:**\n1. **Metrics Tracking**: LLM token usage, search API calls, cost estimation not persisted\n1. **Error Pattern Tracking**: No database schema for error patterns\n1. **Model Knowledge Toggle**: ##FH_ALLOW_MODEL_KNOWLEDGE=false## not fully respected\n1. **Provider-Specific Optimization**: Same prompts used for all LLM providers\n\n**Low Priority:**\n1. **URL Analyses**: URL string highlighted in reports as \"claim\"\n1. **LLM Fallback**: Config documented but not implemented\n1. **Rich Report Mode**: ##FH_REPORT_STYLE=rich## documented but not implemented\n\n=== Recent Fixes (January 2026) ===\n\n**v2.6.25:**\n* Question-to-statement handling improvements\n* ArticleSummary data generation logic\n* UI layout improvements for summary page\n\n**v2.6.24:**\n* Fixed critical ##isValidImpliedClaim## bug\n* Rating direction instructions strengthened\n* Centrality over-marking reduced\n* Question label misapplication fixed\n\n**v2.6.23:**\n* Input neutrality divergence fixed (4% -> 1%)\n* Canonicalization context detection corrected\n* Generic recency detection enhanced\n\n**v2.6.18-v2.6.22:**\n* Runner resilience with exponential backoff\n* Job lifecycle tests added\n* Analyzer modularization started\n* KeyFactors aggregation fixed\n* PDF fetch error handling improved\n\n----\n\n== Quality & Optimization ==\n\n=== Quality Gates ===\n\n> For detailed quality gates reference, see [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]].\n\n**Gate 1: Claim Validation**\n* Filters out opinions, predictions, low-specificity claims\n* Keeps central claims regardless of specificity\n* Tracks excluded claims with reasons\n* Stats included in ##qualityGates.gate1Stats##\n\n**Gate 4: Verdict Confidence Assessment**\n* Requires minimum number of sources\n* Source quality threshold\n* Agreement threshold between sources\n* Central claims remain publishable even if low confidence\n* Stats included in ##qualityGates.gate4Stats##\n\n**Implementation Location:**\n* Gate 1: Applied in ##understandClaim()## during claim extraction\n* Gate 4: Applied in ##generateVerdicts()## during verdict generation\n\n=== Verdict Calculation ===\n\nSee Calculations.md (see Calculations.md in local docs) for detailed verdict calculation methodology, including:\n* 7-point scale mapping\n* MIXED vs UNVERIFIED distinction\n* Counter-evidence handling\n* Aggregation hierarchy (Evidence -> Claims -> KeyFactors -> Contexts -> Overall)\n* Dependency handling\n* Pseudoscience escalation\n* Benchmark guard\n\n=== Cost Optimization Opportunities ===\n\n**Multi-Tier Model Strategy** (not yet implemented):\n* Use cheaper models (Claude Haiku) for extraction tasks\n* Use premium models (Claude Sonnet) for reasoning tasks\n* Estimated savings: 50-70% on LLM costs\n\n**Claim Caching** (not yet implemented):\n* Cache normalized claim verdicts\n* Reuse verdicts across analyses\n* Estimated savings: 30-50% on repeat claims\n\n**Search Optimization:**\n* Limit sources by setting Pipeline config ##analysisMode## (quick vs deep) and iteration limits in UCM (Admin -> Config -> Pipeline). Defaults live in ##apps/web/src/lib/analyzer/config.ts##.\n* Use Search config ##domainWhitelist## to improve relevance\n* Use Search config ##dateRestrict## for recent topics\n\n=== Performance Characteristics ===\n\n**Typical Analysis Time:**\n* Short text (1-2 claims): 30-60 seconds\n* Medium article (5-10 claims): 2-5 minutes\n* Long article (20+ claims): 5-15 minutes\n\n**LLM Calls:**\n* Understanding: 1 call\n* Research: 2-6 calls (per source)\n* Verdict: 1-3 calls (depending on claim count)\n* Total: Typically 10-20 calls per analysis\n\n**Search Queries:**\n* Typically 3-6 queries per analysis\n* Fetches 4-8 sources total\n* Parallel source fetching with 5-second timeout per source\n\n----\n\n== Future Enhancements ==\n\n=== Planned Improvements ===\n\n**Security (Pre-Release):**\n* SSRF protection implementation\n* Authentication and authorization system\n* Rate limiting and quota enforcement\n* CORS tightening for production\n\n**Performance:**\n* Tiered LLM model routing\n* Claim-level caching and separated architecture\n* Parallel verdict generation\n* Optimized prompt templates per provider\n\n**Features:**\n* Quality gate visualization in UI\n* Metrics dashboard with cost tracking\n* Error pattern analysis\n* Historical track record for sources\n* Multi-language support\n\n**Data Model:**\n* Normalized database tables for claims/evidence/sources/verdicts\n* Provenance chain tracking\n* Normalized ClaimAssessmentBoundary persistence\n\n----\n\n== Testing Infrastructure ==\n\n=== Promptfoo Test Coverage (v2.8.2) ===\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Promptfoo Test Coverage.WebHome\"/}}\n\n=== Test Summary ===\n\n|= Config |= Description |= Test Cases |= Prompts Covered\n| ##source-reliability## | Source reliability evaluation | 7 | 1\n| ##verdict## | Verdict generation accuracy | 5 | 1\n| ##text-analysis## | LLM text analysis pipeline | 26 | 4\n| **Total** | | **38** | **6**\n\n=== Running Tests ===\n\n{{code language=\"bash\"}}\n# Run all tests\nnpm run promptfoo:all\n\n# Run specific test suite\nnpm run promptfoo:sr              # Source reliability\nnpm run promptfoo:verdict         # Verdict generation\nnpm run promptfoo:text-analysis   # Text analysis pipeline\n\n# View results\nnpm run promptfoo:view\n{{/code}}\n\nSee: Promptfoo Testing Guide (in User Guides)\n\n----\n\n== References ==\n\n=== Related Documentation ===\n\n* **Context Detection**: [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]] - Consolidated context detection guide\n* **Quality Gates**: [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] - Quality gates reference\n* **Calculations**: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] - Verdict calculation methodology\n* **KeyFactors Design**: [[KeyFactors Design>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome]] - KeyFactors implementation details\n* **Source Reliability**: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] - Source scoring system\n* **Prompt Architecture**: [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]] - Modular prompt composition system\n* **Promptfoo Testing**: [[Promptfoo Testing>>FactHarbor.Product Development.DevOps.Tooling.Promptfoo Testing.WebHome]] - Prompt testing guide\n* **Pipeline Architecture**: [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] - Twin-path pipeline design\n* **Getting Started**: [[Getting Started>>FactHarbor.Product Development.DevOps.Guidelines.Getting Started.WebHome]] - Setup and installation\n* **LLM Configuration**: [[LLM Configuration>>FactHarbor.Product Development.DevOps.Subsystems and Components.LLM Configuration.WebHome]] - Provider configuration\n\n=== Key Environment Variables ===\n\n**Configuration Management:**\n* **LLM Provider Selection:** Configure via UCM pipeline config (##pipeline.llmProvider##)\n* **Analysis Behavior:** Configure via UCM (pipeline/search/calculation/SR configs)\n* **DEPRECATED:** ##LLM_PROVIDER## environment variable is no longer used (removed 2026-02-02)\n\n**Environment variables are only for:**\n* API keys and secrets (ANTHROPIC_API_KEY, OPENAI_API_KEY, etc.)\n* Deployment settings (PORT, NODE_ENV, etc.)\n* Infrastructure config (DATABASE_URL, FH_ADMIN_KEY, etc.)\n\n|= Variable |= Default |= Purpose\n| ##ANTHROPIC_API_KEY## | - | Anthropic Claude API key (required if using Anthropic)\n| ##OPENAI_API_KEY## | - | OpenAI API key (required if using OpenAI)\n| ##GOOGLE_GENERATIVE_AI_API_KEY## | - | Google Gemini API key (required if using Google)\n| ##MISTRAL_API_KEY## | - | Mistral API key (required if using Mistral)\n| ##SERPAPI_API_KEY## | - | SerpAPI search key (required if using SerpAPI)\n| ##GOOGLE_CSE_API_KEY## | - | Google Custom Search API key (required if using Google CSE)\n| ##GOOGLE_CSE_ID## | - | Google Custom Search Engine ID (required if using Google CSE)\n| ##FH_RUNNER_MAX_CONCURRENCY## | ##3## | Max parallel analysis jobs\n| ##FH_ADMIN_KEY## | - | Admin endpoints authentication\n| ##FH_INTERNAL_RUNNER_KEY## | - | Internal job execution authentication\n| ~~##LLM_PROVIDER##~~ | ~~-~~ | **DEPRECATED** (use UCM ##pipeline.llmProvider##)\n\n----\n\n== Recent Updates ==\n\n=== v2.6.38 (January 26, 2026) ===\n* **Context Overlap Detection**: LLM-driven merge heuristics with temporal guidance clarification\n* **Defensive Validation**: Context count warnings (5+ threshold) and claim assignment validation\n* **UI Reliability Signals**: ##articleVerdictReliability## field added to signal when overall average is meaningful\n* **Transparency**: De-emphasize unreliable averages, emphasize individual context verdicts in UI\n\n----\n\n**Last Updated**: February 3, 2026\n**Document Status**: Living document - updated as architecture evolves\n\n----\n\n**Navigation:** [[Implementation>>FactHarbor.Product Development.Specification.Implementation.WebHome]] | Related: [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n", "Product Development.Specification.Implementation.WebHome": "= Implementation =\n\n{{warning}}\n**Documentation Restructured.** Implementation content has been consolidated into the [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] section for a unified reading experience. Use the mapping below to find the new locations.\n{{/warning}}\n\n== Page Mapping ==\n\n|= Old Location |= New Location\n| AKEL Pipeline Flow | [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]] (overview) and [[ClaimAssessmentBoundary Pipeline Detail>>FactHarbor.Product Development.Diagrams.ClaimAssessmentBoundary Pipeline Detail.WebHome]] (detail)\n| Pipeline Architecture | [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]] (overview)\n| Twin-Path Architecture | [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]\n| Context and EvidenceScope Detection | [[Context Detection>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Context Detection.WebHome]]\n| Quality Gates Reference | [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]]\n| Source Reliability System (5 sub-pages) | [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]\n| KeyFactors Design | [[KeyFactors Design>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.KeyFactors Design.WebHome]]\n| Schema Migration Strategy | [[Schema Migration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Schema Migration.WebHome]]\n\n== Remaining Pages ==\n\nThese pages were not moved and remain in this section:\n\n* [[Implementation Status and Quality>>FactHarbor.Product Development.Specification.Implementation.Implementation Status and Quality.WebHome]]  Spec alignment, working features, known gaps, quality gates, performance, testing infrastructure\n\n----\n\n**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.Codegen Contract.WebHome": "= POC1 Codegen Contract =\n\n----\n\n== Version History ==\n\n|=Version|=Date|=Changes\n|0.4.1|2025-12-24|Applied 9 critical fixes: file format notice, verdict taxonomy, canonicalization algorithm, Stage 1 cost policy, BullMQ fix, language in cache key, historical claims TTL, idempotency, copyright policy\n|0.4|2025-12-24|**BREAKING:** 3-stage pipeline with claim-level caching, user tier system, cache-only mode for free users, Redis cache architecture\n|0.3.1|2025-12-24|Fixed single-prompt strategy, SSE clarification, schema canonicalization, cost constraints\n|0.3|2025-12-24|Added complete API endpoints, LLM config, risk tiers, scraping details\n\n----\n\n{{info}}\n**Canonical Data Model:** See [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] for the canonical data model specification. This page documents the POC1 API contract and endpoint schemas.\n{{/info}}\n\n== POC1 Codegen Contract (Canonical) ==\n\n{{info}}\nThis section is the **authoritative, code-generation-ready contract** for POC1.\nIf any other page conflicts with this section, **this section wins**.\n{{/info}}\n\n=== Canonical outputs ===\n* **result.json**: schema-validated, machine-readable output\n* **report.md**: deterministic template rendering from ``result.json`` (LLM must not free-write the final report)\n\n=== Locked enums ===\n**Scenario verdict** (``ScenarioVerdict.verdict_label``):\n* ``Highly likely`` | ``Likely`` | ``Unclear`` | ``Unlikely`` | ``Highly unlikely`` | ``Unsubstantiated``\n\n**Claim verdict** (``ClaimVerdict.verdict_label``):\n* ``Supported`` | ``Refuted`` | ``Inconclusive``\n\n**Mapping rule (summary):**\n* Primary-interpretation scenario:\n** ``Highly likely`` / ``Likely``  ``Supported``\n** ``Highly unlikely`` / ``Unlikely``  ``Refuted``\n** ``Unclear`` / ``Unsubstantiated``  ``Inconclusive``\n* If scenarios materially disagree (assumption-dependent outcomes)  ``Inconclusive`` (explain why)\n\n=== Deterministic claim normalization (cache key) ===\n* Normalization version: ``v1norm1``\n* Cache namespace: ``claim:v1norm1:{language}:{sha256(canonical_claim_text)}``\n* Normative reference implementation is defined in [[Data Schemas and Cache>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome]] section 5.1.1 (no ellipses; must match exactly).\n\n=== Idempotency ===\nClients SHOULD send:\n* Header: ``Idempotency-Key: <client-generated-uuid>`` (preferred)\nor\n* Body: ``client.request_id``\n\nServer rules:\n* Same key + same request body  return existing job (``200``) and include ``idempotent=true``.\n* Same key + different request body  ``409`` ``VALIDATION_ERROR``.\n\nIdempotency TTL: 24 hours.\n\n=== Minimal OpenAPI 3.1 (authoritative for codegen) ===\n{{code language=\"yaml\"}}\nopenapi: 3.1.0\ninfo:\n  title: FactHarbor POC1 API\n  version: 0.9.106\nservers:\n  - url: /\npaths:\n  /v1/analyze:\n    post:\n      summary: Create analysis job\n      parameters:\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Idempotency-Key\n          required: false\n          schema: { type: string }\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/AnalyzeRequest'\n      responses:\n        '202':\n          description: Accepted\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/JobCreated'\n        '4XX':\n          description: Error\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorEnvelope'\n  /v1/jobs/{job_id}:\n    get:\n      summary: Get job status\n      parameters:\n        - in: path\n          name: job_id\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Job'\n        '404':\n          description: Not Found\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorEnvelope'\n    delete:\n      summary: Cancel job (best-effort) and delete artifacts\n      parameters:\n        - in: path\n          name: job_id\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n      responses:\n        '204': { description: No Content }\n        '404':\n          description: Not Found\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorEnvelope'\n  /v1/jobs/{job_id}/events:\n    get:\n      summary: Job progress via SSE (no token streaming)\n      parameters:\n        - in: path\n          name: job_id\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n      responses:\n        '200':\n          description: text/event-stream\n  /v1/jobs/{job_id}/result:\n    get:\n      summary: Get final JSON result\n      parameters:\n        - in: path\n          name: job_id\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n      responses:\n        '200':\n          description: OK\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/AnalysisResult'\n        '409':\n          description: Not ready\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorEnvelope'\n  /v1/jobs/{job_id}/report:\n    get:\n      summary: Download report (markdown)\n      parameters:\n        - in: path\n          name: job_id\n          required: true\n          schema: { type: string }\n        - in: header\n          name: Authorization\n          required: true\n          schema: { type: string }\n      responses:\n        '200':\n          description: text/markdown\n        '409':\n          description: Not ready\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ErrorEnvelope'\n  /v1/health:\n    get:\n      summary: Health check\n      responses:\n        '200':\n          description: OK\ncomponents:\n  schemas:\n    AnalyzeRequest:\n      type: object\n      properties:\n        input_url: { type: ['string', 'null'] }\n        input_text: { type: ['string', 'null'] }\n        options:\n          type: object\n          properties:\n            max_claims: { type: integer, minimum: 1, maximum: 50, default: 5 }\n            cache_preference:\n              type: string\n              enum: [prefer_cache, allow_partial, cache_only, skip_cache]\n              default: prefer_cache\n            browsing:\n              type: string\n              enum: [on, off]\n              default: on\n            output_report: { type: boolean, default: true }\n        client:\n          type: object\n          properties:\n            request_id: { type: string }\n    JobCreated:\n      type: object\n      required: [job_id, status, created_at, links]\n      properties:\n        job_id: { type: string }\n        status: { type: string }\n        created_at: { type: string }\n        links:\n          type: object\n          properties:\n            self: { type: string }\n            events: { type: string }\n            result: { type: string }\n            report: { type: string }\n    Job:\n      type: object\n      required: [job_id, status, created_at, updated_at]\n      properties:\n        job_id: { type: string }\n        status:\n          type: string\n          enum: [QUEUED, RUNNING, SUCCEEDED, FAILED, CANCELED]\n        created_at: { type: string }\n        updated_at: { type: string }\n    AnalysisResult:\n      type: object\n      properties:\n        job_id: { type: string }\n    ErrorEnvelope:\n      type: object\n      properties:\n        error:\n          type: object\n          properties:\n            code: { type: string }\n            message: { type: string }\n            details: { type: object }\n{{/code}}\n\n----\n\n**Navigation:** [[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] | Next: [[Pipeline Architecture>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome": "= Data Schemas and Cache =\n\n== 4. Data Schemas ==\n\n=== 4.1 Stage 1 Output: ClaimExtraction ===\n\n{{{{\n \"job_id\": \"01J...ULID\",\n \"stage\": \"stage1_extraction\",\n \"article_metadata\": {\n \"title\": \"Article title\",\n \"source_url\": \"https://example.com/article\",\n \"extracted_text_length\": 5234,\n \"language\": \"en\"\n },\n \"claims\": [\n {\n \"claim_id\": \"C1\",\n \"claim_text\": \"Original claim text from article\",\n \"canonical_claim\": \"Normalized, deduplicated phrasing\",\n \"claim_hash\": \"sha256:abc123...\",\n \"is_central_to_thesis\": true,\n \"claim_type\": \"causal\",\n \"evaluability\": \"evaluable\",\n \"risk_tier\": \"B\",\n \"domain\": \"public_health\"\n }\n ],\n \"article_thesis\": \"Main argument detected\",\n \"cost\": 0.003\n}\n}}}\n\n----\n\n=== 4.5 Verdict Label Taxonomy ===\n\nFactHarbor uses **three distinct verdict taxonomies** depending on analysis level:\n\n==== 4.5.1 Scenario Verdict Labels (Stage 2) ====\n\nUsed for individual scenario verdicts within a claim.\n\n**Enum Values:**\n\n* Highly Likely - Probability 0.85-1.0, high confidence\n* Likely - Probability 0.65-0.84, moderate-high confidence\n* Unclear - Probability 0.35-0.64, or low confidence\n* Unlikely - Probability 0.16-0.34, moderate-high confidence\n* Highly Unlikely - Probability 0.0-0.15, high confidence\n* Unsubstantiated - Insufficient evidence to determine probability\n\n==== 4.5.2 Claim Verdict Labels (Rollup) ====\n\nUsed when summarizing a claim across all scenarios.\n\n**Enum Values:**\n\n* Supported - Majority of scenarios are Likely or Highly Likely\n* Refuted - Majority of scenarios are Unlikely or Highly Unlikely\n* Inconclusive - Mixed scenarios or majority Unclear/Unsubstantiated\n\n**Mapping Logic:**\n\n* If 60% scenarios are (Highly Likely | Likely)  Supported\n* If 60% scenarios are (Highly Unlikely | Unlikely)  Refuted\n* Otherwise  Inconclusive\n\n==== 4.5.3 Article Verdict Labels (Stage 3) ====\n\nUsed for holistic article-level assessment.\n\n**Enum Values:**\n\n* WELL-SUPPORTED - Article thesis logically follows from supported claims\n* MISLEADING - Claims may be true but article commits logical fallacies\n* REFUTED - Central claims are refuted, invalidating thesis\n* UNCERTAIN - Insufficient evidence or highly mixed claim verdicts\n\n**Note:** Article verdict considers **claim centrality** (central claims override supporting claims).\n\n==== 4.5.4 API Field Mapping ====\n\n|=Level|=API Field|=Enum Name\n|Scenario|scenarios[].verdict.label|scenario_verdict_label\n|Claim|claims[].rollup_verdict (optional)|claim_verdict_label\n|Article|article_holistic_assessment.overall_verdict|article_verdict_label\n\n----\n\n== 5. Cache Architecture ==\n\n=== 5.1 Redis Cache Design ===\n\n**Technology:** Redis 7.0+ (in-memory key-value store)\n\n**Cache Key Schema:**\n\n{{{claim:v1norm1:{language}:{sha256(canonical_claim)}\n}}}\n\n**Example:**\n\n{{{Claim (English): \"COVID vaccines are 95% effective\"\nCanonical: \"covid vaccines are 95 percent effective\"\nLanguage: \"en\"\nSHA256: abc123...def456\nKey: claim:v1norm1:en:abc123...def456\n}}}\n\n**Rationale:** Prevents cross-language collisions and enables per-language cache analytics.\n\n**Data Structure:**\n\n{{{SET claim:v1norm1:en:abc123...def456 '{...ClaimAnalysis JSON...}'\nEXPIRE claim:v1norm1:en:abc123...def456 7776000 # 90 days\n}}}\n\n----\n\n=== 5.1.1 Canonical Claim Normalization (v1norm1) ===\n\nThe cache key depends on deterministic claim normalization. **All implementations MUST follow this algorithm exactly.**\n\n**Normalization version:** ``v1norm1``\n\n**Algorithm (v1norm1):**\n1. Unicode normalize: NFD\n2. Lowercase\n3. Strip diacritics\n4. Normalize apostrophes: ``'`` and ``'``  ``'``\n5. Replace percent sign: ``%``  `` percent``\n6. Collapse whitespace\n7. Remove punctuation **except apostrophes**\n8. Expand contractions (fixed list below)\n9. Remove remaining apostrophes\n10. Collapse whitespace again\n\n{{code language=\"python\"}}\nimport re\nimport unicodedata\n\n# Canonical claim normalization for deduplication.\n# Version: v1norm1\n#\n# IMPORTANT:\n# - Any change to these rules REQUIRES a new normalization version.\n# - Cache keys MUST include the normalization version to avoid collisions.\n\nCONTRACTIONS_V1NORM1 = {\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"can't\": \"cannot\",\n    \"won't\": \"will not\",\n    \"shouldn't\": \"should not\",\n    \"wouldn't\": \"would not\",\n    \"isn't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"wasn't\": \"was not\",\n    \"weren't\": \"were not\",\n    \"haven't\": \"have not\",\n    \"hasn't\": \"has not\",\n    \"hadn't\": \"had not\",\n    \"it's\": \"it is\",\n    \"that's\": \"that is\",\n    \"there's\": \"there is\",\n    \"i'm\": \"i am\",\n    \"we're\": \"we are\",\n    \"they're\": \"they are\",\n    \"you're\": \"you are\",\n    \"i've\": \"i have\",\n    \"we've\": \"we have\",\n    \"they've\": \"they have\",\n    \"you've\": \"you have\",\n    \"i'll\": \"i will\",\n    \"we'll\": \"we will\",\n    \"they'll\": \"they will\",\n    \"you'll\": \"you will\",\n}\n\ndef normalize_claim(text: str) -> str:\n    if text is None:\n        return \"\"\n\n    # 1) Unicode normalization (NFD)\n    text = unicodedata.normalize(\"NFD\", text)\n\n    # 2) Lowercase\n    text = text.lower()\n\n    # 3) Strip diacritics\n    text = \"\".join(c for c in text if unicodedata.category(c) != \"Mn\")\n\n    # 4) Normalize apostrophes\n    text = text.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\")\n\n    # 5) Normalize percent sign\n    text = text.replace(\"%\", \" percent\")\n\n    # 6) Collapse whitespace\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    # 7) Remove punctuation except apostrophes\n    text = re.sub(r\"[^\\w\\s']\", \"\", text)\n\n    # 8) Expand contractions\n    for k, v in CONTRACTIONS_V1NORM1.items():\n        text = re.sub(rf\"\\b{re.escape(k)}\\b\", v, text)\n\n    # 9) Remove remaining apostrophes (after contraction expansion)\n    text = text.replace(\"'\", \"\")\n\n    # 10) Final whitespace normalization\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    return text\n{{/code}}\n\n**Canonical claim hash input (normative):**\n* ``claim_hash = sha256_hex_lower( \"v1norm1|<language>|\" + canonical_claim_text )``\n* Cache key: ``claim:v1norm1:<language>:<claim_hash>``\n\n**Normalization Examples:**\n\n|= Input |= Normalized Output\n| \"Biden won the 2020 election\" | {{code}}biden won the 2020 election{{/code}}\n| \"Biden won the 2020 election!\" | {{code}}biden won the 2020 election{{/code}}\n| \"Biden  won   the 2020  election\" | {{code}}biden won the 2020 election{{/code}}\n| \"Biden didn't win the 2020 election\" | {{code}}biden did not win the 2020 election{{/code}}\n| \"BIDEN WON THE 2020 ELECTION\" | {{code}}biden won the 2020 election{{/code}}\n\n**Versioning:** Algorithm version is {{code}}v1norm1{{/code}}. Changes to the algorithm require a new version identifier.\n\n=== 5.1.2 Copyright & Data Retention Policy ===\n\n**Evidence Excerpt Storage:**\n\nTo comply with copyright law and fair use principles:\n\n**What We Store:**\n\n* **Metadata only:** Title, author, publisher, URL, publication date\n* **Short excerpts:** Max 25 words per quote, max 3 quotes per evidence item\n* **Summaries:** AI-generated bullet points (not verbatim text)\n* **No full articles:** Never store complete article text beyond job processing\n\n**Total per Cached Claim:**\n\n* Scenarios: 2 per claim\n* Evidence items: 6 per scenario (12 total)\n* Quotes: 3 per evidence  25 words = 75 words per item\n* **Maximum stored verbatim text:** ~~900 words per claim (12  75)\n\n**Retention:**\n\n* Cache TTL: 90 days\n* Job outputs: 24 hours (then archived or deleted)\n* No persistent full-text article storage\n\n**Rationale:**\n\n* Short excerpts for citation = fair use\n* Summaries are transformative (not copyrightable)\n* Limited retention (90 days max)\n* No commercial republication of excerpts\n\n**DMCA Compliance:**\n\n* Cache invalidation endpoint available for rights holders\n* Contact: dmca@factharbor.org\n\n----\n\n== Summary ==\n\nThis specification covers the data schemas and cache architecture for the POC1 API:\n\n* **Data Schemas**: Stage 1 output (ClaimExtraction), verdict label taxonomy (3-level: Scenario, Claim, Article), API field mapping\n* **Cache Architecture**: Redis cache design, canonical claim normalization (v1norm1), copyright & data retention policy\n\n**Full specification sections:**\n\n* [[Codegen Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Codegen Contract.WebHome]]  OpenAPI spec and locked enums\n* [[Pipeline Architecture>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome]]  3-stage pipeline and scoring algorithms\n* [[LLM Abstraction Layer>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome]]  Provider abstraction and failover\n* [[REST API Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome]]  Endpoints and response schemas\n\n----\n\n**Navigation:** [[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] | Prev: [[REST API Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome": "= LLM Abstraction Layer =\n\n== 6.1 Design Principle ==\n\n**FactHarbor uses provider-agnostic LLM abstraction** to avoid vendor lock-in and enable:\n\n* **Provider switching:** Change LLM providers without code changes\n* **Cost optimization:** Use different providers for different stages\n* **Resilience:** Automatic fallback if primary provider fails\n* **Cross-checking:** Compare outputs from multiple providers\n* **A/B testing:** Test new models without deployment changes\n\n**Implementation:** All LLM calls go through an abstraction layer that routes to configured providers.\n\n----\n\n== 6.2 LLM Provider Interface ==\n\n**Abstract Interface:**\n\n{{{\ninterface LLMProvider {\n  // Core methods\n  complete(prompt: string, options: CompletionOptions): Promise<CompletionResponse>\n  stream(prompt: string, options: CompletionOptions): AsyncIterator<StreamChunk>\n\n  // Provider metadata\n  getName(): string\n  getMaxTokens(): number\n  getCostPer1kTokens(): { input: number, output: number }\n\n  // Health check\n  isAvailable(): Promise<boolean>\n}\n\ninterface CompletionOptions {\n  model?: string\n  maxTokens?: number\n  temperature?: number\n  stopSequences?: string[]\n  systemPrompt?: string\n}\n}}}\n\n----\n\n== 6.3 Supported Providers (POC1) ==\n\n**Primary Provider (Default):**\n\n* **Anthropic Claude API**\n  * Models (examples; not normative): Provider-default FAST model, Provider-default REASONING model, Provider-default HEAVY model (optional)\n  * Used by default in POC1\n  * Best quality for holistic analysis\n\n**Secondary Providers (Future):**\n\n* **OpenAI API**\n  * Models: GPT-4o, GPT-4o-mini\n  * For cost comparison\n\n* **Google Vertex AI**\n  * Models: Gemini 1.5 Pro, Gemini 1.5 Flash\n  * For diversity in evidence gathering\n\n* **Local Models** (Post-POC)\n  * Models: Llama 3.1, Mistral\n  * For privacy-sensitive deployments\n\n----\n\n== 6.4 Provider Configuration ==\n\n**Environment Variables:**\n\n{{{\n# Primary provider\nLLM_PRIMARY_PROVIDER=anthropic\nANTHROPIC_API_KEY=sk-ant-...\n\n# Fallback provider\nLLM_FALLBACK_PROVIDER=openai\nOPENAI_API_KEY=sk-...\n\n# Provider selection per stage\nLLM_STAGE1_PROVIDER=anthropic\nLLM_STAGE1_MODEL=claude-haiku-4\nLLM_STAGE2_PROVIDER=anthropic\nLLM_STAGE2_MODEL=claude-sonnet-4-5-20250929\nLLM_STAGE3_PROVIDER=anthropic\nLLM_STAGE3_MODEL=claude-sonnet-4-5-20250929\n\n# Cost limits\nLLM_MAX_COST_PER_REQUEST=1.00\n}}}\n\n**Database Configuration (Alternative):**\n\n{{{{\n{\n  \"providers\": [\n    {\n      \"name\": \"anthropic\",\n      \"api_key_ref\": \"vault://anthropic-api-key\",\n      \"enabled\": true,\n      \"priority\": 1\n    },\n    {\n      \"name\": \"openai\",\n      \"api_key_ref\": \"vault://openai-api-key\",\n      \"enabled\": true,\n      \"priority\": 2\n    }\n  ],\n  \"stage_config\": {\n    \"stage1\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-haiku-4-5-20251001\",\n      \"max_tokens\": 4096,\n      \"temperature\": 0.0\n    },\n    \"stage2\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-sonnet-4-5-20250929\",\n      \"max_tokens\": 16384,\n      \"temperature\": 0.3\n    },\n    \"stage3\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-sonnet-4-5-20250929\",\n      \"max_tokens\": 8192,\n      \"temperature\": 0.2\n    }\n  }\n}\n}}}\n\n----\n\n== 6.5 Stage-Specific Models (POC1 Defaults) ==\n\n**Stage 1: Claim Extraction**\n\n* **Default:** Anthropic Provider-default FAST model\n* **Alternative:** OpenAI GPT-4o-mini, Google Gemini 1.5 Flash\n* **Rationale:** Fast, cheap, simple task\n* **Cost:** ~$0.003 per article\n\n**Stage 2: Claim Analysis** (CACHEABLE)\n\n* **Default:** Anthropic Provider-default REASONING model\n* **Alternative:** OpenAI GPT-4o, Google Gemini 1.5 Pro\n* **Rationale:** High-quality analysis, cached 90 days\n* **Cost:** ~$0.081 per NEW claim\n\n**Stage 3: Holistic Assessment**\n\n* **Default:** Anthropic Provider-default REASONING model\n* **Alternative:** OpenAI GPT-4o, Provider-default HEAVY model (optional) (for high-stakes)\n* **Rationale:** Complex reasoning, logical fallacy detection\n* **Cost:** ~$0.030 per article\n\n**Cost Comparison (Example):**\n\n|=Stage|=Anthropic (Default)|=OpenAI Alternative|=Google Alternative\n|Stage 1|Provider-default FAST model ($0.003)|GPT-4o-mini ($0.002)|Gemini Flash ($0.002)\n|Stage 2|Provider-default REASONING model ($0.081)|GPT-4o ($0.045)|Gemini Pro ($0.050)\n|Stage 3|Provider-default REASONING model ($0.030)|GPT-4o ($0.018)|Gemini Pro ($0.020)\n|**Total (0% cache)**|**$0.114**|**$0.065**|**$0.072**\n\n**Note:** POC1 uses Anthropic exclusively for consistency. Multi-provider support planned for POC2.\n\n----\n\n== 6.6 Failover Strategy ==\n\n**Automatic Failover:**\n\n{{{\nasync function completeLLM(stage: string, prompt: string): Promise<string> {\n  const primaryProvider = getProviderForStage(stage)\n  const fallbackProvider = getFallbackProvider()\n\n  try {\n    return await primaryProvider.complete(prompt)\n  } catch (error) {\n    if (error.type === 'rate_limit' || error.type === 'service_unavailable') {\n      logger.warn(`Primary provider failed, using fallback`)\n      return await fallbackProvider.complete(prompt)\n    }\n    throw error\n  }\n}\n}}}\n\n**Fallback Priority:**\n\n1. **Primary:** Configured provider for stage\n2. **Secondary:** Fallback provider (if configured)\n3. **Cache:** Return cached result (if available for Stage 2)\n4. **Error:** Return 503 Service Unavailable\n\n----\n\n== 6.7 Provider Selection API ==\n\n**Admin Endpoint:** POST /admin/v1/llm/configure\n\n**Update provider for specific stage:**\n\n{{{{\n{\n  \"stage\": \"stage2\",\n  \"provider\": \"openai\",\n  \"model\": \"gpt-4o\",\n  \"max_tokens\": 16384,\n  \"temperature\": 0.3\n}\n}}}\n\n**Response:** 200 OK\n\n{{{{\n{\n  \"message\": \"LLM configuration updated\",\n  \"stage\": \"stage2\",\n  \"previous\": {\n    \"provider\": \"anthropic\",\n    \"model\": \"claude-sonnet-4-5-20250929\"\n  },\n  \"current\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-4o\"\n  },\n  \"cost_impact\": {\n    \"previous_cost_per_claim\": 0.081,\n    \"new_cost_per_claim\": 0.045,\n    \"savings_percent\": 44\n  }\n}\n}}}\n\n**Get current configuration:**\n\nGET /admin/v1/llm/config\n\n{{{{\n{\n  \"providers\": [\"anthropic\", \"openai\"],\n  \"primary\": \"anthropic\",\n  \"fallback\": \"openai\",\n  \"stages\": {\n    \"stage1\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-haiku-4-5-20251001\",\n      \"cost_per_request\": 0.003\n    },\n    \"stage2\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-sonnet-4-5-20250929\",\n      \"cost_per_new_claim\": 0.081\n    },\n    \"stage3\": {\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-sonnet-4-5-20250929\",\n      \"cost_per_request\": 0.030\n    }\n  }\n}\n}}}\n\n----\n\n== 6.8 Implementation Notes ==\n\n**Provider Adapter Pattern:**\n\n{{{\nclass AnthropicProvider implements LLMProvider {\n  async complete(prompt: string, options: CompletionOptions) {\n    const response = await anthropic.messages.create({\n      model: options.model || 'claude-sonnet-4-5-20250929',\n      max_tokens: options.maxTokens || 4096,\n      messages: [{ role: 'user', content: prompt }],\n      system: options.systemPrompt\n    })\n    return response.content[0].text\n  }\n}\n\nclass OpenAIProvider implements LLMProvider {\n  async complete(prompt: string, options: CompletionOptions) {\n    const response = await openai.chat.completions.create({\n      model: options.model || 'gpt-4o',\n      max_tokens: options.maxTokens || 4096,\n      messages: [\n        { role: 'system', content: options.systemPrompt },\n        { role: 'user', content: prompt }\n      ]\n    })\n    return response.choices[0].message.content\n  }\n}\n}}}\n\n**Provider Registry:**\n\n{{{\nconst providers = new Map<string, LLMProvider>()\nproviders.set('anthropic', new AnthropicProvider())\nproviders.set('openai', new OpenAIProvider())\nproviders.set('google', new GoogleProvider())\n\nfunction getProvider(name: string): LLMProvider {\n  return providers.get(name) || providers.get(config.primaryProvider)\n}\n}}}\n\n----\n\n**Navigation:** [[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] | Prev: [[Pipeline Architecture>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome]] | Next: [[REST API Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome": "= Pipeline Architecture (POC1) =\n\n== 1. Core Objective (POC1) ==\n\nThe primary technical goal of POC1 is to validate **Approach 1 (Single-Pass Holistic Analysis)** while implementing **claim-level caching** to achieve cost sustainability.\n\nThe system must prove that AI can identify an article's **Main Thesis** and determine if supporting claims logically support that thesis without committing fallacies.\n\n=== Success Criteria: ===\n\n* Test with 30 diverse articles\n* Target: 70% accuracy detecting misleading articles\n* Cost: <$0.25 per NEW analysis (uncached)\n* Cost: $0.00 for cached claim reuse\n* Cache hit rate: 50% after 1,000 articles\n* Processing time: <2 minutes (standard depth)\n\n=== Economic Model: ===\n\n* **Free tier:** $10 credit per month (~~40-140 articles depending on cache hits)\n* **After limit:** Cache-only mode (instant, free access to cached claims)\n* **Paid tier:** Unlimited new analyses\n\n----\n\n== 2. Architecture Overview ==\n\n=== 2.1 3-Stage Pipeline with Caching ===\n\nFactHarbor POC1 uses a **3-stage architecture** designed for claim-level caching and cost efficiency:\n\n{{mermaid}}\ngraph TD\n A[Article Input] --> B[Stage 1: Extract Claims]\n B --> C{For Each Claim}\n C --> D[Check Cache]\n D -->|Cache HIT| E[Return Cached Verdict]\n D -->|Cache MISS| F[Stage 2: Analyze Claim]\n F --> G[Store in Cache]\n G --> E\n E --> H[Stage 3: Holistic Assessment]\n H --> I[Final Report]\n{{/mermaid}}\n\n==== Stage 1: Claim Extraction (FAST model, no cache) ====\n\n* **Input:** Article text\n* **Output:** 5 canonical claims (normalized, deduplicated)\n* **Model:** Provider-default FAST model (default, configurable via LLM abstraction layer)\n* **Cost:** $0.003 per article\n* **Cache strategy:** No caching (article-specific)\n\n==== Stage 2: Claim Analysis (REASONING model, CACHED) ====\n\n* **Input:** Single canonical claim\n* **Output:** Scenarios + Evidence + Verdicts\n* **Model:** Provider-default REASONING model (default, configurable via LLM abstraction layer)\n* **Cost:** $0.081 per NEW claim\n* **Cache strategy:** Redis, 90-day TTL\n* **Cache key:** claim:v1norm1:{language}:{sha256(canonical_claim)}\n\n==== Stage 3: Holistic Assessment (REASONING model, no cache) ====\n\n* **Input:** Article + Claim verdicts (from cache or Stage 2)\n* **Output:** Article verdict + Fallacies + Logic quality\n* **Model:** Provider-default REASONING model (default, configurable via LLM abstraction layer)\n* **Cost:** $0.030 per article\n* **Cache strategy:** No caching (article-specific)\n\n\n\n**Note:** Stage 3 implements **Approach 1 (Single-Pass Holistic Analysis)** from the [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]]. While claim analysis (Stage 2) is cached for efficiency, the holistic assessment maintains the integrated evaluation philosophy of Approach 1.\n\n=== Total Cost Formula: ===\n\n{{{Cost = $0.003 (extraction) + (N_new_claims  $0.081) + $0.030 (holistic)\n\nExamples:\n- 0 new claims (100% cache hit): $0.033\n- 1 new claim (80% cache hit): $0.114\n- 3 new claims (40% cache hit): $0.276\n- 5 new claims (0% cache hit): $0.438\n}}}\n\n----\n\n=== 2.2 User Tier System ===\n\n|=Tier|=Monthly Credit|=After Limit|=Cache Access|=Analytics\n|**Free**|$10|Cache-only mode| Full|Basic\n|**Pro** (future)|$50|Continues| Full|Advanced\n|**Enterprise** (future)|Custom|Continues| Full + Priority|Full\n\n**Free Tier Economics:**\n\n* $10 credit = 40-140 articles analyzed (depending on cache hit rate)\n* Average 70 articles/month at 70% cache hit rate\n* After limit: Cache-only mode\n\n----\n\n=== 2.3 Cache-Only Mode (Free Tier Feature) ===\n\nWhen free users reach their $10 monthly limit, they enter **Cache-Only Mode**:\n\n\n\n==== Stage 3: Holistic Assessment - Complete Specification ====\n\n===== 3.3.1 Overview =====\n\n**Purpose:** Synthesize individual claim analyses into an overall article assessment, identifying logical fallacies, reasoning quality, and publication readiness.\n\n**Approach:** **Single-Pass Holistic Analysis** (Approach 1 from Comparison Matrix)\n\n**Why This Approach for POC1:**\n*  **1 API call** (vs 2 for Two-Pass or Judge)\n*  **Low cost** ($0.030 per article)\n*  **Fast** (4-6 seconds)\n*  **Low complexity** (simple implementation)\n*  **Medium reliability** (acceptable for POC1, will improve in POC2/Production)\n\n**Alternative Approaches Considered:**\n\n|= Approach |= API Calls |= Cost |= Speed |= Complexity |= Reliability |= Best For\n| **1. Single-Pass**  | 1 |  Low |  Fast |  Low |  Medium | **POC1**\n| 2. Two-Pass | 2 |  Med |  Slow |  Med |  High | POC2/Prod\n| 3. Structured | 1 |  Low |  Fast |  Med |  High | POC1 (alternative)\n| 4. Weighted | 1 |  Low |  Fast |  Low |  Medium | POC1 (alternative)\n| 5. Heuristics | 1 |  Lowest |  Fastest |  Med |  Medium | Any\n| 6. Hybrid | 1 |  Low |  Fast |  Med-High |  High | POC2\n| 7. Judge | 2 |  Med |  Slow |  Med |  High | Production\n\n**POC1 Choice:** Approach 1 (Single-Pass) for speed and simplicity. Will upgrade to Approach 2 (Two-Pass) or 6 (Hybrid) in POC2 for higher reliability.\n\n===== 3.3.2 What Stage 3 Evaluates =====\n\nStage 3 performs **integrated holistic analysis** considering:\n\n**1. Claim-Level Aggregation:**\n* Verdict distribution (how many TRUE vs FALSE vs DISPUTED)\n* Average confidence across all claims\n* Claim interdependencies (do claims support/contradict each other?)\n* Critical claim identification (which claims are most important?)\n\n**2. Contextual Factors:**\n* **Source credibility**: Is the article from a reputable publisher?\n* **Author expertise**: Does the author have relevant credentials?\n* **Publication date**: Is information current or outdated?\n* **Claim coherence**: Do claims form a logical narrative?\n* **Missing context**: Are important caveats or qualifications missing?\n\n**3. Logical Fallacies:**\n* **Cherry-picking**: Selective evidence presentation\n* **False equivalence**: Treating unequal things as equal\n* **Straw man**: Misrepresenting opposing arguments\n* **Ad hominem**: Attacking person instead of argument\n* **Slippery slope**: Assuming extreme consequences without justification\n* **Circular reasoning**: Conclusion assumes premise\n* **False dichotomy**: Presenting only two options when more exist\n\n**4. Reasoning Quality:**\n* **Evidence strength**: Quality and quantity of supporting evidence\n* **Logical coherence**: Arguments follow logically\n* **Transparency**: Assumptions and limitations acknowledged\n* **Nuance**: Complexity and uncertainty appropriately addressed\n\n**5. Publication Readiness:**\n* **Risk tier assignment**: A (high risk), B (medium), or C (low risk)\n* **Publication mode**: DRAFT_ONLY, AI_GENERATED, or HUMAN_REVIEWED\n* **Required disclaimers**: What warnings should accompany this content?\n\n===== 3.3.3 Implementation: Single-Pass Approach =====\n\n**Input:**\n* Original article text (full content)\n* Stage 2 claim analyses (array of ClaimAnalysis objects)\n* Article metadata (URL, title, author, date, source)\n\n**Processing:**\n\n{{code language=\"python\"}}\n# Pseudo-code for Stage 3 (Single-Pass)\n\ndef stage3_holistic_assessment(article, claim_analyses, metadata):\n    \"\"\"\n    Single-pass holistic assessment using Provider-default REASONING model.\n\n    Approach 1: One comprehensive prompt that asks the LLM to:\n    1. Review all claim verdicts\n    2. Identify patterns and dependencies\n    3. Detect logical fallacies\n    4. Assess reasoning quality\n    5. Determine credibility score and risk tier\n    6. Generate publication recommendations\n    \"\"\"\n\n    # Construct comprehensive prompt\n    prompt = f\"\"\"\nYou are analyzing an article for factual accuracy and logical reasoning.\n\nARTICLE METADATA:\n- Title: {metadata['title']}\n- Source: {metadata['source']}\n- Date: {metadata['date']}\n- Author: {metadata['author']}\n\nARTICLE TEXT:\n{article}\n\nINDIVIDUAL CLAIM ANALYSES:\n{format_claim_analyses(claim_analyses)}\n\nYOUR TASK:\nPerform a holistic assessment considering:\n\n1. CLAIM AGGREGATION:\n   - Review the verdict for each claim\n   - Identify any interdependencies between claims\n   - Determine which claims are most critical to the article's thesis\n\n2. CONTEXTUAL EVALUATION:\n   - Assess source credibility\n   - Evaluate author expertise\n   - Consider publication timeliness\n   - Identify missing context or important caveats\n\n3. LOGICAL FALLACIES:\n   - Identify any logical fallacies present\n   - For each fallacy, provide:\n     * Type of fallacy\n     * Where it occurs in the article\n     * Why it's problematic\n     * Severity (minor/moderate/severe)\n\n4. REASONING QUALITY:\n   - Evaluate evidence strength\n   - Assess logical coherence\n   - Check for transparency in assumptions\n   - Evaluate handling of nuance and uncertainty\n\n5. CREDIBILITY SCORING:\n   - Calculate overall credibility score (0.0-1.0)\n   - Assign risk tier:\n     * A (high risk): 0.5 credibility OR severe fallacies\n     * B (medium risk): 0.5-0.8 credibility OR moderate issues\n     * C (low risk): >0.8 credibility AND no significant issues\n\n6. PUBLICATION RECOMMENDATIONS:\n   - Determine publication mode:\n     * DRAFT_ONLY: Tier A, multiple severe issues\n     * AI_GENERATED: Tier B/C, acceptable quality with disclaimers\n     * HUMAN_REVIEWED: Complex or borderline cases\n   - List required disclaimers\n   - Explain decision rationale\n\nOUTPUT FORMAT:\nReturn a JSON object matching the ArticleAssessment schema.\n\"\"\"\n\n    # Call LLM\n    response = llm_client.complete(\n        model=\"claude-sonnet-4-5-20250929\",\n        prompt=prompt,\n        max_tokens=4000,\n        response_format=\"json\"\n    )\n\n    # Parse and validate response\n    assessment = parse_json(response.content)\n    validate_article_assessment_schema(assessment)\n\n    return assessment\n{{/code}}\n\n**Prompt Engineering Notes:**\n\n1. **Structured Instructions**: Break down task into 6 clear sections\n2. **Context-Rich**: Provide article + all claim analyses + metadata\n3. **Explicit Criteria**: Define credibility scoring and risk tiers precisely\n4. **JSON Schema**: Request structured output matching ArticleAssessment schema\n5. **Examples** (in production): Include 2-3 example assessments for consistency\n\n===== 3.3.4 Credibility Scoring Algorithm =====\n\n**Base Score Calculation:**\n\n{{code language=\"python\"}}\ndef calculate_credibility_score(claim_analyses, fallacies, contextual_factors):\n    \"\"\"\n    Calculate overall credibility score (0.0-1.0).\n\n    This is a GUIDELINE for the LLM, not strict code.\n    The LLM has flexibility to adjust based on context.\n    \"\"\"\n\n    # 1. Claim Verdict Score (60% weight)\n    verdict_weights = {\n        \"TRUE\": 1.0,\n        \"PARTIALLY_TRUE\": 0.7,\n        \"DISPUTED\": 0.5,\n        \"UNSUPPORTED\": 0.3,\n        \"FALSE\": 0.0,\n        \"UNVERIFIABLE\": 0.4\n    }\n\n    claim_scores = [\n        verdict_weights[c.verdict.label] * c.verdict.confidence\n        for c in claim_analyses\n    ]\n    avg_claim_score = sum(claim_scores) / len(claim_scores)\n    claim_component = avg_claim_score * 0.6\n\n    # 2. Fallacy Penalty (20% weight)\n    fallacy_penalties = {\n        \"minor\": -0.05,\n        \"moderate\": -0.15,\n        \"severe\": -0.30\n    }\n\n    fallacy_score = 1.0\n    for fallacy in fallacies:\n        fallacy_score += fallacy_penalties[fallacy.severity]\n\n    fallacy_score = max(0.0, min(1.0, fallacy_score))\n    fallacy_component = fallacy_score * 0.2\n\n    # 3. Contextual Factors (20% weight)\n    context_adjustments = {\n        \"source_credibility\": {\"positive\": +0.1, \"neutral\": 0, \"negative\": -0.1},\n        \"author_expertise\": {\"positive\": +0.1, \"neutral\": 0, \"negative\": -0.1},\n        \"timeliness\": {\"positive\": +0.05, \"neutral\": 0, \"negative\": -0.05},\n        \"transparency\": {\"positive\": +0.05, \"neutral\": 0, \"negative\": -0.05}\n    }\n\n    context_score = 1.0\n    for factor in contextual_factors:\n        adjustment = context_adjustments.get(factor.factor, {}).get(factor.impact, 0)\n        context_score += adjustment\n\n    context_score = max(0.0, min(1.0, context_score))\n    context_component = context_score * 0.2\n\n    # 4. Combine components\n    final_score = claim_component + fallacy_component + context_component\n\n    # 5. Apply confidence modifier\n    avg_confidence = sum(c.verdict.confidence for c in claim_analyses) / len(claim_analyses)\n    final_score = final_score * (0.8 + 0.2 * avg_confidence)\n\n    return max(0.0, min(1.0, final_score))\n{{/code}}\n\n**Note:** This algorithm is a **guideline** provided to the LLM in the system prompt. The LLM has flexibility to adjust based on specific article context, but should generally follow this structure for consistency.\n\n===== 3.3.5 Risk Tier Assignment =====\n\n**Automatic Risk Tier Rules:**\n\n{{code}}\nRisk Tier A (High Risk - Requires Review):\n- Credibility score  0.5, OR\n- Any severe fallacies detected, OR\n- Multiple (3+) moderate fallacies, OR\n- 50%+ of claims are FALSE or UNSUPPORTED\n\nRisk Tier B (Medium Risk - May Publish with Disclaimers):\n- Credibility score 0.5-0.8, OR\n- 1-2 moderate fallacies, OR\n- 20-49% of claims are DISPUTED or PARTIALLY_TRUE\n\nRisk Tier C (Low Risk - Safe to Publish):\n- Credibility score > 0.8, AND\n- No severe or moderate fallacies, AND\n- <20% disputed/problematic claims, AND\n- No critical missing context\n{{/code}}\n\n===== 3.3.6 Output: ArticleAssessment Schema =====\n\n(See [[Data Schemas and Cache>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome]] for complete JSON schema)\n\n===== 3.3.7 Performance Metrics =====\n\n**POC1 Targets:**\n* **Processing time**: 4-6 seconds per article\n* **Cost**: $0.030 per article (Sonnet 4.5 tokens)\n* **Quality**: 70-80% agreement with human reviewers (acceptable for POC)\n* **API calls**: 1 per article\n\n**Future Improvements (POC2/Production):**\n* Upgrade to Two-Pass (Approach 2): +15% accuracy, +$0.020 cost\n* Add human review sampling: 10% of Tier B articles\n* Implement Judge approach (Approach 7) for Tier A: Highest quality\n\n===== 3.3.8 Example Stage 3 Execution =====\n\n**Input:**\n* Article: \"Biden won the 2020 election\"\n* Claim analyses: [{claim: \"Biden won\", verdict: \"TRUE\", confidence: 0.95}]\n\n**Stage 3 Processing:**\n1. Analyzes single claim with high confidence\n2. Checks for contextual factors (source credibility)\n3. Searches for logical fallacies (none found)\n4. Calculates credibility: 0.6 * 0.95 + 0.2 * 1.0 + 0.2 * 1.0 = 0.97\n5. Assigns risk tier: C (low risk)\n6. Recommends: AI_GENERATED publication mode\n\n**Output:**\n{{code language=\"json\"}}\n{\n  \"article_id\": \"a1\",\n  \"overall_assessment\": {\n    \"credibility_score\": 0.97,\n    \"risk_tier\": \"C\",\n    \"summary\": \"Article makes single verifiable claim with strong evidence support\",\n    \"confidence\": 0.95\n  },\n  \"claim_aggregation\": {\n    \"total_claims\": 1,\n    \"verdict_distribution\": {\"TRUE\": 1},\n    \"avg_confidence\": 0.95\n  },\n  \"contextual_factors\": [\n    {\"factor\": \"source_credibility\", \"impact\": \"positive\", \"description\": \"Reputable news source\"}\n  ],\n  \"recommendations\": {\n    \"publication_mode\": \"AI_GENERATED\",\n    \"requires_review\": false,\n    \"suggested_disclaimers\": []\n  }\n}\n{{/code}}\n\n==== What Cache-Only Mode Provides: ====\n\n **Claim Extraction (Platform-Funded):**\n\n* Stage 1 extraction runs at $0.003 per article\n* **Cost: Absorbed by platform** (not charged to user credit)\n* Rationale: Extraction is necessary to check cache, and cost is negligible\n* Rate limit: Max 50 extractions/day in cache-only mode (prevents abuse)\n\n **Instant Access to Cached Claims:**\n\n* Any claim that exists in cache  Full verdict returned\n* Cost: $0 (no LLM calls)\n* Response time: <100ms\n\n **Partial Article Analysis:**\n\n* Check each claim against cache\n* Return verdicts for ALL cached claims\n* For uncached claims: Return \"status\": \"cache_miss\"\n\n **Cache Coverage Report:**\n\n* \"3 of 5 claims available in cache (60% coverage)\"\n* Links to cached analyses\n* Estimated cost to complete: $0.162 (2 new claims)\n\n **Not Available in Cache-Only Mode:**\n\n* New claim analysis (Stage 2 LLM calls blocked)\n* Full holistic assessment (Stage 3 blocked if any claims missing)\n\n==== User Experience Example: ====\n\n{{{{\n \"status\": \"cache_only_mode\",\n \"message\": \"Monthly credit limit reached. Showing cached results only.\",\n \"cache_coverage\": {\n \"claims_total\": 5,\n \"claims_cached\": 3,\n \"claims_missing\": 2,\n \"coverage_percent\": 60\n },\n \"cached_claims\": [\n {\"claim_id\": \"C1\", \"verdict\": \"Likely\", \"confidence\": 0.82},\n {\"claim_id\": \"C2\", \"verdict\": \"Highly Likely\", \"confidence\": 0.91},\n {\"claim_id\": \"C4\", \"verdict\": \"Unclear\", \"confidence\": 0.55}\n ],\n \"missing_claims\": [\n {\"claim_id\": \"C3\", \"claim_text\": \"...\", \"estimated_cost\": \"$0.081\"},\n {\"claim_id\": \"C5\", \"claim_text\": \"...\", \"estimated_cost\": \"$0.081\"}\n ],\n \"upgrade_options\": {\n \"top_up\": \"$5 for 20-70 more articles\",\n \"pro_tier\": \"$50/month unlimited\"\n }\n}\n}}}\n\n**Design Rationale:**\n\n* Free users still get value (cached claims often answer their question)\n* Demonstrates FactHarbor's value (partial results encourage upgrade)\n* Sustainable for platform (no additional cost)\n* Fair to all users (everyone contributes to cache)\n\n----\n\n**Navigation:** [[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] | Prev: [[Codegen Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Codegen Contract.WebHome]] | Next: [[LLM Abstraction Layer>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome": "= REST API Contract =\n\n== 3.1 User Credit Tracking ==\n\n**Endpoint:** GET /v1/user/credit\n\n**Response:** 200 OK\n\n{{{{\n \"user_id\": \"user_abc123\",\n \"tier\": \"free\",\n \"credit_limit\": 10.00,\n \"credit_used\": 7.42,\n \"credit_remaining\": 2.58,\n \"reset_date\": \"2025-02-01T00:00:00Z\",\n \"cache_only_mode\": false,\n \"usage_stats\": {\n \"articles_analyzed\": 67,\n \"claims_from_cache\": 189,\n \"claims_newly_analyzed\": 113,\n \"cache_hit_rate\": 0.626\n }\n}\n}}}\n\n----\n\n\n\n==== Stage 2 Output Schema: ClaimAnalysis ====\n\n**Complete schema for each claim's analysis result:**\n\n{{code language=\"json\"}}\n{\n  \"claim_id\": \"claim_abc123\",\n  \"claim_text\": \"Biden won the 2020 election\",\n  \"scenarios\": [\n    {\n      \"scenario_id\": \"scenario_1\",\n      \"description\": \"Interpreting 'won' as Electoral College victory\",\n      \"verdict\": {\n        \"label\": \"TRUE\",\n        \"confidence\": 0.95,\n        \"explanation\": \"Joe Biden won 306 electoral votes vs Trump's 232\"\n      },\n      \"evidence\": {\n        \"supporting\": [\n          {\n            \"text\": \"Biden certified with 306 electoral votes\",\n            \"source_url\": \"https://www.archives.gov/electoral-college/2020\",\n            \"source_title\": \"2020 Electoral College Results\",\n            \"credibility_score\": 0.98\n          }\n        ],\n        \"opposing\": []\n      }\n    }\n  ],\n  \"recommended_scenario\": \"scenario_1\",\n  \"metadata\": {\n    \"analysis_timestamp\": \"2024-12-24T18:00:00Z\",\n    \"model_used\": \"claude-sonnet-4-5-20250929\",\n    \"processing_time_seconds\": 8.5\n  }\n}\n{{/code}}\n\n**Required Fields:**\n* **claim_id**: Unique identifier matching Stage 1 output\n* **claim_text**: The exact claim being analyzed\n* **scenarios**: Array of interpretation scenarios (minimum 1)\n * **scenario_id**: Unique ID for this scenario\n * **description**: Clear interpretation of the claim\n * **verdict**: Verdict object with label, confidence, explanation\n * **evidence**: Supporting and opposing evidence arrays\n* **recommended_scenario**: ID of the primary/recommended scenario\n* **metadata**: Processing metadata (timestamp, model, timing)\n\n**Optional Fields:**\n* Additional context, warnings, or quality scores\n\n**Minimum Viable Example:**\n\n{{code language=\"json\"}}\n{\n  \"claim_id\": \"c1\",\n  \"claim_text\": \"The sky is blue\",\n  \"scenarios\": [{\n    \"scenario_id\": \"s1\",\n    \"description\": \"Under clear daytime conditions\",\n    \"verdict\": {\"label\": \"TRUE\", \"confidence\": 0.99, \"explanation\": \"Rayleigh scattering\"},\n    \"evidence\": {\"supporting\": [], \"opposing\": []}\n  }],\n  \"recommended_scenario\": \"s1\",\n  \"metadata\": {\"analysis_timestamp\": \"2024-12-24T18:00:00Z\"}\n}\n{{/code}}\n\n\n\n==== Stage 3 Output Schema: ArticleAssessment ====\n\n**Complete schema for holistic article-level assessment:**\n\n{{code language=\"json\"}}\n{\n  \"article_id\": \"article_xyz789\",\n  \"overall_assessment\": {\n    \"credibility_score\": 0.72,\n    \"risk_tier\": \"B\",\n    \"summary\": \"Article contains mostly accurate claims with one disputed claim requiring expert review\",\n    \"confidence\": 0.85\n  },\n  \"claim_aggregation\": {\n    \"total_claims\": 5,\n    \"verdict_distribution\": {\n      \"TRUE\": 3,\n      \"PARTIALLY_TRUE\": 1,\n      \"DISPUTED\": 1,\n      \"FALSE\": 0,\n      \"UNSUPPORTED\": 0,\n      \"UNVERIFIABLE\": 0\n    },\n    \"avg_confidence\": 0.82\n  },\n  \"contextual_factors\": [\n    {\n      \"factor\": \"Source credibility\",\n      \"impact\": \"positive\",\n      \"description\": \"Published by reputable news organization\"\n    },\n    {\n      \"factor\": \"Claim interdependence\",\n      \"impact\": \"neutral\",\n      \"description\": \"Claims are independent; no logical chains\"\n    }\n  ],\n  \"recommendations\": {\n    \"publication_mode\": \"AI_GENERATED\",\n    \"requires_review\": false,\n    \"review_reason\": null,\n    \"suggested_disclaimers\": [\n      \"One claim (Claim 4) has conflicting expert opinions\"\n    ]\n  },\n  \"metadata\": {\n    \"holistic_timestamp\": \"2024-12-24T18:00:10Z\",\n    \"model_used\": \"claude-sonnet-4-5-20250929\",\n    \"processing_time_seconds\": 4.2,\n    \"cache_used\": false\n  }\n}\n{{/code}}\n\n**Required Fields:**\n* **article_id**: Unique identifier for this article\n* **overall_assessment**: Top-level assessment\n * **credibility_score**: 0.0-1.0 composite score\n * **risk_tier**: A, B, or C (per AKEL quality gates)\n * **summary**: Human-readable assessment\n * **confidence**: How confident the holistic assessment is\n* **claim_aggregation**: Statistics across all claims\n * **total_claims**: Count of claims analyzed\n * **verdict_distribution**: Count per verdict label\n * **avg_confidence**: Average confidence across verdicts\n* **contextual_factors**: Array of contextual considerations\n* **recommendations**: Publication decision support\n * **publication_mode**: DRAFT_ONLY, AI_GENERATED, or HUMAN_REVIEWED\n * **requires_review**: Boolean flag\n * **suggested_disclaimers**: Array of disclaimer texts\n* **metadata**: Processing metadata\n\n**Minimum Viable Example:**\n\n{{code language=\"json\"}}\n{\n  \"article_id\": \"a1\",\n  \"overall_assessment\": {\n    \"credibility_score\": 0.95,\n    \"risk_tier\": \"C\",\n    \"summary\": \"All claims verified as true\",\n    \"confidence\": 0.98\n  },\n  \"claim_aggregation\": {\n    \"total_claims\": 1,\n    \"verdict_distribution\": {\"TRUE\": 1},\n    \"avg_confidence\": 0.99\n  },\n  \"contextual_factors\": [],\n  \"recommendations\": {\n    \"publication_mode\": \"AI_GENERATED\",\n    \"requires_review\": false,\n    \"suggested_disclaimers\": []\n  },\n  \"metadata\": {\"holistic_timestamp\": \"2024-12-24T18:00:00Z\"}\n}\n{{/code}}\n\n== 3.2 Create Analysis Job (3-Stage) ==\n\n**Endpoint:** POST /v1/analyze\n\n=== Idempotency Support: ===\n\nTo prevent duplicate job creation on network retries, clients SHOULD include **either**:\n\n* Header: ``Idempotency-Key: <client-generated-uuid>`` (preferred)\n* OR body: ``client.request_id``\n\n**Example request (header):**\n{{code language=\"text\"}}\nPOST /v1/analyze\nAuthorization: Bearer <API_KEY>\nIdempotency-Key: 0f3c6c0e-2d2b-4b4a-9d6f-1a1f6b0c9f7e\nContent-Type: application/json\n{{/code}}\n\n**Example request (body):**\n{{code language=\"json\"}}\n{\n  \"input_url\": \"https://example.org/article\",\n  \"options\": { \"max_claims\": 5, \"cache_preference\": \"prefer_cache\" },\n  \"client\": { \"request_id\": \"0f3c6c0e-2d2b-4b4a-9d6f-1a1f6b0c9f7e\" }\n}\n{{/code}}\n\n**Server behavior:**\n* Same idempotency key + same request body  return existing job (``200``) and include:\n  ``idempotent=true`` and ``original_request_at``.\n* Same key + different body  ``409`` with ``VALIDATION_ERROR`` describing the mismatch.\n\n**Idempotency TTL:** 24 hours (minimum).\n\n=== Request Body: ===\n\n{{{{\n \"input_type\": \"url\",\n \"input_url\": \"https://example.com/medical-report-01\",\n \"input_text\": null,\n \"options\": {\n \"browsing\": \"on\",\n \"depth\": \"standard\",\n \"max_claims\": 5,\n\n* **cache_preference** (optional): Cache usage preference\n * **Type:** string\n * **Enum:** {{code}}[\"prefer_cache\", \"allow_partial\", \"skip_cache\"]{{/code}}\n * **Default:** {{code}}\"prefer_cache\"{{/code}}\n * **Semantics:**\n  * {{code}}\"prefer_cache\"{{/code}}: Use full cache if available, otherwise run all stages\n  * {{code}}\"allow_partial\"{{/code}}: Use cached Stage 2 results if available, rerun only Stage 3\n  * {{code}}\"skip_cache\"{{/code}}: Always rerun all stages (ignore cache)\n * **Behavior:** When set to {{code}}\"allow_partial\"{{/code}} and Stage 2 cached results exist:\n  * Stage 1 & 2 are skipped\n  * Stage 3 (holistic assessment) runs fresh with cached claim analyses\n  * Response includes {{code}}\"cache_used\": true{{/code}} and {{code}}\"stages_cached\": [\"stage1\", \"stage2\"]{{/code}}\n\n \"scenarios_per_claim\": 2,\n \"max_evidence_per_scenario\": 6,\n \"context_aware_analysis\": true\n },\n \"client\": {\n \"request_id\": \"optional-client-tracking-id\",\n \"source_label\": \"optional\"\n }\n}\n}}}\n\n**Options:**\n\n* browsing: on | off (retrieve web sources or just output queries)\n* depth: standard | deep (evidence thoroughness)\n* max_claims: 1-10 (default: **5** for cost control)\n* scenarios_per_claim: 1-5 (default: **2** for cost control)\n* max_evidence_per_scenario: 3-10 (default: **6**)\n* context_aware_analysis: true | false (experimental)\n\n**Response:** 202 Accepted\n\n{{{{\n \"job_id\": \"01J...ULID\",\n \"status\": \"QUEUED\",\n \"created_at\": \"2025-12-24T10:31:00Z\",\n \"estimated_cost\": 0.114,\n \"cost_breakdown\": {\n \"stage1_extraction\": 0.003,\n \"stage2_new_claims\": 0.081,\n \"stage2_cached_claims\": 0.000,\n \"stage3_holistic\": 0.030\n },\n \"cache_info\": {\n \"claims_to_extract\": 5,\n \"estimated_cache_hits\": 4,\n \"estimated_new_claims\": 1\n },\n \"links\": {\n \"self\": \"/v1/jobs/01J...ULID\",\n \"result\": \"/v1/jobs/01J...ULID/result\",\n \"report\": \"/v1/jobs/01J...ULID/report\",\n \"events\": \"/v1/jobs/01J...ULID/events\"\n }\n}\n}}}\n\n**Error Responses:**\n\n402 Payment Required - Free tier limit reached, cache-only mode\n\n{{{{\n \"error\": \"credit_limit_reached\",\n \"message\": \"Monthly credit limit reached. Entering cache-only mode.\",\n \"cache_only_mode\": true,\n \"credit_remaining\": 0.00,\n \"reset_date\": \"2025-02-01T00:00:00Z\",\n \"action\": \"Resubmit with cache_preference=allow_partial for cached results\"\n}\n}}}\n\n----\n\n**Navigation:** [[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] | Prev: [[LLM Abstraction Layer>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome]] | Next: [[Data Schemas and Cache>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome]]\n", "Product Development.Specification.POC.API-and-Schemas.WebHome": "= POC1 API & Schemas Specification =\n\n----\n\n{{info}}\n**Canonical Data Model:** See [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] for the canonical data model specification. This section documents the POC1 API contract and endpoint schemas.\n{{/info}}\n\n== Contents ==\n\n* **[[Codegen Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Codegen Contract.WebHome]]**  Authoritative code-generation contract: locked enums, OpenAPI 3.1, idempotency, canonical outputs\n* **[[Pipeline Architecture>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome]]**  3-stage pipeline (Extract  Analyze  Assess), cost model, user tiers, cache-only mode, credibility scoring algorithm\n* **[[LLM Abstraction Layer>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.LLM Abstraction Layer.WebHome]]**  Provider-agnostic abstraction: interface, supported providers, stage-specific models, failover strategy, admin API\n* **[[REST API Contract>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.REST API Contract.WebHome]]**  API endpoints (POST /v1/analyze, GET /v1/jobs, etc.), request/response schemas, Stage 2 & 3 output schemas\n* **[[Data Schemas and Cache>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.Data Schemas and Cache.WebHome]]**  ClaimExtraction schema, verdict label taxonomy (3-level), Redis cache design, canonical claim normalization (v1norm1), copyright policy\n\n== Version History ==\n\n|=Version|=Date|=Changes\n|0.4.1|2025-12-24|Applied 9 critical fixes: file format notice, verdict taxonomy, canonicalization algorithm, Stage 1 cost policy, BullMQ fix, language in cache key, historical claims TTL, idempotency, copyright policy\n|0.4|2025-12-24|**BREAKING:** 3-stage pipeline with claim-level caching, user tier system, cache-only mode for free users, Redis cache architecture\n|0.3.1|2025-12-24|Fixed single-prompt strategy, SSE clarification, schema canonicalization, cost constraints\n|0.3|2025-12-24|Added complete API endpoints, LLM config, risk tiers, scraping details\n\n== Related ==\n\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]  Canonical data model specification\n* [[POC1 Requirements>>FactHarbor.Product Development.Specification.POC.Requirements]]  Full POC1 requirements document\n* [[POC1 Summary>>FactHarbor.Product Development.Specification.POC.Summary]]  Executive summary\n", "Product Development.Specification.POC.Article-Verdict-Problem": "= The Article Verdict Problem =\n\n**Context:** Context-Aware Analysis Investigation \n**Date:** December 23, 2025 \n**Status:** Implemented  AnalysisContext-based multi-context analysis (evolved beyond the single-pass approach initially chosen)\n\n==  Executive Summary ==\n\n**The Problem:** An article's overall credibility is not simply the average of its individual claim verdicts. An article with mostly accurate facts can still be misleading if the conclusion doesn't follow from the evidence.\n\n**Investigation Scope:** 7 solution approaches analyzed for performance, cost, and complexity.\n\n**Chosen Solution:** Single-Pass Holistic Analysis (Approach 1) for POC1 testing\n- Enhance AI prompt to evaluate logical structure\n- Zero additional cost or architecture changes\n- Test with 30 articles to validate approach\n- Mark as experimental - doesn't block POC1 success\n\n**Fallback Plan:** If Approach 1 shows <70% accuracy, implement Weighted Aggregation (Approach 4) or defer to POC2 with Hybrid approach (Approach 6).\n\n==  The Core Problem ==\n\n=== Problem Statement ===\n\n> \"An analysis and verdict of the whole article is not the same as a summary of the analysis and verdicts of the parts (the claims).\"\n\n=== Why This Matters ===\n\n**Example: The Misleading Article**\n\n{{{\nArticle: \"Coffee Cures Cancer!\"\n\nIndividual Claims:\n[1] Coffee contains antioxidants   WELL-SUPPORTED (95%)\n[2] Antioxidants fight cancer   WELL-SUPPORTED (85%)\n[3] Therefore, coffee cures cancer   REFUTED (10%)\n\nSimple Aggregation:\n- Verdict counts: 2 supported, 1 refuted\n- Average confidence: 63% (2/3 claims somewhat supported)\n- Naive conclusion: \"Mostly accurate article\"\n\nReality:\n- The MAIN CLAIM (coffee cures cancer) is FALSE\n- Article commits logical fallacy (correlation  causation)\n- Article is MISLEADING despite containing accurate facts\n- Readers could be harmed by false medical claim\n\nCorrect Assessment:\n- Article verdict: MISLEADING / REFUTED\n- Reason: Makes unsupported causal claim from correlational evidence\n}}}\n\n=== Why Simple Aggregation Fails ===\n\n**Pattern 1: False Central Claim**\n- 4 supporting facts (all true) \n- 1 main conclusion (false) \n- Simple average: 80% accurate\n- Reality: Core argument is false  Article is MISLEADING\n\n**Pattern 2: Accurate Facts, Wrong Conclusion**\n- All individual facts are verifiable\n- Conclusion doesn't follow from facts\n- Logical fallacy (e.g., correlation  causation)\n- Simple average looks good, article is dangerous\n\n**Pattern 3: Misleading Framing**\n- Facts are accurate\n- Selective presentation creates false impression\n- Headline doesn't match content\n- Simple average misses the problem\n\n==  Chosen Solution: Single-Pass Holistic Analysis (POC1) ==\n\n=== Approach Overview ===\n\n**How it works:**\n- AI analyzes the entire article in ONE API call\n- Evaluates both individual claims AND overall article credibility\n- No pipeline changes - just enhanced prompting\n\n=== AI Prompt Enhancement ===\n\n**Add to existing prompt:**\n{{{\nAfter analyzing individual claims, evaluate the article as a whole:\n\n1. What is the article's main argument or conclusion?\n2. Does this conclusion logically follow from the evidence presented?\n3. Are there logical fallacies? (correlationcausation, cherry-picking, etc.)\n4. Even if individual facts are accurate, is the article's framing misleading?\n5. Should the article verdict differ from the average of claim verdicts?\n\nProvide:\n- Individual claim verdicts\n- Overall article verdict (may differ from claim average)\n- Explanation if article verdict differs from claim pattern\n}}}\n\n=== Expected AI Output ===\n\n{{code language=\"json\"}}\n{\n \"claims\": [\n {\"text\": \"Coffee contains antioxidants\", \"verdict\": \"SUPPORTED\", \"confidence\": 95},\n {\"text\": \"Antioxidants fight cancer\", \"verdict\": \"SUPPORTED\", \"confidence\": 85},\n {\"text\": \"Coffee cures cancer\", \"verdict\": \"REFUTED\", \"confidence\": 10}\n ],\n \"article_analysis\": {\n \"main_argument\": \"Coffee cures cancer\",\n \"logical_assessment\": \"Article makes causal claim not supported by evidence\",\n \"fallacy_detected\": \"correlation presented as causation\",\n \"article_verdict\": \"MISLEADING\",\n \"differs_from_claims\": true,\n \"reasoning\": \"Despite two accurate supporting facts, the main conclusion is unsupported\"\n }\n}\n{{/code}}\n\n=== Performance & Cost ===\n\n**Performance:**\n- Same as baseline POC1 (single API call)\n- Fast response time\n- No additional latency\n\n**Cost:**\n- Same as baseline POC1\n- ~$0.015-0.025 per analysis\n- No cost increase (just longer prompt)\n\n**Architecture:**\n- Zero changes to system architecture\n- Just prompt engineering\n- Easy to implement and test\n\n=== POC1 Testing Plan ===\n\n**Test Set: 30 Articles**\n- 10 straightforward (verdict = average works fine)\n- 10 misleading (accurate facts, wrong conclusion)\n- 10 complex/nuanced cases\n\n**Success Criteria:**\n- AI correctly identifies 70% of misleading articles\n- AI doesn't over-flag straightforward articles\n- Reasoning is comprehensible\n\n**Success  Ship it in POC2 as standard feature**\n\n**Partial Success (50-70%)  Try Approach 4 (Weighted Aggregation) or plan Approach 6 (Hybrid) for POC2**\n\n**Failure (<50%)  Defer to POC2 with more sophisticated approach**\n\n=== Why This Approach for POC1 ===\n\n**Advantages:**\n Zero additional cost (no extra API calls)\n No architecture changes (just prompt)\n Fast to implement and test\n Fail-fast learning (find out if AI can do this)\n If it works  problem solved with minimal effort\n If it fails  informed decision for POC2\n\n**Risks:**\n AI might miss subtle logical issues\n Relies entirely on AI's reasoning capability\n Less structured than multi-pass approaches\n\n**Mitigation:**\n- Mark as \"experimental\" in POC1\n- Don't block POC1 success on this feature\n- Use results to inform POC2 design\n- Have fallback approaches ready\n\n==  Complete Analysis: All Solution Approaches ==\n\nWe investigated 7 approaches for solving this problem. Here's the complete overview:\n\n=== Approach 1: Single-Pass Holistic Analysis  CHOSEN FOR POC1 ===\n\n**Concept:** AI analyzes article and evaluates both claims and overall credibility in one call.\n\n**Pros:** Simplest, fastest, cheapest, no architecture changes\n**Cons:** Relies on AI capability, might miss subtle issues\n**Cost:** ~$0.020/analysis | **Speed:** Fast | **Complexity:** LOW\n\n**When to use:** POC1 testing - validate if AI can do this at all\n\n=== Approach 2: Two-Pass Sequential Analysis ===\n\n**Concept:** Pass 1 extracts claims, Pass 2 analyzes logical structure given the claims.\n\n**Pros:** More focused analysis, better debugging, higher reliability\n**Cons:** Slower (two API calls), more expensive, more complex\n**Cost:** ~$0.030/analysis | **Speed:** Slower | **Complexity:** MEDIUM\n\n**When to use:** POC2 if Approach 1 fails, or production for highest quality\n\n=== Approach 3: Structured Output with Explicit Relationships ===\n\n**Concept:** AI outputs claim relationships explicitly (main claim, supporting claims, dependencies, logical validity).\n\n**Pros:** Explicit structure, easier to validate, single API call\n**Cons:** Complex prompt, relies on AI identifying relationships correctly\n**Cost:** ~$0.023/analysis | **Speed:** Fast | **Complexity:** MEDIUM\n\n**When to use:** POC1 if structured data valuable for UI/debugging\n\n=== Approach 4: Weighted Aggregation with Importance Scores ===\n\n**Concept:** AI assigns importance weight (0-1) to each claim. Article verdict = weighted average.\n\n**Pros:** Simple math, easy to explain, single API call\n**Cons:** Reduces to number (loses nuance), doesn't identify fallacies explicitly\n**Cost:** ~$0.020/analysis | **Speed:** Fast | **Complexity:** LOW\n\n**When to use:** POC1 fallback if Approach 1 doesn't work well\n\n=== Approach 5: Post-Processing Heuristics ===\n\n**Concept:** Rule-based detection of logical issues after claim extraction (e.g., \"if article contains 'causes' but only correlational evidence, flag causal fallacy\").\n\n**Pros:** Cheapest, deterministic, explainable, no extra API calls\n**Cons:** Brittle rules, high maintenance, false positives/negatives\n**Cost:** ~$0.018/analysis | **Speed:** Fast | **Complexity:** MEDIUM\n\n**When to use:** Add to any other approach for robustness\n\n=== Approach 6: Hybrid (Weighted Aggregation + Heuristics)  RECOMMENDED FOR POC2 ===\n\n**Concept:** Combine AI-assigned weights with rule-based fallacy detection.\n\n**Pros:** Best of both worlds, robust, still single API call, cost-effective\n**Cons:** More complex than single approach, need to tune interaction\n**Cost:** ~$0.020/analysis | **Speed:** Fast | **Complexity:** MED-HIGH\n\n**When to use:** POC2 for robust production-ready solution\n\n=== Approach 7: LLM-as-Judge (Verification Pass) ===\n\n**Concept:** Pass 1 generates verdict, Pass 2 verifies if verdict matches article content.\n\n**Pros:** AI checks AI, high reliability, catches mistakes\n**Cons:** Slower (two calls), more expensive, verification might also err\n**Cost:** ~$0.030/analysis | **Speed:** Slower | **Complexity:** MEDIUM\n\n**When to use:** Production if quality is paramount\n\n=== Comparison Matrix ===\n\n| Approach | API Calls | Cost | Speed | Complexity | Reliability | Best For |\n|----------|-----------|------|-------|------------|-------------|----------|\n| 1. Single-Pass  | 1 |  Low |  Fast |  Low |  Medium | POC1 |\n| 2. Two-Pass | 2 |  Med |  Slow |  Med |  High | POC2/Prod |\n| 3. Structured | 1 |  Low |  Fast |  Med |  High | POC1 |\n| 4. Weighted | 1 |  Low |  Fast |  Low |  Medium | POC1 |\n| 5. Heuristics | 1 |  Lowest |  Fastest |  Med |  Medium | Any |\n| 6. Hybrid  | 1 |  Low |  Fast |  Med-High |  High | POC2 |\n| 7. Judge | 2 |  Med |  Slow |  Med |  High | Production |\n\n=== Phased Recommendation ===\n\n**POC1 (Immediate):**\n- Test Approach 1 (Single-Pass Holistic)\n- Mark as experimental\n- Gather data on AI capability\n\n**POC2 (If POC1 validates approach):**\n- Upgrade to Approach 6 (Hybrid)\n- Add heuristics for robustness\n- Target 85%+ accuracy\n\n**Production (Post-POC2):**\n- If quality issues: Consider Approach 7 (LLM-as-Judge)\n- If quality acceptable: Keep Approach 6\n- Target 90%+ accuracy\n\n==  Decision Framework ==\n\n=== POC1 Evaluation Criteria ===\n\nAfter testing with 30 articles:\n\n**If AI Accuracy 70%:**\n-  Approach validated!\n-  Ship as standard feature in POC2\n-  Consider adding heuristics (Approach 6) for robustness\n\n**If AI Accuracy 50-70%:**\n-  Promising but needs improvement\n-  Try Approach 4 (Weighted Aggregation) in POC1\n-  Plan Approach 6 (Hybrid) for POC2\n\n**If AI Accuracy <50%:**\n-  Current AI can't do this reliably\n-  Defer to POC2 or post-POC2\n-  Consider Approach 2 or 7 (two-pass) for production\n\n=== Why This Matters for POC1 ===\n\n**Testing this in POC1:**\n- Validates core capability (can AI do nuanced reasoning?)\n- Informs POC2 architecture decisions\n- Zero cost to try (just prompt enhancement)\n- Fail-fast principle (test hardest part first)\n\n**Not testing this in POC1:**\n- Keeps POC1 scope minimal\n- Focuses on core claim extraction\n- But misses early learning opportunity\n\n**Decision:** Test it, mark as experimental, don't block POC1 success on it.\n\n==  Implementation Notes ==\n\n=== What AKEL Must Do ===\n\n**For POC1 (Approach 1):**\n1. Enhanced prompt with logical analysis section\n2. Parse AI output for both claim-level and article-level verdicts\n3. Display both verdicts to user\n4. Track accuracy on test set\n\n**No architecture changes needed.**\n\n=== What Gets Displayed to Users ===\n\n**Output Format:**\n{{{\nANALYSIS SUMMARY (4-6 sentences, context-aware):\n\"This article argues that coffee cures cancer based on evidence about\nantioxidants. We analyzed 3 claims: two supporting facts about coffee's\nchemical properties are well-supported, but the main causal claim is\nrefuted by current evidence. The article confuses correlation with\ncausation. Overall assessment: MISLEADING - makes an unsupported\nmedical claim despite citing some accurate facts.\"\n\nCLAIMS VERDICTS:\n[1] Coffee contains antioxidants: WELL-SUPPORTED (95%)\n[2] Antioxidants fight cancer: WELL-SUPPORTED (85%)\n[3] Coffee cures cancer: REFUTED (10%)\n\nARTICLE VERDICT: MISLEADING\nThe article's main conclusion is not supported by the evidence presented.\n}}}\n\n=== Error Handling ===\n\n**If AI fails to provide article-level analysis:**\n- Fall back to claim-average verdict\n- Log failure for analysis\n- Don't break the analysis\n\n**If AI over-flags straightforward articles:**\n- Review prompt tuning\n- Consider adding confidence threshold\n- Track false positive rate\n\n==  Testing Strategy ==\n\n=== Test Set Composition ===\n\n**Category 1: Straightforward Articles (10 articles)**\n- Clear claims with matching overall message\n- Verdict = average should work fine\n- Tests that we don't over-flag\n\n**Category 2: Misleading Articles (10 articles)**\n- Accurate facts, unsupported conclusion\n- Logical fallacies present\n- Verdict  average\n- Core test of capability\n\n**Category 3: Complex/Nuanced (10 articles)**\n- Gray areas\n- Multiple valid interpretations\n- Tests nuance handling\n\n=== Success Metrics ===\n\n**Quantitative:**\n- 70% accuracy on Category 2 (misleading articles)\n- 30% false positives on Category 1 (straightforward)\n- 50% accuracy on Category 3 (complex)\n\n**Qualitative:**\n- Reasoning is comprehensible to humans\n- False positives are explainable\n- False negatives reveal clear AI limitations\n\n=== Documentation ===\n\n**For each test case, record:**\n- Article summary\n- AI's claim verdicts\n- AI's article verdict\n- AI's reasoning\n- Human judgment (correct/incorrect)\n- Notes on why AI succeeded/failed\n\n==  Key Insights ==\n\n=== What This Tests ===\n\n**Core Capability:**\nCan AI understand that article credibility depends on:\n1. Logical structure (does conclusion follow?)\n2. Claim importance (main vs. supporting)\n3. Reasoning quality (sound vs. fallacious)\n\n**Not just:**\n- Accuracy of individual facts\n- Simple averages\n- Keyword matching\n\n=== Why This Is Important ===\n\n**For FactHarbor's Mission:**\n- Prevents misleading \"mostly accurate\" verdicts\n- Catches dangerous misinformation (medical, financial)\n- Provides nuanced analysis users can trust\n\n**For POC Validation:**\n- Tests most challenging capability\n- If AI can do this, everything else is easier\n- If AI can't, we know early and adjust\n\n=== Strategic Value ===\n\n**If Approach 1 works (70% accuracy):**\n-  Solved complex problem with zero architecture changes\n-  No cost increase\n-  Differentiation from competitors who only check facts\n-  Foundation for more sophisticated features\n\n**If Approach 1 doesn't work:**\n-  Learned AI limitations early\n-  Informed decision for POC2\n-  Can plan proper solution (Approach 6 or 7)\n\n==  Lessons Learned ==\n\n=== From Investigation ===\n\n**AI Capabilities:**\n- Modern LLMs (Sonnet 4.5) can do nuanced reasoning\n- But reliability varies significantly\n- Need to test, not assume\n\n**Cost-Performance Trade-offs:**\n- Single-pass approaches: Fast and cheap but less reliable\n- Multi-pass approaches: Slower and expensive but more robust\n- Hybrid approaches: Best balance for production\n\n**Architecture Decisions:**\n- Don't over-engineer before validating need\n- Test simplest approach first\n- Have fallback plans ready\n\n=== For POC1 ===\n\n**Keep It Simple:**\n- Test Approach 1 with minimal changes\n- Mark as experimental\n- Use results to guide POC2\n\n**Fail Fast:**\n- 30-article test set reveals capability quickly\n- Better to learn in POC1 than after building complex architecture\n\n**Document Everything:**\n- Track AI failures\n- Understand patterns\n- Inform future improvements\n\n==  Summary ==\n\n**Problem:** Article credibility  average of claim verdicts\n\n**Investigation:** 7 approaches analyzed for cost, speed, reliability\n\n**Chosen Solution:** Single-Pass Holistic Analysis (Approach 1)\n- Test in POC1 with enhanced prompt\n- Zero cost increase, no architecture changes\n- Validate if AI can do nuanced reasoning\n\n**Success Criteria:** 70% accuracy detecting misleading articles\n\n**Fallback Plans:**\n- POC1: Try Approach 4 (Weighted Aggregation)\n- POC2: Implement Approach 6 (Hybrid)\n- Production: Consider Approach 7 (LLM-as-Judge)\n\n**Next Steps:**\n1. Create 30-article test set\n2. Enhance AI prompt\n3. Test and measure accuracy\n4. Use results to inform POC2 design\n\n**This is POC1's key experimental feature!** \n", "Product Development.Specification.POC.Requirements": "= POC Requirements =\n\n{{info}}\n**POC1 Architecture:** 3-stage AKEL pipeline (Extract  Analyze  Holistic) with Redis caching, credit tracking, and LLM abstraction layer.\n\nSee [[POC1 API Specification>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] for complete technical details.\n{{/info}}\n\n\n\n**Status:**  Approved for Development \n**Version:** 2.0 (Updated after Specification Cross-Check) \n**Goal:** Prove that AI can extract claims and determine verdicts automatically without human intervention\n\n== 1. POC Overview ==\n\n=== 1.1 What POC Tests ===\n\n**Core Question:**\n> Can AI automatically extract factual claims from articles and evaluate them with reasonable verdicts?\n\n**What we're proving:**\n* AI can identify factual claims from text\n* AI can evaluate those claims and produce verdicts\n* Output is comprehensible and useful\n* Fully automated approach is viable\n\n**What we're NOT testing:**\n* Scenario generation (deferred to POC2)\n* Evidence display (deferred to POC2)\n* Production scalability\n* Perfect accuracy\n* Complete feature set\n\n=== 1.2 Scenarios Deferred to POC2 ===\n\n**Intentional Simplification:**\n\nScenarios are a core component of the full FactHarbor system (Claims  Scenarios  Evidence  Verdicts), but are **deliberately excluded from POC1**.\n\n**Rationale:**\n* **POC1 tests:** Can AI extract claims and generate verdicts?\n* **POC2 will add:** Scenario generation and management\n* **Open questions remain:** Should scenarios be separate entities? How are they sequenced with evidence gathering? What's the optimal workflow?\n\n**Design Decision:**\n\nProve basic AI capability first, then add scenario complexity based on POC1 learnings. This is good engineering: test the hardest part (AI fact-checking) before adding architectural complexity.\n\n**No Risk:**\n\nScenarios are additive complexity, not foundational. Deferring them to POC2 allows:\n* Faster POC1 validation\n* Learning from POC1 to inform scenario design\n* Iterative approach: fail fast if basic AI doesn't work\n* Flexibility to adjust scenario architecture based on POC1 insights\n\n**Full System Workflow (Future):**\n{{code}}\nClaims  Scenarios  Evidence  Verdicts\n{{/code}}\n\n**POC1 Simplified Workflow:**\n{{code}}\nClaims  Verdicts (scenarios implicit in reasoning)\n{{/code}}\n\n== 2. POC Output Specification ==\n\n=== 2.1 Component 1: ANALYSIS SUMMARY (Context-Aware) ===\n\n**What:** Context-aware overview that considers both individual claims AND their relationship to the article's main argument\n\n**Length:** 4-6 sentences \n\n**Content (Required Elements):**\n1. **Article's main thesis/claim** - What is the article trying to argue or prove?\n2. **Claim count and verdicts** - How many claims analyzed, distribution of verdicts\n3. **Central vs. supporting claims** - Which claims are central to the article's argument?\n4. **Relationship assessment** - Do the claims support the article's conclusion?\n5. **Overall credibility** - Final assessment considering claim importance\n\n**Critical Innovation:**\n\nPOC1 tests whether AI can understand that **article credibility  simple average of claim verdicts**. An article might:\n* Make accurate supporting facts but draw unsupported conclusions\n* Have one false central claim that invalidates the whole argument\n* Misframe accurate information to mislead\n\n**Good Example (Context-Aware):**\n{{code}}\nThis article argues that coffee cures cancer based on its antioxidant \ncontent. We analyzed 3 factual claims: 2 about coffee's chemical \nproperties are well-supported, but the main causal claim is refuted \nby current evidence. The article confuses correlation with causation. \nOverall assessment: MISLEADING - makes an unsupported medical claim \ndespite citing some accurate facts.\n{{/code}}\n\n**Poor Example (Simple Aggregation - Don't Do This):**\n{{code}}\nThis article makes 3 claims. 2 are well-supported and 1 is refuted.\nOverall assessment: mostly accurate (67% accurate).\n{{/code}}\n This misses that the refuted claim IS the article's main point!\n\n**What POC1 Tests:**\n\nCan AI identify and assess:\n*  The article's main thesis/conclusion?\n*  Which claims are central vs. supporting?\n*  Whether the evidence supports the conclusion?\n*  Overall credibility considering logical structure?\n\n**If AI Cannot Do This:**\n\nThat's valuable to learn in POC1! We'll:\n* Note as limitation\n* Fall back to simple aggregation with warning\n* Design explicit article-level analysis for POC2\n\n=== 2.2 Component 2: CLAIMS IDENTIFICATION ===\n\n**What:** List of factual claims extracted from article \n**Format:** Numbered list \n**Quantity:** 3-5 claims \n**Requirements:**\n* Factual claims only (not opinions/questions)\n* Clearly stated\n* Automatically extracted by AI\n\n**Example:**\n{{code}}\nCLAIMS IDENTIFIED:\n\n[1] Coffee reduces diabetes risk by 30%\n[2] Coffee improves heart health\n[3] Decaf has same benefits as regular\n[4] Coffee prevents Alzheimer's completely\n{{/code}}\n\n=== 2.3 Component 3: CLAIMS VERDICTS ===\n\n**What:** Verdict for each claim identified \n**Format:** Per claim structure \n\n**Required Elements:**\n* **Verdict Label:** ~~WELL-SUPPORTED / PARTIALLY SUPPORTED / UNCERTAIN / REFUTED~~ \n  //**Current Implementation (v2.6.33):** 7-point symmetric scale://\n  * TRUE (86-100%) / MOSTLY-TRUE (72-85%) / LEANING-TRUE (58-71%)\n  * MIXED (43-57%, high confidence) / UNVERIFIED (43-57%, low confidence)\n  * LEANING-FALSE (29-42%) / MOSTLY-FALSE (15-28%) / FALSE (0-14%)\n* **Confidence Score:** 0-100%\n* **Brief Reasoning:** 1-3 sentences explaining why\n* **Risk Tier:** A (High) / B (Medium) / C (Low) - for demonstration\n\n**Example:**\n{{code}}\nVERDICTS:\n\n[1] WELL-SUPPORTED (85%) [Risk: C]\nMultiple studies confirm 25-30% risk reduction with regular consumption.\n\n[2] UNCERTAIN (65%) [Risk: B]\nEvidence is mixed. Some studies show benefits, others show no effect.\n\n[3] PARTIALLY SUPPORTED (60%) [Risk: C]\nSome benefits overlap, but caffeine-related benefits are reduced in decaf.\n\n[4] REFUTED (90%) [Risk: B]\nNo evidence for complete prevention. Claim is significantly overstated.\n{{/code}}\n\n**Risk Tier Display:**\n* **Tier A (Red):** High Risk - Medical/Legal/Safety/Elections\n* **Tier B (Yellow):** Medium Risk - Policy/Science/Causality \n* **Tier C (Green):** Low Risk - Facts/Definitions/History\n\n**Note:** Risk tier shown for demonstration purposes in POC. Full system uses risk tiers to determine review workflow.\n\n=== 2.4 Component 4: ARTICLE SUMMARY (Optional) ===\n\n**What:** Brief summary of original article content \n**Length:** 3-5 sentences \n**Tone:** Neutral (article's position, not FactHarbor's analysis)\n\n**Example:**\n{{code}}\nARTICLE SUMMARY:\n\nHealth News Today article discusses coffee benefits, citing studies \non diabetes and Alzheimer's. Author highlights research linking coffee \nto disease prevention. Recommends 2-3 cups daily for optimal health.\n{{/code}}\n\n=== 2.5 Component 5: USAGE STATISTICS (Cost Tracking) ===\n\n**What:** LLM usage metrics for cost optimization and scaling decisions\n\n**Purpose:** \n* Understand cost per analysis\n* Identify optimization opportunities\n* Project costs at scale\n* Inform architecture decisions\n\n**Display Format:**\n{{code}}\nUSAGE STATISTICS:\n Article: 2,450 words (12,300 characters)\n Input tokens: 15,234\n Output tokens: 892\n Total tokens: 16,126\n Estimated cost: $0.24 USD\n Response time: 8.3 seconds\n Cost per claim: $0.048\n Model: claude-sonnet-4-20250514\n{{/code}}\n\n**Why This Matters:**\n\nAt scale, LLM costs are critical:\n* 10,000 articles/month  $200-500/month\n* 100,000 articles/month  $2,000-5,000/month\n* Cost optimization can reduce expenses 30-50%\n\n**What POC1 Learns:**\n* How cost scales with article length\n* Prompt optimization opportunities (caching, compression)\n* Output verbosity tradeoffs\n* Model selection strategy (FAST vs. REASONING roles)\n* Article length limits (if needed)\n\n**Implementation:**\n* Claude API already returns usage data\n* No extra API calls needed\n* Display to user + log for aggregate analysis\n* Test with articles of varying lengths\n\n**Critical for GO/NO-GO:** Unit economics must be viable at scale!\n\n=== 2.6 Total Output Size ===\n\n**Combined:** ~220-350 words\n* Analysis Summary (Context-Aware): 60-90 words (4-6 sentences)\n* Claims Identification: 30-50 words\n* Claims Verdicts: 100-150 words\n* Article Summary: 30-50 words (optional)\n\n**Note:** Analysis summary is slightly longer (4-6 sentences vs. 3-5) to accommodate context-aware assessment of article structure and logical reasoning.\n\n== 3. What's NOT in POC Scope ==\n\n=== 3.1 Feature Exclusions ===\n\nThe following are **explicitly excluded** from POC:\n\n**Content Features:**\n*  Scenarios (deferred to POC2)\n*  Evidence display (supporting/opposing lists)\n*  Source links (clickable references)\n*  Detailed reasoning chains\n*  Source quality ratings (shown but not detailed)\n*  Contradiction detection (basic only)\n*  Risk assessment (shown but not workflow-integrated)\n\n**Platform Features:**\n*  User accounts / authentication\n*  Saved history\n*  Search functionality\n*  Claim comparison\n*  User contributions\n*  Commenting system\n*  Social sharing\n\n**Technical Features:**\n*  Browser extensions\n*  Mobile apps\n*  API endpoints\n*  Webhooks\n*  Export features (PDF, CSV)\n\n**Quality Features:**\n*  Accessibility (WCAG compliance)\n*  Multilingual support\n*  Mobile optimization\n*  Media verification (images/videos)\n\n**Production Features:**\n*  Security hardening\n*  Privacy compliance (GDPR)\n*  Terms of service\n*  Monitoring/logging\n*  Error tracking\n*  Analytics\n*  A/B testing\n\n== 4. POC Simplifications vs. Full System ==\n\n=== 4.1 Architecture Comparison ===\n\n**POC Architecture (Simplified):**\n{{code}}\nUser Input  Single AKEL Call  Output Display\n (all processing)\n{{/code}}\n\n**Full System Architecture:**\n{{code}}\nUser Input  Claim Extractor  Claim Classifier  Scenario Generator \n Evidence Summarizer  Contradiction Detector  Verdict Generator \n Quality Gates  Publication  Output Display\n{{/code}}\n\n**Key Differences:**\n\n|=Aspect|=POC1|=Full System\n|Processing|Single API call|Multi-component pipeline\n|Scenarios|None (implicit)|Explicit entities with versioning\n|Evidence|Basic retrieval|Comprehensive with quality scoring\n|Quality Gates|Simplified (4 basic checks)|Full validation infrastructure\n|Workflow|3 steps (input/process/output)|6 phases with gates\n|Data Model|Stateless (no database)|PostgreSQL + Redis + S3\n|Architecture|Single prompt to Claude|AKEL Orchestrator + Components\n\n=== 4.2 Workflow Comparison ===\n\n**POC1 Workflow:**\n1. User submits text/URL\n2. Single AKEL call (all processing in one prompt)\n3. Display results\n**Total: 3 steps, ~10-18 seconds**\n\n**Full System Workflow:**\n1. **Claim Submission** (extraction, normalization, clustering)\n2. **Scenario Building** (definitions, assumptions, boundaries)\n3. **Evidence Handling** (retrieval, assessment, linking)\n4. **Verdict Creation** (synthesis, reasoning, approval)\n5. **Public Presentation** (summaries, landscapes, deep dives)\n6. **Time Evolution** (versioning, re-evaluation triggers)\n**Total: 6 phases with quality gates, ~10-30 seconds**\n\n=== 4.3 Why POC is Simplified ===\n\n**Engineering Rationale:**\n\n1. **Test core capability first:** Can AI do basic fact-checking without humans?\n2. **Fail fast:** If AI can't generate reasonable verdicts, pivot early\n3. **Learn before building:** POC1 insights inform full architecture\n4. **Iterative approach:** Add complexity only after validating foundations\n5. **Resource efficiency:** Don't build full system if core concept fails\n\n**Acceptable Trade-offs:**\n\n*  POC proves AI capability (most risky assumption)\n*  POC validates user comprehension (can people understand output?)\n*  POC doesn't validate full workflow (test in Beta)\n*  POC doesn't validate scale (test in Beta)\n*  POC doesn't validate scenario architecture (design in POC2)\n\n=== 4.4 Gap Between POC1 and POC2/Beta ===\n\n**What needs to be built for POC2:**\n* Scenario generation component\n* Evidence Model structure (full)\n* Scenario-evidence linking\n* Multi-interpretation comparison\n* Truth landscape visualization\n\n**What needs to be built for Beta:**\n* Multi-component AKEL pipeline\n* Quality gate infrastructure\n* Review workflow system\n* Audit sampling framework\n* Production data model\n* Federation architecture (Release 1.0)\n\n**POC1  POC2 is significant architectural expansion.**\n\n== 5. Publication Mode & Labeling ==\n\n=== 5.1 POC Publication Mode ===\n\n**Mode:** Mode 2 (AI-Generated, No Prior Human Review)\n\nPer FactHarbor Specification Section 11 \"POC v1 Behavior\":\n* Produces public AI-generated output\n* No human approval gate\n* Clear AI-Generated labeling\n* All quality gates active (simplified)\n* Risk tier classification shown (demo)\n\n=== 5.2 User-Facing Labels ===\n\n**Primary Label (top of analysis):**\n{{code}}\n\n [AI-GENERATED - POC/DEMO] \n \n This analysis was produced entirely by AI and has not \n been human-reviewed. Use for demonstration purposes. \n \n Source: AI/AKEL v1.0 (POC) \n Review Status: Not Reviewed (Proof-of-Concept) \n Quality Gates: 4/4 Passed (Simplified) \n Last Updated: [timestamp] \n\n{{/code}}\n\n**Per-Claim Risk Labels:**\n* **[Risk: A]**  High Risk (Medical/Legal/Safety)\n* **[Risk: B]**  Medium Risk (Policy/Science)\n* **[Risk: C]**  Low Risk (Facts/Definitions)\n\n=== 5.3 Display Requirements ===\n\n**Must Show:**\n* AI-Generated status (prominent)\n* POC/Demo disclaimer\n* Risk tier per claim\n* Confidence scores (0-100%)\n* Quality gate status (passed/failed)\n* Timestamp\n\n**Must NOT Claim:**\n* Human review\n* Production quality\n* Medical/legal advice\n* Authoritative verdicts\n* Complete accuracy\n\n=== 5.4 Mode 2 vs. Full System Publication ===\n\n|=Element|=POC Mode 2|=Full System Mode 2|=Full System Mode 3\n|Label|AI-Generated (POC)|AI-Generated|AKEL-Generated\n|Review|None|None|Human-Reviewed\n|Quality Gates|4 (simplified)|6 (full)|6 (full) + Human\n|Audit|None (POC)|Sampling (5-50%)|Pre-publication\n|Risk Display|Demo only|Workflow-integrated|Validated\n|User Actions|View only|Flag for review|Trust rating\n\n== 6. Quality Gates (Simplified Implementation) ==\n\n=== 6.1 Overview ===\n\nPer FactHarbor Specification Section 6, all AI-generated content must pass quality gates before publication. POC implements **simplified versions** of the 4 mandatory gates.\n\n**Full System Has 4 Gates:**\n1. Source Quality\n2. Contradiction Search (MANDATORY)\n3. Uncertainty Quantification\n4. Structural Integrity\n\n**POC Implements Simplified Versions:**\n* Focus on demonstrating concept\n* Basic implementations sufficient\n* Failures displayed to user (not blocking)\n* Full system has comprehensive validation\n\n=== 6.2 Gate 1: Source Quality (Basic) ===\n\n**Full System Requirements:**\n* Primary sources identified and accessible\n* Source reliability scored against whitelist\n* Citation completeness verified\n* Publication dates checked\n* Author credentials validated\n\n**POC Implementation:**\n*  At least 2 sources found\n*  Sources accessible (URLs valid)\n*  No whitelist checking\n*  No credential validation\n*  No comprehensive reliability scoring\n\n**Pass Criteria:** 2 accessible sources found\n\n**Failure Handling:** Display error message, don't generate verdict\n\n=== 6.3 Gate 2: Contradiction Search (Basic) ===\n\n**Full System Requirements:**\n* Counter-evidence actively searched\n* Reservations and limitations identified\n* Alternative interpretations explored\n* Bubble detection (echo chambers, conspiracy theories)\n* Cross-cultural and international perspectives\n* Academic literature (supporting AND opposing)\n\n**POC Implementation:**\n*  Basic search for counter-evidence\n*  Identify obvious contradictions\n*  No comprehensive academic search\n*  No bubble detection\n*  No systematic alternative interpretation search\n*  No international perspective verification\n\n**Pass Criteria:** Basic contradiction search attempted\n\n**Failure Handling:** Note \"limited contradiction search\" in output\n\n=== 6.4 Gate 3: Uncertainty Quantification (Basic) ===\n\n**Full System Requirements:**\n* Confidence scores calculated for all claims/verdicts\n* Limitations explicitly stated\n* Data gaps identified and disclosed\n* Strength of evidence assessed\n* Alternative scenarios considered\n\n**POC Implementation:**\n*  Confidence scores (0-100%)\n*  Basic uncertainty acknowledgment\n*  No detailed limitation disclosure\n*  No data gap identification\n*  No alternative scenario consideration (deferred to POC2)\n\n**Pass Criteria:** Confidence score assigned\n\n**Failure Handling:** Show \"Confidence: Unknown\" if calculation fails\n\n=== 6.5 Gate 4: Structural Integrity (Basic) ===\n\n**Full System Requirements:**\n* No hallucinations detected (fact-checking against sources)\n* Logic chain valid and traceable\n* References accessible and verifiable\n* No circular reasoning\n* Premises clearly stated\n\n**POC Implementation:**\n*  Basic coherence check\n*  References accessible\n*  No comprehensive hallucination detection\n*  No formal logic validation\n*  No premise extraction and verification\n\n**Pass Criteria:** Output is coherent and references are accessible\n\n**Failure Handling:** Display error message\n\n=== 6.6 Quality Gate Display ===\n\n**POC shows simplified status:**\n{{code}}\nQuality Gates: 4/4 Passed (Simplified)\n Source Quality: 3 sources found\n Contradiction Search: Basic search completed\n Uncertainty: Confidence scores assigned\n Structural Integrity: Output coherent\n{{/code}}\n\n**If any gate fails:**\n{{code}}\nQuality Gates: 3/4 Passed (Simplified)\n Source Quality: 3 sources found\n Contradiction Search: Search failed - limited evidence\n Uncertainty: Confidence scores assigned\n Structural Integrity: Output coherent\n\nNote: This analysis has limited evidence. Use with caution.\n{{/code}}\n\n=== 6.7 Simplified vs. Full System ===\n\n|=Gate|=POC (Simplified)|=Full System\n|Source Quality|2 sources accessible|Whitelist scoring, credentials, comprehensiveness\n|Contradiction|Basic search|Systematic academic + media + international\n|Uncertainty|Confidence % assigned|Detailed limitations, data gaps, alternatives\n|Structural|Coherence check|Hallucination detection, logic validation, premise check\n\n**POC Goal:** Demonstrate that quality gates are possible, not perfect implementation.\n\n== 7. AKEL Architecture Comparison ==\n\n=== 7.1 POC AKEL (Simplified) ===\n\n**Implementation:**\n* Single provider API call (REASONING model)\n* One comprehensive prompt\n* All processing in single request\n* No separate components\n* No orchestration layer\n\n**Prompt Structure:**\n{{code}}\nTask: Analyze this article and provide:\n\n1. Extract 3-5 factual claims\n2. For each claim:\n - Determine verdict (WELL-SUPPORTED/PARTIALLY/UNCERTAIN/REFUTED)\n - Assign confidence score (0-100%)\n - Assign risk tier (A/B/C)\n - Write brief reasoning (1-3 sentences)\n3. Generate analysis summary (3-5 sentences)\n4. Generate article summary (3-5 sentences)\n5. Run basic quality checks\n\nReturn as structured JSON.\n{{/code}}\n\n**Processing Time:** 10-18 seconds (estimate)\n\n=== 7.2 Full System AKEL (Production) ===\n\n**Architecture:**\n{{code}}\nAKEL Orchestrator\n Claim Extractor\n Claim Classifier (with risk tier assignment)\n Scenario Generator\n Evidence Summarizer\n Contradiction Detector\n Quality Gate Validator\n Audit Sampling Scheduler\n Federation Sync Adapter (Release 1.0+)\n{{/code}}\n\n**Processing:**\n* Parallel processing where possible\n* Separate component calls\n* Quality gates between phases\n* Audit sampling selection\n* Cross-node coordination (federated mode)\n\n**Processing Time:** 10-30 seconds (full pipeline)\n\n=== 7.3 Why POC Uses Single Call ===\n\n**Advantages:**\n*  Simpler to implement\n*  Faster POC development\n*  Easier to debug\n*  Proves AI capability\n*  Good enough for concept validation\n\n**Limitations:**\n*  No component reusability\n*  No parallel processing\n*  All-or-nothing (can't partially succeed)\n*  Harder to improve individual components\n*  No audit sampling\n\n**Acceptable Trade-off:**\n\nPOC tests \"Can AI do this?\" not \"How should we architect it?\"\n\nFull component architecture comes in Beta after POC validates concept.\n\n=== 7.4 Evolution Path ===\n\n**POC1:** Single prompt  Prove concept\n**POC2:** Add scenario component  Test full pipeline\n**Beta:** Multi-component AKEL  Production architecture\n**Release 1.0:** Full AKEL + Federation  Scale\n\n== 8. Functional Requirements ==\n\n=== FR-POC-1: Article Input ===\n\n**Requirement:** User can submit article for analysis\n\n**Functionality:**\n* Text input field (paste article text, up to 5000 characters)\n* URL input field (paste article URL)\n* \"Analyze\" button to trigger processing\n* Loading indicator during analysis\n\n**Excluded:**\n* No user authentication\n* No claim history\n* No search functionality\n* No saved templates\n\n**Acceptance Criteria:**\n* User can paste text from article\n* User can paste URL of article\n* System accepts input and triggers analysis\n\n=== FR-POC-2: Claim Extraction (Fully Automated) ===\n\n**Requirement:** AI automatically extracts 3-5 factual claims\n\n**Functionality:**\n* AI reads article text\n* AI identifies factual claims (not opinions/questions)\n* AI extracts 3-5 most important claims\n* System displays numbered list\n\n**Critical:** NO MANUAL EDITING ALLOWED\n* AI selects which claims to extract\n* AI identifies factual vs. non-factual\n* System processes claims as extracted\n* No human curation or correction\n\n**Error Handling:**\n* If extraction fails: Display error message\n* User can retry with different input\n* No manual intervention to fix extraction\n\n**Acceptance Criteria:**\n* AI extracts 3-5 claims automatically\n* Claims are factual (not opinions)\n* Claims are clearly stated\n* No manual editing required\n\n=== FR-POC-3: Verdict Generation (Fully Automated) ===\n\n**Requirement:** AI automatically generates verdict for each claim\n\n**Functionality:**\n* For each claim, AI:\n * Evaluates claim based on available evidence/knowledge\n * Determines verdict: ~~WELL-SUPPORTED / PARTIALLY SUPPORTED / UNCERTAIN / REFUTED~~ //(Now: 7-point scale - see Section 2.3)//\n * Assigns confidence score (0-100%)\n * Assigns risk tier (A/B/C)\n * Writes brief reasoning (1-3 sentences)\n* System displays verdict for each claim\n\n**Critical:** NO MANUAL EDITING ALLOWED\n* AI computes verdicts based on evidence\n* AI generates confidence scores\n* AI writes reasoning\n* No human review or adjustment\n\n**Error Handling:**\n* If verdict generation fails: Display error message\n* User can retry\n* No manual intervention to adjust verdicts\n\n**Acceptance Criteria:**\n* Each claim has a verdict\n* Confidence score is displayed (0-100%)\n* Risk tier is displayed (A/B/C)\n* Reasoning is understandable (1-3 sentences)\n* Verdict is defensible given reasoning\n* All generated automatically by AI\n\n=== FR-POC-4: Analysis Summary (Fully Automated) ===\n\n**Requirement:** AI generates brief summary of analysis\n\n**Functionality:**\n* AI summarizes findings in 3-5 sentences:\n * How many claims found\n * Distribution of verdicts\n * Overall assessment\n* System displays at top of results\n\n**Critical:** NO MANUAL EDITING ALLOWED\n\n**Acceptance Criteria:**\n* Summary is coherent\n* Accurately reflects analysis\n* 3-5 sentences\n* Automatically generated\n\n=== FR-POC-5: Article Summary (Fully Automated, Optional) ===\n\n**Requirement:** AI generates brief summary of original article\n\n**Functionality:**\n* AI summarizes article content (not FactHarbor's analysis)\n* 3-5 sentences\n* System displays\n\n**Note:** Optional - can skip if time limited\n\n**Critical:** NO MANUAL EDITING ALLOWED\n\n**Acceptance Criteria:**\n* Summary is neutral (article's position)\n* Accurately reflects article content\n* 3-5 sentences\n* Automatically generated\n\n=== FR-POC-6: Publication Mode Display ===\n\n**Requirement:** Clear labeling of AI-generated content\n\n**Functionality:**\n* Display Mode 2 publication label\n* Show POC/Demo disclaimer\n* Display risk tiers per claim\n* Show quality gate status\n* Display timestamp\n\n**Acceptance Criteria:**\n* Label is prominent and clear\n* User understands this is AI-generated POC output\n* Risk tiers are color-coded\n* Quality gate status is visible\n\n=== FR-POC-7: Quality Gate Execution ===\n\n**Requirement:** Execute simplified quality gates\n\n**Functionality:**\n* Check source quality (basic)\n* Attempt contradiction search (basic)\n* Calculate confidence scores\n* Verify structural integrity (basic)\n* Display gate results\n\n**Acceptance Criteria:**\n* All 4 gates attempted\n* Pass/fail status displayed\n* Failures explained to user\n* Gates don't block publication (POC mode)\n\n== 9. Non-Functional Requirements ==\n\n=== NFR-POC-1: Fully Automated Processing ===\n\n**Requirement:** Complete AI automation with zero manual intervention\n\n**Critical Rule:** NO MANUAL EDITING AT ANY STAGE\n\n**What this means:**\n* Claims: AI selects (no human curation)\n* Scenarios: N/A (deferred to POC2)\n* Evidence: AI evaluates (no human selection)\n* Verdicts: AI determines (no human adjustment)\n* Summaries: AI writes (no human editing)\n\n**Pipeline:**\n{{code}}\nUser Input  AKEL Processing  Output Display\n \n ZERO human editing\n{{/code}}\n\n**If AI output is poor:**\n*  Do NOT manually fix it\n*  Document the failure\n*  Improve prompts and retry\n*  Accept that POC might fail\n\n**Why this matters:**\n* Tests whether AI can do this without humans\n* Validates scalability (humans can't review every analysis)\n* Honest test of technical feasibility\n\n=== NFR-POC-2: Performance ===\n\n**Requirement:** Analysis completes in reasonable time\n\n**Acceptable Performance:**\n* Processing time: 1-5 minutes (acceptable for POC)\n* Display loading indicator to user\n* Show progress if possible (\"Extracting claims...\", \"Generating verdicts...\")\n\n**Not Required:**\n* Production-level speed (< 30 seconds)\n* Optimization for scale\n* Caching\n\n**Acceptance Criteria:**\n* Analysis completes within 5 minutes\n* User sees loading indicator\n* No timeout errors\n\n=== NFR-POC-3: Reliability ===\n\n**Requirement:** System works for manual testing sessions\n\n**Acceptable:**\n* Occasional errors (< 20% failure rate)\n* Manual restart if needed\n* Display error messages clearly\n\n**Not Required:**\n* 99.9% uptime\n* Automatic error recovery\n* Production monitoring\n\n**Acceptance Criteria:**\n* System works for test demonstrations\n* Errors are handled gracefully\n* User receives clear error messages\n\n=== NFR-POC-4: Environment ===\n\n**Requirement:** Runs on simple infrastructure\n\n**Acceptable:**\n* Single machine or simple cloud setup\n* No distributed architecture\n* No load balancing\n* No redundancy\n* Local development environment viable\n\n**Not Required:**\n* Production infrastructure\n* Multi-region deployment\n* Auto-scaling\n* Disaster recovery\n\n=== NFR-POC-5: Cost Efficiency Tracking ===\n\n**Requirement:** Track and display LLM usage metrics to inform optimization decisions\n\n**Must Track:**\n* Input tokens (article + prompt)\n* Output tokens (generated analysis)\n* Total tokens\n* Estimated cost (USD)\n* Response time (seconds)\n* Article length (words/characters)\n\n**Must Display:**\n* Usage statistics in UI (Component 5)\n* Cost per analysis\n* Cost per claim extracted\n\n**Must Log:**\n* Aggregate metrics for analysis\n* Cost distribution by article length\n* Token efficiency trends\n\n**Purpose:**\n* Understand unit economics\n* Identify optimization opportunities\n* Project costs at scale\n* Inform architecture decisions (caching, model selection, etc.)\n\n**Acceptance Criteria:**\n*  Usage data displayed after each analysis\n*  Metrics logged for aggregate analysis\n*  Cost calculated accurately (Claude API pricing)\n*  Test cases include varying article lengths\n*  POC1 report includes cost analysis section\n\n**Success Target:**\n* Average cost per analysis < $0.05 USD\n* Cost scaling behavior understood (linear/exponential)\n* 2+ optimization opportunities identified\n\n**Critical:** Unit economics must be viable for scaling decision!\n\n== 10. Technical Architecture ==\n\n=== 10.1 System Components ===\n\n**Frontend:**\n* Simple HTML form (text input + URL input + button)\n* Loading indicator\n* Results display page (single page, no tabs/navigation)\n\n**Backend:**\n* Single API endpoint\n* Calls provider API (REASONING model; configured via LLM abstraction)\n* Parses response\n* Returns JSON to frontend\n\n**Data Storage:**\n* None required (stateless POC)\n* Optional: Simple file storage or SQLite for demo examples\n\n**External Services:**\n* Claude API (Anthropic) - required\n* Optional: URL fetch service for article text extraction\n\n=== 10.2 Processing Flow ===\n\n{{code}}\n1. User submits text or URL\n \n2. Backend receives request\n \n3. If URL: Fetch article text\n \n4. Call Claude API with single prompt:\n \"Extract claims, evaluate each, provide verdicts\"\n \n5. Claude API returns:\n - Analysis summary\n - Claims list\n - Verdicts for each claim (with risk tiers)\n - Article summary (optional)\n - Quality gate results\n \n6. Backend parses response\n \n7. Frontend displays results with Mode 2 labeling\n{{/code}}\n\n**Key Simplification:** Single API call does entire analysis\n\n=== 10.3 AI Prompt Strategy ===\n\n**Single Comprehensive Prompt:**\n{{code}}\nTask: Analyze this article and provide:\n\n1. Identify the article's main thesis/conclusion\n - What is the article trying to argue or prove?\n - What is the primary claim or conclusion?\n\n2. Extract 3-5 factual claims from the article\n - Note which claims are CENTRAL to the main thesis\n - Note which claims are SUPPORTING facts\n\n3. For each claim:\n - Determine verdict (7-point scale: TRUE/MOSTLY-TRUE/LEANING-TRUE/MIXED/UNVERIFIED/LEANING-FALSE/MOSTLY-FALSE/FALSE)\n - Assign confidence score (0-100%)\n - Assign risk tier (A: Medical/Legal/Safety, B: Policy/Science, C: Facts/Definitions)\n - Write brief reasoning (1-3 sentences)\n\n4. Assess relationship between claims and main thesis:\n - Do the claims actually support the article's conclusion?\n - Are there logical leaps or unsupported inferences?\n - Is the article's framing misleading even if individual facts are accurate?\n\n5. Run quality gates:\n - Check: 2 sources found\n - Attempt: Basic contradiction search\n - Calculate: Confidence scores\n - Verify: Structural integrity\n\n6. Write context-aware analysis summary (4-6 sentences):\n - State article's main thesis\n - Report claims found and verdict distribution\n - Note if central claims are problematic\n - Assess whether evidence supports conclusion\n - Overall credibility considering claim importance\n\n7. Write article summary (3-5 sentences: neutral summary of article content)\n\nReturn as structured JSON with quality gate results.\n{{/code}}\n\n**One prompt generates everything.**\n\n**Critical Addition:**\n\nSteps 1, 2 (marking central claims), 4, and 6 are NEW for context-aware analysis. These test whether AI can distinguish between \"accurate facts poorly reasoned\" vs. \"genuinely credible article.\"\n\n=== 10.4 Technology Stack Suggestions ===\n\n**Frontend:**\n* HTML + CSS + JavaScript (minimal framework)\n* OR: Next.js (if team prefers)\n* Hosted: Local machine OR Vercel/Netlify free tier\n\n**Backend:**\n* Python Flask/FastAPI (simple REST API)\n* OR: Next.js API routes (if using Next.js)\n* Hosted: Local machine OR Railway/Render free tier\n\n**AKEL Integration:**\n* Claude API via Anthropic SDK\n* Model: Provider-default REASONING model or latest available\n\n**Database:**\n* None (stateless acceptable)\n* OR: SQLite if want to store demo examples\n* OR: JSON files on disk\n\n**Deployment:**\n* Local development environment sufficient for POC\n* Optional: Deploy to cloud for remote demos\n\n== 11. Success Criteria ==\n\n=== 11.1 Minimum Success (POC Passes) ===\n\n**Required for GO decision:**\n*  AI extracts 3-5 factual claims automatically\n*  AI provides verdict for each claim automatically\n*  Verdicts are reasonable (70% make logical sense)\n*  Analysis summary is coherent\n*  Output is comprehensible to reviewers\n*  Team/advisors understand the output\n*  Team agrees approach has merit\n*  **Minimal or no manual editing needed** (< 30% of analyses require manual intervention)\n*  **Cost efficiency acceptable** (average cost per analysis < $0.05 USD target)\n*  **Cost scaling understood** (data collected on article length vs. cost)\n*  **Optimization opportunities identified** (2 potential improvements documented)\n\n**Quality Definition:**\n* \"Reasonable verdict\" = Defensible given general knowledge\n* \"Coherent summary\" = Logically structured, grammatically correct\n* \"Comprehensible\" = Reviewers understand what analysis means\n\n=== 11.2 POC Fails If ===\n\n**Automatic NO-GO if any of these:**\n*  Claim extraction poor (< 60% accuracy - extracts non-claims or misses obvious ones)\n*  Verdicts nonsensical (< 60% reasonable - contradictory or random)\n*  Output incomprehensible (reviewers can't understand analysis)\n*  **Requires manual editing for most analyses** (> 50% need human correction)\n*  Team loses confidence in AI-automated approach\n\n=== 11.3 Quality Thresholds ===\n\n**POC quality expectations:**\n\n|=Component|=Quality Threshold|=Definition\n|Claim Extraction|(% class=\"success\" %)70% accuracy(%%) |Identifies obvious factual claims, may miss some edge cases\n|Verdict Logic|(% class=\"success\" %)70% defensible(%%) |Verdicts are logical given reasoning provided\n|Reasoning Clarity|(% class=\"success\" %)70% clear(%%) |1-3 sentences are understandable and relevant\n|Overall Analysis|(% class=\"success\" %)70% useful(%%) |Output helps user understand article claims\n\n**Analogy:** \"B student\" quality (70-80%), not \"A+\" perfection yet\n\n**Not expecting:**\n* 100% accuracy\n* Perfect claim coverage\n* Comprehensive evidence gathering\n* Flawless verdicts\n* Production polish\n\n**Expecting:**\n* Reasonable claim extraction\n* Defensible verdicts\n* Understandable reasoning\n* Useful output\n\n== 12. Test Cases ==\n\n=== 12.1 Test Case 1: Simple Factual Claim ===\n\n**Input:** \"Coffee reduces the risk of type 2 diabetes by 30%\"\n\n**Expected Output:**\n* Extract claim correctly\n* Provide verdict: WELL-SUPPORTED or PARTIALLY SUPPORTED\n* Confidence: 70-90%\n* Risk tier: C (Low)\n* Reasoning: Mentions studies or evidence\n\n**Success:** Verdict is reasonable and reasoning makes sense\n\n=== 12.2 Test Case 2: Complex News Article ===\n\n**Input:** News article URL with multiple claims about politics/health/science\n\n**Expected Output:**\n* Extract 3-5 key claims\n* Verdict for each (may vary: some supported, some uncertain, some refuted)\n* Coherent analysis summary\n* Article summary\n* Risk tiers assigned appropriately\n\n**Success:** Claims identified are actually from article, verdicts are reasonable\n\n=== 12.3 Test Case 3: Controversial Topic ===\n\n**Input:** Article on contested political or scientific topic\n\n**Expected Output:**\n* Balanced analysis\n* Acknowledges uncertainty where appropriate\n* Doesn't overstate confidence\n* Reasoning shows awareness of complexity\n\n**Success:** Analysis is fair and doesn't show obvious bias\n\n=== 12.4 Test Case 4: Clearly False Claim ===\n\n**Input:** Article with obviously false claim (e.g., \"The Earth is flat\")\n\n**Expected Output:**\n* Extract claim\n* Verdict: REFUTED\n* High confidence (> 90%)\n* Risk tier: C (Low - established fact)\n* Clear reasoning\n\n**Success:** AI correctly identifies false claim with high confidence\n\n=== 12.5 Test Case 5: Genuinely Uncertain Claim ===\n\n**Input:** Article with claim where evidence is genuinely mixed\n\n**Expected Output:**\n* Extract claim\n* Verdict: UNCERTAIN\n* Moderate confidence (40-60%)\n* Reasoning explains why uncertain\n\n**Success:** AI recognizes uncertainty and doesn't overstate confidence\n\n=== 12.6 Test Case 6: High-Risk Medical Claim ===\n\n**Input:** Article making medical claims\n\n**Expected Output:**\n* Extract claim\n* Verdict: [appropriate based on evidence]\n* Risk tier: A (High - medical)\n* Red label displayed\n* Clear disclaimer about not being medical advice\n\n**Success:** Risk tier correctly assigned, appropriate warnings shown\n\n== 13. POC Decision Gate ==\n\n=== 13.1 Decision Framework ===\n\nAfter POC testing complete, team makes one of three decisions:\n\n**Option A: GO (Proceed to POC2)**\n\n**Conditions:**\n* AI quality 70% without manual editing\n* Basic claim  verdict pipeline validated\n* Internal + advisor feedback positive\n* Technical feasibility confirmed\n* Team confident in direction\n* Clear path to improving AI quality to 90%\n\n**Next Steps:**\n* Plan POC2 development (add scenarios)\n* Design scenario architecture\n* Expand to Evidence Model structure\n* Test with more complex articles\n\n**Option B: NO-GO (Pivot or Stop)**\n\n**Conditions:**\n* AI quality < 60%\n* Requires manual editing for most analyses (> 50%)\n* Feedback indicates fundamental flaws\n* Cost/effort not justified by value\n* No clear path to improvement\n\n**Next Steps:**\n* **Pivot:** Change to hybrid human-AI approach (accept manual review required)\n* **Stop:** Conclude approach not viable, revisit later\n\n**Option C: ITERATE (Improve POC)**\n\n**Conditions:**\n* Concept has merit but execution needs work\n* Specific improvements identified\n* Addressable with better prompts/approach\n* AI quality between 60-70%\n\n**Next Steps:**\n* Improve AI prompts\n* Test different approaches\n* Re-run POC with improvements\n* Then make GO/NO-GO decision\n\n=== 13.2 Decision Criteria Summary ===\n\n{{code}}\nAI Quality < 60%  NO-GO (approach doesn't work)\nAI Quality 60-70%  ITERATE (improve and retry)\nAI Quality 70%  GO (proceed to POC2)\n{{/code}}\n\n== 14. Key Risks & Mitigations ==\n\n=== 14.1 Risk: AI Quality Not Good Enough ===\n\n**Likelihood:** Medium-High \n**Impact:** POC fails \n\n**Mitigation:**\n* Extensive prompt engineering and testing\n* Use best available AI models (role-based selection; configured via LLM abstraction)\n* Test with diverse article types\n* Iterate on prompts based on results\n\n**Acceptance:** This is what POC tests - be ready for failure\n\n=== 14.2 Risk: AI Consistency Issues ===\n\n**Likelihood:** Medium \n**Impact:** Works sometimes, fails other times \n\n**Mitigation:**\n* Test with 10+ diverse articles\n* Measure success rate honestly\n* Improve prompts to increase consistency\n\n**Acceptance:** Some variability OK if average quality 70%\n\n=== 14.3 Risk: Output Incomprehensible ===\n\n**Likelihood:** Low-Medium \n**Impact:** Users can't understand analysis \n\n**Mitigation:**\n* Create clear explainer document\n* Iterate on output format\n* Test with non-technical reviewers\n* Simplify language if needed\n\n**Acceptance:** Iterate until comprehensible\n\n=== 14.4 Risk: API Rate Limits / Costs ===\n\n**Likelihood:** Low \n**Impact:** System slow or expensive \n\n**Mitigation:**\n* Monitor API usage\n* Implement retry logic\n* Estimate costs before scaling\n\n**Acceptance:** POC can be slow and expensive (optimization later)\n\n=== 14.5 Risk: Scope Creep ===\n\n**Likelihood:** Medium \n**Impact:** POC becomes too complex \n\n**Mitigation:**\n* Strict scope discipline\n* Say NO to feature additions\n* Keep focus on core question\n\n**Acceptance:** POC is minimal by design\n\n== 15. POC Philosophy ==\n\n=== 15.1 Core Principles ===\n\n**1. Build Less, Learn More**\n* Minimum features to test hypothesis\n* Don't build unvalidated features\n* Focus on core question only\n\n**2. Fail Fast**\n* Quick test of hardest part (AI capability)\n* Accept that POC might fail\n* Better to discover issues early\n* Honest assessment over optimistic hope\n\n**3. Test First, Build Second**\n* Validate AI can do this before building platform\n* Don't assume it will work\n* Let results guide decisions\n\n**4. Automation First**\n* No manual editing allowed\n* Tests scalability, not just feasibility\n* Proves approach can work at scale\n\n**5. Honest Assessment**\n* Don't cherry-pick examples\n* Don't manually fix bad outputs\n* Document failures openly\n* Make data-driven decisions\n\n=== 15.2 What POC Is ===\n\n Testing AI capability without humans \n Proving core technical concept \n Fast validation of approach \n Honest assessment of feasibility \n\n=== 15.3 What POC Is NOT ===\n\n Building a product \n Production-ready system \n Feature-complete platform \n Perfectly accurate analysis \n Polished user experience \n\n== 16. Success = Clear Path Forward ==\n\n**If POC succeeds (70% AI quality):**\n*  Approach validated\n*  Proceed to POC2 (add scenarios)\n*  Design full Evidence Model structure\n*  Test multi-scenario comparison\n*  Focus on improving AI quality from 70%  90%\n\n**If POC fails (< 60% AI quality):**\n*  Learn what doesn't work\n*  Pivot to different approach\n*  OR wait for better AI technology\n*  Avoid wasting resources on non-viable approach\n\n**Either way, POC provides clarity.**\n\n== 17. Related Pages ==\n\n* [[User Needs>>FactHarbor.Product Development.Requirements.User Needs.WebHome]]\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[Gap Analysis>>FactHarbor.Product Development.Requirements.GapAnalysis]]\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]\n* [[Workflows>>FactHarbor.Product Development.Specification.Workflows.WebHome]]\n\n**Document Status:**  Ready for POC Development (Version 2.0 - Updated with Spec Alignment)\n\n\n=== NFR-POC-11: LLM Provider Abstraction (POC1) ===\n\n**Requirement:** POC1 MUST implement LLM abstraction layer with support for multiple providers.\n\n**POC1 Implementation:**\n\n* **Primary Provider:** Anthropic Claude API\n * Stage 1: Provider-default FAST model\n * Stage 2: Provider-default REASONING model (cached)\n * Stage 3: Provider-default REASONING model\n\n* **Provider Interface:** Abstract LLMProvider interface implemented\n\n* **Configuration:** Environment variables for provider selection\n * {{code}}LLM_PRIMARY_PROVIDER=anthropic{{/code}}\n * {{code}}LLM_STAGE1_MODEL=claude-haiku-4{{/code}}\n * {{code}}LLM_STAGE2_MODEL=claude-sonnet-3-5{{/code}}\n\n* **Failover:** Basic error handling with cache fallback for Stage 2\n\n* **Cost Tracking:** Log provider name and cost per request\n\n**Future (POC2/Beta):**\n\n* Secondary provider (OpenAI) with automatic failover\n* Admin API for runtime provider switching\n* Cost comparison dashboard\n* Cross-provider output verification\n\n**Success Criteria:**\n\n* All LLM calls go through abstraction layer (no direct API calls)\n* Provider can be changed via environment variable without code changes\n* Cost tracking includes provider name in logs\n* Stage 2 falls back to cache on provider failure\n\n**Implementation:** See [[POC1 API & Schemas Specification>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] Section 6\n\n**Dependencies:**\n* NFR-14 (Main Requirements)\n* Design Decision 9\n* Architecture Section 2.2\n\n**Priority:** HIGH (P1)\n\n**Rationale:** Even though POC1 uses single provider, abstraction must be in place from start to avoid costly refactoring later.\n\n", "Product Development.Specification.POC.Specification": "= POC1: Core Workflow with Quality Gates =\n\n**Version:** 0.9.86-POC1 \n**Phase:** Proof of Concept 1 - Core Workflow Validation \n**Status:** Production Specification \n**Based on:** FactHarbor V0.9.69 + Quality Gate Enhancements\n\n---\n\n== Purpose ==\n\nThis specification defines POC1 scope and requirements. POC1 proves that the AKEL workflow can produce credible, quality-assured outputs by demonstrating:\n\n1. **Automated Claim Extraction** - AKEL reliably identifies fact-checkable claims from articles\n2. **Evidence-Based Verdicts** - AKEL generates verdicts supported by real evidence\n3. **Quality Validation** - Quality gates prevent hallucinations and low-confidence outputs\n4. **Core Workflow Viability** - The Article  Claims  Evidence  Verdicts pipeline works\n\n---\n\n== Success Criteria ==\n\n**POC1 is successful when:**\n\n*  Process 20 test articles without failures\n*  Extract claims that are verifiably factual (not opinions)\n*  Generate verdicts with supporting evidence (minimum 2 sources per verdict)\n*  Hallucination rate below 10% (manual verification)\n*  Quality gates effectively filter non-publishable content\n\n---\n\n== Scope ==\n\n=== In POC1 ===\n\n**Core Functionality:**\n* Article processing and claim extraction\n* Evidence collection and source assessment\n* Verdict generation with quality validation\n* Basic UI to display results\n* Manual quality metrics tracking\n\n**Quality Assurance:**\n* Gate 1: Claim Extraction Validation\n* Gate 4: Verdict Confidence Assessment\n* Quality metrics dashboard (manual)\n\n=== Deferred to POC2 and Later ===\n\n**POC2 (Quality & Reliability):**\n* ~~FR8: Time Evolution (Dropped - Not in V1.0)~~\n* FR10: Human Contributor Override\n* Full NFR11 (Gates 2 & 3)\n* Evidence deduplication (FR54)\n* Quality metrics dashboard (automated)\n\n**Beta 0 (User Testing):**\n* FR11: Audit Trail\n* FR13: In-Article Claim Highlighting\n* NFR13: Public quality metrics\n* FR47: Archive.org Integration\n\n**V1.0 and Later:**\n* User accounts and authentication\n* Corrections system (FR45)\n* Search engine optimization (FR44)\n* Image/video verification (FR46, FR51)\n* API endpoints (UN-14)\n* Security hardening (NFR12)\n* A/B testing (FR49)\n\n---\n\n== Requirements ==\n\n=== NFR11: AKEL Quality Assurance Framework (POC1 Lite Version) ===\n\n**Importance:** CRITICAL \n**Phase:** POC1 (2-gate subset), POC2 (full 4-gate system) \n**Purpose:** Validate AKEL outputs before displaying to users\n\nPOC1 implements **2 critical gates** from the full NFR11 specification:\n\n==== Gate 1: Claim Extraction Validation ====\n\n**Purpose:** Ensure extracted claims are factual assertions that can be verified\n\n**Validation Checks:**\n\n|= Check |= Purpose |= Pass Criteria\n| **Factuality Test** | Can this claim be proven true/false? | Claim must be verifiable\n| **Opinion Detection** | Contains subjective language? | Opinion score  0.3\n| **Specificity Check** | Contains concrete details? | Specificity score  0.3\n| **Future Prediction** | About future events? | Must be about past/present\n\n**Claim Classification:**\n\n*  **FACTUAL** - Verifiable with evidence (proceed to verification)\n*  **OPINION** - Subjective judgment (exclude from analysis)\n*  **PREDICTION** - Future-oriented claim (exclude from analysis)\n*  **AMBIGUOUS** - Too vague to verify (exclude from analysis)\n\n**Implementation Logic:**\n\nThe system evaluates each extracted claim by:\n1. Checking if the claim can be verified using evidence\n2. Detecting opinion markers (e.g., \"I think\", \"beautiful\", \"should\")\n3. Counting specific elements (proper nouns, numbers, dates, locations)\n4. Detecting future-oriented language (e.g., \"will\", \"predicted\", \"expects\")\n\n**Pass Criteria:**\n* Must be factual (verifiable)\n* Opinion score  0.3\n* Specificity score  0.3\n* Claim type = FACTUAL\n\n**Failure Actions:**\n* Non-factual claims are **excluded** from further analysis\n* User sees clear explanation of why claim was excluded\n* No scenarios or verdicts generated for failed claims\n\n**Example Outcomes:**\n\n **Pass:** \"France's GDP was $2.7 trillion in 2023\"\n* Factual: Yes\n* Opinion markers: None\n* Specific elements: France, $2.7 trillion, 2023\n* Type: FACTUAL\n\n **Fail:** \"France has a beautiful culture\"\n* Factual: No (subjective)\n* Opinion markers: \"beautiful\"\n* Type: OPINION\n\n==== Gate 4: Verdict Confidence Assessment ====\n\n**Purpose:** Only display verdicts with sufficient evidence and confidence\n\n**Validation Checks:**\n\n|= Metric |= Minimum Required |= Purpose\n| **Evidence Count** |  2 sources | Multiple source confirmation\n| **Source Quality** | Average  0.6 | Reliable sources only\n| **Evidence Agreement** |  60% supporting | Majority consensus required\n| **Uncertainty Factors** |  3 hedging statements | Confident assertions\n| **Confidence Tier** | MEDIUM or HIGH | Sufficient confidence level\n\n**Confidence Tiers:**\n\n|= Tier |= Evidence |= Avg Quality |= Agreement |= Publishable?\n| **HIGH** | 3+ sources |  0.7 |  80% |  Yes\n| **MEDIUM** | 2+ sources |  0.6 |  60% |  Yes\n| **LOW** | 2+ sources |  0.5 |  40% |  No (needs review)\n| **INSUFFICIENT** | < 2 sources | Any | Any |  No (more research needed)\n\n**Implementation Logic:**\n\nThe system evaluates each verdict by:\n1. Counting supporting and contradicting evidence sources\n2. Calculating average reliability score across all sources\n3. Computing evidence agreement (% supporting vs contradicting)\n4. Counting uncertainty markers in verdict text (e.g., \"may\", \"possibly\")\n5. Assigning confidence tier based on thresholds\n\n**Failure Actions:**\n* LOW or INSUFFICIENT verdicts are **not displayed** to users\n* System logs these cases for manual review\n* Claims marked as \"Insufficient evidence for verdict\"\n\n**Example Outcomes:**\n\n **HIGH Confidence (Publishable):**\n* 4 sources (all reliable news organizations)\n* Average source quality: 0.8\n* Evidence agreement: 90% supporting\n* Uncertainty factors: 0\n* Result: Display verdict with \"High Confidence\" badge\n\n **INSUFFICIENT (Not Publishable):**\n* 1 source\n* Average source quality: 0.7\n* Evidence agreement: 100% (only 1 source)\n* Result: Verdict blocked, needs more evidence\n\n=== NFR11: Acceptance Criteria ===\n\n**For POC1 Success:**\n\n|= Criterion |= Target |= Measurement\n| Claim validation accuracy |  90% | Manual review of 100 claims\n| False positive rate (bad claims passed) |  5% | Count of opinion/prediction claims that passed\n| Verdict publication rate | 60-80% | % of verdicts meeting quality thresholds\n| Hallucination rate | < 10% | Manual verification of evidence accuracy\n| Zero-evidence verdicts | 0% | Automated enforcement by Gate 4\n\n**Quality Gate Bypass:**\n\nFor POC1 testing and debugging **only**:\n* Manual override switch to bypass quality gates\n* **Must be removed** before POC2\n* All bypasses logged for review\n\n---\n\n=== FR7: Automated Verdicts (Enhanced with Quality Gates) ===\n\n**Importance:** CRITICAL \n**Phase:** POC1 (enhanced), V1.0 (complete) \n**Purpose:** Generate evidence-based verdicts automatically\n\n**POC1 Enhancements:**\n\n**Integration with Quality Gates:**\n* Verdicts must pass Gate 4 before display\n* Low-confidence verdicts are held for review\n* Users see confidence tier for each verdict\n\n**Evidence Requirements:**\n* Minimum 2 sources per verdict (enforced by Gate 4)\n* Sources must have quality score  0.6\n* Mix of supporting and contradicting evidence preferred\n\n**Verdict Structure:**\n\nEach verdict includes:\n* **Assessment:** ~~TRUE / FALSE / PARTIALLY TRUE / DISPUTED / UNVERIFIABLE~~\n  //**Current Implementation (v2.6.33):** 7-point symmetric scale://\n  * TRUE (86-100%) / MOSTLY-TRUE (72-85%) / LEANING-TRUE (58-71%)\n  * MIXED (43-57%, high confidence) / UNVERIFIED (43-57%, low confidence)  \n  * LEANING-FALSE (29-42%) / MOSTLY-FALSE (15-28%) / FALSE (0-14%)\n* **Confidence Tier:** HIGH / MEDIUM\n* **Summary:** 2-3 sentence explanation\n* **Supporting Evidence:** List of sources with quotes\n* **Contradicting Evidence:** Opposing viewpoints (if any)\n* **Context:** Additional clarifying information\n* **Quality Indicators:** Evidence count, source quality, agreement percentage\n\n**Example Verdict Output:**\n\n{{info}}\n**Verdict:** PARTIALLY TRUE (Medium Confidence)\n\nThe claim that \"France's GDP was $2.7 trillion in 2023\" is partially accurate. According to World Bank data, France's GDP was approximately $2.78 trillion in 2023, slightly higher than claimed.\n\n**Supporting Evidence (3 sources):**\n* World Bank: $2.78 trillion (Quality: 0.9)\n* IMF: $2.76 trillion (Quality: 0.9)\n* OECD: $2.77 trillion (Quality: 0.8)\n\n**Evidence Agreement:** 100% (all sources support)\n{{/info}}\n\n---\n\n=== FR4: Analysis Summary (Enhanced with Quality Indicators) ===\n\n**Importance:** HIGH \n**Phase:** POC2 (full), POC1 (basic) \n**Purpose:** Provide article-level summary for readers\n\n**POC1 Implementation:**\n\nBasic summary showing:\n* Total claims extracted\n* Claims analyzed (passed Gate 1)\n* Claims excluded (failed Gate 1) with reasons\n* Verdicts generated (passed Gate 4)\n* Verdicts held (failed Gate 4)\n* Overall quality metrics\n\n**Summary Display:**\n\n{{info}}\n**Analysis Summary**\n\n**Claims:** 8 extracted, 6 analyzed, 2 excluded\n* Excluded: 1 opinion, 1 prediction\n\n**Verdicts:** 4 published, 2 held for review\n* High confidence: 2\n* Medium confidence: 2\n* Insufficient evidence: 2 (needs review)\n\n**Quality Metrics:**\n* Average source quality: 0.75\n* Evidence agreement: 78%\n* Analysis confidence: MEDIUM\n{{/info}}\n\n---\n\n== Workflow ==\n\n=== POC1 Workflow Diagram ===\n\n{{code language=\"none\"}}\nArticle Input\n \n[Claim Extraction]\n \n[Gate 1: Claim Validation]  Quality Gate\n  PASS  [Evidence Collection]\n  FAIL  [Exclude from Analysis]\n \n [Verdict Generation]\n \n [Gate 4: Verdict Confidence]  Quality Gate\n  PASS  [Display to User]\n  FAIL  [Hold for Review]\n{{/code}}\n\n**Key Stages:**\n\n1. **Article Input:** User provides article URL or text\n2. **Claim Extraction:** AKEL identifies factual claims (FR1)\n3. **Gate 1 Validation:** Check if claims are fact-checkable\n4. **Evidence Collection:** Gather supporting/contradicting sources (FR5)\n5. **Verdict Generation:** Synthesize evidence into verdict (FR7)\n6. **Gate 4 Validation:** Verify verdict has sufficient confidence\n7. **Display or Hold:** Show verdict to user or flag for review\n\n---\n\n== Data Model Extensions ==\n\n=== ClaimValidationResult ===\n\n**Purpose:** Track Gate 1 validation outcomes\n\n**Fields:**\n\n|= Field |= Type |= Purpose\n| `claimId` | string | Reference to claim\n| `isFactual` | boolean | Can be verified?\n| `opinionScore` | number (0-1) | Opinion detection score\n| `specificityScore` | number (0-1) | Specificity level\n| `futureOriented` | boolean | About future events?\n| `claimType` | enum | FACTUAL / OPINION / PREDICTION / AMBIGUOUS\n| `passed` | boolean | Passed Gate 1?\n| `failureReason` | string | Why it failed (if applicable)\n| `validatedAt` | timestamp | Validation time\n\n---\n\n=== VerdictValidationResult ===\n\n**Purpose:** Track Gate 4 validation outcomes\n\n**Fields:**\n\n|= Field |= Type |= Purpose\n| `verdictId` | string | Reference to verdict\n| `evidenceCount` | number | Total sources\n| `averageSourceQuality` | number (0-1) | Mean quality across sources\n| `evidenceAgreement` | number (0-1) | % supporting vs contradicting\n| `uncertaintyFactors` | number | Count of hedging statements\n| `confidenceTier` | enum | HIGH / MEDIUM / LOW / INSUFFICIENT\n| `publishable` | boolean | Can display to users?\n| `failureReasons` | string[] | Why not publishable (if applicable)\n| `validatedAt` | timestamp | Validation time\n\n---\n\n=== QualityMetrics ===\n\n**Purpose:** Track POC1 quality performance (manual tracking)\n\n**Metrics to Track:**\n\n|= Metric |= Measurement Method |= Target\n| Claims extracted per article | Automated count | 5-15 per article\n| Claims passing Gate 1 | Automated count | 60-80% pass rate\n| Verdicts passing Gate 4 | Automated count | 60-80% pass rate\n| Hallucination rate | Manual review | < 10%\n| Evidence accuracy | Manual verification | > 90%\n| User-reported issues | Manual tracking | < 5% of verdicts\n\n**Manual Quality Review Process:**\n\n1. Select random sample of 20 verdicts\n2. Verify evidence sources are real and accurately quoted\n3. Check verdict assessment matches evidence\n4. Document any hallucinations or errors\n5. Calculate quality metrics\n6. Adjust thresholds if needed\n\n---\n\n== Cache Architecture ==\n\n{{warning}}\n**Implementation Status:** This Redis cache architecture is **NOT YET IMPLEMENTED**. Current implementation stores all data as JSON blobs in SQLite. See `Docs/STATUS/Current_Status.md` for caching roadmap.\n{{/warning}}\n\n**Redis Cache Design (Planned):**\n* Key: {{code}}claim:v1norm1:{language}:{sha256(canonical_claim)}{{/code}}\n* Value: Complete ClaimAnalysis JSON (~15KB, ~5KB compressed)\n* TTL: 90 days\n* Invalidation: Time-based, event-based, version-based\n\n**Canonicalization Algorithm:** Specified in API v0.4.1 Section 5.1.1\n* Unicode normalization (NFC)\n* Lowercase + punctuation removal\n* Whitespace normalization\n* Numeric normalization (95%  95 percent)\n* Common abbreviations (COVID-19  covid)\n* NO entity normalization (v1 limitation)\n\n**Cache Hit Rate Projections:**\n* Articles 0-100: 10% hit rate\n* Articles 100-1,000: 40% hit rate\n* Articles 1,000-10,000: 70% hit rate (**TARGET**)\n* Articles 10,000+: 80-90% hit rate\n\n**Cost Impact:**\n* 70% hit rate: $0.16/article (break-even with monolithic)\n* 80% hit rate: $0.11/article (27% savings)\n* 90% hit rate: $0.07/article (53% savings)\n\n== Implementation Checklist ==\n\n=== Phase 1: Setup (Phase: Core Workflow (Weeks 1-2) ===\n\n* [ ] Implement claim extraction (FR1)\n* [ ] Implement evidence collection (FR5)\n* [ ] Implement verdict generation (FR7)\n* [ ] Test with 5 sample articles\n\n=== Phase 3: Quality Gates (Phase: Testing & Refinement (Phase: Quality Metrics (Phase: Documentation (Phase:**\n\n=== Functional Requirements ===\n\n*  **20 articles processed** without system failures\n*  **Claim extraction works** - average 5-15 claims per article\n*  **Quality gates effective** - 60-80% of claims/verdicts pass validation\n*  **Verdicts have evidence** - 0% verdicts with < 2 sources\n\n=== Quality Requirements ===\n\n*  **Hallucination rate < 10%** - verified through manual review\n*  **Evidence accuracy > 90%** - sources are real and quoted correctly\n*  **Opinion detection works** - < 5% opinion/prediction claims passed Gate 1\n*  **Confidence tiers accurate** - HIGH verdicts are indeed higher quality\n\n=== Technical Requirements ===\n\n*  **Performance acceptable** - verdicts generated in < 60 seconds per article\n*  **API integration stable** - < 5% API call failures\n*  **Code quality** - documented, testable, maintainable\n\n---\n\n---\n\n_End of POC1 Specification_", "Product Development.Specification.POC.Summary": "= POC Summary (POC1 & POC2) =\n\n\n{{info}}\n**This page describes POC1 v0.4+ (3-stage pipeline with caching).**\n\nFor complete implementation details, see [[POC1 API & Schemas Specification>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]].\n{{/info}}\n\n\n\n== 1. POC Specification ==\n\n=== POC Goal\nProve that AI can extract claims and determine verdicts automatically without human intervention.\n\n=== POC Output (4 Components Only)\n\n**1. ANALYSIS SUMMARY**\n- 3-5 sentences\n- How many claims found\n- Distribution of verdicts \n- Overall assessment\n\n**2. CLAIMS IDENTIFICATION**\n- 3-5 numbered factual claims\n- Extracted automatically by AI\n\n**3. CLAIMS VERDICTS**\n- Per claim: Verdict label + Confidence % + Brief reasoning (1-3 sentences)\n- Verdict labels: WELL-SUPPORTED / PARTIALLY SUPPORTED / UNCERTAIN / REFUTED\n\n**4. ARTICLE SUMMARY (optional)**\n- 3-5 sentences\n- Neutral summary of article content\n\n**Total output: ~200-300 words**\n\n=== What's NOT in POC\n\n Scenarios (multiple interpretations) \n Evidence display (supporting/opposing lists) \n Source links \n Detailed reasoning chains \n User accounts, history, search \n Browser extensions, API \n Accessibility, multilingual, mobile \n Export, sharing features \n Any other features\n\n=== Critical Requirement\n\n**FULLY AUTOMATED - NO MANUAL EDITING**\n\nThis is non-negotiable. POC tests whether AI can do this without human intervention.\n\n=== POC Success Criteria\n\n**Passes if:**\n-  AI extracts 3-5 factual claims automatically\n-  AI provides reasonable verdicts (70% make sense)\n-  Output is comprehensible\n-  Team agrees approach has merit\n-  Minimal or no manual editing needed\n\n**Fails if:**\n-  Claim extraction poor (< 60% accuracy)\n-  Verdicts nonsensical (< 60% reasonable)\n-  Requires manual editing for most analyses (> 50%)\n-  Team loses confidence in approach\n\n=== POC Architecture\n\n**Frontend:** Simple input form + results display \n**Backend:** Single API call to Claude (Sonnet 4.5) \n**Processing:** One prompt generates complete analysis \n**Database:** None required (stateless)\n\n=== POC Philosophy\n\n> \"Build less, learn more, decide faster. Test the hardest part first.\"\n\n=== Context-Aware Analysis (Experimental POC1 Feature) ===\n\n**Problem:** Article credibility  simple average of claim verdicts\n\n**Example:** Article with accurate facts (coffee has antioxidants, antioxidants fight cancer) but false conclusion (therefore coffee cures cancer) would score as \"mostly accurate\" with simple averaging, but is actually MISLEADING.\n\n**Solution (POC1 Test):** Approach 1 - Single-Pass Holistic Analysis\n* Enhanced AI prompt to evaluate logical structure\n* AI identifies main argument and assesses if it follows from evidence\n* Article verdict may differ from claim average\n* Zero additional cost, no architecture changes\n\n**Testing:**\n* 30-article test set\n* Success: 70% accuracy detecting misleading articles\n* Marked as experimental\n\n**See:** [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]] for full analysis and solution approaches.\n\n== 2. POC2 Specification ==\n\n=== POC2 Goal ===\nProve that AKEL produces high-quality outputs consistently at scale with complete quality validation.\n\n=== POC2 Enhancements (From POC1) ===\n\n**1. COMPLETE QUALITY GATES (All 4)**\n* Gate 1: Claim Validation (from POC1)\n* Gate 2: Evidence Relevance  NEW\n* Gate 3: Scenario Coherence  NEW \n* Gate 4: Verdict Confidence (from POC1)\n\n**2. EVIDENCE DEDUPLICATION (FR54)**\n* Prevent counting same source multiple times\n* Handle syndicated content (AP, Reuters)\n* Content fingerprinting with fuzzy matching\n* Target: >95% duplicate detection accuracy\n\n**3. CONTEXT-AWARE ANALYSIS (Conditional)**\n* **If POC1 succeeds (70%):** Implement as standard feature\n* **If POC1 promising (50-70%):** Try weighted aggregation approach\n* **If POC1 fails (<50%):** Defer to post-POC2\n* Detects articles with accurate claims but misleading conclusions\n\n**4. QUALITY METRICS DASHBOARD (NFR13)**\n* Track hallucination rates\n* Monitor gate performance\n* Evidence quality metrics\n* Processing statistics\n\n=== What's Still NOT in POC2 ===\n\n User accounts, authentication \n Public publishing interface \n Social sharing features \n Full production security (comes in Beta 0) \n In-article claim highlighting (comes in Beta 0)\n\n=== Success Criteria ===\n\n**Quality:**\n* Hallucination rate <5% (target: <3%)\n* Average quality rating 8.0/10\n* Gates identify >95% of low-quality outputs\n\n**Performance:**\n* All 4 quality gates operational\n* Evidence deduplication >95% accurate\n* Quality metrics tracked continuously\n\n**Context-Aware (if implemented):**\n* Maintains 70% accuracy detecting misleading articles\n* <15% false positive rate\n\n**Total Output Size:** Similar to POC1 (~220-350 words per analysis)\n\n== 2. Key Strategic Recommendations\n\n=== Immediate Actions\n\n**For POC:**\n1. Focus on core functionality only (claims + verdicts)\n2. Create basic explainer (1 page)\n3. Test AI quality without manual editing\n4. Make GO/NO-GO decision\n\n**Planning:**\n1. Define accessibility strategy (when to build)\n2. Decide on multilingual priorities (which languages first)\n3. Research media verification options (partner vs build)\n4. Evaluate browser extension approach\n\n=== Testing Strategy\n\n**POC Tests:** Can AI do this without humans? \n**Beta Tests:** What do users need? What works? What doesn't? \n**Release Tests:** Is it production-ready?\n\n**Key Principle:** Test assumptions before building features.\n\n=== Build Sequence (Priority Order)\n\n**Must Build:**\n1. Core analysis (claims + verdicts)  POC\n2. Educational resources (basic  comprehensive)\n3. Accessibility (WCAG 2.1 AA)  Legal requirement\n\n**Should Build (Validate First):**\n4. Browser extensions  Test demand\n5. Media verification  Pilot with existing tools\n6. Multilingual  Start with 2-3 languages\n\n**Can Build Later:**\n7. Mobile apps  PWA first\n8. ClaimReview schema  After content library\n9. Export features  Based on user requests\n10. Everything else  Based on validation\n\n=== Decision Framework\n\n**For each feature, ask:**\n1. **Importance:** Risk + Impact + Strategy alignment?\n2. **Urgency:** Fail fast + Legal + Promises?\n3. **Validation:** Do we know users want this?\n4. **Priority:** When should we build it?\n\n**Don't build anything without answering these questions.**\n\n== 4. Critical Principles\n\n=== Automation First\n- AI makes content decisions\n- Humans improve algorithms\n- Scale through code, not people\n\n=== Fail Fast\n- Test assumptions quickly\n- Don't build unvalidated features\n- Accept that experiments may fail\n- Learn from failures\n\n=== Evidence Over Authority\n- Transparent reasoning visible\n- No single \"true/false\" verdicts\n- Multiple scenarios shown\n- Assumptions made explicit\n\n=== User Focus\n- Serve users' needs first\n- Build what's actually useful\n- Don't build what's just \"cool\"\n- Measure and iterate\n\n=== Honest Assessment\n- Don't cherry-pick examples\n- Document failures openly\n- Accept limitations\n- No overpromising\n\n== 5. POC Decision Gate\n\n=== After POC, Choose:\n\n**GO (Proceed to Beta):**\n- AI quality 70% without editing\n- Approach validated\n- Team confident\n- Clear path to improvement\n\n**NO-GO (Pivot or Stop):**\n- AI quality < 60%\n- Requires manual editing for most\n- Fundamental flaws identified\n- Not feasible with current technology\n\n**ITERATE (Improve & Retry):**\n- Concept has merit\n- Specific improvements identified\n- Addressable with better prompts\n- Test again after changes\n\n== 6. Key Risks & Mitigations\n\n=== Risk 1: AI Quality Not Good Enough\n**Mitigation:** Extensive prompt testing, use best models \n**Acceptance:** POC might fail - that's what testing reveals\n\n=== Risk 2: Users Don't Understand Output\n**Mitigation:** Create clear explainer, test with real users \n**Acceptance:** Iterate on explanation until comprehensible\n\n=== Risk 3: Approach Doesn't Scale\n**Mitigation:** Start simple, add complexity only when proven \n**Acceptance:** POC proves concept, beta proves scale\n\n=== Risk 4: Legal/Compliance Issues\n**Mitigation:** Plan accessibility early, consult legal experts \n**Acceptance:** Can't launch publicly without compliance\n\n=== Risk 5: Feature Creep\n**Mitigation:** Strict scope discipline, say NO to additions \n**Acceptance:** POC is minimal by design\n\n== 7. Success Metrics\n\n=== POC Success\n- AI output quality 70%\n- Manual editing needed < 30% of time\n- Team confidence: High\n- Decision: GO to beta\n\n=== Platform Success (Later)\n- User comprehension 80%\n- Return user rate 30%\n- Flag rate (user corrections) < 10%\n- Processing time < 30 seconds\n- Error rate < 1%\n\n=== Mission Success (Long-term)\n- Users make better-informed decisions\n- Misinformation spread reduced\n- Public discourse improves\n- Trust in evidence increases\n\n== 8. What Makes FactHarbor Different\n\n=== Not Traditional Fact-Checking\n-  No simple \"true/false\" verdicts\n-  Multiple scenarios with context\n-  Transparent reasoning chains\n-  Explicit assumptions shown\n\n=== Not AI Chatbot\n-  Not conversational\n-  Structured Evidence Models\n-  Reproducible analysis\n-  Verifiable sources\n\n=== Not Just Automation\n-  Not replacing human judgment\n-  Augmenting human reasoning\n-  Making process transparent\n-  Enabling informed decisions\n\n== 9. Core Philosophy\n\n**Three Pillars:**\n\n**1. Scenarios Over Verdicts**\n- Show multiple interpretations\n- Make context explicit\n- Acknowledge uncertainty\n- Avoid false certainty\n\n**2. Transparency Over Authority**\n- Show reasoning, not just conclusions\n- Make assumptions explicit\n- Link to evidence\n- Enable verification\n\n**3. Evidence Over Opinions**\n- Ground claims in sources\n- Show supporting AND opposing evidence\n- Evaluate source quality\n- Avoid cherry-picking\n\n== 10. Next Actions\n\n=== Immediate\n Review this consolidated summary \n Confirm POC scope agreement \n Make strategic decisions on key questions \n Begin POC development \n\n=== Strategic Planning\n Define accessibility approach \n Select initial languages for multilingual \n Research media verification partners \n Evaluate browser extension frameworks \n\n=== Continuous\n Test assumptions before building \n Measure everything \n Learn from failures \n Stay focused on mission \n\n== Summary of Summaries\n\n**POC Goal:** Prove AI can do this automatically \n**POC Scope:** 4 simple components, ~200-300 words \n**POC Critical:** Fully automated, no manual editing \n**POC Success:** 70% quality without human correction \n\n**Gap Analysis:** 18 gaps identified, 2 critical (Accessibility + Education) \n**Framework:** Importance (risk + impact + strategy) + Urgency (fail fast + legal + promises) \n**Key Insight:** Context matters - urgency changes with milestones \n\n**Strategy:** Test first, build second. Fail fast. Stay focused. \n**Philosophy:** Scenarios, transparency, evidence. No false certainty. \n\n== Document Status\n\n**This document supersedes all previous analysis documents.**\n\nAll gap analysis, POC specifications, and strategic frameworks are consolidated here without timeline references.\n\n**For detailed specifications, refer to:**\n- User Needs document (in project knowledge)\n- Requirements document (in project knowledge)\n- This summary (comprehensive overview)\n\n**Previous documents are archived for reference but this is the authoritative summary.**\n\n**End of Consolidated Summary**\n", "Product Development.Specification.POC.WebHome": "= POC Specification =\n\n**This section contains detailed specifications for FactHarbor POC phases.**\n\n**Core Philosophy:** Prove the AKEL automation works before building production features. Each POC phase validates critical technical capabilities.\n\n{{warning}}\n**Critical Innovation in POC1 Context-Aware Analysis:**\n\nPOC1 tests whether AI can understand that an article's overall credibility is not simply the average of its individual claim verdicts. An article with mostly accurate facts can still be misleading if it draws unsupported conclusions.\n\nSee [[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]] for full discussion.\n{{/warning}}\n\n== POC Phase Navigation ==\n\n* **[[POC Requirements>>FactHarbor.Product Development.Specification.POC.Requirements]]** Complete POC1 & POC2 specification (v4.0)\n* **[[POC1 Specification>>FactHarbor.Product Development.Specification.POC.Specification]]** POC1 scope, requirements, and quality gate definitions\n* **[[POC Summary>>FactHarbor.Product Development.Specification.POC.Summary]]** Executive overview\n* **[[Article Verdict Problem>>FactHarbor.Product Development.Specification.POC.Article-Verdict-Problem]]** Why context-aware analysis matters\n* **[[API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]]** POC1 API contract and endpoint specifications\n\n== Roadmap Navigation ==\n\n* **[[POC1 Roadmap>>FactHarbor.Product Development.Planning.POC1.WebHome]]** Core Workflow with Quality Gates\n* **[[POC2 Roadmap>>FactHarbor.Product Development.Planning.POC2.WebHome]]** Robust Quality & Reliability\n* **[[Requirements Roadmap Matrix>>FactHarbor.Product Development.Planning.Requirements-Roadmap-Matrix.WebHome]]** Phase assignments\n\n== Key Specifications ==\n\n**POC1 Goal:** Prove AI can extract claims and evaluate them with context-aware analysis\n\n**POC1 Output:** 5 components\n1. Analysis Summary (context-aware, 4-6 sentences)\n2. Claims Identification (3-5 claims)\n3. Claims Verdicts (verdict + confidence + reasoning)\n4. Article Summary (optional)\n5. Usage Statistics (cost tracking)\n\n**POC1 Philosophy:** \n* Context-aware analysis (article  sum of claims)\n* Cost efficiency tracking (viable at scale)\n* Fail-fast learning (test critical capabilities)\n\n**POC1 Success Criteria:**\n* AI quality 70% (including context-aware assessment)\n* Cost per analysis <$0.05 (target)\n* Minimal manual editing (<30%)\n\n**POC1  POC2 Decision Gate:** GO / NO-GO / ITERATE\n\nSee [[POC Requirements>>FactHarbor.Product Development.Specification.POC.Requirements]] for complete specification.\n\n== POC vs. Full System ==\n\n|= Feature |= POC1 (Actual) |= POC2 |= Full System\n| AnalysisContexts |  Implemented (multi-context detection + KeyFactors) |  + Persistence |  + Enhanced\n| Evidence Display |  With sources + quality scoring |  Enhanced |  + ML validation\n| Quality Gates |  Gate 1 (Claim Validation) + Gate 4 (Verdict Confidence) |  + Gates 2 & 3 |  + ML validation\n| Database |  SQLite |  PostgreSQL |  + Redis + S3\n| User Submissions |  URL/text submission |  Enhanced |  + Batch\n| LLM Providers |  Multi-provider (Anthropic, OpenAI, Google, Mistral) |  + Admin UI |  + Cost optimization\n| A/B Testing |  Implemented |  Enhanced |  + ML-driven\n| API |  ASP.NET Core 8.0 |  Enhanced |  + Public API\n\n**", "Product Development.Specification.Reference.Data Models and Schemas.LLM Schema Mapping.WebHome": "= LLM Schema Mapping Reference =\n\n**Version**: 3.1.0 (Orchestrated pipeline  historical reference)\n**Date**: 2026-02-07\n**Purpose**: Complete mapping of TypeScript -> LLM Prompts -> JSON Schemas\n**Audience**: Prompt Engineers, LLM System Developers\n\n{{warning}}\n**HISTORICAL REFERENCE  Orchestrated Pipeline (removed in v2.11.0)**\n\nThis document describes the LLM schema mappings for the removed Orchestrated pipeline (UNDERSTAND  EXTRACT_EVIDENCE  CONTEXT_REFINEMENT  VERDICT phases). These phases, their Zod schemas, and their references to ##orchestrated.ts## no longer exist in the codebase.\n\n**For the current ClaimAssessmentBoundary (CB) pipeline:**\n* Runtime prompts: ##apps/web/prompts/claimboundary.prompt.md## (13 sections  CLAIM_EXTRACTION_PASS1/2, GENERATE_QUERIES, EXTRACT_EVIDENCE, BOUNDARY_CLUSTERING, VERDICT_ADVOCATE/CHALLENGER/RECONCILIATION, etc.)\n* Zod schemas: inline schema objects in ##apps/web/src/lib/analyzer/claimboundary-pipeline.ts## (one per stage)\n* Types: ##apps/web/src/lib/analyzer/types.ts## (##AtomicClaim##, ##ClaimAssessmentBoundary##, ##EvidenceScope##, ##EvidenceItem##, ##CBClaimVerdict##, ##CBAnalysisResult##)\n* Stage reference: [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]\n{{/warning}}\n\n----\n\n== Overview ==\n\nThis document maps how FactHarbor's TypeScript objects are presented to LLMs (via prompts) and how LLM outputs are validated (via Zod schemas). The **Master Mapping Table** below reflects the current CB pipeline. The phase-by-phase sections are retained as historical reference for the removed Orchestrated pipeline.\n\n----\n\n== Master Mapping Table ==\n\n=== Current (CB Pipeline  v4.0.0-cb) ===\n\n|= TypeScript Type |= Prompt Term |= LLM Output Field |= Zod Schema Location\n| ##AtomicClaim## | \"AtomicClaim\" | ##atomicClaims## | ##claimboundary-pipeline.ts## (Stage 1 schema)\n| ##ClaimAssessmentBoundary## | \"ClaimBoundary\" | ##claimBoundaries## | ##claimboundary-pipeline.ts## (Stage 3 schema)\n| ##EvidenceScope## | \"EvidenceScope\" or \"Scope\" | ##evidenceScope## | ##types.ts## (##EvidenceScopeSchema##)\n| ##EvidenceItem## | \"Evidence\" or \"EvidenceItem\" | ##evidenceItems## | ##claimboundary-pipeline.ts## (Stage 2 schema)\n| ##CBClaimVerdict## | \"ClaimVerdict\" | ##claimVerdicts## | ##verdict-stage.ts## (verdict schema)\n| ##CBAnalysisResult## | \"AnalysisResult\" | (top-level result) | ##claimboundary-pipeline.ts## (result schema)\n\n> **CRITICAL TERMINOLOGY**: \"Scope\" refers to ##EvidenceScope## (per-evidence metadata). \"ClaimBoundary\" or \"ClaimAssessmentBoundary\" refers to the evidence-emergent grouping. **NEVER use \"AnalysisContext\" or \"Context\"** for the top-level frame in CB pipeline code or prompts  those are removed Orchestrated concepts.\n\n=== Removed (Orchestrated Pipeline  no longer exists) ===\n\n|= TypeScript Type |= Formerly Used As |= Replacement\n| ~~##AnalysisContext##~~ | Top-level analytical frame | ##ClaimAssessmentBoundary##\n| ~~##ContextAnswer##~~ | Per-context verdict | (no equivalent  CB uses weighted aggregation)\n| ~~##SubClaim##~~ | Atomic claim unit | ##AtomicClaim##\n\n{{warning}}\n**v4.0 Breaking Change (February 2026):** ##AnalysisContext##, ##ContextAnswer##, ##analysisContexts##, ##contextId## are no longer in the codebase. Legacy Orchestrated field names (##distinctProceedings##, ##relatedProceedingId##, ##proceedingId##, ##supportingFactIds##, ##facts##, ##claims##) were removed in v3.0.\n{{/warning}}\n\n----\n\n== Phase-by-Phase Mappings (Orchestrated Pipeline  Historical) ==\n\n{{warning}}\n**All sections below describe the removed Orchestrated pipeline.** The phases (UNDERSTAND, EXTRACT_EVIDENCE, CONTEXT_REFINEMENT, VERDICT), their schemas, and their references to ##orchestrated.ts## no longer exist. Retained for historical reference only.\n\nThe CB pipeline's equivalent stages are: ##extractClaims##  ##researchEvidence##  ##clusterBoundaries##  ##generateVerdicts##  ##aggregateAssessment##. For current schemas, read ##claimboundary-pipeline.ts## and ##verdict-stage.ts## directly.\n{{/warning}}\n\n=== UNDERSTAND Phase (Orchestrated  removed) ===\n\n**Purpose**: Extract claims and detect preliminary AnalysisContexts\n\n**Input to LLM**:\n\n{{code language=\"typescript\"}}\n// Variables passed to prompt\n{\n  currentDate: string;  // e.g., \"2026-01-18\"\n  isRecent: boolean;    // Temporal relevance flag\n}\n{{/code}}\n\n**Prompt Terms Used**:\n* \"AnalysisContext\" or \"Context\" for top-level analytical frames\n* \"Multi-Context Detection\" for identifying distinct frames\n* \"Claim Extraction\" for factual assertions\n\n**LLM Output Schema (v2.7)**:\n\n{{code language=\"json\"}}\n{\n  \"impliedClaim\": \"string\",\n  \"articleThesis\": \"string\",\n  \"subClaims\": [\n    {\n      \"id\": \"string\",\n      \"text\": \"string\",\n      \"claimRole\": \"attribution\" | \"source\" | \"timing\" | \"core\",\n      \"centrality\": \"HIGH\" | \"MEDIUM\" | \"LOW\",\n      \"isCentral\": boolean\n    }\n  ],\n  \"researchQueries\": [\"string\"],\n  \"analysisContexts\": [\n    {\n      \"id\": \"string\",\n      \"name\": \"string\",\n      \"type\": \"legal\" | \"scientific\" | \"methodological\" | \"general\"\n    }\n  ],\n  \"requiresSeparateAnalysis\": boolean\n}\n{{/code}}\n\n**Zod Validation**: ##UnderstandingSchema## (in ##orchestrated.ts##)\n\n**Key Mappings**:\n* Prompt: \"AnalysisContext\" -> Output: ##analysisContexts## array\n* Prompt: \"requiresSeparateAnalysis\" -> Output: ##requiresSeparateAnalysis## boolean\n\n----\n\n=== EXTRACT_EVIDENCE Phase ===\n\n**Purpose**: Extract verifiable evidence items from fetched sources\n\n**Input to LLM**:\n\n{{code language=\"typescript\"}}\n{\n  currentDate: string;\n  originalClaim: string;      // User's input\n  contextsList: string;       // Stringified list of detected AnalysisContexts\n}\n{{/code}}\n\n**Prompt Terms Used**:\n* \"EvidenceScope\" for per-evidence methodology metadata (NOT an AnalysisContext)\n* \"contextId\" for AnalysisContext assignment\n* \"claimDirection\" for support/contradict/neutral assessment\n\n**LLM Output Schema (v2.7)**:\n\n{{code language=\"json\"}}\n{\n  \"evidenceItems\": [\n    {\n      \"id\": \"string\",\n      \"statement\": \"string\",\n      \"category\": \"evidence\" | \"expert_quote\" | \"statistic\" | \"event\" | \"legal_provision\" | \"criticism\",\n      \"specificity\": \"high\" | \"medium\",\n      \"sourceExcerpt\": \"string (50-200 chars)\",\n      \"claimDirection\": \"supports\" | \"contradicts\" | \"neutral\",\n      \"contextId\": \"string (e.g., CTX_TSE)\",\n      \"evidenceScope\": {\n        \"name\": \"string\",\n        \"methodology\": \"string?\",\n        \"boundaries\": \"string?\",\n        \"geographic\": \"string?\",\n        \"temporal\": \"string?\"\n      } | null\n    }\n  ]\n}\n{{/code}}\n\n**Zod Validation**: ##EvidenceItemSchema## (in ##types.ts##)\n\n**Key Mappings**:\n* Prompt: \"EvidenceScope\" -> Output: ##evidenceScope## object (nullable)\n* Prompt: \"contextId\" -> Output: ##contextId##\n\n----\n\n=== CONTEXT_REFINEMENT Phase ===\n\n**Purpose**: Identify final AnalysisContexts from evidence\n\n**Input to LLM**:\n\n{{code language=\"typescript\"}}\n{\n  evidenceItems: EvidenceItem[];              // All extracted evidence items\n  preliminaryContexts: AnalysisContext[];     // From UNDERSTAND phase\n}\n{{/code}}\n\n**Prompt Terms Used**:\n* \"AnalysisContext\" or \"Context\" (primary term for top-level frames)\n* \"ArticleFrame\" (what NOT to split on)\n* \"EvidenceScope\" (per-evidence metadata - NOT an AnalysisContext)\n* \"analysisContexts\" (output field name)\n\n**LLM Output Schema (v2.7)**:\n\n{{code language=\"json\"}}\n{\n  \"requiresSeparateAnalysis\": boolean,\n  \"analysisContexts\": [\n    {\n      \"id\": \"string\",\n      \"name\": \"string\",\n      \"shortName\": \"string\",\n      \"subject\": \"string\",\n      \"temporal\": \"string\",\n      \"status\": \"concluded\" | \"ongoing\" | \"pending\" | \"unknown\",\n      \"outcome\": \"string\",\n      \"metadata\": {\n        \"institution\": \"string?\",\n        \"jurisdiction\": \"string?\",\n        \"methodology\": \"string?\",\n        \"boundaries\": \"string?\",\n        \"geographic\": \"string?\",\n        \"dataSource\": \"string?\"\n      }\n    }\n  ],\n  \"evidenceContextAssignments\": [\n    {\n      \"evidenceId\": \"string\",\n      \"contextId\": \"string\"    // References AnalysisContext.id\n    }\n  ],\n  \"claimContextAssignments\": [\n    {\n      \"claimId\": \"string\",\n      \"contextId\": \"string\"\n    }\n  ]\n}\n{{/code}}\n\n**Zod Validation**: ##ContextRefinementSchema## (in ##orchestrated.ts##)\n\n**Key Mappings**:\n* Prompt: \"AnalysisContext\" -> Output: ##analysisContexts##\n* Prompt: \"ArticleFrame\" -> (explicitly NOT included in output)\n* Prompt: \"EvidenceScope\" -> (per-evidence metadata, not top-level context)\n\n----\n\n=== VERDICT Phase ===\n\n**Purpose**: Generate truth verdicts per context per claim\n\n**Input to LLM**:\n\n{{code language=\"typescript\"}}\n{\n  currentDate: string;\n  originalClaim: string;\n  claimsList: string;          // Stringified claims\n  contextsList: string;        // Stringified AnalysisContexts\n  allEvidenceItems: string;    // Stringified evidence items\n}\n{{/code}}\n\n**Prompt Terms Used**:\n* \"contextId\" for verdict assignment\n* \"answer\" for truth percentage (0-100)\n* \"keyFactors\" for evidence summary\n\n**LLM Output Schema (v2.7)**:\n\n{{code language=\"json\"}}\n{\n  \"verdicts\": [\n    {\n      \"contextId\": \"string\",\n      \"contextName\": \"string\",\n      \"claimId\": \"string\",\n      \"answer\": number (0-100),\n      \"confidence\": number (0-100),\n      \"truthPercentage\": number (0-100),\n      \"shortAnswer\": \"string\",\n      \"keyFactors\": [\n        {\n          \"factor\": \"string\",\n          \"explanation\": \"string\",\n          \"supports\": \"strongly_supports\" | \"supports\" | \"neutral\" | \"contradicts\" | \"strongly_contradicts\",\n          \"weight\": \"high\" | \"medium\" | \"low\",\n          \"isContested\": boolean,\n          \"contestedBy\": \"string?\",\n          \"factualBasis\": \"established\" | \"disputed\" | \"opinion\" | \"alleged\" | \"unknown\"\n        }\n      ]\n    }\n  ]\n}\n{{/code}}\n\n**Contestation Fields (v2.8):**\n* ##isContested##: Whether there is opposition to this factor\n* ##contestedBy##: Who opposes (e.g., \"opposition party\", \"industry group\")\n* ##factualBasis##: Type of opposition evidence\n** ##established## = Strong documented counter-evidence (weight: 0.5x in v3.1; was 0.3x in Orchestrated)\n** ##disputed## = Some factual counter-evidence (weight: 0.7x in v3.1; was 0.5x in Orchestrated)\n** ##opinion##/##alleged##/##unknown## = DOUBTED, no evidence (weight: 1.0x)\n\nSee [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]] for \"Doubted vs Contested\" distinction.\n\n**Zod Validation**: ##VerdictSchema## (in ##orchestrated.ts##)\n\n----\n\n== Terminology Bridges (Prompt <-> Code) ==\n\n=== ClaimAssessmentBoundary Bridges (CB  current) ===\n\n|= Layer |= Term |= Notes\n| Prompt | \"ClaimBoundary\" or \"ClaimAssessmentBoundary\" | Primary prompt term for top-level frame (NEVER \"AnalysisContext\")\n| LLM Output | ##claimBoundaries## | JSON field name (v4.0-cb)\n| TypeScript | ##ClaimAssessmentBoundary## | Interface name (##types.ts##)\n| Database | (embedded in CBAnalysisResult JSON) | Stored as JSON blob\n\n=== EvidenceScope Bridges (unchanged) ===\n\n|= Layer |= Term |= Notes\n| Prompt | \"EvidenceScope\" or \"Scope\" | Per-evidence metadata (NOT a ClaimAssessmentBoundary)\n| LLM Output | ##evidenceScope## | Consistent across versions\n| TypeScript | ##EvidenceScope## | Interface name (##types.ts##)\n| Database | (embedded in evidence item objects) | Part of ResultJson\n\n=== Removed: AnalysisContext Bridges (Orchestrated  do not use) ===\n\n~~Prompt: \"AnalysisContext\" / LLM Output: ##analysisContexts## / TypeScript: ##AnalysisContext##~~  removed in v4.0-cb.\n\n----\n\n== Validation Flow ==\n\n{{mermaid}}\ngraph TD\n    A[TypeScript Input] -->|Variables| B[Prompt Template]\n    B -->|Prompt String| C[LLM API Call]\n    C -->|JSON String| D[Parse Response]\n    D -->|Raw Object| E[Zod Validation]\n    E -->|Valid?| F{Schema Match?}\n    F -->|Yes| G[TypeScript Output]\n    F -->|No| H[Validation Error]\n    H --> I[Fallback or Retry]\n{{/mermaid}}\n\n=== Schema Validation Checkpoints ===\n\n1. **Pre-LLM**: Variables validated (type-safe TypeScript)\n1. **Post-LLM**: JSON parsed and Zod-validated\n1. **Post-Validation**: TypeScript types enforced\n1. **Runtime**: Additional business logic validation (e.g., ##contextId## exists in context list)\n\n----\n\n== Common Pitfalls ==\n\n=== Pitfall 1: Field Name Mismatch ===\n\n**Wrong** (Prompt says one thing, schema expects another):\n\n{{code language=\"typescript\"}}\n// Prompt says: \"Output as 'contexts'\"\n// But Zod schema expects: analysisContexts\n\n// Result: Validation fails  field names must match exactly\n{{/code}}\n\n**Correct** (Prompt and schema aligned):\n\n{{code language=\"typescript\"}}\n// Prompt says: \"Output as 'analysisContexts'\"\n// Zod schema expects: analysisContexts\n// Result: Validation succeeds\n{{/code}}\n\n=== Pitfall 2: Terminology Confusion in Prompts ===\n\n**Wrong** (Confusing \"scope\" with \"context\"):\n\n{{code}}\n\"Identify the distinct scopes...\"\n// WRONG: \"Scope\" means EvidenceScope (per-evidence metadata), not AnalysisContext\n{{/code}}\n\n**Correct** (Clear CB terminology):\n\n{{code}}\n\"Identify ClaimBoundaries (or ClaimAssessmentBoundaries)...\"\n// \"Scope\" reserved for EvidenceScope (per-evidence source methodology)\n// NEVER use \"AnalysisContext\"  that concept is from the removed Orchestrated pipeline\n{{/code}}\n\n=== Pitfall 3: Missing Glossary ===\n\n**Wrong** (No term definitions in prompt):\n\n{{code}}\n\"Extract the scopes from evidence.\"\n// Ambiguous: Does \"scope\" mean ClaimAssessmentBoundary or EvidenceScope?\n{{/code}}\n\n**Correct** (Explicit glossary with CRITICAL distinction):\n\n{{code}}\n## TERMINOLOGY (CRITICAL)\n- **AtomicClaim**: Single verifiable assertion extracted from user input (stored as atomicClaims)\n- **ClaimBoundary** (or \"ClaimAssessmentBoundary\"): Evidence-emergent grouping of compatible EvidenceScopes (stored as claimBoundaries)\n- **EvidenceScope** (or \"Scope\"): Per-evidence source methodology metadata (NOT a ClaimBoundary)\n{{/code}}\n\n----\n\n== Testing Checklist ==\n\nWhen updating CB pipeline prompts or schemas:\n\n* Prompt terminology uses CB terms: ##AtomicClaim##, ##ClaimBoundary##, ##EvidenceScope##, ##EvidenceItem## (not ##AnalysisContext##, ##SubClaim##, ##ContextAnswer##)\n* Prompt field names match CB Zod schema field names (##atomicClaims##, ##claimBoundaries##, ##evidenceItems##)\n* Glossary section present in all base prompts with CB term definitions\n* Provider-specific sections in ##claimboundary.prompt.md## use same core terms\n* Example outputs in prompts match CB schema structure\n* Validation errors are descriptive (mention expected vs actual field names)\n* Documentation updated (this file, [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]])\n\n----\n\n== References ==\n\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]] - Core definitions\n* Prompt_Engineering_Standards.md - How to write prompts\n* types.ts - TypeScript interfaces\n* Migration decision documented in ##types.ts## comments and [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]\n\n----\n\n**Maintainer**: LLM Expert, Prompt Engineering Team\n**Last Updated**: 2026-02-07\n**Next Review**: After next major version", "Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema.WebHome": "= FactHarbor Metrics Schema =\n\n== Overview ==\n\nThe FactHarbor metrics system collects comprehensive performance, quality, and cost data for every analysis job. This enables:\n\n* Performance monitoring and optimization\n* Cost tracking and forecasting\n* Quality assurance and regression detection\n* A/B testing and validation\n\n== Database Schema ==\n\n=== AnalysisMetrics Table ===\n\n{{code language=\"sql\"}}\nCREATE TABLE AnalysisMetrics (\n  Id UNIQUEIDENTIFIER PRIMARY KEY,\n  JobId UNIQUEIDENTIFIER NOT NULL,\n  MetricsJson NVARCHAR(MAX) NOT NULL,\n  CreatedUtc DATETIME2 NOT NULL,\n  FOREIGN KEY (JobId) REFERENCES Jobs(JobId)\n);\n\nCREATE INDEX IX_Metrics_JobId ON AnalysisMetrics(JobId);\nCREATE INDEX IX_Metrics_Created ON AnalysisMetrics(CreatedUtc);\n{{/code}}\n\n== JSON Schema ==\n\n=== AnalysisMetrics ===\n\n{{code language=\"typescript\"}}\ninterface AnalysisMetrics {\n  // Identification\n  jobId: string;\n  schemaVersion: string;\n  pipelineVariant: 'orchestrated' | 'monolithic-dynamic';\n  timestamp: Date;\n\n  // Performance\n  totalDurationMs: number;\n  phaseTimings: {\n    understand: number;\n    research: number;\n    verdict: number;\n    summary: number;\n    report: number;\n  };\n  llmCalls: LLMCallMetric[];\n  searchQueries: SearchQueryMetric[];\n\n  // Quality Gates\n  gate1Stats: Gate1Metric;\n  gate4Stats: Gate4Metric;\n\n  // Schema Compliance\n  schemaCompliance: SchemaComplianceMetric;\n\n  // Output Quality\n  outputQuality: {\n    claimsExtracted: number;\n    claimsWithVerdicts: number;\n    scopesDetected: number;\n    sourcesFound: number;\n    factsExtracted: number;\n    averageConfidence: number;\n  };\n\n  // Costs (estimated)\n  estimatedCostUSD: number;\n  tokenCounts: {\n    promptTokens: number;\n    completionTokens: number;\n    totalTokens: number;\n  };\n\n  // Configuration\n  config: {\n    llmProvider: string;\n    searchProvider: string;\n    allowModelKnowledge: boolean;\n    isLLMTiering: boolean;\n    isDeterministic: boolean;\n  };\n}\n{{/code}}\n\n=== LLMCallMetric ===\n\n{{code language=\"typescript\"}}\ninterface LLMCallMetric {\n  taskType: 'understand' | 'extract_evidence' | 'context_refinement' | 'verdict' | 'supplemental' | 'other'; // legacy: extract_facts, scope_refinement\n  provider: string;\n  modelName: string;\n  promptTokens: number;\n  completionTokens: number;\n  totalTokens: number;\n  durationMs: number;\n  success: boolean;\n  schemaCompliant: boolean;\n  retries: number;\n  errorMessage?: string;\n  timestamp: Date;\n}\n{{/code}}\n\n=== SearchQueryMetric ===\n\n{{code language=\"typescript\"}}\ninterface SearchQueryMetric {\n  query: string;\n  provider: 'google-cse' | 'serpapi' | 'gemini-grounded';\n  resultsCount: number;\n  durationMs: number;\n  success: boolean;\n  timestamp: Date;\n}\n{{/code}}\n\n=== Gate1Metric ===\n\n{{code language=\"typescript\"}}\ninterface Gate1Metric {\n  totalClaims: number;\n  passedClaims: number;\n  filteredClaims: number;\n  filteredReasons: Record<string, number>; // reason -> count\n  centralClaimsKept: number;\n}\n{{/code}}\n\n=== Gate4Metric ===\n\n{{code language=\"typescript\"}}\ninterface Gate4Metric {\n  totalVerdicts: number;\n  highConfidence: number;\n  mediumConfidence: number;\n  lowConfidence: number;\n  insufficient: number;\n  unpublishable: number;\n}\n{{/code}}\n\n== API Endpoints ==\n\n=== POST /api/fh/metrics ===\n\nStore metrics for an analysis job.\n\n**Request Body:**\n\n{{code language=\"json\"}}\n{\n  \"jobId\": \"uuid\",\n  \"schemaVersion\": \"2.6.33\",\n  \"totalDurationMs\": 45000,\n  ...\n}\n{{/code}}\n\n**Response:**\n\n{{code language=\"json\"}}\n{\n  \"id\": \"metrics-uuid\",\n  \"jobId\": \"job-uuid\"\n}\n{{/code}}\n\n=== GET /api/fh/metrics/{jobId} ===\n\nRetrieve metrics for a specific job.\n\n**Response:** Full AnalysisMetrics JSON\n\n=== GET /api/fh/metrics/summary ===\n\nGet summary statistics for a date range.\n\n**Query Parameters:**\n* ##startDate## (optional): ISO 8601 date\n* ##endDate## (optional): ISO 8601 date\n* ##limit## (optional): Max records to analyze (default: 100)\n\n**Response:**\n\n{{code language=\"json\"}}\n{\n  \"count\": 50,\n  \"avgDuration\": 42000,\n  \"avgCost\": 0.15,\n  \"avgTokens\": 75000,\n  \"schemaComplianceRate\": 96.5,\n  \"gate1PassRate\": 72.3,\n  \"gate4HighConfidenceRate\": 58.7,\n  \"startDate\": \"2026-01-15T00:00:00Z\",\n  \"endDate\": \"2026-01-19T23:59:59Z\"\n}\n{{/code}}\n\n=== DELETE /api/fh/metrics/cleanup ===\n\nDelete metrics older than specified days.\n\n**Query Parameters:**\n* ##daysOld## (optional): Age threshold in days (default: 90)\n\n**Response:**\n\n{{code language=\"json\"}}\n{\n  \"deleted\": 150\n}\n{{/code}}\n\n== Usage in Code ==\n\n=== Collecting Metrics ===\n\n{{code language=\"typescript\"}}\nimport { createMetricsCollector, persistMetrics } from '@/lib/analyzer/metrics';\n\n// Create collector at start of analysis\nconst metrics = createMetricsCollector(jobId, 'orchestrated');\n\n// Track phases\nmetrics.startPhase('understand');\n// ... do understanding work ...\nmetrics.endPhase('understand');\n\n// Record LLM calls\nmetrics.recordLLMCall({\n  taskType: 'understand',\n  provider: 'anthropic',\n  modelName: 'claude-sonnet-4-20250514',\n  promptTokens: 5000,\n  completionTokens: 1500,\n  totalTokens: 6500,\n  durationMs: 2500,\n  success: true,\n  schemaCompliant: true,\n  retries: 0,\n  timestamp: new Date(),\n});\n\n// Set quality gate stats\nmetrics.setGate1Stats({\n  totalClaims: 8,\n  passedClaims: 6,\n  filteredClaims: 2,\n  filteredReasons: { 'opinion': 1, 'low-specificity': 1 },\n  centralClaimsKept: 2,\n});\n\n// Finalize and persist\nconst finalMetrics = metrics.finalize();\nawait persistMetrics(finalMetrics);\n{{/code}}\n\n=== Querying Metrics ===\n\n{{code language=\"typescript\"}}\n// Get metrics for a job\nconst response = await fetch(`/api/fh/metrics/${jobId}`);\nconst metrics: AnalysisMetrics = await response.json();\n\n// Get summary stats\nconst params = new URLSearchParams({\n  startDate: '2026-01-01',\n  endDate: '2026-01-31',\n});\nconst summaryResponse = await fetch(`/api/fh/metrics/summary?${params}`);\nconst summary = await summaryResponse.json();\n{{/code}}\n\n== Dashboard ==\n\nThe metrics dashboard is available at ##/admin/metrics##.\n\nFeatures:\n* Real-time summary statistics\n* Schema compliance tracking\n* Quality gate pass rates\n* Cost and performance trends\n* Time range selection\n\n== Best Practices ==\n\n1. **Always collect metrics**: Metrics should be collected for 100% of analyses\n1. **Async persistence**: Metrics persistence should not block analysis completion\n1. **Error handling**: Failed metrics collection should not fail the analysis\n1. **Privacy**: Metrics do not include sensitive input data\n1. **Retention**: Configure cleanup schedule (default: 90 days)\n\n== Cost Estimation ==\n\nCosts are estimated based on token usage and provider pricing:\n\n|= Provider |= Model |= Input (per 1M) |= Output (per 1M)\n| Anthropic | Sonnet-4 | $3 | $15\n| Anthropic | Haiku | $0.25 | $1.25\n| OpenAI | GPT-4o | $2.50 | $10\n| OpenAI | GPT-4o-mini | $0.15 | $0.60\n| Google | Gemini Pro | $1.25 | $5\n| Google | Gemini Flash | $0.075 | $0.30\n\n**Note:** Prices as of January 2026, subject to change.\n\n== Related Documentation ==\n\n* [[Testing Strategy>>FactHarbor.Product Development.DevOps.Guidelines.Testing Strategy.WebHome]]\n* BASELINE_RESULTS.md\n* AB_TEST_RESULTS.md", "Product Development.Specification.Reference.Data Models and Schemas.WebHome": "= Data Models and Schemas =\n\nSchema definitions and mappings used in FactHarbor's data pipeline. These supplement the [[canonical Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] with LLM-specific response mappings and metrics schema definitions.\n\n== Schemas ==\n\n* [[LLM Schema Mapping>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.LLM Schema Mapping.WebHome]] - How LLM responses map to FactHarbor data structures\n* [[Metrics Schema>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema.WebHome]] - Performance and quality metrics schema definitions\n\n== Related ==\n\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] - Core data model specification\n* [[Schema Migration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Schema Migration.WebHome]] - Migration patterns\n", "Product Development.Specification.Reference.Prompt Engineering.Prompt Guidelines.WebHome": "= Provider-Specific Prompt Guidelines =\n\n**Version**: 2.6.41\n**Last Updated**: 2026-02-02\n**Status**: Implementation Complete\n\n----\n\n== Overview ==\n\nThis document describes the provider-specific prompt optimizations implemented in FactHarbor v2.8. Each LLM provider has unique strengths and tendencies that can be leveraged (or compensated for) through tailored prompt engineering.\n\n----\n\n== Provider Optimization Summary ==\n\n|= Provider |= Strengths |= Compensations |= Key Techniques\n| **Claude (Anthropic)** | Nuanced reasoning, context detection | Over-hedging tendency | XML tags, thinking blocks\n| **GPT (OpenAI)** | Fast structured output, examples | Over-splitting, balanced drift | Few-shot examples, calibration\n| **Gemini (Google)** | Large context, factual extraction | Verbosity | Length limits, checklists\n| **Mistral** | Fast, rule-following | Less nuanced | Step-by-step, templates\n\n----\n\n== Claude (Anthropic) ==\n\n=== Model Variants ===\n\n* **Premium**: ##claude-sonnet-4-20250514## (verdict, reasoning)\n* **Budget**: ##claude-3-5-haiku-20241022## (extraction)\n\n=== Optimization Techniques ===\n\n==== 1. XML Structure Tags ====\n\nClaude excels with XML-structured prompts. Use tags to organize sections:\n\n{{code language=\"xml\"}}\n<claude_optimization>\n## PROMPT STRUCTURE\nThis prompt uses XML tags for optimal Claude comprehension.\n\n<thinking_process>\nBefore generating output, work through:\n1. What type of input is this?\n2. Are there multiple analytical frames?\n3. Which claims need attribution separation?\n</thinking_process>\n\n<output_rules>\n- Return valid JSON matching schema\n- Use empty strings \"\" for optional fields\n</output_rules>\n</claude_optimization>\n{{/code}}\n\n==== 2. Thinking Blocks ====\n\nEncourage explicit reasoning before output:\n\n{{code language=\"xml\"}}\n<thinking_process>\nFor each context, reason through:\n1. What does the USER'S CLAIM state?\n2. What does the EVIDENCE show?\n3. Do they MATCH or CONTRADICT?\n</thinking_process>\n{{/code}}\n\n==== 3. Prefill Technique ====\n\nStart the assistant response with JSON structure to guide output:\n\n{{code language=\"typescript\"}}\n// In llm.ts\ngetClaudePrefill('understand') // Returns: '{\"impliedClaim\":'\n{{/code}}\n\n=== Compensation Strategies ===\n\n* **Avoid over-hedging**: Instruct to be \"direct and confident\"\n* **Prevent peripheral claims**: Focus on \"verifiable assertions\"\n* **Limit query redundancy**: \"Each search query should be distinct\"\n\n----\n\n== GPT (OpenAI) ==\n\n=== Model Variants ===\n\n* **Premium**: ##gpt-4o## (verdict, reasoning)\n* **Budget**: ##gpt-4o-mini## (extraction)\n\n=== Optimization Techniques ===\n\n==== 1. Few-Shot Examples ====\n\nGPT responds exceptionally well to concrete examples:\n\n{{code language=\"typescript\"}}\n`### FEW-SHOT EXAMPLE (Follow this pattern)\n\nInput: \"The WHO spokesperson stated that the new variant is more transmissible\"\nOutput:\n{\n  \"subClaims\": [\n    {\n      \"id\": \"SC1\",\n      \"text\": \"A WHO spokesperson made a public statement...\",\n      \"claimRole\": \"attribution\",\n      \"centrality\": \"low\"\n    },\n    {\n      \"id\": \"SC2\",\n      \"text\": \"The new variant is more transmissible...\",\n      \"claimRole\": \"core\",\n      \"centrality\": \"high\"\n    }\n  ]\n}`\n{{/code}}\n\n==== 2. Explicit Field Lists ====\n\nAlways enumerate required fields:\n\n{{code language=\"typescript\"}}\n`### REQUIRED OUTPUT FIELDS (All must be present)\n- impliedClaim: string (neutral summary)\n- articleThesis: string (what input asserts)\n- subClaims: array with id, text, claimRole, centrality, isCentral\n- researchQueries: array of 4-6 distinct search strings`\n{{/code}}\n\n==== 3. Calibration Tables ====\n\nUse tables to guide verdict scoring:\n\n{{code language=\"typescript\"}}\n`### VERDICT CALIBRATION TABLE\n| Evidence Pattern | Verdict Range |\n|-----------------|---------------|\n| 3+ supporting, 0 counter | 80-95% (TRUE/MOSTLY TRUE) |\n| 2-3 supporting, 1 counter | 65-79% (LEANING TRUE) |\n| Balanced evidence | 43-57% (MIXED) |\n| 1 supporting, 2-3 counter | 21-35% (LEANING FALSE) |\n| 0-1 supporting, 3+ counter | 5-20% (FALSE/MOSTLY FALSE) |`\n{{/code}}\n\n=== Compensation Strategies ===\n\n* **Prevent over-splitting**: Include \"If ANY answer is 'no' -> Don't create that context\"\n* **Counter balance drift**: \"Do NOT artificially center at 50%\"\n* **Ensure schema compliance**: \"Use '' for empty strings, NEVER null\"\n\n----\n\n== Gemini (Google) ==\n\n=== Model Variants ===\n\n* **Premium**: ##gemini-1.5-pro## (verdict, reasoning)\n* **Budget**: ##gemini-1.5-flash## (extraction)\n\n=== Optimization Techniques ===\n\n==== 1. Explicit Length Limits ====\n\nGemini tends toward verbosity. Always specify limits:\n\n{{code language=\"typescript\"}}\n`### OUTPUT LENGTH LIMITS (MUST FOLLOW)\n| Field | Maximum |\n|-------|---------|\n| impliedClaim | 150 characters |\n| claim text | 150 characters |\n| keyFactors.factor | 12 words |\n| keyFactors.explanation | 20 words |\n| shortAnswer | 25 words |`\n{{/code}}\n\n==== 2. Numbered Processes ====\n\nStructure complex tasks as numbered steps:\n\n{{code language=\"typescript\"}}\n`### NUMBERED PROCESS\n1. Read input completely\n2. Identify claim type (statement vs article)\n3. Extract claims with attribution separation\n4. Assess centrality (expect 1-4 HIGH max)\n5. Detect context boundaries if present\n6. Generate 4-6 search queries\n7. Output JSON`\n{{/code}}\n\n==== 3. Schema Checklists ====\n\nForce validation before output:\n\n{{code language=\"typescript\"}}\n`### SCHEMA CHECKLIST (Verify before output)\n- [ ] id: string (SC1, SC2, etc.)\n- [ ] text: string (150 chars)\n- [ ] claimRole: \"attribution\" | \"source\" | \"timing\" | \"core\"\n- [ ] centrality: \"high\" | \"medium\" | \"low\"\n- [ ] dependsOn: array of claim IDs (or empty [])`\n{{/code}}\n\n=== Compensation Strategies ===\n\n* **Prevent verbosity**: \"Keep all text fields concise\"\n* **Force schema compliance**: \"Arrays must be arrays (even single items)\"\n* **Explicit enum values**: List exact allowed strings\n\n----\n\n== Mistral ==\n\n=== Model Variants ===\n\n* **Premium**: ##mistral-large-latest## (verdict, reasoning)\n* **Budget**: ##mistral-small-latest## (extraction)\n\n=== Optimization Techniques ===\n\n==== 1. Step-by-Step Instructions ====\n\nMistral excels at following explicit sequences:\n\n{{code language=\"typescript\"}}\n`### STEP-BY-STEP PROCESS (Follow exactly)\n\n**Step 1:** Read input completely\n\n**Step 2:** Identify input type\n- Statement about facts  \"claim\"\n- News article/long text  \"article\"\n\n**Step 3:** Extract claims using template\nFor each claim found:\n- id: SC{n}\n- text: [the claim statement]\n- claimRole: [pick: attribution | source | timing | core]\n- centrality: [pick: high | medium | low]`\n{{/code}}\n\n==== 2. Field Templates ====\n\nProvide exact structure to fill in:\n\n{{code language=\"typescript\"}}\n`**Step 2c:** Fill verdict template\n{\n  \"contextId\": \"[context ID]\",\n  \"answer\": [0-100 integer],\n  \"confidence\": [0-100 integer],\n  \"shortAnswer\": \"[complete sentence, 25 words]\",\n  \"keyFactors\": [array of 3-5 items]\n}`\n{{/code}}\n\n==== 3. Validation Checklists ====\n\nInclude explicit pre-output verification:\n\n{{code language=\"typescript\"}}\n`### VALIDATION CHECKLIST\n[ ] All claims have unique IDs (SC1, SC2, etc.)\n[ ] Core claims separated from attribution\n[ ] Only 1-4 claims marked as \"high\" centrality\n[ ] 4-6 search queries included\n[ ] JSON is valid`\n{{/code}}\n\n=== Compensation Strategies ===\n\n* **Add explicit guidance**: More detailed than other providers\n* **Use enumerated rules**: List options explicitly\n* **Include examples**: For each classification decision\n\n----\n\n== Budget Model Optimization ==\n\nWhen pipeline config enables tiering, fast-tier models receive simplified prompts:\n\n=== Token Reduction (~40%) ===\n\n|= Component |= Full Prompt |= Budget Prompt\n| Examples | 2-3 per task | 1 per task\n| Glossaries | Full terminology | Inline definitions only\n| Provider variant | Full section | Single-line hint\n| Reasoning guidance | Detailed | Skip complex reasoning\n\n=== Budget Model Detection ===\n\n{{code language=\"typescript\"}}\n// In prompt-builder.ts\nconst budgetModels = [\n  'claude-3-5-haiku', 'claude-3-haiku',\n  'gpt-4o-mini', 'gpt-3.5-turbo',\n  'gemini-1.5-flash', 'gemini-flash',\n  'mistral-small', 'mistral-medium',\n];\n{{/code}}\n\n=== Budget Prompt Example ===\n\n{{code language=\"typescript\"}}\n`## FAST MODE\n\n**Task**: Extract claims, generate queries. Skip complex reasoning.\n\n**Example**:\nInput: \"Expert X says Y is harmful\"\nOutput: 2 claims\n1. {id: \"SC1\", text: \"Expert X made statements about Y\", claimRole: \"attribution\", centrality: \"low\"}\n2. {id: \"SC2\", text: \"Y is harmful\", claimRole: \"core\", centrality: \"high\"}\n\n**Rules**:\n- Separate WHO SAID from WHAT THEY SAID\n- Only 1-2 claims = \"high\" centrality\n- Generate 4 queries (2 supporting, 2 contradicting)\n- Output valid JSON`\n{{/code}}\n\n----\n\n== Structured Output Hardening ==\n\n=== Provider-Specific JSON Guidance ===\n\nEach provider receives tailored JSON output instructions:\n\n|= Provider |= Key Guidance\n| Claude | Empty strings \"\" not null, array fields as arrays\n| GPT | Include ALL required fields, use \"\" not null\n| Gemini | Length limits, no explanatory text outside JSON\n| Mistral | Field naming exact match, correct JSON syntax\n\n=== Schema Retry Prompts ===\n\nWhen schema validation fails, provider-specific retry prompts include:\n* Summary of errors found\n* Excerpt of original output\n* Provider-specific fix guidance\n\n=== Claude Prefill Strings ===\n\n{{code language=\"typescript\"}}\ngetClaudePrefill('understand')    // '{\"impliedClaim\":'\ngetClaudePrefill('extract_evidence') // '{\"evidenceItems\":['\ngetClaudePrefill('verdict')       // '{\"contextId\":'\n{{/code}}\n\n----\n\n== Testing ==\n\n=== Test Coverage ===\n\nAll provider optimizations are validated by ##prompt-optimization.test.ts##:\n\n|= Test Category |= Tests\n| Provider variant generation | 16\n| XML tags (Claude) | 3\n| Few-shot examples (GPT) | 3\n| Length limits (Gemini) | 2\n| Step-by-step (Mistral) | 2\n| Budget model optimization | 17\n| Structured output | 16\n| Critical guidance | 12\n| Provider detection | 5\n| Token estimation | 3\n| Standard test cases | 4\n| **Total** | **83**\n\n----\n\n== File Reference ==\n\n|= File |= Purpose\n| ##prompts/providers/anthropic.ts## | Claude-specific variants\n| ##prompts/providers/openai.ts## | GPT-specific variants\n| ##prompts/providers/google.ts## | Gemini-specific variants\n| ##prompts/providers/mistral.ts## | Mistral-specific variants\n| ##prompts/config-adaptations/tiering.ts## | Budget model prompts\n| ##prompts/config-adaptations/structured-output.ts## | JSON guidance, retry prompts\n| ##prompts/prompt-builder.ts## | Prompt composition logic\n| ##prompts/prompt-testing.ts## | A/B testing utilities\n| ##prompts/prompt-optimization.test.ts## | Comprehensive test suite\n\n----\n\n== Best Practices ==\n\n=== Do ===\n\n* Use provider-specific optimization techniques\n* Include explicit examples for GPT\n* Add length limits for Gemini\n* Use step-by-step for Mistral\n* Use XML tags for Claude\n* Test prompts with ##generateTestPrompt()##\n\n=== Don't ===\n\n* Use same prompt for all providers\n* Skip schema compliance guidance\n* Assume LLMs handle null correctly (use \"\")\n* Send verbose prompts to fast-tier models\n* Skip validation checklists\n\n----\n\n**Document Status**: Living document - update as provider behaviors change", "Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting.WebHome": "= Provider-Specific Prompt Formatting =\n\n**Version**: 2.8.0 (Optimized - estimated 20-30% token reduction)\n**Date**: 2026-02-03\n**Status**: Implemented\n**Related**: Evidence Quality Filtering\n\n----\n\n== Table of Contents ==\n\n1. Introduction\n1. Architecture Overview\n1. Provider-Specific Optimizations\n1. Prompt Composition Strategy\n1. Configuration Adaptations\n1. Testing and Validation\n\n----\n\n== 1. Introduction ==\n\n=== Purpose ===\n\nFactHarbor's v2.8 prompt architecture uses **provider-specific formatting** to maximize performance across different LLM providers. Each provider (Anthropic, OpenAI, Google, Mistral) has unique strengths and preferred prompt structures.\n\n=== Problem Statement ===\n\nGeneric prompts perform sub-optimally across providers:\n* Claude excels with XML-structured prompts\n* GPT-4 prefers markdown with clear headings\n* Gemini benefits from example-heavy prompts\n* Mistral requires explicit reasoning guidance\n\nA one-size-fits-all approach leaves significant performance gains untapped.\n\n=== Solution ===\n\n**Dynamic prompt composition** with provider-specific variants:\n\n{{code language=\"typescript\"}}\n// Base prompt (universal logic)\nconst basePrompt = getExtractEvidenceBasePrompt();\n\n// Provider variant (format optimization)\nconst providerVariant = getAnthropicExtractEvidenceVariant();  // or OpenAI, Gemini, Mistral\n\n// Config adaptation (tiering, knowledge mode)\nconst configAdaptation = getTieringExtractEvidenceAdaptation(tier);\n\n// Final composed prompt\nconst finalPrompt = basePrompt + providerVariant + configAdaptation;\n{{/code}}\n\n=== v2.8.0 Optimization Improvements (Feb 2026) ===\n\n**Token Reduction**: All provider variants and base prompts optimized for an estimated 20-30% token reduction while maintaining quality.\n\n**Changes Applied**:\n* **Provider variants**: Removed attribution duplication, condensed examples (estimated ~15-20% reduction)\n* **Base prompts**: Inlined terminology, simplified guidance (estimated ~5-10% additional reduction)\n* **Impact**: Estimated API cost reduction of 20-30% across all providers automatically\n\n**Provider Variant Lengths** (approximate, post-optimization):\n* **Anthropic variants**: ~400-600 tokens each (down from ~600-800 tokens)\n* **OpenAI variants**: ~350-550 tokens each (down from ~500-700 tokens)\n* **Google variants**: ~450-650 tokens each (down from ~600-800 tokens)\n* **Mistral variants**: ~400-600 tokens each (down from ~550-750 tokens)\n\n**Quality**: Validated via manual review and build verification. All critical guidance preserved (death = HIGH centrality, attribution = LOW, multi-context rules intact).\n\nSee Prompt Architecture v2.8.0 for detailed changes.\n\n----\n\n== 2. Architecture Overview ==\n\n{{info}}\n**Testing Harness Only**  The ##prompt-builder.ts## system and the TypeScript prompt modules under ##apps/web/src/lib/analyzer/prompts/## are used exclusively by the testing harness (##prompt-testing.ts##). **Production runtime uses ##loadAndRenderSection()## from UCM-managed ##.prompt.md## files** (see [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]). The composition strategy documented on this page applies to the testing harness only.\n{{/info}}\n\n=== Prompt Builder Module ===\n\n**Location**: ##apps/web/src/lib/analyzer/prompts/prompt-builder.ts## (testing harness)\n\n**Supported Providers**:\n* **Anthropic** (Claude Sonnet 4, Sonnet 3.5, Haiku 3.5)\n* **OpenAI** (GPT-4, GPT-4 Turbo, GPT-3.5 Turbo)\n* **Google** (Gemini 1.5 Pro, Gemini 1.5 Flash)\n* **Mistral** (Mistral Large, Mistral Medium)\n\n=== Task Types ===\n\nPrompts are composed for the following task types in the testing harness:\n1. **extract_evidence** - Evidence extraction from sources\n1. **verdict** - Claim evaluation and verdict assignment\n1. **dynamic_plan** - Dynamic analysis planning (monolithic mode)\n1. **dynamic_analysis** - Dynamic analysis execution (monolithic mode)\n\n{{warning}}\n**Removed task types** (Orchestrated pipeline  no longer exist): ~~**understand**~~, ~~**context_refinement**~~ (AnalysisContext boundary detection), ~~**orchestrated_understand**~~  these task types were part of the removed Orchestrated pipeline.\n{{/warning}}\n\n=== Composition Layers ===\n\n{{code}}\n\n Base Prompt (universal logic)              Task requirements, schema, rules\n\n Provider Variant (format optimization)     XML tags, markdown, examples\n\n Config Adaptation (runtime tuning)         Tiering, knowledge mode, budget\n\n                  \n         Final Composed Prompt\n{{/code}}\n\n----\n\n== 3. Provider-Specific Optimizations ==\n\n=== 3.1 Anthropic Claude ===\n\n**File**: ##apps/web/src/lib/analyzer/prompts/providers/anthropic.ts##\n\n**Optimizations**:\n* **XML-structured prompts** - Claude excels with XML tags for clarity\n* **Format-only variants** - Content guidance stays in base prompts\n* **Output structure hints** - JSON validity and empty-string rules\n* **Nuanced reasoning** - Trust Claude's judgment on complex assessments\n* **Boundary clustering** - Strong at clustering EvidenceScopes into ClaimAssessmentBoundaries\n\n**Example Optimization**:\n\n{{code language=\"xml\"}}\n<claude_optimization>\n## FORMAT\nUse XML tags. Follow schema precisely.\n\n## OUTPUT\n- Valid JSON matching schema\n- Empty strings \"\" for missing optional fields\n- All arrays as arrays (even if empty)\n\n## STRENGTHS\nApply nuanced reasoning. Be direct and confident.\n</claude_optimization>\n{{/code}}\n\n**Why This Works**:\n* Claude's training prioritizes XML tag recognition\n* Format-only structure reduces duplication and keeps guidance centralized\n\n=== 3.2 OpenAI GPT-4 ===\n\n**File**: ##apps/web/src/lib/analyzer/prompts/providers/openai.ts##\n\n**Optimizations**:\n* **Markdown headings** - Clear H2/H3 structure with ##~#~### and ##~#~#~###\n* **Numbered lists** - Explicit step-by-step instructions\n* **Code block examples** - JSON schema with inline comments\n* **Explicit constraints** - \"NEVER\", \"ALWAYS\", \"MUST\" keywords\n* **Function calling** - Structured output via JSON mode\n\n**Example Optimization**:\n\n{{code language=\"markdown\"}}\n## OpenAI Optimization - Extract Evidence Task\n\n### Step-by-Step Process\n\n1. **Read the source content** - Scan for verifiable statements\n2. **Extract specific claims** - Look for concrete facts, not opinions\n3. **Assign probativeValue** - Rate quality (high/medium/low)\n4. **Link to source** - Include excerpt + URL for attribution\n5. **Categorize evidence** - Classify as statistic, expert_quote, event, etc.\n\n### Output Requirements\n\n**REQUIRED Fields**:\n- `id`: Unique identifier (format: \"S{sourceId}-E{number}\")\n- `statement`: The extracted statement (20+ characters)\n- `category`: Evidence category (see schema)\n- `sourceUrl`: Full URL of source\n- `sourceExcerpt`: Relevant excerpt (30+ characters)\n\n**NEVER Extract**:\n- Vague statements (\"some say\", \"many believe\")\n- Opinions without attribution\n- Speculative language (\"might\", \"could\", \"possibly\")\n\n```json\n{\n  \"id\": \"S1-E1\",\n  \"statement\": \"The study found a 25% increase in efficiency\",\n  \"category\": \"statistic\",\n  \"probativeValue\": \"high\",\n  \"sourceUrl\": \"https://example.com/study\",\n  \"sourceExcerpt\": \"Published in Nature (2023), the peer-reviewed study documented...\"\n}\n```\n{{/code}}\n\n**Why This Works**:\n* GPT-4's training emphasizes markdown structure\n* Numbered lists provide clear execution order\n* Code blocks with comments aid JSON schema comprehension\n\n=== 3.3 Google Gemini ===\n\n**File**: ##apps/web/src/lib/analyzer/prompts/providers/google.ts##\n\n**Optimizations**:\n* **Example-heavy prompts** - Multiple before/after examples\n* **Visual formatting** - Use of emojis and bullets for emphasis\n* **Repetition for clarity** - Key rules stated 2-3 times\n* **Concrete over abstract** - Specific examples > general principles\n* **Short paragraphs** - Break complex instructions into small chunks\n\n**Example Optimization**:\n\n{{code language=\"markdown\"}}\n## Gemini Optimization - Extract Evidence Task\n\n###  What You'll Do\n\nExtract verifiable facts from sources. Each fact needs:\n-  A clear statement (what was claimed)\n-  Source attribution (where it came from)\n-  Quality rating (high/medium/low)\n\n###  Good Example\n\n**Input Source**: \"The agency report found Product A failure rate of 2.1% in 2023\"\n\n**Output**:\n```json\n{\n  \"statement\": \"Agency report documented Product A failure rate of 2.1% in 2023\",\n  \"probativeValue\": \"high\",\n  \"sourceUrl\": \"https://example.com/reports/2023\",\n  \"sourceExcerpt\": \"The agency annual report documented a Product A failure rate of 2.1%...\"\n}\n```\n\n###  Bad Example\n\n**Input Source**: \"Some experts say Product A is concerning\"\n\n**Output**: *DO NOT EXTRACT* (vague attribution, no concrete fact)\n\n###  Quality Ratings\n\n**High probativeValue**:\n- Specific data with attribution (\"Study X found Y%\")\n- Expert testimony with credentials (\"Dr. Smith, university professor...\")\n- Official documents with citations (\"Court ruling 2023-456...\")\n\n**Medium probativeValue**:\n- General claims with reasonable attribution (\"Recent studies suggest...\")\n- Moderate specificity (\"Performance improved last quarter\")\n\n**Low probativeValue** (DO NOT EXTRACT):\n- Vague attribution (\"Some say...\", \"Many believe...\")\n- Speculation (\"Could be...\", \"Might have...\")\n{{/code}}\n\n**Why This Works**:\n* Gemini benefits from visual anchors (emojis, bullets)\n* Multiple examples provide pattern recognition\n* Repetition reinforces key rules\n\n=== 3.4 Mistral ===\n\n**File**: ##apps/web/src/lib/analyzer/prompts/providers/mistral.ts##\n\n**Optimizations**:\n* **Explicit reasoning steps** - Chain-of-thought guidance\n* **French-language examples** - Mistral's training includes French corpus\n* **Formal tone** - Academic-style instructions\n* **Strict schema adherence** - Emphasis on JSON structure\n* **Error prevention** - Common pitfalls highlighted\n\n**Example Optimization**:\n\n{{code language=\"markdown\"}}\n## Mistral Optimization - Extract Evidence Task\n\n### Reasoning Chain (Chane de Raisonnement)\n\nFor each source passage:\n\n1. **Identify** verifiable statements (Identifier les dclarations vrifiables)\n2. **Evaluate** probative value (valuer la valeur probante)\n3. **Extract** with source linkage (Extraire avec attribution de source)\n4. **Validate** against schema (Valider selon le schma)\n\n### Schema Adherence (Respect du Schma)\n\nCRITICAL: Every extracted fact MUST conform to this structure:\n\n```typescript\ninterface EvidenceItem {\n  id: string;               // Required: \"S{sourceId}-F{number}\"\n  fact: string;             // Required: Minimum 20 characters\n  category: string;         // Required: From allowed categories\n  sourceUrl: string;        // Required: Full HTTP/HTTPS URL\n  sourceExcerpt: string;    // Required: Minimum 30 characters\n  probativeValue?: string;  // Optional: \"high\" | \"medium\" | \"low\"\n}\n```\n\n### Common Errors to Avoid (Erreurs Courantes  viter)\n\n Missing required fields  Schema validation will fail\n Empty strings for required fields  Use meaningful content\n Relative URLs  Use absolute URLs (https://...)\n Too-short excerpts  Minimum 30 characters required\n{{/code}}\n\n**Why This Works**:\n* Mistral performs well with formal, academic prompting\n* Explicit reasoning chain aids structured thinking\n* Bilingual examples leverage training corpus\n\n----\n\n== 4. Prompt Composition Strategy ==\n\n=== Base Prompts (Universal Layer) ===\n\n**Directory**: ##apps/web/src/lib/analyzer/prompts/base/##\n\n**Files**:\n* ##extract-evidence-base.ts## - Evidence extraction rules\n* ##verdict-base.ts## - Verdict evaluation criteria (testing harness)\n\n{{warning}}\n**Removed base prompt files** (Orchestrated pipeline): ~~##understand-base.ts##~~, ~~##context-refinement-base.ts##~~  these files were for the removed Orchestrated pipeline.\n{{/warning}}\n\n**Content**:\n* Task requirements\n* JSON schema definitions\n* Universal rules (Ground Realism, quality gates)\n* Terminology guidance (EvidenceItem, EvidenceScope, ClaimBoundary)\n\n=== Provider Variants (Format Layer) ===\n\n**Directory**: ##apps/web/src/lib/analyzer/prompts/providers/##\n\n**Files**:\n* ##anthropic.ts## - Claude-specific optimizations\n* ##openai.ts## - GPT-4 optimizations\n* ##google.ts## - Gemini optimizations\n* ##mistral.ts## - Mistral optimizations\n\n**Content**:\n* Format adaptations (XML vs markdown)\n* Examples in provider-preferred style\n* Reasoning guidance tailored to provider strengths\n* Output structure hints\n\n=== Config Adaptations (Runtime Layer) ===\n\n**Directory**: ##apps/web/src/lib/analyzer/prompts/config-adaptations/##\n\n**Files**:\n* ##tiering.ts## - Budget tier adaptations (premium/standard/budget)\n* ##knowledge-mode.ts## - Model knowledge mode (with/without knowledge)\n* ##structured-output.ts## - JSON mode guidance\n\n**Content**:\n* Tier-specific constraints (token limits, search quotas)\n* Knowledge mode instructions (use/ignore model knowledge)\n* Structured output validation\n\n=== Composition Function ===\n\n**Location**: ##apps/web/src/lib/analyzer/prompts/prompt-builder.ts:buildPrompt()##\n\n{{code language=\"typescript\"}}\nexport function buildPrompt(\n  taskType: TaskType,\n  provider: 'anthropic' | 'openai' | 'google' | 'mistral',\n  config?: { tier?: string; knowledgeMode?: string; }\n): string {\n  // 1. Get base prompt (universal logic)\n  const base = getBasePromptForTask(taskType);\n\n  // 2. Get provider variant (format optimization)\n  const variant = getProviderVariantForTask(taskType, provider);\n\n  // 3. Get config adaptation (runtime tuning)\n  const adaptation = getConfigAdaptation(taskType, config);\n\n  // 4. Compose final prompt\n  return base + '\\n\\n' + variant + '\\n\\n' + adaptation;\n}\n{{/code}}\n\n----\n\n== 5. Configuration Adaptations ==\n\n=== 5.1 Tiering Adaptations ===\n\n**Purpose**: Adjust prompts based on budget tier (premium/standard/budget)\n\n**Premium Tier**:\n* Full reasoning guidance\n* Multiple examples\n* Extended search quotas\n* No token limits\n\n**Standard Tier**:\n* Essential reasoning guidance\n* Key examples only\n* Standard search quotas\n* Moderate token limits\n\n**Budget Tier**:\n* Minimal reasoning guidance\n* No examples (schema only)\n* Reduced search quotas\n* Strict token limits\n\n**Example**:\n\n{{code language=\"typescript\"}}\n// Premium tier (full guidance)\nconst premium = `\n## REASONING APPROACH (Premium)\nUse your full capabilities to:\n1. Deeply analyze input for implicit claims\n2. Cluster EvidenceScopes into ClaimAssessmentBoundaries\n3. Generate comprehensive search queries (up to 12)\n4. Extract all relevant evidence (no artificial limits)\n`;\n\n// Budget tier (minimal guidance)\nconst budget = `\n## INSTRUCTIONS (Budget Mode)\n1. Identify core claims only (max 5)\n2. Generate essential search queries (max 6)\n3. Extract most critical evidence only (max 20 items)\nToken limit: 8000 tokens\n`;\n{{/code}}\n\n=== 5.2 Knowledge Mode Adaptations ===\n\n**Purpose**: Control whether LLM can use internal knowledge vs. sources-only\n\n**With Model Knowledge**:\n* LLM can use training data for context\n* Useful for well-known topics\n* Risk: May inject outdated knowledge\n\n**Without Model Knowledge** (Sources-Only):\n* LLM must base all claims on provided sources\n* Required for Ground Realism enforcement\n* Prevents hallucination\n\n**Example**:\n\n{{code language=\"typescript\"}}\n// With knowledge\nconst withKnowledge = `\nYou may use your training knowledge to contextualize claims, but all EVIDENCE\nmust come from provided sources with proper attribution.\n`;\n\n// Without knowledge (sources-only)\nconst withoutKnowledge = `\nCRITICAL: You MUST base all evidence exclusively on the provided sources.\nDO NOT use your training knowledge. If sources don't contain information,\nstate that evidence is insufficient rather than using model knowledge.\n`;\n{{/code}}\n\n----\n\n== 6. Testing and Validation ==\n\n=== Test Suite Location ===\n\n**File**: ##apps/web/test/unit/lib/analyzer/prompts/prompt-optimization.test.ts##\n\n**Coverage**:\n* Provider-specific variant generation\n* Prompt composition correctness\n* Config adaptation merging\n* Schema validation\n\n=== Validation Checks ===\n\n**Automated Checks**:\n1. **Prompt length** - Ensure prompt doesn't exceed provider limits\n1.* Claude: 200k tokens\n1.* GPT-4: 128k tokens\n1.* Gemini: 2M tokens\n1.* Mistral: 128k tokens\n1. **Schema inclusion** - Verify JSON schema present in all prompts\n1. **Required fields** - Check that all required fields documented\n1. **Provider-specific markers** - Validate XML tags (Claude), markdown (GPT-4), etc.\n\n=== Manual Review Process ===\n\n**Before deploying prompt changes**:\n1. Test with representative inputs (3-5 samples per task type)\n1. Compare outputs across providers (consistency check)\n1. Validate JSON schema adherence\n1. Check for unintended behavior changes\n1. Review with domain expert (if terminology changes)\n\n----\n\n== Appendix A: Provider Feature Matrix ==\n\n|= Feature |= Anthropic |= OpenAI |= Google |= Mistral\n| **XML Tags** | Excellent | Poor | Moderate | Poor\n| **Markdown** | Good | Excellent | Good | Good\n| **Thinking Blocks** | Native | Simulated | Simulated | Simulated\n| **Prefill** | Supported | No | No | No\n| **JSON Mode** | Structured | Structured | Structured | Structured\n| **Examples Heavy** | Moderate | Good | Excellent | Moderate\n| **Nuanced Reasoning** | Excellent | Excellent | Good | Good\n| **Schema Adherence** | Excellent | Excellent | Good | Moderate\n| **Boundary Clustering** | Excellent | Good | Moderate | Moderate\n| **Attribution Separation** | Excellent | Good | Moderate | Moderate\n\n**Legend**:\n* Excellent - Strong native support, highly effective\n* Good - Reliable support, effective\n* Moderate - Acceptable support, requires careful prompting\n* Poor/No - Weak support, avoid using\n\n----\n\n== Appendix B: Related Documents ==\n\n* Evidence Quality Filtering - Layer 1 prompt enforcement\n* AGENTS.md - LLM agent rules and guidance\n* Unified Config Management - Runtime configuration\n* ##prompt-builder.ts## - Implementation\n\n----\n\n**Document Version**: 1.0\n**Last Updated**: 2026-01-29\n**Next Review**: When adding new providers or major prompt changes\n**Maintained by**: Plan Coordinator", "Product Development.Specification.Reference.Prompt Engineering.WebHome": "= Prompt Engineering =\n\nPrompt design guidelines and provider-specific formatting rules for LLM prompts used in FactHarbor's AKEL pipeline. Covers best practices for evidence extraction, verdict generation, and source reliability prompts across different LLM providers (Anthropic, OpenAI, Google).\n\n== Reference ==\n\n* [[Provider-Specific Formatting>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting.WebHome]] - Formatting rules specific to each LLM provider (Anthropic, OpenAI, Google)\n* [[Prompt Guidelines>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.Prompt Guidelines.WebHome]] - General prompt engineering best practices and standards\n\n== Related ==\n\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] - AI Knowledge Extraction Layer\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] - System architecture and LLM abstraction\n", "Product Development.Specification.Reference.Terminology.WebHome": "= FactHarbor Terminology Reference =\n\n**Version**: 4.0.0-cb\n**Date**: 2026-02-16\n**Audience**: Developers, Prompt Engineers, LLM Systems\n**Status**: ClaimBoundary pipeline (default) - Orchestrated pipeline removed\n\n----\n\n== Purpose ==\n\nThis document provides the **authoritative glossary** for FactHarbor's ClaimBoundary pipeline terminology across all layers: TypeScript code, JSON schema, database storage, and LLM prompts. Use this as the single source of truth when encountering ambiguous terms.\n\n{{info}}\n**ClaimBoundary is now the default pipeline** (as of Phase 2: Cutover, February 2026). The Orchestrated pipeline (which used AnalysisContext) has been completely removed.\n\n* **All code** uses ClaimBoundary, AtomicClaim, claimBoundaryId.\n* **NEVER use** in new code: AnalysisContext, contextId, analysisContexts.\n* Full architecture: [[ClaimBoundary Pipeline Architecture>>path:/Docs/WIP/ClaimBoundary_Pipeline_Architecture_2026-02-15.md]]\n* Migration state: See [[CB Execution State>>path:/Docs/WIP/CB_Execution_State.md]]\n{{/info}}\n\n----\n\n== Field Mapping Table (v4.0.0-cb) ==\n\n**ClaimBoundary Pipeline**: All code uses ClaimBoundary terminology. AnalysisContext fields removed.\n\n|= Concept |= TypeScript Type |= JSON Field (v4.0.0-cb) |= JSON Field (Legacy - removed) |= Prompt Term\n| Atomic claim | ##AtomicClaim## | ##atomicClaims## | ~~##claims##~~ | AtomicClaim\n| Claim boundary | ##ClaimBoundary## | ##claimBoundaries## | ~~##analysisContexts##~~ | ClaimBoundary\n| Per-evidence metadata | ##EvidenceScope## | ##evidenceScope## | ##evidenceScope## (unchanged) | EvidenceScope\n| Boundary finding | ##BoundaryFinding## | ##boundaryFindings## | (new field) | BoundaryFinding\n| Claim verdict | ##ClaimVerdict## | ##claimVerdicts## | ~~##claimAssessments##~~ | ClaimVerdict\n| Evidence item | ##EvidenceItem## | ##evidenceItems## | ##evidenceItems## (unchanged) | EvidenceItem\n| Evidence statement | ##statement## | ##statement## | ##statement## (unchanged) | statement\n| Evidence ID prefix | ##EV_001, EV_002...## | ##EV_001, EV_002...## | ~~##E1, E2, E3...##~~ | EV-prefix\n| Coverage matrix | ##CoverageMatrix## | ##coverageMatrix## | (new field) | CoverageMatrix\n| Verdict narrative | ##VerdictNarrative## | ##verdictNarrative## | (new field) | VerdictNarrative\n\n**Schema Version**: 3.0.0-cb (resultJson schema)\n\n----\n\n== Pipeline Hierarchy: Understanding the Flow ==\n\nFactHarbor's ClaimBoundary pipeline follows a sequential 5-stage flow:\n\n=== Stage 1: EXTRACT CLAIMS (Two-Pass, Evidence-Grounded) ===\n\n* **What:** Parse input and extract central, verifiable AtomicClaims\n* **When:** First stage of pipeline (before research)\n* **Output:** ##AtomicClaim[]##  only claims with ##isCentral: true##\n* **Key fields:** ##statement##, ##centrality##, ##harmPotential##, ##specificityScore##, ##groundingQuality##, ##expectedEvidenceProfile##\n* **JSON field:** ##atomicClaims##\n\n=== Stage 2: RESEARCH ===\n\n* **What:** Gather evidence for each claim via web search and extraction\n* **When:** After claims extracted, before boundary clustering\n* **Output:** ##EvidenceItem[]##  each with mandatory ##EvidenceScope##\n* **Key principle:** Claims drive all research (no context-driven queries)\n* **ID format:** ##EV_001, EV_002, ...## (EV-prefix for Evidence)\n* **JSON field:** ##evidenceItems##\n\n=== Stage 3: CLUSTER BOUNDARIES ===\n\n* **What:** Organize evidence into ClaimBoundaries by clustering compatible EvidenceScopes\n* **When:** After research complete, before verdict generation\n* **Output:** ##ClaimBoundary[]## + evidence assignments\n* **Key principle:** Boundaries emerge from evidence (not predetermined)\n* **Clustering factors:** Methodology, boundaries, geographic, temporal congruence\n* **JSON field:** ##claimBoundaries##\n\n=== Stage 4: VERDICT (LLM Debate Pattern) ===\n\n* **What:** Generate per-claim verdicts using 5-step debate (advocate  self-consistency  challenge  reconciliation  validation)\n* **When:** After boundary clustering complete\n* **Output:** ##ClaimVerdict[]##  one per AtomicClaim, with ##boundaryFindings[]##\n* **Module:** ##verdict-stage.ts## (separate module)\n* **JSON field:** ##claimVerdicts##\n\n=== Stage 5: AGGREGATE ===\n\n* **What:** Compute coverage matrix, triangulation, and overall assessment with narrative\n* **When:** Final stage\n* **Output:** ##OverallAssessment## with ##VerdictNarrative##\n* **Key artifacts:** ##CoverageMatrix## (claims  boundaries), triangulation scores\n* **JSON field:** ##overallAssessment##\n\n----\n\n**Key Distinctions:**\n\n* **AtomicClaim** = \"What verifiable assertion am I checking?\" (extracted from user input)\n* **EvidenceScope** = \"What methodology/boundaries did THIS source use?\" (per-evidence metadata)\n* **ClaimBoundary** = \"Which evidence items can be grouped together?\" (compatible EvidenceScopes)\n* **BoundaryFinding** = \"What does evidence in THIS boundary say about THIS claim?\" (per-boundary verdict component)\n\n----\n\n== Core Concepts ==\n\n=== 1. ClaimBoundary Pipeline Terminology  NEW PIPELINE (DEFAULT) ===\n\n{{info}}\n**ClaimBoundary is now the default pipeline.** The Orchestrated pipeline (which used AnalysisContext) has been removed as of Phase 2a. All new code must use ClaimBoundary terminology.\n\nFull architecture: [[ClaimBoundary Pipeline Architecture>>path:/Docs/WIP/ClaimBoundary_Pipeline_Architecture_2026-02-15.md]]\n{{/info}}\n\n==== 1.1 AtomicClaim ====\n\n**What it is**: A single verifiable assertion extracted from user input. The atomic unit of analysis in the ClaimBoundary pipeline.\n\n**Why it matters**: Each claim drives independent research. Claims must be specific enough to generate targeted search queries without additional framing.\n\n**Examples (generic)**:\n* \"Entity A performed Action X during Period Y\"\n* \"Metric M for Process P exceeds Value V\"\n* \"Framework F applies to Situation S\"\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface AtomicClaim {\n  id: string;                     // AC_01, AC_02, ...\n  statement: string;              // The verifiable assertion\n  category: \"factual\" | \"evaluative\" | \"procedural\";\n  centrality: \"high\" | \"medium\";\n  harmPotential: \"critical\" | \"high\" | \"medium\" | \"low\";\n  isCentral: boolean;             // true (only central claims survive)\n  claimDirection: \"supports_thesis\" | \"contradicts_thesis\" | \"contextual\";\n  keyEntities: string[];\n  specificityScore: number;       // 0-1 (0.6 required)\n  groundingQuality: \"strong\" | \"moderate\" | \"weak\" | \"none\";\n  expectedEvidenceProfile: {\n    methodologies: string[];\n    expectedMetrics: string[];\n    expectedSourceTypes: SourceType[];\n  };\n}\n{{/code}}\n\n**Variable names**: ##atomicClaim##, ##atomicClaims##\n\n**NEVER call it**: \"context\", \"fact\"\n\n----\n\n==== 1.2 ClaimBoundary ====\n\n**What it is**: An evidence-emergent grouping of compatible EvidenceScopes, created //after// research by clustering evidence with congruent methodology, temporal, boundaries, and geographic dimensions.\n\n**Why it matters**: ClaimBoundaries organize evidence for verdict generation. Instead of pre-creating analytical frames (which caused instability), boundaries emerge from the evidence itself.\n\n**Key principle**: Evidence tells us what boundaries exist. We don't guess.\n\n**Examples (generic)**:\n* \"Methodology A studies\" (evidence using Framework A)\n* \"Jurisdiction J proceedings\" (evidence from legal domain J)\n* \"Period P data\" (evidence from temporal range P)\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface ClaimBoundary {\n  id: string;                     // CB_01, CB_02, ...\n  name: string;                   // Human-readable label\n  shortName: string;              // Short label for UI tabs\n  description: string;            // What this boundary represents\n  methodology?: string;           // Dominant methodology (if applicable)\n  boundaries?: string;            // Scope boundaries\n  geographic?: string;            // Geographic scope\n  temporal?: string;              // Temporal scope\n  internalCoherence: number;      // 0-1: consistency of evidence within\n  evidenceCount: number;          // Number of evidence items\n}\n{{/code}}\n\n**Variable names**: ##claimBoundary##, ##claimBoundaries##, ##claimBoundaryId##\n\n**NEVER call it**: \"context\", \"scope\", \"analysisContext\"\n\n----\n\n==== 1.3 BoundaryFinding ====\n\n**What it is**: Per-boundary quantitative evidence assessment for a specific claim. Each ClaimVerdict contains boundaryFindings[] showing how evidence within each boundary supports or contradicts the claim.\n\n**Why it matters**: Enables multi-perspective verdicts  the same claim may have different support levels across different ClaimBoundaries.\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface BoundaryFinding {\n  boundaryId: string;                           // Which ClaimBoundary\n  truthPercentage: number;                      // 0-100 per-boundary assessment\n  confidence: number;                           // 0-100 per-boundary confidence\n  evidenceDirection: \"supports\" | \"contradicts\" | \"mixed\" | \"neutral\";\n  evidenceCount: number;                        // Evidence items in this boundary for this claim\n}\n{{/code}}\n\n**Used in**: ##ClaimVerdict.boundaryFindings[]##\n\n----\n\n==== 1.4 ClaimVerdict ====\n\n**What it is**: Final verdict for a single AtomicClaim, produced by the 5-step LLM debate pattern (advocate  self-consistency  challenge  reconciliation  validation).\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface ClaimVerdict {\n  id: string;\n  claimId: string;                              // Which AtomicClaim\n  truthPercentage: number;                      // 0-100 overall\n  verdict: string;                              // 7-point scale label\n  confidence: number;                           // 0-100 overall\n  reasoning: string;                            // LLM-generated explanation\n  harmPotential: \"critical\" | \"high\" | \"medium\" | \"low\";\n  isContested: boolean;                         // Documented counter-evidence exists\n  supportingEvidenceIds: string[];\n  contradictingEvidenceIds: string[];\n  boundaryFindings: BoundaryFinding[];          // Per-boundary assessments\n  challengeResponses?: string[];                // From reconciliation step\n}\n{{/code}}\n\n**Variable names**: ##claimVerdict##, ##claimVerdicts##\n\n----\n\n==== 1.5 CoverageMatrix ====\n\n**What it is**: A deterministic claims  boundaries matrix showing which claims have been evaluated in which boundaries.\n\n**Why it matters**: Ensures analytical completeness  every claim must have evidence across relevant boundaries. Logged to job events for transparency.\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface CoverageMatrix {\n  claims: string[];                            // AtomicClaim IDs\n  boundaries: string[];                        // ClaimBoundary IDs\n  coverage: boolean[][];                       // claims[i]  boundaries[j]  has evidence?\n  missingCoverage: Array<{\n    claimId: string;\n    boundaryId: string;\n    reason: string;\n  }>;\n}\n{{/code}}\n\n----\n\n==== 1.6 VerdictNarrative ====\n\n**What it is**: Structured summary of the overall assessment, including boundary disagreements and limitations. LLM-generated (Sonnet, 1 call) during Stage 5: AGGREGATE.\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface VerdictNarrative {\n  headline: string;                           // 1-sentence summary\n  evidenceBaseSummary: string;                // What evidence was found\n  keyFinding: string;                         // Most important conclusion\n  boundaryDisagreements: Array<{\n    boundaryIds: string[];\n    disagreementSummary: string;\n  }>;\n  limitations: string[];                      // Gaps, uncertainties, caveats\n}\n{{/code}}\n\n**Used in**: ##OverallAssessment.verdictNarrative##\n\n----\n\n=== 2. EvidenceScope (Per-Evidence Source Metadata)  SHARED CONCEPT ===\n\n**What it is**: Metadata attached to individual evidence items describing the methodology, boundaries, geography, and time period that **the source document** used when producing that evidence.\n\n**Why it matters**: Evidence stating \"Metric X = Value Y\" from a study using Methodology A with Boundary Set B cannot be directly compared to a study using Methodology C with Boundary Set D. EvidenceScope captures these methodological differences so the pipeline can cluster compatible evidence into ClaimBoundaries.\n\n**Examples (generic)**:\n* Methodology: \"Standard S\", \"Framework F\", \"Protocol P\"\n* Boundaries: \"Full system\", \"Subsystem only\", \"Phases 1-3\"\n* Geographic: \"Jurisdiction J\", \"Region R\", \"Multi-national\"\n* Temporal: \"Period P data\", \"Year Y baseline\", \"Historical range\"\n\n**Primary fields (always extracted when available)**:\n* ##methodology##: The analytical approach used by the source (e.g., \"Standard ISO-X\", \"Regulatory Framework Y\")\n* ##temporal##: When the source data was collected or applies (e.g., \"Data from Period P\", \"Baseline Year Y\")\n\n**Other fields**:\n* ##boundaries##: What was included/excluded in the analysis (e.g., \"Full lifecycle\", \"Operation phase only\")\n* ##geographic##: Geographic scope of the source data (e.g., \"Region R\", \"Country C\")\n* ##additionalDimensions##: Domain-specific scope data (e.g., `{ \"sample_size\": \"N=12000\", \"blinding\": \"double-blind\" }`)\n\n//Note: All fields except ##name## are optional in the TypeScript type (##types.ts:226##). The LLM is instructed to extract ##methodology## and ##temporal## for every evidence item, but extraction is best-effort  ##scopeQuality## on EvidenceItem tracks completeness.//\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\nexport interface EvidenceScope {\n  name: string;                             // Short label (e.g., \"Methodology A\", \"Framework B\")\n  methodology?: string;                     // Optional  extracted when source provides it\n  temporal?: string;                        // Optional  extracted when source provides it\n  boundaries?: string;                      // Optional\n  geographic?: string;                      // Optional\n  sourceType?: SourceType;                  // Optional classification\n  additionalDimensions?: Record<string, string>; // Optional domain-specific data\n}\n\n// Attached to evidence items\nexport interface EvidenceItem {\n  id: string;                               // EV_001, EV_002, ... (EV-prefix)\n  statement: string;                        // The evidence statement\n  evidenceScope: EvidenceScope;             // MANDATORY (not optional)\n  claimBoundaryId?: string;                 // Assigned during CLUSTER stage\n  relevantClaimIds: string[];               // Which claims this evidence relates to\n  isDerivative: boolean;                    // Derives from another source's study\n  derivedFromSourceUrl?: string;            // URL of original source (optional)\n  scopeQuality: \"complete\" | \"partial\" | \"incomplete\"; // Scope metadata quality\n  // ...\n}\n{{/code}}\n\n**JSON field name**:\n\n{{code language=\"json\"}}\n{\n  \"evidenceItems\": [\n    {\n      \"id\": \"EV_001\",\n      \"statement\": \"Entity A achieved Metric X using Method M\",\n      \"evidenceScope\": {\n        \"name\": \"Method M Analysis\",\n        \"methodology\": \"Standard S with Approach A\",\n        \"temporal\": \"Period P data (Year-Year)\",\n        \"boundaries\": \"Full system boundary\",\n        \"geographic\": \"Region R\",\n        \"additionalDimensions\": {\n          \"sample_size\": \"N=12000\",\n          \"confidence_level\": \"95%\"\n        }\n      },\n      \"scopeQuality\": \"complete\",\n      \"claimBoundaryId\": \"CB_01\",\n      \"relevantClaimIds\": [\"AC_01\", \"AC_03\"]\n    }\n  ]\n}\n{{/code}}\n\n**Prompt terminology**:\n* Always: \"EvidenceScope\"\n* NEVER: \"scope\" (ambiguous), \"context\"\n\n**Key differences from ClaimBoundary**:\n* **EvidenceScope** = Per-evidence metadata (\"What methodology did THIS source use?\")\n* **ClaimBoundary** = Cluster of compatible EvidenceScopes (\"Which evidence can be grouped together?\")\n* Multiple evidence items share the same EvidenceScope  they cluster into the same ClaimBoundary\n* EvidenceScopes with incompatible methodology/boundaries/temporal/geographic  separate ClaimBoundaries\n\n----\n\n=== 3. Background Details (Narrative Background) ===\n\n**What it is**: The narrative/rhetorical framing or background context of the input article. This describes **how the article presents** the information, but is NOT a reason to split into separate AnalysisContexts.\n\n**Why it matters**: Helps understand the user's intent and article structure, but does NOT affect verdict logic.\n\n**Examples**:\n* \"Brazilian political crisis following January 8th events\"\n* \"Climate policy debate in European Union\"\n* \"Legal proceedings against former president\"\n\n**Code representation**:\n\n{{code language=\"typescript\"}}\n// Stored as string in ClaimUnderstanding\nexport interface ClaimUnderstanding {\n  backgroundDetails: string; // Narrative background\n  // ...\n}\n{{/code}}\n\n**JSON field name** (v3.1):\n\n{{code language=\"json\"}}\n{\n  \"backgroundDetails\": \"Brazilian political crisis following January 8th events\"\n}\n{{/code}}\n\n**UI Display**: Shown in ##BackgroundBanner.tsx## component with label \"Background\"\n\n**Prompt terminology**:\n* Preferred: \"backgroundDetails\" or \"Background\"\n* Removed: ~~\"ArticleFrame\"~~, ~~\"analysisContext\" (singular)~~ (legacy terms)\n\n**Common confusion**:\n* backgroundDetails is NOT an AnalysisContext (not a reason to split analysis)\n* backgroundDetails does NOT get its own verdict\n* backgroundDetails IS purely descriptive/informational\n\n----\n\n=== 4. Doubted vs Contested (Contestation Classification) - v2.8 ===\n\n**What it is**: A distinction between two types of opposition to a claim, which affects how the opposition impacts the verdict weight.\n\n**Why it matters**: Not all criticism is equal. Political statements without evidence shouldn't reduce a claim's weight as much as documented counter-evidence. This ensures:\n* **Evidence-based contestation** appropriately reduces certainty\n* **Opinion-based doubt** doesn't unfairly penalize well-evidenced claims\n\n**Two Categories**:\n\n|= Category |= factualBasis |= Weight Multiplier |= Example\n| **DOUBTED** | ##\"opinion\"## | 1.0x (full) | \"Government says trial was unfair\" (no specifics)\n| **DOUBTED** | ##\"alleged\"## | 1.0x (full) | \"Critics claim bias\" (no evidence cited)\n| **CONTESTED** | ##\"disputed\"## | 0.7x (reduced) | \"Defense presented conflicting expert testimony\"\n| **CONTESTED** | ##\"established\"## | 0.5x (reduced) | \"Audit found violation of Regulation 47(b)\"\n\n**Weight calculation** (in ##getClaimWeight()##, v3.1  updated in v2.9.0):\n\n{{code language=\"typescript\"}}\nif (claim.isContested) {\n  if (basis === \"established\") weight *= 0.5;  // Strong counter-evidence (updated v2.9.0: was 0.3x)\n  else if (basis === \"disputed\") weight *= 0.7; // Some counter-evidence (updated v2.9.0: was 0.5x)\n  // \"opinion\"/\"alleged\"/\"unknown\" -> full weight (just doubted)\n}\n{{/code}}\n\n**Common confusion**:\n* \"contested\" does NOT mean \"disputed politically\" (that's \"doubted\")\n* \"contested\" means there IS documented counter-evidence\n* Political statements alone do NOT reduce claim weight\n* Only factual counter-evidence reduces claim weight\n\n----\n\n== Terminology Mapping Tables ==\n\n=== Table 1: Primary Entities (v4.0.0-cb) ===\n\n|= Concept |= TypeScript Name (CB) |= JSON Field |= Prompt Term |= Formerly (Orchestrated  removed)\n| Top-level analytical frame | ##ClaimAssessmentBoundary## | ##claimBoundaries## | \"ClaimBoundary\" | ~~##AnalysisContext## / ##analysisContexts##~~\n| Atomic verifiable assertion | ##AtomicClaim## | ##atomicClaims## | \"AtomicClaim\" | ~~##SubClaim##~~\n| Per-evidence source metadata | ##EvidenceScope## | ##evidenceScope## | \"EvidenceScope\" | ##EvidenceScope## (unchanged)\n| Evidence item | ##EvidenceItem## | ##evidenceItems## | \"EvidenceItem\" | ##EvidenceItem## (unchanged)\n| Per-boundary verdict component | ##BoundaryFinding## | ##boundaryFindings## | \"BoundaryFinding\" | (new in CB)\n| Per-claim verdict | ##CBClaimVerdict## | ##claimVerdicts## | \"ClaimVerdict\" | ~~##ClaimVerdict##~~ (restructured)\n| Narrative background | (string) | ##backgroundDetails## | \"Background\" | ##backgroundDetails## (unchanged)\n\n=== Table 2: Reference Fields (v4.0.0-cb) ===\n\n|= Field Purpose |= TypeScript Field |= JSON Field |= Prompt Term |= Valid Values\n| Evidence  ClaimAssessmentBoundary | ##claimBoundaryId?: string## | ##claimBoundaryId## | \"claimBoundaryId\" | Must match ##ClaimAssessmentBoundary.id##\n| Claim  relevant evidence | ##relevantClaimIds: string[]## | ##relevantClaimIds## | \"relevantClaimIds\" | Array of AC-prefix IDs\n| Supporting evidence | ##supportingEvidenceIds## | ##supportingEvidenceIds## | \"supportingEvidenceIds\" | Array of EV-prefix IDs\n\n{{warning}}\n**Removed reference fields** (Orchestrated pipeline  do not use in new code):\n~~##contextId##~~ (claim/evidence  AnalysisContext  removed), ~~##analysisContexts##~~ (top-level context array  removed), ~~##claimAssessments##~~ (renamed to ##claimVerdicts##)\n{{/warning}}\n\n=== Table 3: Special Constants ===\n\n{{info}}\n**Orchestrated-era constants removed**: ~~##CTX_UNSCOPED##~~, ~~##CTX_MAIN##~~, ~~##CTX_GENERAL##~~ no longer exist in the CB pipeline. ClaimAssessmentBoundaries emerge from evidence clustering and do not use fixed fallback constants.\n{{/info}}\n\n----\n\n== Quick Reference: \"Which term should I use?\" ==\n\n=== In TypeScript Code ===\n\n{{code language=\"typescript\"}}\n// CORRECT (v4.0-cb)\nimport { AtomicClaim, ClaimAssessmentBoundary, EvidenceScope, EvidenceItem } from './types';\n\nfunction processBoundaries(boundaries: ClaimAssessmentBoundary[]) {\n  // ...\n}\n\nfunction processEvidence(items: EvidenceItem[]) {\n  // ...\n}\n\n// REMOVED in v4.0-cb - do not use:\n// AnalysisContext     use ClaimAssessmentBoundary\n// SubClaim            use AtomicClaim\n// ContextAnswer       removed (no equivalent in CB)\n// ExtractedFact       use EvidenceItem (removed in v3.0)\n{{/code}}\n\n=== In JSON Schema (Zod) ===\n\n{{code language=\"typescript\"}}\n// CORRECT (v4.0-cb field names)\nconst schema = z.object({\n  atomicClaims: z.array(AtomicClaimSchema),\n  claimBoundaries: z.array(ClaimAssessmentBoundarySchema),\n  evidenceItems: z.array(EvidenceItemSchema),\n  backgroundDetails: z.string(),\n});\n{{/code}}\n\n=== In LLM Prompts ===\n\n{{code language=\"typescript\"}}\n// CORRECT (v4.0-cb terminology)\nconst prompt = `\n## TERMINOLOGY\n\n**AtomicClaim**: Single verifiable assertion extracted from user input (stored as atomicClaims)\n**EvidenceScope**: Per-evidence source metadata (stored as evidenceItem.evidenceScope)\n**ClaimBoundary**: Evidence-emergent grouping of compatible EvidenceScopes (stored as claimBoundaries)\n**EvidenceItem**: Individual evidence with id (EV-prefix) and statement\n\nYour task: Extract AtomicClaims from user input...\n`;\n\n// AVOID (Orchestrated-era terms  pipeline removed)\nconst prompt = `Identify AnalysisContexts from evidence...`;  // Orchestrated-only concept\nconst prompt = `Extract facts...`;  // Use \"evidence\" not \"facts\"\n{{/code}}\n\n=== In UI/Documentation ===\n\n{{code language=\"typescript\"}}\n// CORRECT (v4.0-cb)\n<h2>Claim Boundaries</h2>\n<p>This analysis identified 2 evidence-emergent boundaries:</p>\n\n<BackgroundBanner backgroundDetails={background} />\n\n// REMOVED in v4.0-cb:\n// <h2>Analysis Contexts</h2>   use \"Claim Boundaries\"\n// <ArticleFrameBanner articleFrame={...} />   removed\n{{/code}}\n\n----\n\n== Pipeline Stage Functions (v4.0.0-cb) ==\n\n|= Stage Function |= Description |= Pipeline Module\n| ##extractClaims## | Two-pass LLM claim extraction (PASS1  PASS2); Gate 1 application | ##claimboundary-pipeline.ts## Stage 1\n| ##researchEvidence## | Iterative web research: query generation, fetch, EvidenceItem extraction | ##claimboundary-pipeline.ts## Stage 2\n| ##clusterBoundaries## | LLM clustering of EvidenceScopes into ClaimAssessmentBoundaries | ##claimboundary-pipeline.ts## Stage 3\n| ##generateVerdicts## | Advocate  Challenger  Reconciliation debate per boundary | ##verdict-stage.ts## Stage 4\n| ##aggregateAssessment## | Gate 4, weighted aggregation, VerdictNarrative generation | ##claimboundary-pipeline.ts## Stage 5\n\n{{warning}}\n**Removed Orchestrated task names** (do not use): ~~##extract_evidence##~~, ~~##context_refinement##~~, ~~##understand##~~, ~~##verdict##~~  these are Orchestrated pipeline function names, not CB stages. Also removed: ~~##extract_facts##~~, ~~##scope_refinement##~~ (pre-v3.0 names).\n{{/warning}}\n\n----\n\n== Config Field Names (CB Pipeline) ==\n\n{{warning}}\n**Orchestrated-era config fields removed**: ~~##contextDetectionMethod##~~, ~~##contextDetectionEnabled##~~, ~~##contextDetectionMinConfidence##~~, ~~##contextDetectionMaxContexts##~~, ~~##contextDedupThreshold##~~ no longer exist. These were part of the removed Orchestrated pipeline's context detection system.\n{{/warning}}\n\nCurrent CB pipeline config fields (UCM-managed  see [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] and [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]):\n\n|= Config Field |= Description |= Default\n| ##gate1Enabled## | Enable Gate 1 claim validation | ##true##\n| ##gate1KeepCentralClaims## | Always keep ##isCentral## claims through Gate 1 | ##true##\n| ##gate4Enabled## | Enable Gate 4 verdict confidence assessment | ##true##\n| ##maxResearchIterations## | Max research rounds per ClaimAssessmentBoundary | ##3##\n| ##maxTotalTokens## | Token budget cap across all pipeline stages | ##750000##\n\n----\n\n== Decision Trees ==\n\n=== \"Should I create a ClaimAssessmentBoundary?\" ===\n\nClaimBoundaries are **NOT manually created**  they emerge from EvidenceScope clustering during Stage 3 (##clusterBoundaries##). The decision below applies to the LLM's clustering logic:\n\n{{code}}\nSTART: Can these EvidenceScopes be compared directly for the same claim?\n  |\n  +-- YES -> Same ClaimBoundary (compatible methodology, temporal, geographic)\n  |   Example: Multiple sources using the same analytical framework\n  |\n  +-- NO -> Are they incompatible in methodology, temporal, or geographic scope?\n      |\n      +-- YES -> Separate ClaimBoundaries\n      |   Example: Studies using different analytical frameworks\n      |\n      +-- NO -> Same ClaimBoundary with noted caveats\n          Example: Minor reporting period differences within same framework\n{{/code}}\n\n=== \"Is this an EvidenceScope or backgroundDetails?\" ===\n\n{{code}}\nSTART: Where does this information come from?\n  |\n  +-- FROM SOURCE DOCUMENT -> EvidenceScope\n  |   Example: \"Study used ISO 14040 methodology\"\n  |   Attach to evidenceItem.evidenceScope\n  |\n  +-- FROM USER INPUT / ARTICLE FRAMING -> backgroundDetails\n      Example: \"Article is written as opinion piece\"\n      Store as backgroundDetails string (NOT a ClaimAssessmentBoundary)\n{{/code}}\n\n----\n\n== Common Pitfalls & Solutions ==\n\n=== Pitfall 1: Using \"Scope\" Without Qualifier ===\n\n**Problem**:\n\n{{code language=\"typescript\"}}\n// Ambiguous - which scope?\nfunction getScope(id: string) { ... }\n{{/code}}\n\n**Solution**:\n\n{{code language=\"typescript\"}}\n// Explicit\nfunction getClaimAssessmentBoundary(id: string): ClaimAssessmentBoundary { ... }\nfunction getEvidenceScope(evidence: EvidenceItem): EvidenceScope | null { ... }\n{{/code}}\n\n=== Pitfall 2: Conflating backgroundDetails with ClaimAssessmentBoundary ===\n\n**Problem** (creates spurious boundaries):\n\n{{code language=\"json\"}}\n{\n  \"claimBoundaries\": [\n    { \"name\": \"Article frames as conspiracy theory\" }\n  ]\n}\n{{/code}}\n\n**Solution** (background  string field, boundaries  evidence-emergent):\n\n{{code language=\"json\"}}\n{\n  \"backgroundDetails\": \"Article frames as conspiracy theory\",\n  \"claimBoundaries\": [\n    { \"name\": \"Peer-reviewed study evidence (Framework A)\" }\n  ]\n}\n{{/code}}\n\n=== Pitfall 3: Using Legacy Field Names ===\n\n**Problem** (v4.0-cb schema will reject):\n\n{{code language=\"typescript\"}}\nconst facts = result.facts;               // REMOVED (v3.x)\nconst ctx = understanding.analysisContext; // REMOVED (v4.0)\nconst ctxs = result.analysisContexts;      // REMOVED (v4.0)\n{{/code}}\n\n**Solution**:\n\n{{code language=\"typescript\"}}\nconst evidenceItems = result.evidenceItems;    // v4.0-cb\nconst background = result.backgroundDetails;   // v4.0-cb\nconst boundaries = result.claimBoundaries;     // v4.0-cb\n{{/code}}\n\n----\n\n== Validation Checklist ==\n\nUse this checklist when reviewing code that involves claims/evidence/boundaries:\n\n* Is \"scope\" qualified as ##ClaimAssessmentBoundary## or ##EvidenceScope##?\n* Do JSON field names use v4.0-cb names (##atomicClaims##, ##claimBoundaries##, ##evidenceItems##, ##backgroundDetails##)?\n* Does prompt include terminology glossary header with CB term definitions?\n* Are ##claimBoundaryId## values validated against ##claimBoundaries[]##?\n* Are fallbacks logged (not silent)?\n* Does ##EvidenceScope## capture source methodology (not artificially create new ##ClaimAssessmentBoundaries##)?\n* Is ##backgroundDetails## separate from ##claimBoundaries##?\n* Are evidence IDs using EV-prefix (##EV_001, EV_002, EV_003...##)?\n\n----\n\n== FAQ ==\n\n**Q: When should I use EvidenceScope vs ClaimAssessmentBoundary?**\n\nA: If the information describes **how a source document computed its data** (methodology, temporal bounds, geographic scope), it's ##EvidenceScope##  attached to an evidence item. ##ClaimAssessmentBoundary## is NOT manually assigned; it emerges from Stage 3 clustering of compatible EvidenceScopes. You never \"create\" a ClaimBoundary in code  the LLM creates them.\n\n**Q: Can an evidence item have BOTH claimBoundaryId AND evidenceScope?**\n\nA: Yes. ##claimBoundaryId## says **which ClaimAssessmentBoundary the evidence belongs to** (assigned during Stage 3 clustering), while ##evidenceScope## says **how the source computed the data** (assigned during Stage 2 extraction). They serve orthogonal purposes.\n\n**Q: Should prompts say \"ClaimBoundary\", \"ClaimAssessmentBoundary\", or \"Boundary\"?**\n\nA: Use \"ClaimBoundary\" for schema-facing precision (matches JSON field prefix), \"ClaimAssessmentBoundary\" for full clarity in documentation. \"Boundary\" is acceptable shorthand in prompts where context is clear. **NEVER use \"AnalysisContext\"** in new code or prompts  that term is from the removed Orchestrated pipeline.\n\n**Q: What if the LLM produces zero ClaimBoundaries during Stage 3?**\n\nA: This indicates no compatible EvidenceScopes were clustered. The pipeline has a safety net: if Stage 3 produces zero actionable boundaries, the highest-centrality claim is rescued to prevent silent empty results. First check research quality (Stage 2) and verify the ##BOUNDARY_CLUSTERING## prompt is receiving evidence items.\n\n----\n\n== Related Documentation ==\n\n* [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]  Runtime UCM prompt system for CB pipeline\n* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]]  Pipeline design, stage configuration, CB invariants\n* AGENTS.md  High-level rules for terminology and code standards\n* ##types.ts##  TypeScript interface definitions (AtomicClaim, ClaimAssessmentBoundary, EvidenceScope, EvidenceItem)\n\n----\n\n**Document Maintainer**: Lead Developer\n**Last Reviewed**: 2026-02-19 (Updated for v4.0.0-cb ClaimAssessmentBoundary pipeline)\n**Next Review**: 2026-05 (or after next major version)", "Product Development.Specification.Reference.WebHome": "= Reference =\n\nTechnical reference materials for FactHarbor including terminology, schemas, and prompt engineering guidelines.\n\n== Terminology ==\n\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]] - Authoritative glossary of FactHarbor terms (v3.1)\n\n== [[Data Models and Schemas>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.WebHome]] ==\n\n* [[LLM Schema Mapping>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.LLM Schema Mapping.WebHome]] - LLM response schema mappings\n* [[Metrics Schema>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.Metrics Schema.WebHome]] - System performance metrics schema\n\n== [[Prompt Engineering>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.WebHome]] ==\n\n* [[Provider-Specific Formatting>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.Provider-Specific Formatting.WebHome]] - Provider-specific prompt formatting rules\n* [[Prompt Guidelines>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.Prompt Guidelines.WebHome]] - General prompt engineering guidelines\n\n== Related ==\n\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]] - Core data model specification\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] - AI extraction layer\n", "Product Development.Specification.Review & Data Use.WebHome": "= Review & Data Use =\n\n== 1. User & Role Concepts ==\n\n* **READER**: Default role for anonymous visitors (guest). Can browse, search, and view published analyses.\n* **USER**: Registered role (requires login). Can submit URLs/text for analysis (rate-limited), flag issues, and view submission history.\n* **UCM_ADMINISTRATOR**: Appointed role. Manages UCM configuration (prompt templates, quality thresholds, model selection). All config changes are versioned and auditable.\n* **MODERATOR**: Appointed role. Handles abuse, spam, and harassment. Does NOT manage content quality (that is automated).\n* **TECHNICAL_USER**: Strictly technical identities (services, federation components, background jobs).\n* **FEDERATION_NODE**: Technical entity representing a participating node in the federation (future).\n\n== 2. UCM Configuration Actions ==\n\nThe system logs every significant configuration change:\n* **CONFIG_CHANGE** logs who changed which config, when, and why.\n* Fields: ##ConfigBlobHash##, ##ConfigType##, ##ChangedBy##, ##ChangeReason##, ##Timestamp##.\n* Every analysis job records the config snapshot used via **config_usage**.\n* Reports reference the exact UCM configuration state at time of analysis.\n\n== 3. User Roles ==\n\n=== 3.1 Human Roles ===\n{{include reference=\"FactHarbor.Product Development.Diagrams.Human User Roles.WebHome\"/}}\n\n=== 3.2 Technical Roles ===\n{{include reference=\"FactHarbor.Product Development.Diagrams.Technical and System Users.WebHome\"/}}\n\n== 4. User Class Diagram ==\n{{include reference=\"FactHarbor.Product Development.Diagrams.User Class Diagram.WebHome\"/}}\n", "Product Development.Specification.System-Performance-Metrics": "= System Performance Metrics =\n**What we monitor to ensure AKEL performs well.**\n== 1. Purpose ==\nThese metrics tell us:\n*  Is AKEL performing within acceptable ranges?\n*  Where should we focus improvement efforts?\n*  When do humans need to intervene?\n*  Are our changes improving things?\n**Principle**: Measure to improve, not to judge.\n== 2. Metric Categories ==\n=== 2.1 AKEL Performance ===\n**Processing speed and reliability**\n=== 2.2 Content Quality ===\n**Output quality and user satisfaction**\n=== 2.3 System Health ===\n**Infrastructure and operational metrics**\n=== 2.4 User Experience ===\n**How users interact with the system**\n== 3. AKEL Performance Metrics ==\n=== 3.1 Processing Time ===\n**Metric**: Time from claim submission to verdict publication\n**Measurements**:\n* P50 (median): 50% of claims processed within X seconds\n* P95: 95% of claims processed within Y seconds\n* P99: 99% of claims processed within Z seconds\n**Targets**:\n* P50:  12 seconds\n* P95:  18 seconds\n* P99:  25 seconds\n**Alert thresholds**:\n* P95 > 20 seconds: Monitor closely\n* P95 > 25 seconds: Investigate immediately\n* P95 > 30 seconds: Emergency - intervention required\n**Why it matters**: Slow processing = poor user experience\n**Improvement ideas**:\n* Optimize evidence extraction\n* Better caching\n* Parallel processing\n* Database query optimization\n=== 3.2 Success Rate ===\n**Metric**: % of claims successfully processed without errors\n**Target**:  99%\n**Alert thresholds**:\n* 98-99%: Monitor\n* 95-98%: Investigate\n* <95%: Emergency\n**Common failure causes**:\n* Timeout (evidence extraction took too long)\n* Parse error (claim text unparsable)\n* External API failure (source unavailable)\n* Resource exhaustion (memory/CPU)\n**Why it matters**: Errors frustrate users and reduce trust\n=== 3.3 Evidence Completeness ===\n**Metric**: % of claims where AKEL found sufficient evidence\n**Measurement**: Claims with 3 pieces of evidence from 2 distinct sources\n**Target**:  80%\n**Alert thresholds**:\n* 75-80%: Monitor\n* 70-75%: Investigate\n* <70%: Intervention needed\n**Why it matters**: Incomplete evidence = low confidence verdicts\n**Improvement ideas**:\n* Better search algorithms\n* More source integrations\n* Improved relevance scoring\n=== 3.4 Source Diversity ===\n**Metric**: Average number of distinct sources per claim\n**Target**:  3.0 sources per claim\n**Alert thresholds**:\n* 2.5-3.0: Monitor\n* 2.0-2.5: Investigate\n* <2.0: Intervention needed\n**Why it matters**: Multiple sources increase confidence and reduce bias\n=== 3.5 Scenario Coverage ===\n**Metric**: % of claims with at least one scenario extracted\n**Target**:  75%\n**Why it matters**: Scenarios provide context for verdicts\n== 4. Content Quality Metrics ==\n=== 4.1 Confidence Distribution ===\n**Metric**: Distribution of confidence scores across claims\n**Target**: Roughly normal distribution\n* ~10% very low confidence (0.0-0.3)\n* ~20% low confidence (0.3-0.5)\n* ~40% medium confidence (0.5-0.7)\n* ~20% high confidence (0.7-0.9)\n* ~10% very high confidence (0.9-1.0)\n**Alert thresholds**:\n* >30% very low confidence: Evidence extraction issues\n* >30% very high confidence: Too aggressive/overconfident\n* Heavily skewed distribution: Systematic bias\n**Why it matters**: Confidence should reflect actual uncertainty\n=== 4.2 Contradiction Rate ===\n**Metric**: % of claims with internal contradictions detected\n**Target**:  5%\n**Alert thresholds**:\n* 5-10%: Monitor\n* 10-15%: Investigate\n* >15%: Intervention needed\n**Why it matters**: High contradiction rate suggests poor evidence quality or logic errors\n=== 4.3 User Feedback Ratio ===\n**Metric**: Helpful vs unhelpful user ratings\n**Target**:  70% helpful\n**Alert thresholds**:\n* 60-70%: Monitor\n* 50-60%: Investigate\n* <50%: Emergency\n**Why it matters**: Direct measure of user satisfaction\n=== 4.4 False Positive/Negative Rate ===\n**Metric**: When humans review flagged items, how often was AKEL right?\n**Measurement**:\n* False positive: AKEL flagged for review, but actually fine\n* False negative: Missed something that should've been flagged\n**Target**:\n* False positive rate:  20%\n* False negative rate:  5%\n**Why it matters**: Balance between catching problems and not crying wolf\n== 5. System Health Metrics ==\n=== 5.1 Uptime ===\n**Metric**: % of time system is available and functional\n**Target**:  99.9% (less than 45 minutes downtime per month)\n**Alert**: Immediate notification on any downtime\n**Why it matters**: Users expect 24/7 availability\n=== 5.2 Error Rate ===\n**Metric**: Errors per 1000 requests\n**Target**:  1 error per 1000 requests (0.1%)\n**Alert thresholds**:\n* 1-5 per 1000: Monitor\n* 5-10 per 1000: Investigate\n* >10 per 1000: Emergency\n**Why it matters**: Errors disrupt user experience\n=== 5.3 Database Performance ===\n**Metrics**:\n* Query response time (P95)\n* Connection pool utilization\n* Slow query frequency\n**Targets**:\n* P95 query time:  50ms\n* Connection pool:  80% utilized\n* Slow queries (>1s):  10 per hour\n**Why it matters**: Database bottlenecks slow entire system\n=== 5.4 Cache Hit Rate ===\n**Metric**: % of requests served from cache vs. database\n**Target**:  80%\n**Why it matters**: Higher cache hit rate = faster responses, less DB load\n=== 5.5 Resource Utilization ===\n**Metrics**:\n* CPU utilization\n* Memory utilization\n* Disk I/O\n* Network bandwidth\n**Targets**:\n* Average CPU:  60%\n* Peak CPU:  85%\n* Memory:  80%\n* Disk I/O:  70%\n**Alert**: Any metric consistently >85%\n**Why it matters**: Headroom for traffic spikes, prevents resource exhaustion\n== 6. User Experience Metrics ==\n=== 6.1 Time to First Verdict ===\n**Metric**: Time from user submitting claim to seeing initial verdict\n**Target**:  15 seconds\n**Why it matters**: User perception of speed\n=== 6.2 Claim Submission Rate ===\n**Metric**: Claims submitted per day/hour\n**Monitoring**: Track trends, detect anomalies\n**Why it matters**: Understand usage patterns, capacity planning\n=== 6.3 User Retention ===\n**Metric**: % of users who return after first visit\n**Target**:  30% (1-week retention)\n**Why it matters**: Indicates system usefulness\n=== 6.4 Feature Usage ===\n**Metrics**:\n* % of users who explore evidence\n* % who check scenarios\n* % who view source track records\n**Why it matters**: Understand how users interact with system\n== 7. Metric Dashboard ==\n=== 7.1 Real-Time Dashboard ===\n**Always visible**:\n* Current processing time (P95)\n* Success rate (last hour)\n* Error rate (last hour)\n* System health status\n**Update frequency**: Every 30 seconds\n=== 7.2 Operational Dashboard ===\n**Reviewed regularly**:\n* All AKEL performance metrics\n* Content quality metrics\n* System health trends\n* User feedback summary\n=== 7.3 Trend Reports ===\n**Periodic review**:\n* Trends over time\n* Period-over-period comparisons\n* Improvement priorities\n* Outstanding issues\n=== 7.4 Strategic Reports ===\n**Comprehensive analysis**:\n* Long-term trends\n* Seasonal patterns\n* Strategic metrics\n* Goal progress\n== 8. Alert System ==\n=== 8.1 Alert Levels ===\n**Info**: Metric outside target, but within acceptable range\n* Action: Note in next review\n* Example: P95 processing time 19s (target 18s, acceptable <20s)\n**Warning**: Metric outside acceptable range\n* Action: Investigate within 24 hours\n* Example: Success rate 97% (acceptable >98%)\n**Critical**: Metric severely degraded\n* Action: Investigate immediately\n* Example: Error rate 2% (acceptable <0.5%)\n**Emergency**: System failure or severe degradation\n* Action: Page on-call, all hands\n* Example: Uptime <95%, P95 >30s\n=== 8.2 Alert Channels ===\n**Slack/Discord**: All alerts\n**Email**: Warning and above\n**SMS**: Critical and emergency only\n**PagerDuty**: Emergency only\n=== 8.3 On-Call Rotation ===\n**Technical Coordinator**: Primary on-call\n**Backup**: Designated team member\n**Responsibilities**:\n* Respond to alerts within SLA\n* Investigate and diagnose issues\n* Implement fixes or escalate\n* Document incidents\n== 9. Metric-Driven Improvement ==\n=== 9.1 Prioritization ===\n**Focus improvements on**:\n* Metrics furthest from target\n* Metrics with biggest user impact\n* Metrics easiest to improve\n* Strategic priorities\n=== 9.2 Success Criteria ===\n**Every improvement project should**:\n* Target specific metrics\n* Set concrete improvement goals\n* Measure before and after\n* Document learnings\n**Example**: \"Reduce P95 processing time from 20s to 16s by optimizing evidence extraction\"\n=== 9.3 A/B Testing ===\n**When feasible**:\n* Run two versions\n* Measure metric differences\n* Choose based on data\n* Roll out winner\n== 10. Bias and Fairness Metrics ==\n=== 10.1 Domain Balance ===\n**Metric**: Confidence distribution by domain\n**Target**: Similar distributions across domains\n**Alert**: One domain consistently much lower/higher confidence\n**Why it matters**: Ensure no systematic domain bias\n=== 10.2 Source Type Balance ===\n**Metric**: Evidence distribution by source type\n**Target**: Diverse source types represented\n**Alert**: Over-reliance on one source type\n**Why it matters**: Prevent source type bias\n=== 10.3 Geographic Balance ===\n**Metric**: Source geographic distribution\n**Target**: Multiple regions represented\n**Alert**: Over-concentration in one region\n**Why it matters**: Reduce geographic/cultural bias\n== 11. Experimental Metrics ==\n**New metrics to test**:\n* User engagement time\n* Evidence exploration depth\n* Cross-reference usage\n* Mobile vs desktop usage\n**Process**:\n1. Define metric hypothesis\n2. Implement tracking\n3. Collect data for 1 month\n4. Evaluate usefulness\n5. Add to standard set or discard\n== 12. Anti-Patterns ==\n**Don't**:\n*  Measure too many things (focus on what matters)\n*  Set unrealistic targets (demotivating)\n*  Ignore metrics when inconvenient\n*  Game metrics (destroys their value)\n*  Blame individuals for metric failures\n*  Let metrics become the goal (they're tools)\n**Do**:\n*  Focus on actionable metrics\n*  Set ambitious but achievable targets\n*  Respond to metric signals\n*  Continuously validate metrics still matter\n*  Use metrics for system improvement, not people evaluation\n*  Remember: metrics serve users, not the other way around\n== 13. Related Pages ==\n* [[Automation Philosophy>>FactHarbor.Organisation.Strategy.Automation Philosophy.WebHome]] - Why we monitor systems, not outputs\n* [[Continuous Improvement>>FactHarbor.Organisation.How-We-Work-Together.Continuous-Improvement]] - How we use metrics to improve\n* [[Governance>>FactHarbor.Organisation.Governance.WebHome]] - Periodic performance reviews\n---\n**Remember**: We measure the SYSTEM, not individual outputs. Metrics drive IMPROVEMENT, not judgment.", "Product Development.Specification.WebHome": "{{info title=\"Version 2.10.2\"}}\nThis document describes the **Specification** of FactHarbor. It is a working draft. See [[Project Status>>FactHarbor.Product Development.Planning.Project Status.WebHome]] for current development phase.\n{{/info}}\n\n= Specification =\n\nThis section defines the functional specification and technical architecture of FactHarbor. See also [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] (sibling section).\n\n== 1. Mission ==\n\n**FactHarbor brings clarity and transparency to a world full of unclear, controversial, and misleading information by shedding light on the context, assumptions, and evidence behind claims  empowering people to better understand and judge wisely.**\n\n== 2. Purpose ==\n\nModern society faces a deep informational crisis:\n\n* Misinformation spreads faster than corrections.\n* High-quality evidence is buried under noise.\n* Meanings shift depending on context  but this is rarely made explicit.\n* Users lack tools to understand *why* information conflicts.\n* Claims are evaluated without clearly defined assumptions.\n\nFactHarbor introduces structure, transparency, and evidence-based reasoning. It provides:\n\n* Automated claim extraction and evidence research.\n* Multiple analytical perspectives (ClaimAssessmentBoundaries) for ambiguous claims.\n* Transparent assumptions, definitions, and boundaries.\n* Full evidence provenance  every verdict traceable to sources.\n* Structured verdicts on a 7-point evidence scale (well-supported  refuted).\n* Immutable analysis outputs  quality improves through system improvements, not data edits.\n\n== 3. Core Concepts ==\n\n* **Claim**: A statement needing structured verification.\n* **ClaimAssessmentBoundary**: An evidence-emergent grouping of compatible EvidenceScopes  yielding a separate verdict per boundary.\n* **EvidenceItem**: A piece of evidence extracted from a source, with provenance and reliability scoring.\n* **Verdict**: An evidence-based rating on a 7-point scale, per claim and per ClaimAssessmentBoundary.\n* **AKEL**: The AI Knowledge Extraction Layer  FactHarbor's automated analysis engine.\n* **UCM**: Unified Configuration Management  versioned system configuration that controls analysis behaviour.\n* **Content States**: Published (passed quality gates, visible to all) or Hidden (moderated due to policy violations).\n\n== 4. How It Works ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Claim and Scenario Lifecycle (Overview).WebHome\"/}}\n\nFactHarbor follows an automated analysis lifecycle:\n\n1. **Submission**  User submits a URL or text. Claims are automatically extracted and normalised.\n2. **Analysis**  AKEL researches evidence from the web, evaluates source reliability, and generates structured verdicts.\n3. **Quality gates**  Automated gates validate claims (Gate 1) and assess verdict confidence (Gate 4).\n4. **Publication**  Results that pass quality gates are published with confidence scores, source quality indicators, and clear AI-generated labeling.\n5. **System improvement**  Quality improves through system changes (code, prompts, configuration)  analysis data is immutable.\n\n//No human review of individual analyses. Improve the system, not the data.//\n\n== 5. Reading Guide ==\n\nThis specification is organised from conceptual overview to implementation detail.\n\n=== Functional Specification ===\n\n//What the system does  for stakeholders, new team members, and anyone wanting to understand FactHarbor.//\n\n* [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]]  AI analysis engine: claim extraction, evidence research, verdict generation\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]  Functional requirements, user needs, roles & content states\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]  Entity relationships and target schema\n* [[Workflows>>FactHarbor.Product Development.Specification.Workflows.WebHome]]  Claim submission and analysis lifecycle\n* [[Automation>>FactHarbor.Product Development.Specification.Automation.WebHome]]  Automation philosophy and roadmap\n* [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]]  AI-powered source credibility evaluation: dual-model cross-check, 19-language support, 7-band scale (standalone service planned)\n\n=== Architecture ===\n\n//How it's built  for developers and architects.//\n\n* [[Architecture Overview>>FactHarbor.Product Development.Specification.Architecture.WebHome]]  System design, AKEL pipeline, quality & trust, storage, security\n* [[Architecture Deep Dives>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]]  Implementation details: pipeline variants, quality gates, source reliability, calculations\n\n=== Reference ===\n\n//Look-up material  terminology, schemas, examples.//\n\n* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]]  Authoritative glossary and field mappings\n* [[Data Models and Schemas>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.WebHome]]  Schema definitions and LLM response mappings\n* [[Prompt Engineering>>FactHarbor.Product Development.Specification.Reference.Prompt Engineering.WebHome]]  Prompt design guidelines and provider-specific formatting\n* [[FAQ>>FactHarbor.Product Development.Specification.FAQ.WebHome]]  Frequently asked questions\n* [[Data Examples>>FactHarbor.Product Development.Specification.Examples.WebHome]]  Sample data structures across the entity model\n* [[Review & Data Use>>FactHarbor.Product Development.Specification.Review & Data Use.WebHome]]  Data usage policies\n\n=== Diagrams ===\n\n//Visual index  all diagrams with links to their contextual pages.//\n\n* [[Diagrams>>FactHarbor.Product Development.Diagrams.WebHome]]  Architecture, pipeline, data model, workflow, and automation diagrams\n\n=== Future  After First Public Release ===\n\n//Planned capabilities not yet implemented. Included for architectural context.//\n\n* [[Federation & Decentralization>>FactHarbor.Product Development.Specification.Federation & Decentralization.WebHome]]  Cross-node knowledge alignment (planned for post-V1.0)\n\n=== POC Archive ===\n\n//Historical specifications and early trial reports from the POC development phase.//\n\n* [[Analysis Reports (Early Trials)>>FactHarbor.Product Development.Specification.FH Analysis Reports.WebHome]]  Sample analyses from POC development (v0.9.x), retained for reference and regression testing\n* [[POC Specification>>FactHarbor.Product Development.Specification.POC.WebHome]]  POC phase requirements, API contracts, and schemas\n", "Product Development.Specification.Workflows.WebHome": "= Workflows =\n\n{{info}}\n**Implementation Status:** Only the Claim Submission (Section 2) and Automated Analysis (Section 3) workflows are implemented in the current POC. Sections 4-11 describe the **target production architecture** (Beta/V1.0). See [[AKEL>>FactHarbor.Product Development.Specification.AI Knowledge Extraction Layer (AKEL).WebHome]] for the implemented pipeline.\n{{/info}}\n\nFactHarbor workflows are **simple, automated, focused on continuous improvement**.\n== 1. Core Principles ==\n* **Automated by default**: AI processes everything\n* **Publish immediately**: No centralized approval (removed in V0.9.50)\n* **Quality through monitoring**: Not gatekeeping\n* **Fix systems, not data**: Errors trigger improvements\n* **Human-in-loop**: Only for edge cases and abuse\n== 2. Claim Submission Workflow ==\n\n=== 2.1 Claim Extraction ===\n\nWhen registered users submit content (text, articles, web pages), FactHarbor first extracts individual verifiable claims:\n\n**Input Types:**\n* Single claim: \"The Earth is flat\"\n* Text with multiple claims: \"Climate change is accelerating. Sea levels rose 3mm in 2023. Arctic ice decreased 13% annually.\"\n* URLs: Web pages analyzed for factual claims\n\n**Extraction Process:**\n* LLM analyzes submitted content\n* Identifies distinct, verifiable factual claims\n* Separates claims from opinions, questions, or commentary\n* Each claim becomes independent for processing\n\n**Output:**\n* List of claims with context\n* Each claim assigned unique ID\n* Original context preserved for reference\n\nThis extraction ensures:\n* Each claim receives focused analysis\n* Multiple claims in one submission are all processed\n* Claims are properly isolated for independent verification\n* Context is preserved for accurate interpretation\n\n{{{\nUser submits  Duplicate detection  Categorization  Processing queue  User receives ID\n}}}\n**Timeline**: Seconds\n**No approval needed**\n\n== 2.5 Claim Analysis Workflow ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Claim and Scenario Workflow.WebHome\"/}}\n\n== 3. Automated Analysis Workflow ==\n{{{\nClaim from queue\n\nEvidence gathering (AKEL)\n\nSource evaluation (track record check)\n\nEvidence clustering + ClaimAssessmentBoundary formation\n\nVerdict synthesis\n\nRisk assessment\n\nQuality gates (confidence > 40%? risk < 80%?)\n\nPublish OR Flag for improvement\n}}}\n**Timeline**: 10-30 seconds\n**90%+ published automatically**\n== 3.5 Evidence and Verdict Workflow ==\n{{include reference=\"FactHarbor.Product Development.Diagrams.Evidence and Verdict Workflow.WebHome\"/}}\n== 4. Publication Workflow ==\n**Standard (90%+)**: Pass quality gates  Publish immediately with confidence scores\n**High Risk (<10%)**: Risk > 80%  Moderator review\n**Low Quality**: Confidence < 40%  Improvement queue  Re-process\n== 5. UCM Configuration Workflow ==\n{{{\nUCM Administrator updates config  New immutable blob created  Activated  Jobs reference new config  Quality monitored\n}}}\n**Analysis data is immutable**  quality improvements flow through UCM config changes\n**Every analysis job** records the UCM config snapshot used for reproducibility\n== 5.5 Quality and Audit Workflow ==\n\n{{include reference=\"FactHarbor.Product Development.Diagrams.Quality and Audit Workflow.WebHome\"/}}\n\n== 6. Flagging Workflow ==\n{{{\nUser flags issue  Categorize (abuse/quality)  Automated or manual resolution\n}}}\n**Quality issues**: Add to improvement queue  System fix  Auto re-process\n**Abuse**: Moderator review  Action taken\n== 7. Moderation Workflow ==\n**Automated pre-moderation**: 95% published automatically\n**Moderator queue**: Only high-risk or flagged content\n**Appeal process**: Different moderator  Governing Team if needed\n== 8. System Improvement Workflow ==\n**Improvement cycle**:\n{{{\nReview error patterns\nDevelop fixes\nTest improvements\nDeploy & re-process\nMonitor metrics\n}}}\n**Error capture**:\n{{{\nError detected  Categorize  Root cause  Improvement queue  Pattern analysis\n}}}\n**A/B Testing**:\n{{{\nNew algorithm  Split traffic (90% control, 10% test)  Run test period  Compare metrics  Deploy if better\n}}}\n== 9. Quality Monitoring Workflow ==\n**Continuous**: Calculate metrics, detect anomalies\n**Recurring**: Update source track records, aggregate error patterns\n**Periodic**: System improvement cycle, performance review\n== 10. Source Track Record Workflow ==\n**Initial score**: New source starts at 50 (neutral)\n**Recurring updates**: Calculate accuracy, correction frequency, update score\n**Continuous**: All claims using source recalculated when score changes\n== 11. Re-Processing Workflow ==\n**Triggers**: System improvement deployed, source score updated, new evidence, error fixed\n**Process**: Identify affected claims  Re-run AKEL  Compare  Update if better  Log change\n== 12. Related Pages ==\n* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]\n* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]]\n* [[Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]", "Product Development.TestReports.WebHome": "= Analysis Test Reports =\n\nArchived outputs from test runs. Reports capture the complete analysis result for a given input claim, including verdict, evidence items, ClaimAssessmentBoundary breakdowns, challenge responses, sources, and quality gate outcomes.\n\n\n----\n\n== Reports ==\n\n{{github-files repo=\"robertschaub/FactHarbor\" path=\"Docs/TESTREPORTS\" /}}\n\n{{info}}\n**HTML reports** open via [[htmlpreview.github.io>>https://htmlpreview.github.io/]] which renders static HTML files hosted on GitHub. PDF and JSON files open directly on GitHub.\n{{/info}}\n", "Product Development.WebHome": "= Product Development =\n\nEverything about what FactHarbor does and how it's built  from requirements and specification through planning and development tooling.\n\n== Sections ==\n\n* **[[Requirements>>FactHarbor.Product Development.Requirements.WebHome]]**  Functional requirements, user needs, and role definitions\n* **[[Specification>>FactHarbor.Product Development.Specification.WebHome]]**  Functional specification, technical architecture, data models, and implementation details\n* **[[Planning>>FactHarbor.Product Development.Planning.WebHome]]**  Development phases, project status, and the path from POC to public release\n* **[[DevOps>>FactHarbor.Product Development.DevOps.WebHome]]**  Guidelines, tooling, deployment, and subsystem guides for contributors\n", "WebHome": "= FactHarbor =\n\n(% class=\"wikigeneratedid\" %)\n(((\n(% class=\"box successmessage\" %)\n(((\n**Our Vision**\nA world where decisions and public debate are grounded in evidence so people can move forward with clarity and confidence.\n)))\n)))\n\n(((\n(% class=\"box infomessage\" style=\"background-color: #fde0e4; color: #b71c1c;\" %)\n(((\n**Our Mission**\nFactHarbor brings clarity and transparency to a world full of unclear, contested, and misleading information by shedding light on the context, assumptions, and evidence behind claims.\n)))\n)))\n\n(% class=\"box warningmessage\" %)\n(((\n**Non-profit and Transparent**\nFactHarbor is a **Non-Profit Organisation** and a strictly **Open-Source** project.\nWe serve the public interest with full transparency  no paywalls, no hidden algorithms, and no profit motive.\n)))\n\n{{info}}\n**Project Repository:** [[FactHarbor on GitHub>>https://github.com/robertschaub/FactHarbor]]\n{{/info}}\n\n== Why FactHarbor Exists ==\n\n**The Problem**\nWe live in an environment where information conflicts, misleading content spreads fast, and many people lack the time or tools to verify complex claims. Headlines, soundbites, and viral posts often win over careful reasoning, and fact-checks frequently reduce everything to a simple verdict without explaining the //why// behind it. The result is frustration, confusion, and growing distrust  not just in institutions, but in the very idea that complex questions can be assessed fairly.\n\n**Our Response**\nFactHarbor acts as a navigation system for complex claims. We dont just say true or false  we make assumptions, evidence, and context visible so you can form your own judgement. Instead of asking you to trust an authority, we show you how different conclusions are reached, where the evidence is strong or weak, and where reasonable people might still disagree.\n\n== From Claim to Conclusion  Reasoning You Can Trust Because You Can Inspect ==\n\n**What FactHarbor Does**\nFactHarbor helps people make sense of contested questions without stripping away nuance. Instead of chasing quick binary verdicts, we break topics into clear, interconnected claims. For each claim, we highlight the context and assumptions it depends on and link directly to the evidence that supports or challenges it.\n\n**Where It Helps**\nWhether youre analyzing public policy, science, or everyday decisions, FactHarbor provides a transparent way to compare perspectives. Our underlying model creates reusable claim maps that journalists, educators, and researchers can build on, while remaining accessible to anyone seeking a clear, honest overview.\n\n**Why Its Trustworthy**\nAt our core is a simple principle: reasoning must be as transparent as the result. Our rules for structuring claims and weighing evidence are documented in the open  designed to be reviewed, challenged, and improved by you. Trust comes not from authority, but from a process anyone can inspect.\n\n**AI as a Tool, Not a Judge**\nAI does the heavy lifting: searching the web for evidence, extracting testable claims, assessing sources by track record, actively seeking contradicting evidence, and assembling structured results with confidence scores. It works fast, at scale, and consistently  but it does not make decisions. It follows human-defined rules and policies, and every step of its reasoning is documented and auditable. The output is always a transparent evidence landscape, not a hidden verdict.\n\n**Humans Define the Rules, Users Form Their Own Judgement**\nHumans design and refine the methodology  the rules, prompts, and policies that govern how AI searches, evaluates, and presents evidence. When results fall short, humans improve the system itself, not individual outputs. Fix the process, not the data. This keeps the platform scalable, unbiased, and accountable. Ultimately, FactHarbor presents evidence and reasoning  the judgement is yours.\n\n== How It Works: The Core Concepts ==\n\nFactHarbor structures reasoning into transparent, inspectable steps:\n\n* **Claims**  The system extracts the key assertions from your input and identifies what actually needs to be verified. Related statements are grouped together to avoid duplication and keep the analysis focused.\n* **Analytical Contexts**  A claim might be well-supported in one context but contradicted in another. FactHarbor identifies the different perspectives, assumptions, and definitions that shape how a claim should be evaluated  and analyzes each one independently.\n* **Evidence**  AI searches across fact-checkers, academic studies, reports, and the open web. Each piece of evidence is assessed for quality, relevance, and linked to the specific context it applies to  not just to the claim in general.\n* **Source Reliability**  Every source is scored for credibility using multi-model AI evaluation. Track record matters: a peer-reviewed study carries different weight than a social media post.\n* **Verdicts**  For each analytical perspective, FactHarbor produces an evidence-based rating (e.g., \"Well Supported\", \"Mixed Evidence\", \"Contradicted\") with explicit confidence levels and full reasoning.\n* **Evidence Landscape**  The result is not a single label, but a multi-perspective view showing where a claim is well-supported, where it's contradicted, and where reasonable people may still disagree  with every conclusion traceable back to its sources.\n\n== The Lifecycle: From Input to Verdict ==\n\nData in FactHarbor flows through a structured, auditable process:\n\n* **Submission:** Registered users submit text or URLs; the system normalises the input.\n* **Evidence Research:** AI extracts AtomicClaims and iteratively researches evidence per claim.\n* **Evidence Handling:** Evidence is retrieved, assessed for quality, and linked.\n* **Verdict Creation:** Generating per-boundary, per-claim verdicts via structured debate.\n* **Public Presentation:** The evidence landscape  multi-perspective results with full provenance  is published for users to explore.\n\n== Explore FactHarbor ==\n\n=== [[About FactHarbor>>FactHarbor.About FactHarbor.WebHome]] ===\n\n**//Who We Are.//**\nThe story behind FactHarbor, our mission, and the people driving it.\n\n=== [[Organisation>>FactHarbor.Organisation.WebHome]] ===\n\n**//How We Operate.//**\nGovernance, decision-making, team structure, and how to get involved as a contributor.\n\n=== [[Product Development>>FactHarbor.Product Development.WebHome]] ===\n\n**//The Full Picture.//**\nRequirements, specification, architecture, planning, and development tooling  from what the system does to how it's built and run.\n\n=== [[Analysis Test Reports>>FactHarbor.Product Development.TestReports.WebHome]] ===\n\n**//See It in Action.//**\nArchived analysis outputs from pipeline test runs  full HTML reports, raw JSON payloads, and PDF snapshots. Browse real verdicts, evidence, and quality gate results.\n\n=== [[License and Disclaimer>>FactHarbor.License and Disclaimer.WebHome]] ===\n\n**//Open Source, Open Data.//**\nMulti-license model (MIT, AGPL-3.0, CC BY-SA 4.0), usage terms, and attribution requirements.\n\n=== [[Media Source Database>>FactHarbor.Product Development.Media Source Database.WebHome]] ===\n\n**//AI-Powered Source Credibility.//**\nAn open, transparent credibility database for every media source on the internet  dual-AI cross-check, 19-language support, and a 7-band scale. Currently powering FactHarbor's analysis; planned as a standalone public service and API."}}