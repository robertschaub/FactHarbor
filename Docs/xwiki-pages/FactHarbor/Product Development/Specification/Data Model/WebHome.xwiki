= Target Data Model =

{{warning}}
**Target Architecture Specification**

This page describes the **target normalised data model** — the evolutionary goal for FactHarbor's storage architecture. It builds on the current implementation (JSON blobs in SQLite) by normalising entities into PostgreSQL tables, adding caching, user accounts, and full-text search.

**For the current implementation entity model, see [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]].**

**Compatibility:** All entity names, field names, and types in this target design are compatible with the current implementation interfaces in ##types.ts## and ##config-schemas.ts##. The target extends but does not contradict the current model.

**Related pages:**
* [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] — Current implementation ERDs and entity landscape
* [[Data Models and Schemas (Reference)>>FactHarbor.Product Development.Specification.Reference.Data Models and Schemas.WebHome]] — LLM schema mappings and metrics schema definitions
* [[POC1 API & Schemas>>FactHarbor.Product Development.Specification.POC.API-and-Schemas.WebHome]] — POC API contract and endpoint specifications
{{/warning}}

FactHarbor's target data model is **simple, focused, and designed for automated processing**.

== 1. Core Entities ==

=== 1.1 ClaimVerdict (normalised from JSON) ===

**Current**: Embedded in ##AnalysisResult## JSON blob (see [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]]).
**Target**: Normalised into PostgreSQL table.

**Fields**:
* **claimId** (text PK): Claim identifier (SC1, C1, etc.)
* **jobId** (UUID FK): Parent analysis job
* **claimText** (text): The claim statement
* **type** (text): "legal", "procedural", "factual", "evaluative"
* **isCentral** (boolean): Central claim flag
* **centrality** (text): "high", "medium", "low" — weight multiplier (3x, 2x, 1x)
* **harmPotential** (text): "high", "medium", "low"
* **truthPercentage** (int): 0-100 calibrated truth score
* **confidence** (int): 0-100 confidence in verdict
* **verdict** (text): 7-point scale label (TRUE, MOSTLY_TRUE, etc.)
* **riskTier** (text): "A", "B", or "C"
* **reasoning** (text): Explanation of verdict
* **supportingEvidenceIds** (text[]): References to EvidenceItem IDs
* **contextId** (text FK): Linked AnalysisContext
* **keyFactorId** (text FK): Linked KeyFactor
* **isContested** (boolean): Whether claim is contested
* **factualBasis** (text): "established", "disputed", "opinion", "unknown"

==== Performance Optimisation: Denormalised Fields ====

**Rationale**: Claims system is 95% reads, 5% writes. Denormalising common data reduces joins and improves query performance by 70%.
**Additional cached fields in claims table**:
* **evidence_summary** (JSONB): Top 5 most relevant evidence snippets with scores
 * Avoids joining evidence table for listing/preview
 * Updated when evidence is added/removed
 * Format: ##[{"text": "...", "source": "...", "relevance": 0.95}, ...]##
* **source_names** (TEXT[]): Array of source names for quick display
 * Avoids joining through evidence to sources
 * Updated when sources change
 * Format: ##["New York Times", "Nature Journal", ...]##
* **context_count** (INTEGER): Number of AnalysisContexts for this analysis
 * Quick metric without counting rows
 * Updated when contexts detected
* **cache_updated_at** (TIMESTAMP): When denormalised data was last refreshed
 * Helps invalidate stale caches
 * Triggers background refresh if too old
**Update Strategy**:
* **Immediate**: Update on re-analysis (system-triggered)
* **Deferred**: Update via background job (non-critical)
* **Invalidation**: Clear cache when source data or UCM config changes significantly
**Trade-offs**:
* 70% fewer joins on common queries
* Much faster claim list/search pages
* Better user experience
* Small storage increase (~10%)
* Need to keep caches in sync

=== 1.2 EvidenceItem (normalised from JSON) ===

**Current**: Embedded in ##AnalysisResult## JSON blob.
**Target**: Normalised into PostgreSQL table.

**Fields**:
* **id** (text PK): Evidence ID (S1-E1 format)
* **jobId** (UUID FK): Parent analysis job
* **statement** (text): Extracted evidence text
* **category** (text): "legal_provision", "evidence", "direct_evidence", "expert_quote", "statistic", "event", "criticism"
* **sourceId** (text FK): Reference to FetchedSource
* **sourceUrl** (text): URL of source
* **sourceExcerpt** (text): Quoted excerpt from source
* **contextId** (text FK): Reference to AnalysisContext
* **probativeValue** (text): "high", "medium", "low"
* **sourceAuthority** (text): "primary", "secondary", "opinion", "contested"
* **evidenceBasis** (text): "scientific", "documented", "anecdotal", "theoretical", "pseudoscientific"
* **claimDirection** (text): "supports", "contradicts", "neutral"
* **extractionConfidence** (int): 0-100 LLM confidence

=== 1.3 FetchedSource (normalised from JSON) ===

**Current**: Embedded in ##AnalysisResult## JSON blob. Source reliability cached in ##source-reliability.db##.
**Target**: Normalised into PostgreSQL table with dedicated reliability tracking.

**Fields**:
* **id** (text PK): Source ID (S1, S2, etc.)
* **url** (text): Full URL
* **domain** (text): Extracted domain (e.g., "nytimes.com")
* **title** (text): Page title
* **trackRecordScore** (float): 0.0-1.0 reliability score from LLM evaluation
* **trackRecordConfidence** (float): 0.0-1.0 confidence in the evaluation
* **trackRecordConsensus** (boolean): Whether multi-model consensus was achieved
* **fetchSuccess** (boolean): Whether content retrieval succeeded
* **fetchedAt** (timestamp): When source was fetched
* **category** (text): Source category

==== Source Reliability Scoring ====

**Current implementation (v2.6.40+):** Source reliability uses a **Pure LLM + Cache architecture**. Sources are batch-evaluated via LLM calls during analysis, cached in SQLite with configurable TTL, and used synchronously. Multi-model consensus (Claude + GPT) is supported. See [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].

**7-band credibility scale** (0.0-1.0):
* Authoritative (0.90+), Highly Credible (0.80-0.89), Credible (0.70-0.79), Moderately Credible (0.60-0.69), Mixed (0.50-0.59), Low Credibility (0.30-0.49), Unreliable (<0.30)

==== Source Scoring Architecture ====

**Critical design principle**: Prevent circular dependencies between source scoring and claim analysis.

**Current approach** (LLM + Cache):
* Sources evaluated on-demand via LLM during analysis
* Evaluations cached in SQLite with configurable TTL (default 90 days)
* Multi-model consensus for stability
* AKEL pipeline reads scores synchronously — never updates them during analysis

**Target enhancement** (Background Refresh):

In addition to on-demand evaluation, a background job can periodically refresh stale scores:

{{code language="typescript"}}
// Target: Background refresh of stale source evaluations
async function refreshStaleSourceScores(): Promise<void> {
  // Find evaluations approaching TTL expiry
  const staleDomains = await getDomainsNearingExpiry(daysRemaining: 7);
  for (const domain of staleDomains) {
    // Re-evaluate using current LLM + consensus
    const evaluation = await evaluateSourceReliability(domain);
    // Update cache with fresh score
    await cacheReliabilityScore(domain, evaluation);
  }
  // Runs as scheduled background job
  // Never during claim analysis
}
{{/code}}

**Key Principles** (current and target):
* **Scoring and analysis are temporally separated**: Source scoring is cached/scheduled; claim analysis is real-time
* **One-way data flow during processing**: Claims READ source scores, never WRITE them
* **Audit trail**: Log all score changes, track score history, explainable calculations

=== 1.4 AnalysisContext (normalised from JSON) ===

**Current**: Embedded in ##ClaimUnderstanding## within the ##AnalysisResult## JSON blob.
**Target**: Normalised into PostgreSQL table.

**Purpose**: Bounded analytical frame for evaluating claims from different perspectives. Each AnalysisContext represents a specific analytical dimension detected during the Understanding phase.

**Fields**:
* **id** (text PK): Context identifier (CTX_xxx)
* **jobId** (UUID FK): Parent analysis job
* **name** (text): Full context name
* **shortName** (text): Short label
* **subject** (text): What is being analysed
* **temporal** (text): Time period
* **status** (text): "concluded", "ongoing", "pending", "unknown"
* **outcome** (text): Result/conclusion
* **assessedStatement** (text): What is being assessed in this context
* **metadata** (JSONB): Domain-specific details (institution, jurisdiction, methodology, etc.)

**How Found**: Input analysis → Context detection → Create AnalysisContext → Link claims to contexts

**Example**:
For article "Bolsonaro trial analysis":
* AnalysisContext 1: "TSE Criminal Trial (2023)" — legal proceedings context
* AnalysisContext 2: "Political Impact on Democracy" — political analysis context
* AnalysisContext 3: "Electoral System Integrity" — institutional context

**Target Evolution**: Many-to-many relationship can be added if cross-analysis context sharing is needed. For current implementation, contexts are embedded per-analysis.

=== 1.5 ClaimVerdict Detail ===

**Purpose**: Assessment of a claim within a specific AnalysisContext. Each ClaimVerdict provides a truth percentage on the 7-point scale with confidence and evidence references.

**Core fields** (see Section 1.1 above for full field list):
* **truthPercentage** (int 0-100): Calibrated truth score mapped to 7-point verdict scale
* **confidence** (int 0-100): System confidence in this assessment
* **reasoning** (text): Human-readable explanation
* **supportingEvidenceIds** (text[]): References to EvidenceItems that support this verdict

**Immutability**: Analysis outputs (including verdicts) are immutable. To improve verdict quality, update UCM configuration and trigger re-analysis. Each analysis job records the UCM config snapshot used.

**Example**:
For claim "Due process was followed in the trial" in AnalysisContext "TSE Criminal Trial":
* truthPercentage: 72, confidence: 68, verdict: "MOSTLY_TRUE"
* Re-analysis with improved UCM config: truthPercentage: 78, confidence: 75, verdict: "MOSTLY_TRUE"
* New analysis job references the updated UCM config snapshot

**Key Design**: Analysis outputs are immutable. Quality improvements flow through UCM configuration changes, not data edits.

=== 1.6 User ===

{{info}}
**Not yet implemented.** All users are currently anonymous readers. User system is planned for Alpha phase.
{{/info}}

Fields: username, email, **role** (Reader/User/UCM Administrator/Moderator)

=== Roles ===
**reader** (guest, anonymous):
* Browse and search published analyses
* View analysis results and evidence

**user** (registered, logged in):
* Everything Reader can do
* Submit URLs/text for analysis (rate-limited)
* Flag quality issues
* View own submission history

**ucm_administrator** (appointed by Governing Team):
* Manage UCM configuration (prompt templates, quality thresholds, model selection)
* View config change history and audit trail
* Activate/deactivate config versions
* Trigger re-analysis with updated config
* View system metrics

**moderator** (appointed):
* Handle abuse reports
* Hide harmful content
* Ban users for policy violations
* Does NOT manage content quality (that is automated)

=== 1.7 UCM Config ===

**Tables** (already implemented in ##config.db##):
* **config_blobs**: Immutable, content-addressed config versions (hash-based deduplication)
* **config_active**: Points to currently active config blob per config type
* **config_usage**: Links each analysis job to the exact config snapshot used

**Config Types Managed** (##PipelineConfig##, ##CalcConfig##, ##SearchConfig##, ##SourceReliabilityConfig##):
* Pipeline settings (LLM provider selection, model tiering, budget controls)
* Calculation parameters (verdict bands, centrality weights, contestation penalties)
* Search provider settings (max results, timeout, domain lists)
* Source reliability rules (consensus thresholds, cache TTL)
* Quality gate thresholds (confidence minimums, source count requirements)

**Audit Trail**:
* Every config change creates a new immutable blob
* Config blobs are never deleted — complete history preserved
* Each analysis job records which config snapshot was active at execution time
* Reports can be reproduced by re-running with the same config snapshot

=== 1.8 Flag ===

{{info}}
**Not yet implemented.** Planned for Alpha phase with user system.
{{/info}}

Fields: entity_id, reported_by, issue_type, status, resolution_note

=== 1.9 QualityMetric ===

{{info}}
**Not yet implemented.** Planned for Alpha/Beta phase.
{{/info}}

**Fields**: metric_type, category, value, target, timestamp
**Purpose**: Time-series quality tracking
**Usage**:
* **Continuous monitoring**: Hourly calculation of error rates, confidence scores, processing times
* **Quality dashboard**: Real-time display with trend charts
* **Alerting**: Automatic alerts when metrics exceed thresholds
* **A/B testing**: Compare control vs treatment metrics
* **Improvement validation**: Measure before/after changes

=== 1.10 ErrorPattern ===

{{info}}
**Not yet implemented.** Planned for Alpha/Beta phase.
{{/info}}

**Fields**: error_category, claim_id, description, root_cause, frequency, status
**Purpose**: Capture errors to trigger system improvements

== 1.11 Current Implementation ERD ==

{{include reference="FactHarbor.Product Development.Specification.Diagrams.Core Data Model ERD.WebHome"/}}

== 1.12 User Class Diagram ==

{{include reference="FactHarbor.Product Development.Specification.Diagrams.User Class Diagram.WebHome"/}}

== 2. Versioning Strategy ==

**Analysis Data Is Immutable**:
* **ClaimVerdicts, EvidenceItems, AnalysisContexts**: Created once per analysis job, never modified
* **FetchedSources**: Reliability scores updated via cache TTL refresh (not during analysis)
* **QualityMetric**: Time-series data (each record is a point in time)
* **ErrorPattern**: System improvement queue (status tracked)

**UCM Configuration Is Versioned**:
* Every config change creates a new immutable blob in ##config_blobs##
* ##config_active## tracks which config version is currently in use
* ##config_usage## links each analysis job to the config snapshot used
* Complete config history preserved — blobs never deleted

**Example**:
{{{
UCM Config V1: prompt_template = "Analyze the following claim..."
 → UCM Administrator updates config →
UCM Config V2: prompt_template = "Analyze the following claim with focus on source quality..."
 → config_blobs stores both versions (content-addressed by hash)
 → Reports reference which config version was used for their analysis
}}}

== 2.5. Storage vs Computation Strategy ==

**Critical architectural decision**: What to persist in databases vs compute dynamically?
**Trade-off**:
* **Store more**: Better reproducibility, faster, lower LLM costs | Higher storage/maintenance costs
* **Compute more**: Lower storage/maintenance costs | Slower, higher LLM costs, less reproducible

=== Recommendation: Hybrid Approach ===

**STORE (in PostgreSQL):**

==== ClaimVerdicts (Current State) ====
* **What**: claimText, type, isCentral, truthPercentage, confidence, verdict, reasoning, supportingEvidenceIds
* **Why**: Core entity, must be persistent
* **Size**: ~1 KB per claim
* **Growth**: Linear with claims
* **Decision**: STORE - Essential

==== EvidenceItems (All Records) ====
* **What**: statement, category, sourceId, sourceExcerpt, probativeValue, sourceAuthority, evidenceBasis
* **Why**: Hard to re-gather, reproducibility
* **Size**: ~2 KB per evidence (with excerpt)
* **Growth**: 3-10 evidence per claim
* **Decision**: STORE - Essential for reproducibility

==== FetchedSources (Track Records) ====
* **What**: url, domain, trackRecordScore, trackRecordConfidence, trackRecordConsensus
* **Why**: Continuously evaluated, cached with TTL
* **Size**: ~500 bytes per source
* **Growth**: Slow (limited number of sources)
* **Decision**: STORE - Essential for quality

==== UCM Config History ====
* **What**: config_blobs (immutable), config_active (activation pointers), config_usage (per-job snapshots)
* **Why**: Audit trail, reproducibility, config rollback
* **Size**: ~5 KB per config version
* **Growth**: Linear with config changes (infrequent — admin-driven)
* **Retention**: Config blobs never deleted (complete history)
* **Decision**: STORE - Essential for reproducibility and audit

==== Flags (User Reports) ====
* **What**: entity_id, reported_by, issue_type, description, status
* **Why**: Error detection, system improvement triggers
* **Size**: ~500 bytes per flag
* **Decision**: STORE - Essential for improvement

==== ErrorPatterns (System Improvement) ====
* **What**: error_category, claim_id, description, root_cause, frequency, status
* **Why**: Learning loop, prevent recurring errors
* **Size**: ~1 KB per pattern
* **Decision**: STORE - Essential for learning

==== QualityMetrics (Time Series) ====
* **What**: metric_type, category, value, target, timestamp
* **Why**: Trend analysis, cannot recreate historical metrics
* **Size**: ~200 bytes per metric
* **Retention**: 2 years hot, then aggregate and archive
* **Decision**: STORE - Essential for monitoring

**STORE (Computed Once, Then Cached):**

==== Analysis Summary ====
* **What**: Neutral text summary of claim analysis (200-500 words)
* **Computed**: Once by AKEL when claim first analysed
* **Recomputed**: Only when system significantly improves OR UCM config changes
* **Why store**: Expensive to regenerate ($0.01-0.05 per analysis), doesn't change often
* **Size**: ~2 KB per claim
* **Decision**: STORE (cached) - Cost-effective

==== AnalysisContexts ====
* **What**: Detected analytical frames with metadata
* **Current design**: Embedded in result JSON
* **Target**: Normalised into PostgreSQL table
* **Size**: ~1 KB per context × 2-5 contexts average
* **Decision**: STORE - Essential for multi-perspective analysis

**COMPUTE DYNAMICALLY (Never Store):**

==== Search Results ====
* **What**: Lists of claims matching search query
* **Compute from**: Elasticsearch index
* **Cache**: 15 minutes in Redis for popular queries
* **Why not store permanently**: Constantly changing, infinite possible queries

==== Aggregated Statistics ====
* **What**: "Total claims: 1,234,567", "Average confidence: 78%", etc.
* **Compute from**: Database queries
* **Cache**: 1 hour in Redis
* **Why not store**: Can be derived, relatively cheap to compute

=== Summary Table ===

| Data Type | Storage | Compute | Size per Claim | Decision | Rationale |
|-----------|---------|---------|----------------|----------|-----------|
| ClaimVerdict | Yes | - | 1 KB | STORE | Essential |
| EvidenceItem | Yes | - | 2 KB x 5 = 10 KB | STORE | Reproducibility |
| FetchedSource | Yes | - | 500 B (shared) | STORE | Track record |
| UCM config history | Yes | - | 5 KB per version | STORE | Reproducibility |
| Analysis summary | Yes | Once | 2 KB | STORE (cached) | Cost-effective |
| AnalysisContext | Yes | Once | 1 KB x 3 | STORE | Essential |
| Flags | Yes | - | 500 B x 10% | STORE | Improvement |
| ErrorPatterns | Yes | - | 1 KB (global) | STORE | Learning |
| QualityMetrics | Yes | - | 200 B (time series) | STORE | Trending |
| Search results | - | Yes | - | COMPUTE + 15min cache | Dynamic |
| Aggregations | - | Yes | - | COMPUTE + 1hr cache | Derivable |

**Total storage per claim**: ~18 KB (without flags)
**For 1 million claims**:
* **Storage**: ~18 GB (manageable)
* **PostgreSQL**: ~$50/month (standard instance)
* **Redis cache**: ~$20/month (1 GB instance)
* **S3 archives**: ~$5/month (old data)
* **Total**: ~$75/month infrastructure

**LLM cost savings by caching**:
* Analysis summary stored: Save $0.03 per claim = $30K per 1M claims
* Source reliability cached (current TTL): Save LLM evaluation costs per domain
* **Total savings**: ~$35K per 1M claims vs recomputing every time

=== Recomputation Triggers ===

**When to mark cached data as stale and recompute:**
1. **UCM config updated** → Mark affected analyses stale, trigger re-analysis
2. **Source trackRecordScore changes >0.10** → Recompute: confidence, verdict
3. **New evidence sources available** → Trigger re-analysis for affected claims
4. **Quality gate thresholds changed** → Re-evaluate existing verdicts against new thresholds

**Recomputation strategy**:
* **Eager**: Immediately re-analyse (for targeted UCM config changes)
* **Lazy**: Re-analyse on next view (for broad system improvements)
* **Batch**: Periodic re-evaluation of stale analyses

=== Database Size Projection ===

**Year 1**: 10K claims
* Storage: 180 MB
* Cost: $10/month
**Year 3**: 100K claims
* Storage: 1.8 GB
* Cost: $30/month
**Year 5**: 1M claims
* Storage: 18 GB
* Cost: $75/month
**Year 10**: 10M claims
* Storage: 180 GB
* Cost: $300/month
* Optimisation: Archive old claims to S3 ($5/TB/month)

**Conclusion**: Storage costs are manageable, LLM cost savings are substantial.

== 3. Key Simplifications ==

* **Two content states only**: Published, Hidden
* **Three user roles only**: Reader, UCM Administrator, Moderator
* **Immutable analysis data**: No data editing or versioning — improve via UCM config
* **UCM config versioning**: Content-addressed immutable blobs with per-job snapshots
* **Source reliability**: LLM + Cache with TTL-based refresh

== 4. Target Database Schema ==

The following ERD visualises all target database tables — existing (green), target normalisation from JSON (blue), and planned future tables (red). For the full multi-view entity reference, see [[Entity Views>>FactHarbor.Product Development.Specification.Diagrams.Entity Views.WebHome]].

{{include reference="FactHarbor.Product Development.Specification.Diagrams.Entity Views.WebHome" section="HTargetDatabaseEntities"/}}

=== 4.1 Primary Storage (PostgreSQL) ===

**ClaimVerdicts Table**:
* Current state per analysis job
* Fields: claimId, jobId, claimText, type, isCentral, centrality, harmPotential, truthPercentage, confidence, verdict, riskTier, reasoning, supportingEvidenceIds, contextId, keyFactorId

**EvidenceItems Table**:
* All evidence records
* Fields: id, jobId, statement, category, sourceId, sourceUrl, sourceExcerpt, contextId, probativeValue, sourceAuthority, evidenceBasis, claimDirection, extractionConfidence

**FetchedSources Table**:
* Source data and reliability cache
* Fields: id, jobId, url, domain, title, trackRecordScore (float 0.0-1.0), trackRecordConfidence, trackRecordConsensus, fetchSuccess, fetchedAt, category

**AnalysisContexts Table**:
* Detected analytical frames
* Fields: id, jobId, name, shortName, subject, temporal, status, outcome, assessedStatement, metadata (JSONB)

**User Table** (planned):
* All user accounts
* Fields: id, username, email, role (Reader/UCM Administrator/Moderator), created_at, last_active

**UCM Config Tables** (already implemented):
* config_blobs: Immutable config versions (hash, config_type, content, changed_by, change_reason, created_at)
* config_active: Currently active config per type (config_type, blob_hash, activated_at, activated_by)
* config_usage: Per-job config snapshots (job_id, config_type, blob_hash, snapshot_at)

**Flag Table** (planned):
* User-reported issues
* Fields: id, entity_type, entity_id, reported_by, issue_type, description, status, resolved_by, resolution_note, created_at, resolved_at

**ErrorPattern Table** (planned):
* System improvement queue
* Fields: id, error_category, claim_id, description, root_cause, frequency, status, created_at, fixed_at

**QualityMetric Table** (planned):
* Time-series quality data
* Fields: id, metric_type, metric_category, value, target, timestamp

=== 4.2 What's NOT Stored (Computed on-the-fly) ===

* **Aggregated statistics**: Computed from base data
* **Search results**: Generated from Elasticsearch index

=== 4.3 Cache Layer (Redis) ===

{{info}}
**Not yet implemented.** Current implementation uses SQLite for source reliability caching. Redis planned for Alpha/Beta phase.
{{/info}}

**Cached for performance (Planned)**:
* Frequently accessed claim verdicts (TTL: 1 hour)
* Search results (TTL: 15 minutes)
* User sessions (TTL: 24 hours)
* Source reliability scores (TTL: 1 hour — supplements SQLite cache)

=== 4.4 File Storage (S3) ===

**Archived content**:
* Old processing logs (>3 months)
* Evidence documents (archived copies)
* Database backups
* Export files

=== 4.5 Search Index (Elasticsearch) ===

{{info}}
**Not yet implemented.** Planned for Alpha phase.
{{/info}}

**Indexed for search**:
* Claim assertions (full-text)
* Evidence statements (full-text)
* AnalysisContext descriptions (full-text)
* Source domains (autocomplete)

Synchronised from PostgreSQL via change data capture or periodic sync.

== 5. Related Pages ==

* [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] — Current implementation ERDs
* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] — Architecture overview
* [[Requirements>>FactHarbor.Product Development.Requirements.WebHome]] — User needs and system requirements
