= Provider-Specific Prompt Formatting =

**Version**: 2.8.0 (Optimized - estimated 20-30% token reduction)
**Date**: 2026-02-03
**Status**: Implemented
**Related**: Evidence Quality Filtering

----

== Table of Contents ==

1. Introduction
1. Architecture Overview
1. Provider-Specific Optimizations
1. Prompt Composition Strategy
1. Configuration Adaptations
1. Testing and Validation

----

== 1. Introduction ==

=== Purpose ===

FactHarbor's v2.8 prompt architecture uses **provider-specific formatting** to maximize performance across different LLM providers. Each provider (Anthropic, OpenAI, Google, Mistral) has unique strengths and preferred prompt structures.

=== Problem Statement ===

Generic prompts perform sub-optimally across providers:
* Claude excels with XML-structured prompts
* GPT-4 prefers markdown with clear headings
* Gemini benefits from example-heavy prompts
* Mistral requires explicit reasoning guidance

A one-size-fits-all approach leaves significant performance gains untapped.

=== Solution ===

**Dynamic prompt composition** with provider-specific variants:

{{code language="typescript"}}
// Base prompt (universal logic)
const basePrompt = getExtractEvidenceBasePrompt();

// Provider variant (format optimization)
const providerVariant = getAnthropicExtractEvidenceVariant();  // or OpenAI, Gemini, Mistral

// Config adaptation (tiering, knowledge mode)
const configAdaptation = getTieringExtractEvidenceAdaptation(tier);

// Final composed prompt
const finalPrompt = basePrompt + providerVariant + configAdaptation;
{{/code}}

=== v2.8.0 Optimization Improvements (Feb 2026) ===

**Token Reduction**: All provider variants and base prompts optimized for an estimated 20-30% token reduction while maintaining quality.

**Changes Applied**:
* **Provider variants**: Removed attribution duplication, condensed examples (estimated ~15-20% reduction)
* **Base prompts**: Inlined terminology, simplified guidance (estimated ~5-10% additional reduction)
* **Impact**: Estimated API cost reduction of 20-30% across all providers automatically

**Provider Variant Lengths** (approximate, post-optimization):
* **Anthropic variants**: ~400-600 tokens each (down from ~600-800 tokens)
* **OpenAI variants**: ~350-550 tokens each (down from ~500-700 tokens)
* **Google variants**: ~450-650 tokens each (down from ~600-800 tokens)
* **Mistral variants**: ~400-600 tokens each (down from ~550-750 tokens)

**Quality**: Validated via manual review and build verification. All critical guidance preserved (death = HIGH centrality, attribution = LOW, multi-context rules intact).

See Prompt Architecture v2.8.0 for detailed changes.

----

== 2. Architecture Overview ==

{{info}}
**Testing Harness Only** â€” The ##prompt-builder.ts## system and the TypeScript prompt modules under ##apps/web/src/lib/analyzer/prompts/## are used exclusively by the testing harness (##prompt-testing.ts##). **Production runtime uses ##loadAndRenderSection()## from UCM-managed ##.prompt.md## files** (see [[Prompt Architecture>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]). The composition strategy documented on this page applies to the testing harness only.
{{/info}}

=== Prompt Builder Module ===

**Location**: ##apps/web/src/lib/analyzer/prompts/prompt-builder.ts## (testing harness)

**Supported Providers**:
* **Anthropic** (Claude Sonnet 4, Sonnet 3.5, Haiku 3.5)
* **OpenAI** (GPT-4, GPT-4 Turbo, GPT-3.5 Turbo)
* **Google** (Gemini 1.5 Pro, Gemini 1.5 Flash)
* **Mistral** (Mistral Large, Mistral Medium)

=== Task Types ===

Prompts are composed for the following task types in the testing harness:
1. **extract_evidence** - Evidence extraction from sources
1. **verdict** - Claim evaluation and verdict assignment
1. **dynamic_plan** - Dynamic analysis planning (monolithic mode)
1. **dynamic_analysis** - Dynamic analysis execution (monolithic mode)

{{warning}}
**Removed task types** (Orchestrated pipeline â€” no longer exist): ~~**understand**~~, ~~**context_refinement**~~ (AnalysisContext boundary detection), ~~**orchestrated_understand**~~ â€” these task types were part of the removed Orchestrated pipeline.
{{/warning}}

=== Composition Layers ===

{{code}}
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Base Prompt (universal logic)           â”‚  â† Task requirements, schema, rules
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Provider Variant (format optimization)  â”‚  â† XML tags, markdown, examples
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Config Adaptation (runtime tuning)      â”‚  â† Tiering, knowledge mode, budget
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         Final Composed Prompt
{{/code}}

----

== 3. Provider-Specific Optimizations ==

=== 3.1 Anthropic Claude ===

**File**: ##apps/web/src/lib/analyzer/prompts/providers/anthropic.ts##

**Optimizations**:
* **XML-structured prompts** - Claude excels with XML tags for clarity
* **Format-only variants** - Content guidance stays in base prompts
* **Output structure hints** - JSON validity and empty-string rules
* **Nuanced reasoning** - Trust Claude's judgment on complex assessments
* **Boundary clustering** - Strong at clustering EvidenceScopes into ClaimAssessmentBoundaries

**Example Optimization**:

{{code language="xml"}}
<claude_optimization>
## FORMAT
Use XML tags. Follow schema precisely.

## OUTPUT
- Valid JSON matching schema
- Empty strings "" for missing optional fields
- All arrays as arrays (even if empty)

## STRENGTHS
Apply nuanced reasoning. Be direct and confident.
</claude_optimization>
{{/code}}

**Why This Works**:
* Claude's training prioritizes XML tag recognition
* Format-only structure reduces duplication and keeps guidance centralized

=== 3.2 OpenAI GPT-4 ===

**File**: ##apps/web/src/lib/analyzer/prompts/providers/openai.ts##

**Optimizations**:
* **Markdown headings** - Clear H2/H3 structure with ##~#~### and ##~#~#~###
* **Numbered lists** - Explicit step-by-step instructions
* **Code block examples** - JSON schema with inline comments
* **Explicit constraints** - "NEVER", "ALWAYS", "MUST" keywords
* **Function calling** - Structured output via JSON mode

**Example Optimization**:

{{code language="markdown"}}
## OpenAI Optimization - Extract Evidence Task

### Step-by-Step Process

1. **Read the source content** - Scan for verifiable statements
2. **Extract specific claims** - Look for concrete facts, not opinions
3. **Assign probativeValue** - Rate quality (high/medium/low)
4. **Link to source** - Include excerpt + URL for attribution
5. **Categorize evidence** - Classify as statistic, expert_quote, event, etc.

### Output Requirements

**REQUIRED Fields**:
- `id`: Unique identifier (format: "S{sourceId}-E{number}")
- `statement`: The extracted statement (20+ characters)
- `category`: Evidence category (see schema)
- `sourceUrl`: Full URL of source
- `sourceExcerpt`: Relevant excerpt (30+ characters)

**NEVER Extract**:
- Vague statements ("some say", "many believe")
- Opinions without attribution
- Speculative language ("might", "could", "possibly")

```json
{
  "id": "S1-E1",
  "statement": "The study found a 25% increase in efficiency",
  "category": "statistic",
  "probativeValue": "high",
  "sourceUrl": "https://example.com/study",
  "sourceExcerpt": "Published in Nature (2023), the peer-reviewed study documented..."
}
```
{{/code}}

**Why This Works**:
* GPT-4's training emphasizes markdown structure
* Numbered lists provide clear execution order
* Code blocks with comments aid JSON schema comprehension

=== 3.3 Google Gemini ===

**File**: ##apps/web/src/lib/analyzer/prompts/providers/google.ts##

**Optimizations**:
* **Example-heavy prompts** - Multiple before/after examples
* **Visual formatting** - Use of emojis and bullets for emphasis
* **Repetition for clarity** - Key rules stated 2-3 times
* **Concrete over abstract** - Specific examples > general principles
* **Short paragraphs** - Break complex instructions into small chunks

**Example Optimization**:

{{code language="markdown"}}
## Gemini Optimization - Extract Evidence Task

### ğŸ“‹ What You'll Do

Extract verifiable facts from sources. Each fact needs:
- âœ… A clear statement (what was claimed)
- âœ… Source attribution (where it came from)
- âœ… Quality rating (high/medium/low)

### âœ… Good Example

**Input Source**: "The agency report found Product A failure rate of 2.1% in 2023"

**Output**:
```json
{
  "statement": "Agency report documented Product A failure rate of 2.1% in 2023",
  "probativeValue": "high",
  "sourceUrl": "https://example.com/reports/2023",
  "sourceExcerpt": "The agency annual report documented a Product A failure rate of 2.1%..."
}
```

### âŒ Bad Example

**Input Source**: "Some experts say Product A is concerning"

**Output**: *DO NOT EXTRACT* (vague attribution, no concrete fact)

### ğŸ“Š Quality Ratings

**High probativeValue**:
- Specific data with attribution ("Study X found Y%")
- Expert testimony with credentials ("Dr. Smith, university professor...")
- Official documents with citations ("Court ruling 2023-456...")

**Medium probativeValue**:
- General claims with reasonable attribution ("Recent studies suggest...")
- Moderate specificity ("Performance improved last quarter")

**Low probativeValue** (DO NOT EXTRACT):
- Vague attribution ("Some say...", "Many believe...")
- Speculation ("Could be...", "Might have...")
{{/code}}

**Why This Works**:
* Gemini benefits from visual anchors (emojis, bullets)
* Multiple examples provide pattern recognition
* Repetition reinforces key rules

=== 3.4 Mistral ===

**File**: ##apps/web/src/lib/analyzer/prompts/providers/mistral.ts##

**Optimizations**:
* **Explicit reasoning steps** - Chain-of-thought guidance
* **French-language examples** - Mistral's training includes French corpus
* **Formal tone** - Academic-style instructions
* **Strict schema adherence** - Emphasis on JSON structure
* **Error prevention** - Common pitfalls highlighted

**Example Optimization**:

{{code language="markdown"}}
## Mistral Optimization - Extract Evidence Task

### Reasoning Chain (ChaÃ®ne de Raisonnement)

For each source passage:

1. **Identify** verifiable statements (Identifier les dÃ©clarations vÃ©rifiables)
2. **Evaluate** probative value (Ã‰valuer la valeur probante)
3. **Extract** with source linkage (Extraire avec attribution de source)
4. **Validate** against schema (Valider selon le schÃ©ma)

### Schema Adherence (Respect du SchÃ©ma)

CRITICAL: Every extracted fact MUST conform to this structure:

```typescript
interface EvidenceItem {
  id: string;               // Required: "S{sourceId}-F{number}"
  fact: string;             // Required: Minimum 20 characters
  category: string;         // Required: From allowed categories
  sourceUrl: string;        // Required: Full HTTP/HTTPS URL
  sourceExcerpt: string;    // Required: Minimum 30 characters
  probativeValue?: string;  // Optional: "high" | "medium" | "low"
}
```

### Common Errors to Avoid (Erreurs Courantes Ã  Ã‰viter)

âŒ Missing required fields â†’ Schema validation will fail
âŒ Empty strings for required fields â†’ Use meaningful content
âŒ Relative URLs â†’ Use absolute URLs (https://...)
âŒ Too-short excerpts â†’ Minimum 30 characters required
{{/code}}

**Why This Works**:
* Mistral performs well with formal, academic prompting
* Explicit reasoning chain aids structured thinking
* Bilingual examples leverage training corpus

----

== 4. Prompt Composition Strategy ==

=== Base Prompts (Universal Layer) ===

**Directory**: ##apps/web/src/lib/analyzer/prompts/base/##

**Files**:
* ##extract-evidence-base.ts## - Evidence extraction rules
* ##verdict-base.ts## - Verdict evaluation criteria (testing harness)

{{warning}}
**Removed base prompt files** (Orchestrated pipeline): ~~##understand-base.ts##~~, ~~##context-refinement-base.ts##~~ â€” these files were for the removed Orchestrated pipeline.
{{/warning}}

**Content**:
* Task requirements
* JSON schema definitions
* Universal rules (Ground Realism, quality gates)
* Terminology guidance (EvidenceItem, EvidenceScope, ClaimBoundary)

=== Provider Variants (Format Layer) ===

**Directory**: ##apps/web/src/lib/analyzer/prompts/providers/##

**Files**:
* ##anthropic.ts## - Claude-specific optimizations
* ##openai.ts## - GPT-4 optimizations
* ##google.ts## - Gemini optimizations
* ##mistral.ts## - Mistral optimizations

**Content**:
* Format adaptations (XML vs markdown)
* Examples in provider-preferred style
* Reasoning guidance tailored to provider strengths
* Output structure hints

=== Config Adaptations (Runtime Layer) ===

**Directory**: ##apps/web/src/lib/analyzer/prompts/config-adaptations/##

**Files**:
* ##tiering.ts## - Budget tier adaptations (premium/standard/budget)
* ##knowledge-mode.ts## - Model knowledge mode (with/without knowledge)
* ##structured-output.ts## - JSON mode guidance

**Content**:
* Tier-specific constraints (token limits, search quotas)
* Knowledge mode instructions (use/ignore model knowledge)
* Structured output validation

=== Composition Function ===

**Location**: ##apps/web/src/lib/analyzer/prompts/prompt-builder.ts:buildPrompt()##

{{code language="typescript"}}
export function buildPrompt(
  taskType: TaskType,
  provider: 'anthropic' | 'openai' | 'google' | 'mistral',
  config?: { tier?: string; knowledgeMode?: string; }
): string {
  // 1. Get base prompt (universal logic)
  const base = getBasePromptForTask(taskType);

  // 2. Get provider variant (format optimization)
  const variant = getProviderVariantForTask(taskType, provider);

  // 3. Get config adaptation (runtime tuning)
  const adaptation = getConfigAdaptation(taskType, config);

  // 4. Compose final prompt
  return base + '\n\n' + variant + '\n\n' + adaptation;
}
{{/code}}

----

== 5. Configuration Adaptations ==

=== 5.1 Tiering Adaptations ===

**Purpose**: Adjust prompts based on budget tier (premium/standard/budget)

**Premium Tier**:
* Full reasoning guidance
* Multiple examples
* Extended search quotas
* No token limits

**Standard Tier**:
* Essential reasoning guidance
* Key examples only
* Standard search quotas
* Moderate token limits

**Budget Tier**:
* Minimal reasoning guidance
* No examples (schema only)
* Reduced search quotas
* Strict token limits

**Example**:

{{code language="typescript"}}
// Premium tier (full guidance)
const premium = `
## REASONING APPROACH (Premium)
Use your full capabilities to:
1. Deeply analyze input for implicit claims
2. Cluster EvidenceScopes into ClaimAssessmentBoundaries
3. Generate comprehensive search queries (up to 12)
4. Extract all relevant evidence (no artificial limits)
`;

// Budget tier (minimal guidance)
const budget = `
## INSTRUCTIONS (Budget Mode)
1. Identify core claims only (max 5)
2. Generate essential search queries (max 6)
3. Extract most critical evidence only (max 20 items)
Token limit: 8000 tokens
`;
{{/code}}

=== 5.2 Knowledge Mode Adaptations ===

**Purpose**: Control whether LLM can use internal knowledge vs. sources-only

**With Model Knowledge**:
* LLM can use training data for context
* Useful for well-known topics
* Risk: May inject outdated knowledge

**Without Model Knowledge** (Sources-Only):
* LLM must base all claims on provided sources
* Required for Ground Realism enforcement
* Prevents hallucination

**Example**:

{{code language="typescript"}}
// With knowledge
const withKnowledge = `
You may use your training knowledge to contextualize claims, but all EVIDENCE
must come from provided sources with proper attribution.
`;

// Without knowledge (sources-only)
const withoutKnowledge = `
CRITICAL: You MUST base all evidence exclusively on the provided sources.
DO NOT use your training knowledge. If sources don't contain information,
state that evidence is insufficient rather than using model knowledge.
`;
{{/code}}

----

== 6. Testing and Validation ==

=== Test Suite Location ===

**File**: ##apps/web/test/unit/lib/analyzer/prompts/prompt-optimization.test.ts##

**Coverage**:
* Provider-specific variant generation
* Prompt composition correctness
* Config adaptation merging
* Schema validation

=== Validation Checks ===

**Automated Checks**:
1. **Prompt length** - Ensure prompt doesn't exceed provider limits
1.* Claude: 200k tokens
1.* GPT-4: 128k tokens
1.* Gemini: 2M tokens
1.* Mistral: 128k tokens
1. **Schema inclusion** - Verify JSON schema present in all prompts
1. **Required fields** - Check that all required fields documented
1. **Provider-specific markers** - Validate XML tags (Claude), markdown (GPT-4), etc.

=== Manual Review Process ===

**Before deploying prompt changes**:
1. Test with representative inputs (3-5 samples per task type)
1. Compare outputs across providers (consistency check)
1. Validate JSON schema adherence
1. Check for unintended behavior changes
1. Review with domain expert (if terminology changes)

----

== Appendix A: Provider Feature Matrix ==

|= Feature |= Anthropic |= OpenAI |= Google |= Mistral
| **XML Tags** | Excellent | Poor | Moderate | Poor
| **Markdown** | Good | Excellent | Good | Good
| **Thinking Blocks** | Native | Simulated | Simulated | Simulated
| **Prefill** | Supported | No | No | No
| **JSON Mode** | Structured | Structured | Structured | Structured
| **Examples Heavy** | Moderate | Good | Excellent | Moderate
| **Nuanced Reasoning** | Excellent | Excellent | Good | Good
| **Schema Adherence** | Excellent | Excellent | Good | Moderate
| **Boundary Clustering** | Excellent | Good | Moderate | Moderate
| **Attribution Separation** | Excellent | Good | Moderate | Moderate

**Legend**:
* Excellent - Strong native support, highly effective
* Good - Reliable support, effective
* Moderate - Acceptable support, requires careful prompting
* Poor/No - Weak support, avoid using

----

== Appendix B: Related Documents ==

* Evidence Quality Filtering - Layer 1 prompt enforcement
* AGENTS.md - LLM agent rules and guidance
* Unified Config Management - Runtime configuration
* ##prompt-builder.ts## - Implementation

----

**Document Version**: 1.0
**Last Updated**: 2026-01-29
**Next Review**: When adding new providers or major prompt changes
**Maintained by**: Plan Coordinator