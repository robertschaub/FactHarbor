= FactHarbor Terminology Reference =

**Version**: 4.0.0-cb
**Date**: 2026-02-16
**Audience**: Developers, Prompt Engineers, LLM Systems
**Status**: ClaimBoundary pipeline (default) - Orchestrated pipeline removed

----

== Purpose ==

This document provides the **authoritative glossary** for FactHarbor's ClaimBoundary pipeline terminology across all layers: TypeScript code, JSON schema, database storage, and LLM prompts. Use this as the single source of truth when encountering ambiguous terms.

{{info}}
**ClaimBoundary is now the default pipeline** (as of Phase 2: Cutover, February 2026). The Orchestrated pipeline (which used AnalysisContext) has been completely removed.

* **All code** uses ClaimBoundary, AtomicClaim, claimBoundaryId.
* **NEVER use** in new code: AnalysisContext, contextId, analysisContexts.
* Full architecture: [[ClaimBoundary Pipeline Architecture>>path:/Docs/WIP/ClaimBoundary_Pipeline_Architecture_2026-02-15.md]]
* Migration state: See [[CB Execution State>>path:/Docs/WIP/CB_Execution_State.md]]
{{/info}}

----

== Field Mapping Table (v4.0.0-cb) ==

**ClaimBoundary Pipeline**: All code uses ClaimBoundary terminology. AnalysisContext fields removed.

|= Concept |= TypeScript Type |= JSON Field (v4.0.0-cb) |= JSON Field (Legacy - removed) |= Prompt Term
| Atomic claim | ##AtomicClaim## | ##atomicClaims## | ~~##claims##~~ | AtomicClaim
| Claim boundary | ##ClaimBoundary## | ##claimBoundaries## | ~~##analysisContexts##~~ | ClaimBoundary
| Per-evidence metadata | ##EvidenceScope## | ##evidenceScope## | ##evidenceScope## (unchanged) | EvidenceScope
| Boundary finding | ##BoundaryFinding## | ##boundaryFindings## | (new field) | BoundaryFinding
| Claim verdict | ##ClaimVerdict## | ##claimVerdicts## | ~~##claimAssessments##~~ | ClaimVerdict
| Evidence item | ##EvidenceItem## | ##evidenceItems## | ##evidenceItems## (unchanged) | EvidenceItem
| Evidence statement | ##statement## | ##statement## | ##statement## (unchanged) | statement
| Evidence ID prefix | ##EV_001, EV_002...## | ##EV_001, EV_002...## | ~~##E1, E2, E3...##~~ | EV-prefix
| Coverage matrix | ##CoverageMatrix## | ##coverageMatrix## | (new field) | CoverageMatrix
| Verdict narrative | ##VerdictNarrative## | ##verdictNarrative## | (new field) | VerdictNarrative

**Schema Version**: 3.0.0-cb (resultJson schema)

----

== Pipeline Hierarchy: Understanding the Flow ==

FactHarbor's ClaimBoundary pipeline follows a sequential 5-stage flow:

=== Stage 1: EXTRACT CLAIMS (Two-Pass, Evidence-Grounded) ===

* **What:** Parse input and extract central, verifiable AtomicClaims
* **When:** First stage of pipeline (before research)
* **Output:** ##AtomicClaim[]## — only claims with ##isCentral: true##
* **Key fields:** ##statement##, ##centrality##, ##harmPotential##, ##specificityScore##, ##groundingQuality##, ##expectedEvidenceProfile##
* **JSON field:** ##atomicClaims##

=== Stage 2: RESEARCH ===

* **What:** Gather evidence for each claim via web search and extraction
* **When:** After claims extracted, before boundary clustering
* **Output:** ##EvidenceItem[]## — each with mandatory ##EvidenceScope##
* **Key principle:** Claims drive all research (no context-driven queries)
* **ID format:** ##EV_001, EV_002, ...## (EV-prefix for Evidence)
* **JSON field:** ##evidenceItems##

=== Stage 3: CLUSTER BOUNDARIES ===

* **What:** Organize evidence into ClaimBoundaries by clustering compatible EvidenceScopes
* **When:** After research complete, before verdict generation
* **Output:** ##ClaimBoundary[]## + evidence assignments
* **Key principle:** Boundaries emerge from evidence (not predetermined)
* **Clustering factors:** Methodology, boundaries, geographic, temporal congruence
* **JSON field:** ##claimBoundaries##

=== Stage 4: VERDICT (LLM Debate Pattern) ===

* **What:** Generate per-claim verdicts using 5-step debate (advocate → self-consistency → challenge → reconciliation → validation)
* **When:** After boundary clustering complete
* **Output:** ##ClaimVerdict[]## — one per AtomicClaim, with ##boundaryFindings[]##
* **Module:** ##verdict-stage.ts## (separate module)
* **JSON field:** ##claimVerdicts##

=== Stage 5: AGGREGATE ===

* **What:** Compute coverage matrix, triangulation, and overall assessment with narrative
* **When:** Final stage
* **Output:** ##OverallAssessment## with ##VerdictNarrative##
* **Key artifacts:** ##CoverageMatrix## (claims × boundaries), triangulation scores
* **JSON field:** ##overallAssessment##

----

**Key Distinctions:**

* **AtomicClaim** = "What verifiable assertion am I checking?" (extracted from user input)
* **EvidenceScope** = "What methodology/boundaries did THIS source use?" (per-evidence metadata)
* **ClaimBoundary** = "Which evidence items can be grouped together?" (compatible EvidenceScopes)
* **BoundaryFinding** = "What does evidence in THIS boundary say about THIS claim?" (per-boundary verdict component)

----

== Core Concepts ==

=== 1. ClaimBoundary Pipeline Terminology — NEW PIPELINE (DEFAULT) ===

{{info}}
**ClaimBoundary is now the default pipeline.** The Orchestrated pipeline (which used AnalysisContext) has been removed as of Phase 2a. All new code must use ClaimBoundary terminology.

Full architecture: [[ClaimBoundary Pipeline Architecture>>path:/Docs/WIP/ClaimBoundary_Pipeline_Architecture_2026-02-15.md]]
{{/info}}

==== 1.1 AtomicClaim ====

**What it is**: A single verifiable assertion extracted from user input. The atomic unit of analysis in the ClaimBoundary pipeline.

**Why it matters**: Each claim drives independent research. Claims must be specific enough to generate targeted search queries without additional framing.

**Examples (generic)**:
* "Entity A performed Action X during Period Y"
* "Metric M for Process P exceeds Value V"
* "Framework F applies to Situation S"

**Code representation**:

{{code language="typescript"}}
export interface AtomicClaim {
  id: string;                     // AC_01, AC_02, ...
  statement: string;              // The verifiable assertion
  category: "factual" | "evaluative" | "procedural";
  centrality: "high" | "medium";
  harmPotential: "critical" | "high" | "medium" | "low";
  isCentral: boolean;             // true (only central claims survive)
  claimDirection: "supports_thesis" | "contradicts_thesis" | "contextual";
  keyEntities: string[];
  specificityScore: number;       // 0-1 (≥0.6 required)
  groundingQuality: "strong" | "moderate" | "weak" | "none";
  expectedEvidenceProfile: {
    methodologies: string[];
    expectedMetrics: string[];
    expectedSourceTypes: SourceType[];
  };
}
{{/code}}

**Variable names**: ##atomicClaim##, ##atomicClaims##

**NEVER call it**: "context", "fact"

----

==== 1.2 ClaimBoundary ====

**What it is**: An evidence-emergent grouping of compatible EvidenceScopes, created //after// research by clustering evidence with congruent methodology, temporal, boundaries, and geographic dimensions.

**Why it matters**: ClaimBoundaries organize evidence for verdict generation. Instead of pre-creating analytical frames (which caused instability), boundaries emerge from the evidence itself.

**Key principle**: Evidence tells us what boundaries exist. We don't guess.

**Examples (generic)**:
* "Methodology A studies" (evidence using Framework A)
* "Jurisdiction J proceedings" (evidence from legal domain J)
* "Period P data" (evidence from temporal range P)

**Code representation**:

{{code language="typescript"}}
export interface ClaimBoundary {
  id: string;                     // CB_01, CB_02, ...
  name: string;                   // Human-readable label
  shortName: string;              // Short label for UI tabs
  description: string;            // What this boundary represents
  methodology?: string;           // Dominant methodology (if applicable)
  boundaries?: string;            // Scope boundaries
  geographic?: string;            // Geographic scope
  temporal?: string;              // Temporal scope
  internalCoherence: number;      // 0-1: consistency of evidence within
  evidenceCount: number;          // Number of evidence items
}
{{/code}}

**Variable names**: ##claimBoundary##, ##claimBoundaries##, ##claimBoundaryId##

**NEVER call it**: "context", "scope", "analysisContext"

----

==== 1.3 BoundaryFinding ====

**What it is**: Per-boundary quantitative evidence assessment for a specific claim. Each ClaimVerdict contains boundaryFindings[] showing how evidence within each boundary supports or contradicts the claim.

**Why it matters**: Enables multi-perspective verdicts — the same claim may have different support levels across different ClaimBoundaries.

**Code representation**:

{{code language="typescript"}}
export interface BoundaryFinding {
  boundaryId: string;                           // Which ClaimBoundary
  truthPercentage: number;                      // 0-100 per-boundary assessment
  confidence: number;                           // 0-100 per-boundary confidence
  evidenceDirection: "supports" | "contradicts" | "mixed" | "neutral";
  evidenceCount: number;                        // Evidence items in this boundary for this claim
}
{{/code}}

**Used in**: ##ClaimVerdict.boundaryFindings[]##

----

==== 1.4 ClaimVerdict ====

**What it is**: Final verdict for a single AtomicClaim, produced by the 5-step LLM debate pattern (advocate → self-consistency → challenge → reconciliation → validation).

**Code representation**:

{{code language="typescript"}}
export interface ClaimVerdict {
  id: string;
  claimId: string;                              // Which AtomicClaim
  truthPercentage: number;                      // 0-100 overall
  verdict: string;                              // 7-point scale label
  confidence: number;                           // 0-100 overall
  reasoning: string;                            // LLM-generated explanation
  harmPotential: "critical" | "high" | "medium" | "low";
  isContested: boolean;                         // Documented counter-evidence exists
  supportingEvidenceIds: string[];
  contradictingEvidenceIds: string[];
  boundaryFindings: BoundaryFinding[];          // Per-boundary assessments
  challengeResponses?: string[];                // From reconciliation step
}
{{/code}}

**Variable names**: ##claimVerdict##, ##claimVerdicts##

----

==== 1.5 CoverageMatrix ====

**What it is**: A deterministic claims × boundaries matrix showing which claims have been evaluated in which boundaries.

**Why it matters**: Ensures analytical completeness — every claim must have evidence across relevant boundaries. Logged to job events for transparency.

**Code representation**:

{{code language="typescript"}}
export interface CoverageMatrix {
  claims: string[];                            // AtomicClaim IDs
  boundaries: string[];                        // ClaimBoundary IDs
  coverage: boolean[][];                       // claims[i] × boundaries[j] → has evidence?
  missingCoverage: Array<{
    claimId: string;
    boundaryId: string;
    reason: string;
  }>;
}
{{/code}}

----

==== 1.6 VerdictNarrative ====

**What it is**: Structured summary of the overall assessment, including boundary disagreements and limitations. LLM-generated (Sonnet, 1 call) during Stage 5: AGGREGATE.

**Code representation**:

{{code language="typescript"}}
export interface VerdictNarrative {
  headline: string;                           // 1-sentence summary
  evidenceBaseSummary: string;                // What evidence was found
  keyFinding: string;                         // Most important conclusion
  boundaryDisagreements: Array<{
    boundaryIds: string[];
    disagreementSummary: string;
  }>;
  limitations: string[];                      // Gaps, uncertainties, caveats
}
{{/code}}

**Used in**: ##OverallAssessment.verdictNarrative##

----

=== 2. EvidenceScope (Per-Evidence Source Metadata) — SHARED CONCEPT ===

**What it is**: Metadata attached to individual evidence items describing the methodology, boundaries, geography, and time period that **the source document** used when producing that evidence.

**Why it matters**: Evidence stating "Metric X = Value Y" from a study using Methodology A with Boundary Set B cannot be directly compared to a study using Methodology C with Boundary Set D. EvidenceScope captures these methodological differences so the pipeline can cluster compatible evidence into ClaimBoundaries.

**Examples (generic)**:
* Methodology: "Standard S", "Framework F", "Protocol P"
* Boundaries: "Full system", "Subsystem only", "Phases 1-3"
* Geographic: "Jurisdiction J", "Region R", "Multi-national"
* Temporal: "Period P data", "Year Y baseline", "Historical range"

**Core fields (REQUIRED)**:
* ##methodology##: REQUIRED — The analytical approach used by the source (e.g., "Standard ISO-X", "Regulatory Framework Y")
* ##temporal##: REQUIRED — When the source data was collected or applies (e.g., "Data from Period P", "Baseline Year Y")

**Optional fields**:
* ##boundaries##: What was included/excluded in the analysis (e.g., "Full lifecycle", "Operation phase only")
* ##geographic##: Geographic scope of the source data (e.g., "Region R", "Country C")
* ##additionalDimensions##: Domain-specific scope data (e.g., `{ "sample_size": "N=12000", "blinding": "double-blind" }`)

**Code representation**:

{{code language="typescript"}}
export interface EvidenceScope {
  name: string;                             // Short label (e.g., "Methodology A", "Framework B")
  methodology: string;                      // REQUIRED
  temporal: string;                         // REQUIRED
  boundaries?: string;                      // Optional
  geographic?: string;                      // Optional
  sourceType?: SourceType;                  // Optional classification
  additionalDimensions?: Record<string, string>; // Optional domain-specific data
}

// Attached to evidence items
export interface EvidenceItem {
  id: string;                               // EV_001, EV_002, ... (EV-prefix)
  statement: string;                        // The evidence statement
  evidenceScope: EvidenceScope;             // MANDATORY (not optional)
  claimBoundaryId?: string;                 // Assigned during CLUSTER stage
  relevantClaimIds: string[];               // Which claims this evidence relates to
  isDerivative: boolean;                    // Derives from another source's study
  derivedFromSourceUrl?: string;            // URL of original source (optional)
  scopeQuality: "complete" | "partial" | "incomplete"; // Scope metadata quality
  // ...
}
{{/code}}

**JSON field name**:

{{code language="json"}}
{
  "evidenceItems": [
    {
      "id": "EV_001",
      "statement": "Entity A achieved Metric X using Method M",
      "evidenceScope": {
        "name": "Method M Analysis",
        "methodology": "Standard S with Approach A",
        "temporal": "Period P data (Year-Year)",
        "boundaries": "Full system boundary",
        "geographic": "Region R",
        "additionalDimensions": {
          "sample_size": "N=12000",
          "confidence_level": "95%"
        }
      },
      "scopeQuality": "complete",
      "claimBoundaryId": "CB_01",
      "relevantClaimIds": ["AC_01", "AC_03"]
    }
  ]
}
{{/code}}

**Prompt terminology**:
* Always: "EvidenceScope"
* NEVER: "scope" (ambiguous), "context"

**Key differences from ClaimBoundary**:
* **EvidenceScope** = Per-evidence metadata ("What methodology did THIS source use?")
* **ClaimBoundary** = Cluster of compatible EvidenceScopes ("Which evidence can be grouped together?")
* Multiple evidence items share the same EvidenceScope → they cluster into the same ClaimBoundary
* EvidenceScopes with incompatible methodology/boundaries/temporal/geographic → separate ClaimBoundaries

----

=== 3. Background Details (Narrative Background) ===

**What it is**: The narrative/rhetorical framing or background context of the input article. This describes **how the article presents** the information, but is NOT a reason to split into separate AnalysisContexts.

**Why it matters**: Helps understand the user's intent and article structure, but does NOT affect verdict logic.

**Examples**:
* "Brazilian political crisis following January 8th events"
* "Climate policy debate in European Union"
* "Legal proceedings against former president"

**Code representation**:

{{code language="typescript"}}
// Stored as string in ClaimUnderstanding
export interface ClaimUnderstanding {
  backgroundDetails: string; // Narrative background
  // ...
}
{{/code}}

**JSON field name** (v3.1):

{{code language="json"}}
{
  "backgroundDetails": "Brazilian political crisis following January 8th events"
}
{{/code}}

**UI Display**: Shown in ##BackgroundBanner.tsx## component with label "Background"

**Prompt terminology**:
* Preferred: "backgroundDetails" or "Background"
* Removed: ~~"ArticleFrame"~~, ~~"analysisContext" (singular)~~ (legacy terms)

**Common confusion**:
* backgroundDetails is NOT an AnalysisContext (not a reason to split analysis)
* backgroundDetails does NOT get its own verdict
* backgroundDetails IS purely descriptive/informational

----

=== 4. Doubted vs Contested (Contestation Classification) - v2.8 ===

**What it is**: A distinction between two types of opposition to a claim, which affects how the opposition impacts the verdict weight.

**Why it matters**: Not all criticism is equal. Political statements without evidence shouldn't reduce a claim's weight as much as documented counter-evidence. This ensures:
* **Evidence-based contestation** appropriately reduces certainty
* **Opinion-based doubt** doesn't unfairly penalize well-evidenced claims

**Two Categories**:

|= Category |= factualBasis |= Weight Multiplier |= Example
| **DOUBTED** | ##"opinion"## | 1.0x (full) | "Government says trial was unfair" (no specifics)
| **DOUBTED** | ##"alleged"## | 1.0x (full) | "Critics claim bias" (no evidence cited)
| **CONTESTED** | ##"disputed"## | 0.5x (reduced) | "Defense presented conflicting expert testimony"
| **CONTESTED** | ##"established"## | 0.3x (heavily reduced) | "Audit found violation of Regulation 47(b)"

**Weight calculation** (in ##getClaimWeight()##):

{{code language="typescript"}}
if (claim.isContested) {
  if (basis === "established") weight *= 0.3;  // Strong counter-evidence
  else if (basis === "disputed") weight *= 0.5; // Some counter-evidence
  // "opinion"/"alleged"/"unknown" -> full weight (just doubted)
}
{{/code}}

**Common confusion**:
* "contested" does NOT mean "disputed politically" (that's "doubted")
* "contested" means there IS documented counter-evidence
* Political statements alone do NOT reduce claim weight
* Only factual counter-evidence reduces claim weight

----

== Terminology Mapping Tables ==

=== Table 1: Primary Entities (v3.1) ===

|= Concept |= TypeScript Name |= JSON Field |= Prompt Term |= UI Label |= Database Column
| Top-level analytical frame | ##AnalysisContext## | ##analysisContexts## | "AnalysisContext" | "Contexts" | ##ResultJson.analysisContexts##
| Per-evidence source metadata | ##EvidenceScope## | ##evidenceScope## | "evidenceScope" | (not displayed separately) | ##ResultJson.evidenceItems[].evidenceScope##
| Narrative background | (string) | ##backgroundDetails## | "Background" | "Background" | ##ResultJson.understanding.backgroundDetails##
| Evidence item | ##EvidenceItem## | ##evidenceItems## | "EvidenceItem" | "Evidence" | ##ResultJson.evidenceItems##

=== Table 2: Reference Fields (v3.1) ===

|= Field Purpose |= TypeScript Field |= JSON Field |= Prompt Term |= Valid Values
| Evidence -> AnalysisContext | ##contextId?: string## | ##contextId## | "contextId" | Must match ##AnalysisContext.id##
| Claim -> AnalysisContext | ##contextId?: string## | ##contextId## | "contextId" | Must match ##AnalysisContext.id##
| Verdict -> AnalysisContext | ##contextId: string## | ##contextId## | "contextId" | Must match ##AnalysisContext.id##
| Supporting evidence | ##supportingEvidenceIds## | ##supportingEvidenceIds## | "supportingEvidenceIds" | Array of E-prefix IDs

=== Table 3: Special Constants ===

|= Constant |= Value |= Meaning |= When to Use
| ##UNSCOPED_ID## | ##"CTX_UNSCOPED"## | Evidence doesn't map to any detected context | When evidence is general/background
| ##CTX_MAIN## | ##"CTX_MAIN"## | Fallback context for single-context analysis | When no distinct contexts detected
| ##CTX_GENERAL## | ##"CTX_GENERAL"## | General context (cross-cutting) | When evidence applies to all contexts

----

== Quick Reference: "Which term should I use?" ==

=== In TypeScript Code ===

{{code language="typescript"}}
// CORRECT (v3.1)
import { AnalysisContext, EvidenceScope, EvidenceItem } from './types';

function processContexts(contexts: AnalysisContext[]) {
  // ...
}

function processEvidence(items: EvidenceItem[]) {
  // ...
}

// REMOVED in v3.0 - do not use:
// ExtractedFact (use EvidenceItem)
// DistinctProceeding (use AnalysisContext)
{{/code}}

=== In JSON Schema (Zod) ===

{{code language="typescript"}}
// CORRECT (v3.1 field names)
const schema = z.object({
  analysisContexts: z.array(AnalysisContextSchema),
  backgroundDetails: z.string(),
  evidenceItems: z.array(EvidenceItemSchema),
});
{{/code}}

=== In LLM Prompts ===

{{code language="typescript"}}
// CORRECT (v3.1 terminology)
const prompt = `
## TERMINOLOGY

**AnalysisContext**: Top-level bounded analytical frame (stored as analysisContexts)
**EvidenceScope**: Per-evidence source metadata (stored as evidenceItem.evidenceScope)
**Background**: Narrative background (stored as backgroundDetails)
**EvidenceItem**: Individual evidence with id (E1, E2...) and statement

Your task: Identify AnalysisContexts from evidence...
`;

// AVOID (legacy terms)
const prompt = `Identify scopes from evidence...`;  // Which "scope"?
const prompt = `Extract facts...`;  // Use "evidence" not "facts"
{{/code}}

=== In UI/Documentation ===

{{code language="typescript"}}
// CORRECT
<h2>Analysis Contexts</h2>
<p>This analysis involves 2 distinct analytical frames:</p>

<BackgroundBanner backgroundDetails={background} />

// REMOVED in v3.0:
// <ArticleFrameBanner articleFrame={...} />
{{/code}}

----

== Internal Task Names (v3.1) ==

|= Task Name |= Description |= Used In
| ##extract_evidence## | Extract evidence items from sources | llm.ts, model-tiering.ts
| ##context_refinement## | Refine AnalysisContext assignments | llm.ts, model-tiering.ts
| ##understand## | Initial claim understanding | llm.ts, model-tiering.ts
| ##verdict## | Generate verdicts | llm.ts, model-tiering.ts

**Note:** Legacy task names ##extract_facts## and ##scope_refinement## were renamed in v3.1.

----

== Config Field Names (v3.1) ==

|= Config Field |= Description |= Default
| ##contextDetectionMethod## | Method for detecting contexts | ##"heuristic"##
| ##contextDetectionEnabled## | Enable context detection | ##true##
| ##contextDetectionMinConfidence## | Minimum confidence threshold | ##0.7##
| ##contextDetectionMaxContexts## | Maximum contexts to detect | ##5##
| ##contextDedupThreshold## | Threshold for deduplication | ##0.85##

**Note:** Legacy field names ##scopeDetection*## and ##scopeDedup*## were renamed in v3.0.

----

== Decision Trees ==

=== "Should I create a new AnalysisContext?" ===

{{code}}
START: Do the facts involve distinct analytical frames?
  |
  +-- YES -> Are the methodologies/boundaries INCOMPATIBLE?
  |   |
  |   +-- YES -> CREATE separate AnalysisContexts
  |   |   Example: WTW vs TTW (different system boundaries)
  |   |
  |   +-- NO -> SINGLE AnalysisContext
  |       Example: Multiple studies using same WTW methodology
  |
  +-- NO -> Are they different LEGAL proceedings analyzing different matters?
      |
      +-- YES -> CREATE separate AnalysisContexts
      |   Example: TSE electoral case vs STF criminal case
      |
      +-- NO -> SINGLE AnalysisContext
          Example: Different viewpoints on same legal case
{{/code}}

=== "Is this an EvidenceScope or AnalysisContext?" ===

{{code}}
START: Where does this information come from?
  |
  +-- FROM SOURCE DOCUMENT -> EvidenceScope
  |   Example: "Study used ISO 14040 methodology"
  |   Attach to evidenceItem.evidenceScope
  |
  +-- FROM INPUT/USER -> AnalysisContext?
      |
      +-- Does it define a DISTINCT analytical frame?
          |
          +-- YES -> AnalysisContext
          |   Example: "Compare US EPA vs EU REACH"
          |
          +-- NO -> backgroundDetails
              Example: "Article is written as opinion piece"
{{/code}}

----

== Common Pitfalls & Solutions ==

=== Pitfall 1: Using "Scope" Without Qualifier ===

**Problem**:

{{code language="typescript"}}
// Ambiguous - which scope?
function getScope(id: string) { ... }
{{/code}}

**Solution**:

{{code language="typescript"}}
// Explicit
function getAnalysisContext(id: string): AnalysisContext { ... }
function getEvidenceScope(evidence: EvidenceItem): EvidenceScope | null { ... }
{{/code}}

=== Pitfall 2: Conflating backgroundDetails with AnalysisContext ===

**Problem**:

{{code language="json"}}
{
  "analysisContexts": [
    { "name": "Article frames as conspiracy theory" }
  ]
}
{{/code}}

**Solution**:

{{code language="json"}}
{
  "backgroundDetails": "Article frames as conspiracy theory",
  "analysisContexts": [
    { "name": "Central Bank Policy Analysis" }
  ]
}
{{/code}}

=== Pitfall 3: Using Legacy Field Names ===

**Problem** (v3.0+ will fail):

{{code language="typescript"}}
const facts = result.facts;  // REMOVED
const context = understanding.analysisContext;  // RENAMED
{{/code}}

**Solution**:

{{code language="typescript"}}
const evidenceItems = result.evidenceItems;  // v3.1
const background = understanding.backgroundDetails;  // v3.1
{{/code}}

----

== Validation Checklist ==

Use this checklist when reviewing code that involves contexts/evidence:

* Is "scope" qualified as AnalysisContext or EvidenceScope?
* Do JSON field names use v3.1 names (##analysisContexts##, ##evidenceItems##, ##backgroundDetails##)?
* Does prompt include terminology glossary header?
* Are ##contextId## values validated against ##analysisContexts[]##?
* Are fallbacks logged (not silent)?
* Does EvidenceScope capture source methodology (not create new AnalysisContexts)?
* Is backgroundDetails separate from AnalysisContexts?
* Are evidence IDs using E-prefix (##E1, E2, E3...##)?

----

== FAQ ==

**Q: When should I use EvidenceScope vs AnalysisContext?**

A: If the information describes **how a source document computed its data** (methodology, boundaries), it's EvidenceScope. If it describes **a distinct analytical frame requiring separate verdicts**, it's AnalysisContext.

**Q: What's the difference between CTX_UNSCOPED and CTX_GENERAL?**

A: ##CTX_UNSCOPED## means the evidence doesn't map to any detected context (background info). ##CTX_GENERAL## means the evidence applies across all contexts (cross-cutting evidence). In practice, both are grouped together in display.

**Q: Can an evidence item have BOTH contextId AND evidenceScope?**

A: Yes! ##contextId## says **which AnalysisContext the evidence supports**, while ##evidenceScope## says **how the source computed the data**. They're orthogonal concepts.

**Q: Should prompts say "AnalysisContext", "Context", or "Framework"?**

A: Use "AnalysisContext" for precision or "context" for brevity. **NEVER use "framework"** when referring to the architectural concept of ##AnalysisContext##. The term "framework" is reserved for descriptive English phrases like "regulatory frameworks".

----

== Related Documentation ==

* v2-to-v3-migration-guide.md - Migration guide for v3.0/v3.1
* AGENTS.md - High-level rules for context detection
* types.ts - TypeScript interface definitions
* Pipeline Variants documentation - Pipeline design (see [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]])

----

**Document Maintainer**: Lead Developer
**Last Reviewed**: 2026-02-04 (Updated for v3.1)
**Next Review**: 2026-05 (or after next major version)