= Source Reliability: Architecture and Verdicts =

== Architecture ==

=== System Overview ===

{{mermaid}}
flowchart TB
    subgraph analysis [FactHarbor Analysis Pipeline]
        AN[orchestrated.ts<br/>Analyzer]
        PF[prefetchSourceReliability<br/>Batch Prefetch]
        SR[source-reliability.ts<br/>Sync Lookup + Weighting]
    end

    subgraph cache [Source Reliability Cache]
        SQLITE[(SQLite<br/>source-reliability.db)]
        MAP[In-Memory Map<br/>prefetchedScores]
    end

    subgraph evaluation [LLM Evaluation - Sequential Refinement]
        EVAL["/api/internal/evaluate-source"]
        LLM1[Claude<br/>Initial Evaluation]
        LLM2[OpenAI mini model<br/>Cross-check and Refine]
        FINAL[Final Result]
    end

    AN -->|1. Extract URLs| PF
    PF -->|2. Batch lookup| SQLITE
    SQLITE -->|3. Cache hits| MAP
    PF -->|4. Cache miss| EVAL
    EVAL --> LLM1
    LLM1 -->|Initial result| LLM2
    LLM2 -->|Refined result| FINAL
    FINAL -->|5. Store score| SQLITE
    FINAL -->|6. Populate| MAP
    AN -->|7. Sync lookup| SR
    SR -->|8. Read from| MAP
    SR -->|9. Apply to| VERDICTS[Verdict Weighting]
{{/mermaid}}

=== Integration Pattern: Batch Prefetch + Sync Lookup ===

The Source Reliability system uses a **two-phase pattern** to avoid async operations in the analyzer's hot path.

==== The Problem ====

The FactHarbor analyzer (##orchestrated.ts##) is a complex synchronous pipeline. Adding ##await## calls mid-pipeline for source reliability lookups would require major refactoring, complicate error handling, and risk introducing race conditions.

==== The Solution: Two-Phase Pattern ====

Separate the async work (cache lookup, LLM calls) from the sync analysis:

|= Phase |= When |= Nature |= What It Does
| **Phase 1: Prefetch** | Before analysis starts | Async | Batch lookup all source URLs, populate in-memory map
| **Phase 2: Lookup** | During analysis | Sync | Read from pre-populated map (instant, no I/O)
| **Phase 3: Weighting** | After verdicts generated | Sync | Adjust truth percentages based on source scores

{{mermaid}}
sequenceDiagram
    participant User
    participant Analyzer as orchestrated.ts
    participant Prefetch as prefetchSourceReliability()
    participant Cache as SQLite Cache
    participant LLM as LLM Endpoint
    participant Map as In-Memory Map
    participant Lookup as getTrackRecordScore()
    participant Weight as applyEvidenceWeighting()

    Note over User,Weight: PHASE 1: Async Prefetch (before source fetching)
    User->>Analyzer: Submit claim for analysis
    Analyzer->>Analyzer: Search for sources
    Analyzer->>Prefetch: await prefetchSourceReliability(urls)

    loop For each unique domain
        Prefetch->>Cache: Batch lookup
        alt Cache Hit
            Cache-->>Prefetch: Return cached score
            Prefetch->>Map: Store score
        else Cache Miss + Important Source
            Prefetch->>LLM: Evaluate source (internal API)
            LLM-->>Prefetch: Score + confidence
            Prefetch->>Cache: Save (TTL: 90 days)
            Prefetch->>Map: Store score
        else Cache Miss + Unimportant Source
            Prefetch->>Map: Store null
        end
    end

    Prefetch-->>Analyzer: Done

    Note over User,Weight: PHASE 2: Sync Lookup (during source fetching)
    loop For each source URL
        Analyzer->>Lookup: getTrackRecordScore(url)
        Lookup->>Map: Read from map
        Map-->>Lookup: Score or null
        Lookup-->>Analyzer: Return immediately (no I/O)
        Analyzer->>Analyzer: Assign trackRecordScore to FetchedSource
    end

    Note over User,Weight: PHASE 3: Evidence Weighting (after verdicts)
    Analyzer->>Weight: applyEvidenceWeighting(verdicts, facts, sources)
    Weight->>Weight: Calculate avg source score per verdict
    Weight->>Weight: Adjust truthPercentage and confidence
    Weight-->>Analyzer: Weighted verdicts

    Analyzer-->>User: Analysis complete
{{/mermaid}}

=== Phase 1 Detail: Prefetch Flow ===

{{mermaid}}
flowchart TD
    subgraph prefetch [Phase 1: Async Prefetch]
        URLS[Extract Source URLs] --> DEDUP[Deduplicate Domains]
        DEDUP --> BATCH[Batch Cache Lookup]
        BATCH --> LOOP{For Each Domain}
        LOOP --> HIT{Cache Hit?}
        HIT -->|Yes| MAP[Add to In-Memory Map]
        HIT -->|No| RATE{Rate Limit OK?}
        RATE -->|No| SKIP[Store null in Map]
        RATE -->|Yes| FILTER{isImportantSource?}
        FILTER -->|Blog/Spam TLD| SKIP
        FILTER -->|Legitimate| LLM[Multi-Model LLM<br/>Internal API Only]
        LLM --> CONF{Confidence >= 0.8?}
        CONF -->|No| SKIP
        CONF -->|Yes| CONS{Models Agree?<br/>Diff <= 0.20}
        CONS -->|No| SKIP
        CONS -->|Yes| SAVE[Cache + Add to Map]
    end

    style RATE fill:#f99,color:#000
    style FILTER fill:#ff9,color:#000
    style SKIP fill:#ddd,color:#000
    style SAVE fill:#9f9,color:#000
{{/mermaid}}

=== Why This Pattern Works ===

|= Concern |= How Pattern Addresses It
| **No async ripple** | Only ONE ##await## at pipeline boundary, rest stays sync
| **Batch efficiency** | Single batch cache lookup instead of N individual calls
| **LLM cost control** | Filter + rate limit applied during prefetch
| **Graceful degradation** | Unknown sources get ##null##, analysis continues
| **No blocking** | Sync lookups are instant map reads

----

== How It Affects Verdicts ==

Source reliability scores directly influence verdict calculations through **evidence weighting**.

=== Formula ===

{{code}}
adjustedTruth = 50 + (originalTruth - 50) x avgSourceScore
adjustedConfidence = confidence x (0.5 + avgSourceScore / 2)
{{/code}}

=== Effect on Verdicts (7-Band Scale) ===

|= Source Credibility Band |= Score (Weight) |= Effect on Verdict
| **Highly Reliable (0.86+)** | ~95-100% | Verdict fully preserved
| **Reliable (0.72-0.86)** | ~75-90% | Verdict mostly preserved
| **Leaning Reliable (0.58-0.72)** | ~60-75% | Moderate preservation
| **Mixed (0.43-0.57)** | ~40-60% | Variable track record (neutral center)
| **Leaning Unreliable (0.29-0.43)** | ~30-45% | Pulls toward neutral
| **Unreliable (0.15-0.29)** | ~15-30% | Strong pull toward neutral
| **Highly Unreliable (0.00-0.15)** | ~0-15% | Maximum skepticism
| **Unknown (null)** | 50% | Uses default score (neutral)

=== Example ===

{{code}}
Original verdict: 80% (Strong True)
Source credibility: 0.5 (Mixed - variable track record)

Adjusted = 50 + (80 - 50) x 0.5
         = 50 + 30 x 0.5
         = 50 + 15
         = 65% (Leaning True)
{{/code}}

=== Multi-Source Averaging ===

When a verdict has evidence from multiple sources:

{{code}}
Verdict with facts from:
  - reuters.com (score: 0.95)
  - bbc.com (score: 0.88)

Average score = (0.95 + 0.88) / 2 = 0.915
{{/code}}

----

**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Overview and Quick Start>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Overview and Quick Start.WebHome]] | Next: [[Configuration and Scoring>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome]]
