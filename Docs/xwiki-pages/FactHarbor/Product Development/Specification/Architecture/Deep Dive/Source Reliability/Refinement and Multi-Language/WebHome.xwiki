= Source Reliability: Refinement and Multi-Language =

== v1.1 Prompt Improvements (January 2026) ==

**Date**: 2026-01-24
**Focus**: Prompt structure, quantification, and consistency across models

Version 1.1 improved the LLM evaluation prompt for source reliability assessments to increase stability, consistency, and effectiveness across different models.

=== Key Improvements ===

|= # |= Improvement |= Impact |= Description
| 1 | Restructured Prompt Hierarchy | High | Moved critical rules to top with visual indicator, ensuring LLMs see most important constraints first
| 2 | Quantified "Insufficient Data" Thresholds | High | Added specific numeric thresholds (e.g., "Zero fact-checker assessments AND <=1 weak mention") to reduce inter-model disagreement
| 3 | Mechanistic Confidence Scoring Formula | High | Created step-by-step calculation formula with base (0.40) + additive factors, making confidence reproducible across models
| 4 | Numeric Negative Evidence Caps | Medium-High | Made caps explicit with numbers (e.g., "3+ failures -> score <= 0.42") to prevent lenient scoring
| 5 | Quantified Recency Weighting | Medium | Converted subjective terms to multipliers (0-12mo: 1.0x, 12-24mo: 0.8x, 2-5yr: 0.5x, >5yr: 0.2x)
| 6 | Evidence Quality Hierarchy | Medium | Created three-tier hierarchy (HIGH/MEDIUM/LOW weight) to prevent single weak sources from dominating
| 7 | Enhanced Calibration Examples | Medium | Added confidence calculations and reasoning to examples, showing models exactly how to apply formulas
| 8 | Improved Source Type Positioning | Low-Medium | Renamed section to "SOURCE TYPE CLASSIFICATION (classify FIRST, then evaluate within category)"
| 9 | Enhanced System Message | Low-Medium | Made system message tactical with specific responsibilities (evidence-only, caps, formula)
| 10 | Expanded Validation Checklist | Low-Medium | Added checklist items for all critical rules to help models catch errors before responding

=== Mechanistic Confidence Formula ===

**Base**: 0.40

**ADD**:
* +0.15 per independent fact-checker assessment (max +0.45 for 3+)
* +0.10 if most evidence is within last 12 months
* +0.10 if evidence shows consistent pattern (3+ sources agree)
* +0.05 per additional corroborating source beyond first (max +0.15)

**SUBTRACT**:
* -0.15 if evidence is contradictory/mixed signals
* -0.10 if evidence is mostly >2 years old

**Final confidence**: clamp result to [0.0, 1.0]

**THRESHOLD**: If calculated confidence < 0.50, strongly consider outputting ##score=null## and ##factualRating="insufficient_data"##

=== Negative Evidence Caps (v1.1) ===

|= Evidence Type |= Score Cap |= Rating Cap
| Evidence of fabricated stories/disinformation | <= 0.14 | highly_unreliable
| 3+ documented fact-checker failures | <= 0.42 | leaning_unreliable
| 1-2 documented failures from reputable fact-checkers | <= 0.57 | mixed
| Political/ideological bias WITHOUT documented failures | No cap | Note in bias field only

=== Evidence Quality Hierarchy ===

**HIGH WEIGHT** (can establish verdict alone):
* Explicit fact-checker assessments (MBFC, Snopes, PolitiFact, etc.)
* Documented corrections/retractions by the source
* Journalism reviews from reputable organizations

**MEDIUM WEIGHT** (support but don't establish alone):
* Newsroom analyses of editorial standards
* Academic studies on source reliability
* Awards/recognition from journalism organizations

**LOW WEIGHT** (context only, cannot trigger caps):
* Single blog posts or forum discussions
* Passing mentions without substantive analysis
* Generic references without reliability details

----

== v1.2 Hardening (January 2026) ==

Version 1.2 introduces significant improvements to scoring accuracy, especially for propaganda and misinformation sources.

=== Key Changes ===

|= Feature |= Description
| **Entity-Level Evaluation** | Prioritize organization reputation over domain-only metrics when the domain is a primary outlet for an established organization
| **SOURCE TYPE SCORE CAPS** | Prompt-driven caps (UCM configurable): ##propaganda_outlet##/##known_disinformation## -> <=14%, ##state_controlled_media##/##platform_ugc## -> <=42%; code validates and warns but does not override
| **Adaptive Evidence Queries** | Negative-signal queries (##propaganda##, ##disinformation##, ##false claims##) added when initial results are sparse
| **Brand Variant Matching** | Improved relevance filtering: handles brand variants, suffix stripping
| **Mechanistic Confidence** | Formula-based confidence scoring: base 0.40 + factors (fact-checkers, recency, corroboration)
| **Asymmetric Confidence Gating** | High scores require higher confidence (skeptical default)
| **Unified Thresholds** | Admin + pipeline + evaluator use same defaults (confidence: 0.8)

=== Source Type Caps (Prompt-Driven, UCM Configurable) ===

{{code}}
propaganda_outlet       -> score <= 0.14 (highly_unreliable)
known_disinformation    -> score <= 0.14 (highly_unreliable)
state_controlled_media  -> score <= 0.42 (leaning_unreliable)
platform_ugc            -> score <= 0.42 (leaning_unreliable)
{{/code}}

These caps are defined in the SR prompt template (UCM configurable). Code-side validation adds a caveat if the LLM exceeds a cap but does **not** override the score (prompt is authoritative since v2.8.3).

=== Asymmetric Confidence Requirements ===

High reliability scores require stronger evidence (skeptical default):

|= Rating |= Min Confidence
| highly_reliable | 0.85
| reliable | 0.75
| leaning_reliable | 0.65
| mixed | 0.55
| leaning_unreliable | 0.50
| unreliable | 0.45
| highly_unreliable | 0.40

----

== Sequential Refinement Architecture ==

The source reliability system uses **sequential refinement** for accurate entity-level evaluation.

=== Architecture ===

{{code}}
Evidence Pack -> Claude (Initial Evaluation) -> Initial Result
                         |
Evidence Pack + Initial Result -> OpenAI mini model (Cross-check and Refine) -> Final Result
{{/code}}

**Key characteristics**:
* The secondary model can catch what the initial model missed (especially entity recognition)
* Explicit cross-checking of entity identification
* Baseline score adjustments for known organization types
* Reasoning transparency through refinement logic

=== Refinement Process ===

The secondary OpenAI model (default: ##gpt-4o-mini##) receives:
1. The original evidence pack
1. The complete initial evaluation from Claude
1. Instructions to cross-check, sharpen entity identification, and refine the score

=== Shared Prompt Sections ===

Both LLM1 (initial evaluation) and LLM2 (refinement) use **shared prompt constants** for consistency:

|= Section |= Purpose
| Rating Scale | Score -> rating mapping (0.86+ = highly_reliable, etc.)
| Evidence Signals | Positive/neutral signal definitions
| Bias Values | Political and other bias enum values
| Source Types | Classification definitions and triggers
| Score Caps | Hard limits for severe source types

=== Refinement Adjustment Rules ===

* **UPWARD adjustment** requires positive signals PRESENT in evidence (academic citations, professional use, independent authoritative mentions)
* **DOWNWARD adjustment** if negative signals were missed or underweighted
* **NO adjustment** if evidence is simply sparse (sparse does not equal positive)
* Absence of negative evidence alone does NOT justify upward adjustment

----

== Multi-Language Support ==

The system includes automatic language detection and multi-language search queries to find regional fact-checker assessments.

=== Language Detection ===

The system detects the **actual publication language** (not TLD) by:

1. Fetching the homepage (5s timeout)
1. Checking ##<html lang="...">## attribute
1. Checking ##<meta http-equiv="content-language">##
1. Checking ##<meta property="og:locale">##
1. If all fail: LLM analyzes content sample

Results are cached per domain.

=== Multi-Language Queries ===

When a non-English language is detected:

1. **LLM translates** key fact-checking terms (cached per language)
1. **Dual-language searches** are performed:
1*. English queries (always, for international coverage)
1*. Translated queries (for regional fact-checkers)

=== Supported Languages ===

German, French, Spanish, Portuguese, Italian, Dutch, Polish, Russian, Swedish, Norwegian, Danish, Finnish, Czech, Hungarian, Turkish, Japanese, Chinese, Korean, Arabic.

=== Regional Fact-Checkers (Tier 1) ===

|= Language |= Fact-Checkers
| German | CORRECTIV, Mimikama, dpa-Faktencheck, Faktenfinder (ARD)
| French | AFP Factuel, Les Decodeurs (Le Monde), Liberation CheckNews
| Spanish | Maldita.es, Newtral, EFE Verifica
| Portuguese | Aos Fatos, Lupa, Poligrafo
| Italian | Pagella Politica, ANSA Fact-checking
| Dutch | Nu.nl Factcheck, Nieuwscheckers

=== Cost Impact ===

|= Component |= Cost per Evaluation
| Language detection (page fetch) | Free
| Translation (LLM, cached) | ~$0.001 per new language
| Additional searches | ~2-4 extra queries

=== Response Fields ===

|= Field |= Type |= Description
| ##refinementApplied## | boolean | Whether the score was adjusted by cross-check
| ##refinementNotes## | string | Summary of cross-check findings
| ##originalScore## | number | Score before refinement (if changed)

----

**Navigation:** [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Configuration and Scoring>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Configuration and Scoring.WebHome]] | Next: [[Admin and Implementation>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.Admin and Implementation.WebHome]]
