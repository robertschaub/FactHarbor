= Context and EvidenceScope Detection =

{{info}}
**Developer Reference** — How FactHarbor detects, refines, and merges analytical contexts and evidence scope metadata across the pipeline.

**Key File**: ##apps/web/src/lib/analyzer/analysis-contexts.ts##
{{/info}}

----

== 1. Overview ==

This guide explains:

* **What** contexts and evidence scopes are (definitions)
* **When** to use AnalysisContext vs EvidenceScope (decision tree)
* **How** contexts are detected across pipeline phases (flow)
* **Why** the system uses principle-based detection (approach)
* **Where** the implementation lives (code references)

----

== 2. Terminology ==

=== 2.1 Core Definitions ===

**AnalysisContext** (Top-Level Bounded Analytical Frame):
* A distinct analytical frame requiring separate analysis
* **Scope**: Top-level question or thesis being analyzed
* **Example**: "Legal proceeding A fairness" vs "Legal proceeding B fairness" are distinct contexts
* **Storage**: ##article.analysisContexts## array, ##claim.contextId##, ##verdict.contextId##

**EvidenceScope** (Per-Evidence Source Methodology):
* Metadata about a single evidence item's boundaries and methodology
* **Scope**: Per-evidence source constraints (methodology, boundaries, temporal, geographic)
* **Example**: "Study used Framework A with full system boundary"
* **Storage**: ##evidenceItem.evidenceScope## object

**Key Distinction**: AnalysisContext = "What question am I answering?" vs EvidenceScope = "What boundaries did this evidence source use?"

=== 2.2 Terminology Usage Rules ===

**FactHarbor Entities** (use in prompts and code):
* **Evidence**: Information extracted from sources
* **Verdict**: Conclusions/assessments produced by analysis
* **AnalysisContext**: Top-level bounded analytical frame
* **EvidenceScope**: Per-Evidence source methodology metadata

**Avoid in prompts**:
* ~~fact/facts~~ -> Use "Evidence" / "Evidence items"
* ~~scope~~ (ambiguous) -> Use "AnalysisContext" or "EvidenceScope" explicitly

----

== 3. AnalysisContext vs EvidenceScope Decision Tree ==

{{mermaid}}
flowchart TD
    START{"What am I describing?"}

    START -->|"Top-level<br/>question/thesis"| Q1{"Does it require<br/>separate analysis?"}
    START -->|"Evidence source<br/>metadata"| SCOPE["Use EvidenceScope"]

    Q1 -->|"Yes — different questions"| CTX["Use AnalysisContext"]
    Q1 -->|"No — same question,<br/>different perspectives"| NOCTX["Single AnalysisContext"]

    CTX --> EX1["Example:<br/>'Proceeding A fairness' vs<br/>'Proceeding B fairness'<br/>= 2 AnalysisContexts"]

    NOCTX --> EX2["Example:<br/>'Is claim X true?' with<br/>sources from different orgs<br/>= 1 AnalysisContext"]

    SCOPE --> EX3["Example:<br/>evidenceScope: methodology<br/>= 'Framework A',<br/>boundaries = 'Full system'"]

    style CTX fill:#c8e6c9,color:#000
    style SCOPE fill:#fff9c4,color:#000
    style NOCTX fill:#e3f2fd,color:#000
{{/mermaid}}

=== 3.1 When to Create Multiple AnalysisContexts ===

Create separate AnalysisContexts when:
1. **Different Questions**: Evidence answers fundamentally different questions
1*. "Was Proceeding A fair?" vs "Was Proceeding B fair?"

1. **Incompatible Methodologies**: Combining conclusions would be misleading
1*. "Efficiency using Framework A boundary" vs "Efficiency using Framework B boundary"

1. **Distinct Temporal Subjects**: Time periods are the primary subject (not incidental)
1*. "Policy effectiveness in Period 1" vs "Policy effectiveness in Period 2"

=== 3.2 When to Use EvidenceScope (Not AnalysisContext) ===

Use EvidenceScope metadata when:
1. **Source Methodology Markers**: Evidence states its analytical boundaries
1*. "This study uses ISO 14040 methodology"

1. **Provenance Tracking**: Distinguishing evidence source constraints

1. **Verdict Enrichment**: Noting source boundaries without splitting contexts

----

== 4. Detection Pipeline Flow ==

=== 4.1 Multi-Phase Context Detection ===

{{mermaid}}
flowchart TB
    subgraph UNDERSTAND["Phase 1: UNDERSTAND"]
        Input[User Input] --> InitialDetection["Initial Context Detection<br/>From input text alone<br/>No evidence yet"]
        InitialDetection --> Claims["Claim Extraction<br/>Claims tagged with contextId"]
    end

    subgraph RESEARCH["Phase 2: RESEARCH"]
        Claims --> Search[Web Search]
        Search --> Sources[Source Documents]
    end

    subgraph EXTRACT["Phase 3: EXTRACT_EVIDENCE"]
        Sources --> EV[Evidence Extraction]
        EV --> ES["EvidenceScope Metadata<br/>Per-evidence boundaries captured"]
    end

    subgraph REFINE["Phase 4: CONTEXT_REFINEMENT"]
        InitialDetection --> Merge["Merge/Refine Contexts<br/>Evidence-based discovery<br/>Overlap detection<br/>LLM-driven merge"]
        EV --> Merge
        ES -.->|"Informs context<br/>discovery"| Merge
        Merge --> FinalContexts["Final Contexts<br/>Max 5 contexts<br/>Warnings if 4-5"]
    end

    style InitialDetection fill:#fff3e0,color:#000
    style ES fill:#fff9c4,color:#000
    style Merge fill:#c8e6c9,color:#000
    style FinalContexts fill:#e3f2fd,color:#000
{{/mermaid}}

//Orange = initial detection from input. Yellow = EvidenceScope metadata. Green = context refinement. Blue = final contexts.//

=== 4.2 Context Detection Points ===

|= Phase |= Context Detection |= Uses Evidence? |= Source
| **UNDERSTAND** | Initial detection from input text | No | Input analysis
| **EXTRACT_EVIDENCE** | EvidenceScope metadata capture | Yes | Source documents
| **CONTEXT_REFINEMENT** | Evidence-based discovery and merge | Yes | EvidenceScope patterns

=== 4.3 Data Flow ===

**Phase 1: Initial Context Detection**
{{code language="typescript"}}
interface AnalysisContext {
  id: string;              // e.g., "CTX_INST_A", "CTX_METHOD_X"
  name: string;            // Human-readable name
  shortName: string;       // Abbreviation
  institution?: string;    // Court, agency, organization
  methodology?: string;    // Standard/method used
  boundaries?: string;     // What's included/excluded
  temporal?: string;       // Time period
  subject: string;         // What's being analyzed
  status: "concluded" | "ongoing" | "pending" | "unknown";
}
{{/code}}

**Phase 3: EvidenceScope Capture**
{{code language="typescript"}}
interface EvidenceScope {
  name: string;         // "Framework A", "Court B"
  methodology: string;  // "Standard X", "Full system boundary"
  boundaries: string;   // "Full system", "Subsystem only"
  geographic: string;   // "Region A"
  temporal: string;     // "Period 1"
}
{{/code}}

----

== 5. Principle-Based Detection Rules ==

=== 5.1 The Single Incompatibility Test ===

Instead of hardcoding specific terms or categories, FactHarbor uses **ONE universal test**:

> **"Would combining findings from this source with other sources be MISLEADING because they measure or analyze fundamentally different things?"**

* **YES** -> Extract EvidenceScope (document what makes it incompatible)
* **NO** -> Don't extract (boundaries are compatible enough)

=== 5.2 Key Principles ===

1. **Selective extraction**: Most analyses have 0-1 significant EvidenceScope patterns, max 2-3
1. **Explicit statements only**: Don't invent boundaries the source didn't state
1. **Incompatibility focus**: Only flag boundaries that would cause apples-to-oranges comparisons
1. **Synonym recognition**: Sources use various terms (scope, delimitations, limitations, inclusion criteria)

=== 5.3 Pattern-Based Heuristics ===

**Comparative claims** (input asks "A vs B"):
* Evidence likely has data for A and data for B
* Check if A's scope differs from B's scope
* If yes, separate contexts

**Multi-process subjects** (input mentions institutions/proceedings):
* Evidence may reference multiple formal processes
* Different processes = different analytical frames
* Check if institutional references cluster

**Historical comparisons** (input asks about change over time):
* Evidence may cover different time periods
* Distinct events (not just dates) = distinct contexts

----

== 6. Overlap Detection and Merge Heuristics ==

=== 6.1 Context Overlap Problem ===

**Problem**: Over-splitting contexts leads to high context counts, redundant analyses, and confusing verdicts.

**Solution**: LLM-driven merge heuristics with explicit guidance.

=== 6.2 Merge Criteria ===

**Merge contexts when**:
1. **Subject Overlap**: Both contexts address the same core question
1*. "Institution A handling of Issue X" + "Institution A process for Issue X" -> Merge

1. **Temporal Continuity**: Time periods are parts of a continuous event
1*. "Policy Phase 1 (2020)" + "Policy Phase 2 (2021)" -> Merge if phases are related

1. **Methodological Compatibility**: Boundaries/standards are compatible
1*. "Study using Framework A, full boundary" + "Study using Framework A, subsystem" -> Keep separate

=== 6.3 Temporal Guidance ===

**Temporal markers create contexts when**:
* Time period is the PRIMARY SUBJECT (e.g., "Compare 2000s vs 1970s")
* NOT when it's incidental (e.g., "Study conducted in 2023")

----

== 7. Context Count and Reliability ==

=== 7.1 Thresholds and Limits ===

|= Context Count |= Status |= Meaning
| **1-3** | Healthy | Typical for most analyses
| **4-5** | High | Complex analytical frame with multiple incompatible boundaries
| **5+** | Limit | System enforces max via ##contextDetectionMaxContexts## config

=== 7.2 Multi-Context Average Reliability ===

**Single Context (Most Common)**: Average = context verdict (identical). Overall verdict fully represents the analysis.

**Multiple Contexts (Distinct Frames)**: Average may not be meaningful if contexts answer different questions. Example: "Legal trial fairness (85%)" + "Scientific validity (30%)" = 57.5% average — which doesn't represent either question well.

**Architecture Decision (v2.6.38)**:
* Simple averaging chosen over complex weighting schemes
* Transparency via ##articleVerdictReliability## flag (##"high"## or ##"low"##) and explicit messaging
* Individual context verdicts always preserved and displayed
* When reliability is low, de-emphasize overall average and highlight individual context verdicts

----

== 8. Implementation Reference ==

=== 8.1 Core Files ===

|= File |= Purpose
| ##apps/web/src/lib/analyzer/analysis-contexts.ts## | Heuristic pre-detection, ##detectContexts()##
| ##apps/web/src/lib/analyzer/orchestrated.ts## | Orchestrated pipeline (primary)
| ##apps/web/src/lib/analyzer/monolithic-dynamic.ts## | Dynamic monolithic pipeline

**Prompt Templates**:
* ##apps/web/src/lib/analyzer/prompts/base/extract-evidence-base.ts## — EvidenceScope detection guidance
* ##apps/web/src/lib/analyzer/prompts/base/context-refinement-base.ts## — Context discovery and merge guidance

=== 8.2 Key Functions ===

{{code language="typescript"}}
// Heuristic pre-detection (optional)
detectContexts(inputText: string): DetectedContext[]

// Format detected contexts for LLM hint
formatDetectedContextsHint(contexts: DetectedContext[]): string

// Aggregation
calculateWeightedVerdictAverage(verdicts: ClaimVerdict[]): number
{{/code}}

=== 8.3 Configuration ===

{{code language="json"}}
{
  "contextDetectionMaxContexts": 5,
  "contextDetectionMergeThreshold": 0.7
}
{{/code}}

----

== 9. Examples ==

=== 9.1 Institutional Boundary -> Distinct Contexts ===

**Input**: "Is Decision X justified?"
**EvidenceScope**: Source A: Institution A / Standard A; Source B: Institution B / Standard B
**Result**: 2 AnalysisContexts (different formal bodies/standards)

=== 9.2 Methodology Boundary -> Distinct Contexts ===

**Input**: "Is Method X more effective than Method Y?"
**EvidenceScope**: Source A: Framework A / Full system; Source B: Framework B / Subsystem
**Result**: 2 AnalysisContexts (incompatible methodological boundaries)

=== 9.3 Temporal Mention vs Temporal Subject ===

**Input**: "Did Policy Z have the intended outcome?"
**Result**: Split only if temporal is primary subject, not incidental date mention.

=== 9.4 Overlap -> Merge ===

**Input**: "Is Claim Q supported?"
**EvidenceScope**: Both sources declare same boundary, methodology, and time window
**Result**: 1 AnalysisContext (strong overlap -> merge)

----

== 10. Related Documentation ==

* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] — Verdict calculation methodology, AnalysisContext aggregation
* [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] — Quality gates reference (Gate 1, Gate 4)
* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] — Pipeline architecture and context routing
* [[Terminology>>FactHarbor.Product Development.Specification.Reference.Terminology.WebHome]] — AnalysisContext vs EvidenceScope definitions
* [[Evidence Quality Filtering>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] — Evidence filtering and classification

----

**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Quality Gates>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] | Next: [[Source Reliability>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]
