= Prompt Architecture =

{{info}}
**Developer Reference** — UCM-managed prompt system: all runtime LLM prompts load from ##.prompt.md## files via ##loadAndRenderSection()##. TypeScript prompt modules under ##prompts/## are retained for testing only.

**Key Files**: ##apps/web/prompts/*.prompt.md## (runtime), ##apps/web/src/lib/analyzer/prompt-loader.ts## (loader), ##apps/web/src/lib/analyzer/prompts/## (testing harness only)
{{/info}}

----

== 1. Overview ==

FactHarbor uses **UCM-managed prompt files** (##.prompt.md##) as the single source of truth for all LLM prompts at runtime (**v2.8.2**). Per AGENTS.md String Usage Boundary, all text that goes into LLM prompts must be UCM-managed, not hardcoded in TypeScript.

=== 1.1 Runtime Architecture (Production) ===

Both pipelines load prompts via ##loadAndRenderSection()## from UCM-managed ##.prompt.md## files:

1. **Section-based prompts** — Each ##.prompt.md## file contains named sections (##~#~# SECTION_NAME##) with variable substitution (##$~{variableName}##)
1. **Provider-specific structured output** — Separate sections per provider (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##)
1. **UCM database backing** — Prompt files are seeded into SQLite config DB on first use and can be edited via Admin UI

=== 1.2 Legacy Testing Harness (Test-Only) ===

A separate TypeScript prompt composition system (##buildPrompt()## in ##prompt-builder.ts##) exists under ##apps/web/src/lib/analyzer/prompts/## but is **only used by the ##prompt-testing.ts## test harness**. It is NOT called by any production pipeline.

----

== 2. Directory Structure ==

=== 2.1 Runtime Prompts (UCM-Managed) ===

{{code language="none"}}
apps/web/prompts/
+-- claimboundary.prompt.md         # ClaimAssessmentBoundary pipeline prompts (13 sections)
+-- monolithic-dynamic.prompt.md    # Monolithic Dynamic pipeline prompts (7 sections)
+-- source-reliability.prompt.md    # Source reliability evaluation prompts
+-- text-analysis/                  # LLM text analysis pipeline prompts
    +-- *.prompt.md
    +-- README.md
{{/code}}

Each ##.prompt.md## file has YAML frontmatter (version, pipeline, variables, requiredSections) followed by ##~#~# SECTION_NAME## headers with prompt content. The ##loadAndRenderSection(pipeline, sectionName, variables)## function extracts and renders sections with variable substitution.

=== 2.2 Testing Harness (TypeScript, Not Used in Production) ===

{{code language="none"}}
apps/web/src/lib/analyzer/prompts/  # TESTING HARNESS ONLY
+-- prompt-builder.ts              # Composition engine (testing only)
+-- prompt-testing.ts              # Testing utilities
+-- OUTPUT_SCHEMAS.md              # Schema documentation
+-- base/                          # Task-specific base prompts (testing only)
+-- providers/                     # LLM-specific variants (testing only)
+-- config-adaptations/            # Configuration adaptations (testing only)
{{/code}}

----

== 3. Prompt Loading Flow (Runtime) ==

{{mermaid}}
flowchart LR
    UCM["UCM Database<br/>(config_blobs)"] --> LOAD["loadPromptConfig()<br/>Load active prompt"]
    LOAD --> PARSE["extractSections()<br/>Parse ## headers"]
    PARSE --> RENDER["renderSection()<br/>Variable substitution"]
    RENDER --> FINAL["Rendered Prompt<br/>Section"]

    style UCM fill:#e3f2fd,color:#000
    style LOAD fill:#c8e6c9,color:#000
    style PARSE fill:#fff9c4,color:#000
    style RENDER fill:#ffe0b2,color:#000
    style FINAL fill:#e1bee7,color:#000
{{/mermaid}}

//Blue = UCM database, Green = config loader, Yellow = section parser, Orange = variable substitution, Purple = final output.//

Provider-specific structured output is loaded as a separate section (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##) and concatenated to the main prompt:

{{code language="typescript"}}
const provider = detectProvider(model.modelName);
const section = `STRUCTURED_OUTPUT_${provider.toUpperCase()}`;
const rendered = await loadAndRenderSection("monolithic-dynamic", section, {});
{{/code}}

----

== 4. Pipeline Prompt Sections ==

=== 4.1 ClaimAssessmentBoundary Pipeline ===

**File**: ##apps/web/prompts/claimboundary.prompt.md## (13 sections, pipeline: "claimboundary", version: 1.0.0)

Each section covers exactly one LLM call point. Sections are mapped to the 5 pipeline stages:

|= Section |= Purpose |= Stage |= Model Tier
| ##CLAIM_EXTRACTION_PASS1## | Initial atomic claim extraction from user input | Stage 1: ##extractClaims## | Haiku
| ##CLAIM_EXTRACTION_PASS2## | Claim consolidation, dependency analysis, counter-claim tagging | Stage 1: ##extractClaims## | Haiku
| ##CLAIM_VALIDATION## | Gate 1: filter opinions, predictions, unanswerable claims | Stage 1: ##extractClaims## | Haiku
| ##GENERATE_QUERIES## | Generate research queries per ##AtomicClaim## | Stage 2: ##researchEvidence## | Haiku
| ##RELEVANCE_CLASSIFICATION## | Assess search result relevance to each claim | Stage 2: ##researchEvidence## | Haiku
| ##EXTRACT_EVIDENCE## | Extract ##EvidenceItems## from fetched source content | Stage 2: ##researchEvidence## | Haiku
| ##BOUNDARY_CLUSTERING## | Cluster ##EvidenceScopes## into ##ClaimAssessmentBoundaries## | Stage 3: ##clusterBoundaries## | Haiku
| ##VERDICT_ADVOCATE## | Construct pro-thesis verdict for a boundary | Stage 4: ##generateVerdicts## | Sonnet
| ##VERDICT_CHALLENGER## | Identify counter-evidence and weaknesses in Advocate verdict | Stage 4: ##generateVerdicts## | Sonnet
| ##VERDICT_RECONCILIATION## | Synthesize Advocate/Challenger into final verdict + confidence | Stage 4: ##generateVerdicts## | Sonnet
| ##VERDICT_GROUNDING_VALIDATION## | Advisory: verify verdict reasoning traces back to evidence | Stage 4: ##generateVerdicts## | Haiku
| ##VERDICT_DIRECTION_VALIDATION## | Advisory: verify ##claimDirection## alignment with verdict | Stage 4: ##generateVerdicts## | Haiku
| ##VERDICT_NARRATIVE## | Generate human-readable verdict narrative | Stage 5: ##aggregateAssessment## | Sonnet

The Advocate → Challenger → Reconciliation pattern in Stage 4 is the CB pipeline's core design: reliable initial verdicts emerge from structured debate, eliminating the need for post-hoc auto-correction.

{{info}}
**Note**: ##orchestrated.prompt.md## (40+ sections) was removed in v2.11.0 alongside the Orchestrated pipeline. Archive available at ##Docs/xwiki-pages-ARCHIVE/...##.
{{/info}}

=== 4.2 Monolithic Dynamic Pipeline ===

**File**: ##apps/web/prompts/monolithic-dynamic.prompt.md##

|= Section |= Purpose |= Variables
| ##DYNAMIC_PLAN## | System prompt for planning/research phase | ##$~{currentDate}##
| ##DYNAMIC_ANALYSIS## | System prompt for analysis/verdict phase | ##$~{currentDate}##
| ##DYNAMIC_ANALYSIS_USER## | User message template with input + sources | ##$~{TEXT_TO_ANALYZE}##, ##$~{SOURCE_SUMMARY}##
| ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON output guidance for Claude | (none)
| ##STRUCTURED_OUTPUT_OPENAI## | JSON output guidance for GPT | (none)
| ##STRUCTURED_OUTPUT_GOOGLE## | JSON output guidance for Gemini | (none)
| ##STRUCTURED_OUTPUT_MISTRAL## | JSON output guidance for Mistral | (none)

----

== 5. Provider-Specific Optimizations ==

Provider-specific structured output guidance is embedded as named sections in each ##.prompt.md## file. The pipeline code dynamically selects the right section based on the detected provider.

|= Provider |= Section Name |= Key Guidance
| **Anthropic (Claude)** | ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON format rules, empty strings for optional fields, field validation
| **OpenAI (GPT)** | ##STRUCTURED_OUTPUT_OPENAI## | Field completeness, common GPT errors to avoid
| **Google (Gemini)** | ##STRUCTURED_OUTPUT_GOOGLE## | Length enforcement, no explanatory text outside JSON
| **Mistral** | ##STRUCTURED_OUTPUT_MISTRAL## | Validation checklist format, field type verification

The ##claimboundary.prompt.md## and ##monolithic-dynamic.prompt.md## files each contain ##STRUCTURED_OUTPUT_<PROVIDER>## sections for provider-specific output guidance.

----

== 6. Configuration Adaptations ==

=== 6.1 LLM Tiering (##isLLMTiering##) ===

When tiering is enabled with a fast-tier model:
* Adjusts complexity expectations per task
* Optimizes token usage for fast-tier models (Haiku, Mini, Flash)

=== 6.2 Knowledge Mode (##allowModelKnowledge##) ===

Managed via UCM prompt sections:
* **##allowModelKnowledge=true##**: Loads ##KNOWLEDGE_INSTRUCTION_ALLOW_MODEL## — LLM may supplement evidence with training knowledge
* **##allowModelKnowledge=false##**: Loads ##KNOWLEDGE_INSTRUCTION_EVIDENCE_ONLY## — LLM must rely strictly on fetched evidence

----

== 7. Provider Detection ==

The ##detectProvider()## function (in ##prompt-builder.ts##) maps model names to provider types:

{{code language="typescript"}}
function detectProvider(modelName: string): ProviderType {
  const lowerName = modelName.toLowerCase();

  if (lowerName.includes('claude')) return 'anthropic';
  if (lowerName.includes('gpt'))    return 'openai';
  if (lowerName.includes('gemini')) return 'google';
  if (lowerName.includes('mistral')) return 'mistral';

  return 'anthropic'; // Default fallback
}
{{/code}}

The ##isBudgetModel()## function identifies fast-tier models (Haiku, Mini, Flash, Mistral Small/Medium) for tiering decisions.

----

== 8. Adding a New Provider ==

1. Add a ##STRUCTURED_OUTPUT_NEWPROVIDER## section to ##claimboundary.prompt.md## and ##monolithic-dynamic.prompt.md##
1. Update ##detectProvider()## in ##prompt-builder.ts## to recognize new model IDs
1. Update ##isBudgetModel()## if the provider has fast-tier models
1. Test with ##prompt-optimization.test.ts## and ##monolithic-dynamic-prompt.test.ts##

----

== 9. Token Optimization ==

=== 9.1 v2.8.0 Achievements ===

**20-30% token reduction** across prompts through a two-phase optimization:

|= Phase |= Strategy |= Token Savings
| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved
| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved

=== 9.2 Design Principles ===

* **Prompt files are authoritative** — code should not duplicate or override prompt logic
* **Variable substitution** — dynamic values injected at render time, not hardcoded
* **Section composition** — complex prompts assembled from multiple sections (e.g., system + mode + provider hint)

----

== 10. Changelog ==

=== v2.11.0: ClaimAssessmentBoundary Pipeline (Feb 17, 2026) ===

##claimboundary.prompt.md## (13 sections) introduced as the primary runtime prompt file.

* ##orchestrated.prompt.md## (40+ sections) removed alongside the Orchestrated pipeline
* CB pipeline uses 5-stage structure: extractClaims → researchEvidence → clusterBoundaries → generateVerdicts → aggregateAssessment
* Verdict generation uses Advocate/Challenger/Reconciliation debate pattern (3 Sonnet calls)
* Advisory validation sections (##VERDICT_GROUNDING_VALIDATION##, ##VERDICT_DIRECTION_VALIDATION##) added — non-blocking, Haiku tier

=== v2.8.2: Prompt Externalization to UCM (Feb 13, 2026) ===

All runtime LLM prompts load from UCM-managed ##.prompt.md## files, compliant with AGENTS.md String Usage Boundary.

* Search relevance mode instructions moved from inline code to prompt file sections
* Monolithic-dynamic system prompts moved from ##buildPrompt()## to ##loadAndRenderSection()##
* 4 provider-specific structured output sections added to ##monolithic-dynamic.prompt.md##
* TypeScript prompt modules retained for testing harness only

=== v2.8.0: Token Optimization (Feb 3, 2026) ===

**20-30% token reduction** across prompts through two-phase optimization.

|= Phase |= Strategy |= Token Savings
| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved
| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved

----

== 11. Related Documentation ==

* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] — Pipeline architecture and shared primitives that consume these prompts
* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] — Verdict calculation methodology driven by prompt outputs
* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] — CB pipeline (default) and Monolithic Dynamic pipeline showing where each prompt is invoked
* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] — Confidence scoring that depends on prompt-extracted evidence quality
* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] — Architecture overview

----

**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] | Next: [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]
