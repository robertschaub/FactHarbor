= Prompt Architecture =

{{info}}
**Developer Reference** — UCM-managed prompt system: all runtime LLM prompts load from ##.prompt.md## files via ##loadAndRenderSection()##. TypeScript prompt modules under ##prompts/## are retained for testing only.

**Key Files**: ##apps/web/prompts/*.prompt.md## (runtime), ##apps/web/src/lib/analyzer/prompt-loader.ts## (loader), ##apps/web/src/lib/analyzer/prompts/## (testing harness only)
{{/info}}

----

== 1. Overview ==

FactHarbor uses **UCM-managed prompt files** (##.prompt.md##) as the single source of truth for all LLM prompts at runtime (**v2.8.2**). Per AGENTS.md String Usage Boundary, all text that goes into LLM prompts must be UCM-managed, not hardcoded in TypeScript.

=== 1.1 Runtime Architecture (Production) ===

Both pipelines load prompts via ##loadAndRenderSection()## from UCM-managed ##.prompt.md## files:

1. **Section-based prompts** — Each ##.prompt.md## file contains named sections (##~#~# SECTION_NAME##) with variable substitution (##$~{variableName}##)
1. **Provider-specific structured output** — Separate sections per provider (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##)
1. **UCM database backing** — Prompt files are seeded into SQLite config DB on first use and can be edited via Admin UI

=== 1.2 Legacy Testing Harness (Test-Only) ===

A separate TypeScript prompt composition system (##buildPrompt()## in ##prompt-builder.ts##) exists under ##apps/web/src/lib/analyzer/prompts/## but is **only used by the ##prompt-testing.ts## test harness**. It is NOT called by any production pipeline.

----

== 2. Directory Structure ==

=== 2.1 Runtime Prompts (UCM-Managed) ===

{{code language="none"}}
apps/web/prompts/
+-- orchestrated.prompt.md          # Orchestrated pipeline prompts (all sections)
+-- monolithic-dynamic.prompt.md    # Monolithic Dynamic pipeline prompts
+-- source-reliability.prompt.md    # Source reliability evaluation prompts
+-- text-analysis/                  # LLM text analysis pipeline prompts
    +-- *.prompt.md
    +-- README.md
{{/code}}

Each ##.prompt.md## file has YAML frontmatter (version, pipeline, variables, requiredSections) followed by ##~#~# SECTION_NAME## headers with prompt content. The ##loadAndRenderSection(pipeline, sectionName, variables)## function extracts and renders sections with variable substitution.

=== 2.2 Testing Harness (TypeScript, Not Used in Production) ===

{{code language="none"}}
apps/web/src/lib/analyzer/prompts/  # TESTING HARNESS ONLY
+-- prompt-builder.ts              # Composition engine (testing only)
+-- prompt-testing.ts              # Testing utilities
+-- OUTPUT_SCHEMAS.md              # Schema documentation
+-- base/                          # Task-specific base prompts (testing only)
+-- providers/                     # LLM-specific variants (testing only)
+-- config-adaptations/            # Configuration adaptations (testing only)
{{/code}}

----

== 3. Prompt Loading Flow (Runtime) ==

{{mermaid}}
flowchart LR
    UCM["UCM Database<br/>(config_blobs)"] --> LOAD["loadPromptConfig()<br/>Load active prompt"]
    LOAD --> PARSE["extractSections()<br/>Parse ## headers"]
    PARSE --> RENDER["renderSection()<br/>Variable substitution"]
    RENDER --> FINAL["Rendered Prompt<br/>Section"]

    style UCM fill:#e3f2fd,color:#000
    style LOAD fill:#c8e6c9,color:#000
    style PARSE fill:#fff9c4,color:#000
    style RENDER fill:#ffe0b2,color:#000
    style FINAL fill:#e1bee7,color:#000
{{/mermaid}}

//Blue = UCM database, Green = config loader, Yellow = section parser, Orange = variable substitution, Purple = final output.//

Provider-specific structured output is loaded as a separate section (e.g., ##STRUCTURED_OUTPUT_ANTHROPIC##) and concatenated to the main prompt:

{{code language="typescript"}}
const provider = detectProvider(model.modelName);
const section = `STRUCTURED_OUTPUT_${provider.toUpperCase()}`;
const rendered = await loadAndRenderSection("monolithic-dynamic", section, {});
{{/code}}

----

== 4. Pipeline Prompt Sections ==

=== 4.1 Monolithic Dynamic Pipeline ===

**File**: ##apps/web/prompts/monolithic-dynamic.prompt.md##

|= Section |= Purpose |= Variables
| ##DYNAMIC_PLAN## | System prompt for planning/research phase | ##$~{currentDate}##
| ##DYNAMIC_ANALYSIS## | System prompt for analysis/verdict phase | ##$~{currentDate}##
| ##DYNAMIC_ANALYSIS_USER## | User message template with input + sources | ##$~{TEXT_TO_ANALYZE}##, ##$~{SOURCE_SUMMARY}##
| ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON output guidance for Claude | (none)
| ##STRUCTURED_OUTPUT_OPENAI## | JSON output guidance for GPT | (none)
| ##STRUCTURED_OUTPUT_GOOGLE## | JSON output guidance for Gemini | (none)
| ##STRUCTURED_OUTPUT_MISTRAL## | JSON output guidance for Mistral | (none)

=== 4.2 Orchestrated Pipeline ===

**File**: ##apps/web/prompts/orchestrated.prompt.md##

Uses many more sections. Key sections include ##UNDERSTAND##, ##EXTRACT_EVIDENCE##, ##VERDICT##, ##CONTEXT_REFINEMENT##, ##SEARCH_RELEVANCE_BATCH_SYSTEM##, ##SEARCH_RELEVANCE_MODE_STRICT##, ##SEARCH_RELEVANCE_MODE_MODERATE##, ##SEARCH_RELEVANCE_MODE_RELAXED##, etc. See the prompt file for the complete list.

----

== 5. Provider-Specific Optimizations ==

Provider-specific structured output guidance is embedded as named sections in each ##.prompt.md## file. The pipeline code dynamically selects the right section based on the detected provider.

|= Provider |= Section Name |= Key Guidance
| **Anthropic (Claude)** | ##STRUCTURED_OUTPUT_ANTHROPIC## | JSON format rules, empty strings for optional fields, field validation
| **OpenAI (GPT)** | ##STRUCTURED_OUTPUT_OPENAI## | Field completeness, common GPT errors to avoid
| **Google (Gemini)** | ##STRUCTURED_OUTPUT_GOOGLE## | Length enforcement, no explanatory text outside JSON
| **Mistral** | ##STRUCTURED_OUTPUT_MISTRAL## | Validation checklist format, field type verification

----

== 6. Configuration Adaptations ==

Configuration adaptations adjust prompt behavior based on runtime pipeline configuration.

=== 6.1 LLM Tiering (##isLLMTiering##) ===

When tiering is enabled with a fast-tier model:
* Adjusts complexity expectations per task
* Optimizes token usage for fast-tier models (Haiku, Mini, Flash)

=== 6.2 Knowledge Mode (##allowModelKnowledge##) ===

* **##allowModelKnowledge=true##**: LLM may supplement evidence with training knowledge
* **##allowModelKnowledge=false##**: LLM must rely strictly on fetched evidence

----

== 7. Provider Detection ==

The ##detectProvider()## function (in ##prompt-builder.ts##) maps model names to provider types:

{{code language="typescript"}}
function detectProvider(modelName: string): ProviderType {
  const lowerName = modelName.toLowerCase();

  if (lowerName.includes('claude')) return 'anthropic';
  if (lowerName.includes('gpt'))    return 'openai';
  if (lowerName.includes('gemini')) return 'google';
  if (lowerName.includes('mistral')) return 'mistral';

  return 'anthropic'; // Default fallback
}
{{/code}}

----

== 8. Adding a New Provider ==

1. Add a ##STRUCTURED_OUTPUT_NEWPROVIDER## section to the relevant ##.prompt.md## files
1. Update ##detectProvider()## in ##prompt-builder.ts## to recognize new model IDs
1. Update ##isBudgetModel()## if the provider has fast-tier models
1. Test with ##prompt-optimization.test.ts## and ##monolithic-dynamic-prompt.test.ts##

----

== 9. Changelog ==

=== 9.1 v2.8.2: Prompt Externalization to UCM (Feb 13, 2026) ===

All runtime LLM prompts now load from UCM-managed ##.prompt.md## files, compliant with AGENTS.md String Usage Boundary.

* Monolithic-dynamic system prompts moved from ##buildPrompt()## to ##loadAndRenderSection()##
* Orchestrated search relevance mode instructions moved from inline code to prompt file sections
* 4 provider-specific structured output sections added to ##monolithic-dynamic.prompt.md##
* TypeScript prompt modules retained for testing harness only
* 27 new CI-safe tests validate prompt file structure and content

=== 9.2 v2.8.0: Token Optimization (Feb 3, 2026) ===

**20-30% token reduction** across prompts through two-phase optimization.

|= Phase |= Strategy |= Token Savings
| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved
| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved

----

== 10. Related Documentation ==

* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] — Pipeline architecture and shared primitives that consume these prompts
* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] — Verdict calculation methodology driven by prompt outputs
* [[Orchestrated Pipeline>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Orchestrated Pipeline.WebHome]] — AKEL pipeline flow showing where each prompt task type is invoked
* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] — Confidence scoring that depends on prompt-extracted evidence quality
* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] — Architecture overview

----

**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] | Next: [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]
