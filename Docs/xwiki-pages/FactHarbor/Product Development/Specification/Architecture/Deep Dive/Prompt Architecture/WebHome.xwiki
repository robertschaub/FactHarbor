= Prompt Architecture =

{{info}}
**Developer Reference** — Modular prompt composition system: base prompts + provider variants + configuration adaptations, producing optimized prompts per LLM provider and pipeline task.

**Key Files**: ##apps/web/src/lib/analyzer/prompts/prompt-builder.ts##, ##apps/web/src/lib/analyzer/prompts/base/##, ##apps/web/src/lib/analyzer/prompts/providers/##
{{/info}}

----

== 1. Overview ==

FactHarbor uses a modular prompt architecture (**v2.8.1**) that separates prompt concerns into three composable layers:

1. **Base prompts** — Task-specific instructions shared across all providers (understand, extract evidence, verdict, etc.)
1. **Provider variants** — LLM-specific optimizations (Claude, GPT, Gemini, Mistral)
1. **Configuration adaptations** — Runtime adjustments based on pipeline config (tiering, knowledge mode, structured output)

This separation ensures that domain logic is defined once in base prompts, while provider-specific formatting and config-driven simplifications are applied as composable overlays.

----

== 2. Directory Structure ==

{{code language="none"}}
apps/web/src/lib/analyzer/prompts/
+-- prompt-builder.ts              # Main composition engine
+-- prompt-testing.ts              # Testing utilities
+-- OUTPUT_SCHEMAS.md              # Schema documentation
+-- base/                          # Task-specific base prompts
|   +-- understand-base.ts
|   +-- extract-evidence-base.ts
|   +-- verdict-base.ts
|   +-- context-refinement-base.ts
|   +-- dynamic-plan-base.ts
|   +-- dynamic-analysis-base.ts
|   +-- orchestrated-understand.ts
|   +-- orchestrated-supplemental.ts
+-- providers/                     # LLM-specific optimizations
|   +-- anthropic.ts               # Claude (XML tags, nuanced reasoning)
|   +-- openai.ts                  # GPT (few-shot examples, JSON mode)
|   +-- google.ts                  # Gemini (length limits, checklists)
|   +-- mistral.ts                 # Mistral (step-by-step, templates)
+-- config-adaptations/            # Runtime adjustments
    +-- tiering.ts                 # Fast-tier simplified prompts
    +-- knowledge-mode.ts          # Allow/disallow LLM training knowledge
    +-- structured-output.ts       # Structured output guidance per provider
{{/code}}

----

== 3. Prompt Composition Flow ==

The ##buildPrompt()## function in ##prompt-builder.ts## accepts a ##PromptContext## and composes the final prompt through three additive layers.

{{mermaid}}
flowchart LR
    INPUT["buildPrompt(context)"] --> BASE["1. Base Prompt<br/>Task-specific instructions<br/>(understand, verdict, etc.)"]
    BASE --> PROVIDER["2. Provider Variant<br/>LLM-specific format<br/>(Anthropic, OpenAI, etc.)"]
    PROVIDER --> CONFIG["3. Config Adaptations<br/>Runtime adjustments<br/>(tiering, knowledge mode)"]
    CONFIG --> FINAL["Final Composed Prompt"]

    style INPUT fill:#e3f2fd,color:#000
    style BASE fill:#c8e6c9,color:#000
    style PROVIDER fill:#fff9c4,color:#000
    style CONFIG fill:#ffe0b2,color:#000
    style FINAL fill:#e1bee7,color:#000
{{/mermaid}}

//Green = base template, Yellow = provider variant, Orange = config adaptations. Layers are concatenated in order.//

=== 3.1 PromptContext Interface ===

{{code language="typescript"}}
interface PromptContext {
  task: TaskType;
  provider: ProviderType;       // 'anthropic' | 'openai' | 'google' | 'mistral'
  modelName: string;
  config: {
    allowModelKnowledge: boolean;
    isLLMTiering: boolean;
    isBudgetModel: boolean;     // Haiku, Mini, Flash
  };
  variables: {
    currentDate?: string;
    originalClaim?: string;
    contextsList?: string;
    isRecent?: boolean;
    textToAnalyze?: string;
    sourceSummary?: string;
    keyFactorHints?: Array<{ factor: string; category: string; evaluationCriteria: string }>;
    minCoreClaimsPerContext?: number;
    hasScopes?: boolean;
  };
}
{{/code}}

=== 3.2 Fast-Tier Short Circuit ===

When ##isLLMTiering=true## AND ##isBudgetModel=true##, the builder bypasses the normal three-layer composition and returns ultra-simplified prompts (~40% fewer tokens) for ##understand##, ##extract_evidence##, and ##verdict## tasks. Other tasks fall back to the standard composition path.

{{mermaid}}
flowchart TD
    START["buildPrompt(context)"] --> CHECK{"isLLMTiering AND<br/>isBudgetModel?"}
    CHECK -->|"Yes + core task"| BUDGET["getBudgetPrompt()<br/>Ultra-simplified<br/>+ minimal provider hint"]
    CHECK -->|"No"| NORMAL["Standard 3-layer<br/>composition"]
    CHECK -->|"Yes + other task"| NORMAL

    style BUDGET fill:#fff9c4,color:#000
    style NORMAL fill:#c8e6c9,color:#000
{{/mermaid}}

----

== 4. Provider-Specific Optimizations ==

Each provider file exports variant functions per task. All concept teaching happens in base prompts; variants **only** specify format preferences and model-specific guidance.

|= Provider |= File |= Key Optimizations |= Style
| **Anthropic (Claude)** | ##providers/anthropic.ts## | XML-structured prompts, thinking blocks for complex reasoning, trust nuanced reasoning | ##<claude_optimization>## XML tags
| **OpenAI (GPT)** | ##providers/openai.ts## | Few-shot examples, explicit field enumeration, bullet lists, JSON mode hints | ##GPT OPTIMIZATION## markdown sections
| **Google (Gemini)** | ##providers/google.ts## | Explicit word/character limits, schema compliance checklists, numbered steps, grounding metadata | ##GEMINI OPTIMIZATION## with length tables
| **Mistral** | ##providers/mistral.ts## | Step-by-step numbered processes, template-based field completion, explicit checklists | ##MISTRAL OPTIMIZATION## with step templates

=== 4.1 Provider Variant Selection ===

Provider variants are applied to the four core tasks: ##understand##, ##extract_evidence##, ##verdict##, and ##context_refinement##. Orchestrated pipeline tasks (##orchestrated_understand##, ##supplemental_claims##, ##supplemental_contexts##) include provider optimization directly in their base templates and skip the variant layer.

=== 4.2 Budget Provider Hints ===

For fast-tier models, full provider variants are replaced with minimal one-line hints:

|= Provider |= Budget Hint
| Anthropic | ##[Claude: Be direct, no hedging. Valid JSON output.]##
| OpenAI | ##[GPT: Follow example patterns exactly. All fields required.]##
| Google | ##[Gemini: Keep outputs concise. Use "" not null for strings.]##
| Mistral | ##[Mistral: Follow numbered steps. Output valid JSON.]##

----

== 5. Task Types ==

|= Task Type |= Pipeline Phase |= Base Prompt File |= Description
| ##understand## | UNDERSTAND | ##understand-base.ts## | Claim extraction, context detection, research query generation
| ##extract_evidence## | RESEARCH | ##extract-evidence-base.ts## | Evidence extraction from fetched source content
| ##verdict## | VERDICT | ##verdict-base.ts## | Claim verdict generation with evidence weighting
| ##context_refinement## | UNDERSTAND | ##context-refinement-base.ts## | Refine and merge analysis contexts
| ##dynamic_plan## | PLAN | ##dynamic-plan-base.ts## | Monolithic pipeline planning phase
| ##dynamic_analysis## | ANALYSIS | ##dynamic-analysis-base.ts## | Monolithic pipeline analysis phase
| ##orchestrated_understand## | UNDERSTAND | ##orchestrated-understand.ts## | Enhanced understand with KeyFactor hints (orchestrated pipeline)
| ##supplemental_claims## | UNDERSTAND | ##orchestrated-supplemental.ts## | Extract supplemental claims per context
| ##supplemental_contexts## | UNDERSTAND | ##orchestrated-supplemental.ts## | Extract supplemental analysis contexts

----

== 6. Configuration Adaptations ==

Configuration adaptations are appended after the provider variant layer. They adjust prompt behavior based on runtime pipeline configuration.

=== 6.1 LLM Tiering (##isLLMTiering##) ===

**File**: ##config-adaptations/tiering.ts##

When tiering is enabled with a fast-tier model, two modes apply:

* **Budget prompts** (##isBudgetModel=true##): Ultra-simplified prompts via ##getBudgetUnderstandPrompt()##, ##getBudgetExtractEvidencePrompt()##, ##getBudgetVerdictPrompt()## -- short-circuits the three-layer composition entirely.
* **Tiering adaptations** (##isBudgetModel=true## but non-core tasks): Lighter adaptations appended via ##getTieringUnderstandAdaptation()##, ##getTieringExtractEvidenceAdaptation()##, ##getTieringVerdictAdaptation()##.

=== 6.2 Knowledge Mode (##allowModelKnowledge##) ===

**File**: ##config-adaptations/knowledge-mode.ts##

Applied **only** to the ##verdict## task:

* **##allowModelKnowledge=true##**: LLM may supplement evidence with training knowledge (adds ##getWithModelKnowledgeAdaptation()##)
* **##allowModelKnowledge=false##**: LLM must rely strictly on fetched evidence (adds ##getWithoutModelKnowledgeAdaptation()##)

=== 6.3 Structured Output Guidance ===

**File**: ##config-adaptations/structured-output.ts##

Applied to all standard tasks (excluding ##orchestrated_*## and ##supplemental_*## tasks, which have structured output built in). Provides provider-specific JSON output formatting guidance.

----

== 7. Token Optimization ==

=== 7.1 v2.8.0 Achievements ===

**20-30% token reduction** across prompts through a two-phase optimization:

|= Phase |= Strategy |= Token Savings
| **Phase 1** | Removed duplication across base/provider, inlined definitions | ~550-700 tokens saved
| **Phase 2** | Condensed rules, shortened instructions | ~230 additional tokens saved

=== 7.2 Design Principles ===

* **Base prompts teach concepts once** — provider variants only add format preferences
* **Budget prompts are standalone** — they do not compose from base + variant layers
* **No concept repetition** — if a rule is in the base prompt, the provider variant must not restate it

----

== 8. Provider Detection ==

The ##detectProvider()## function maps model names to provider types:

{{code language="typescript"}}
function detectProvider(modelName: string): ProviderType {
  const lowerName = modelName.toLowerCase();

  if (lowerName.includes('claude')) return 'anthropic';
  if (lowerName.includes('gpt'))    return 'openai';
  if (lowerName.includes('gemini')) return 'google';
  if (lowerName.includes('mistral')) return 'mistral';

  return 'anthropic'; // Default fallback
}
{{/code}}

The ##isBudgetModel()## function identifies fast-tier models (Haiku, Mini, Flash, Mistral Small/Medium) for tiering decisions.

----

== 9. Adding a New Provider ==

To add support for a new LLM provider:

1. **Create provider file**: ##prompts/providers/newprovider.ts##
1*. Export variant functions: ##getNewProviderUnderstandVariant()##, ##getNewProviderExtractEvidenceVariant()##, ##getNewProviderVerdictVariant()##, ##getNewProviderContextRefinementVariant()##
1*. Follow format-only pattern -- do not duplicate concept teaching from base prompts

1. **Update prompt-builder.ts**:
1*. Add imports for the new variant functions
1*. Add ##'newprovider'## to the ##ProviderType## union type
1*. Add case entries in the ##variantMap## within ##getProviderVariant()##
1*. Add budget provider hint in ##getBudgetProviderHint()##

1. **Update detectProvider()**:
1*. Add model name matching rule (e.g., ##if (lowerName.includes('newmodel')) return 'newprovider';##)

1. **Update isBudgetModel()**:
1*. Add fast-tier model names for the new provider if applicable

1. **Test**:
1*. Run existing tests: ##apps/web/test/unit/lib/analyzer/prompts/prompt-optimization.test.ts##
1*. Add provider-specific test cases to ##prompt-quality.test.ts##
1*. Verify prompt composition produces valid output for all task types

----

== 10. Related Documentation ==

* [[Pipeline Variants>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Pipeline Variants.WebHome]] — Pipeline architecture and shared primitives that consume these prompts
* [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] — Verdict calculation methodology driven by prompt outputs
* [[Orchestrated Pipeline>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Orchestrated Pipeline.WebHome]] — AKEL pipeline flow showing where each prompt task type is invoked
* [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] — Confidence scoring that depends on prompt-extracted evidence quality
* [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] — Architecture overview

----

**Navigation:** [[Deep Dive Index>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Calculations and Verdicts>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Calculations and Verdicts.WebHome]] | Next: [[Confidence Calibration>>FactHarbor.Product Development.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]]
