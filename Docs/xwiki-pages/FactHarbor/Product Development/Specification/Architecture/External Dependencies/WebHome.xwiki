= External Dependencies =

FactHarbor depends on external LLM providers for AI analysis and search providers for evidence retrieval. This page documents the integration architecture, model tiering strategy, and provider health monitoring.

== Dependencies Map ==

{{mermaid}}
flowchart TB
    subgraph FH["FactHarbor"]
        AKEL["AKEL Pipeline"]
        TIERING["Model Tiering\n(model-tiering.ts)"]
        SEARCH_ABS["Search Abstraction\n(web-search.ts)"]
        RETRIEVAL["Content Retrieval\n(retrieval.ts)"]
        HEALTH["Provider Health\n(provider-health.ts)"]
    end

    subgraph LLM["LLM Providers (via Vercel AI SDK)"]
        ANTHROPIC["Anthropic\nClaude Haiku 4.5\nClaude Opus 4.6"]
        OPENAI["OpenAI\nGPT-4.1-mini\nGPT-4.1"]
        GOOGLE["Google\nGemini 2.5-flash\nGemini 2.5-pro"]
        MISTRAL["Mistral\nLarge / Small"]
    end

    subgraph SEARCH["Search Providers"]
        CSE["Google Custom\nSearch Engine"]
        SERP["SerpAPI"]
    end

    subgraph CONTENT["Content Sources"]
        HTML["HTML Pages\n(via cheerio)"]
        PDF["PDF Documents\n(via pdf2json)"]
    end

    AKEL --> TIERING
    TIERING --> ANTHROPIC
    TIERING --> OPENAI
    TIERING --> GOOGLE
    TIERING --> MISTRAL
    AKEL --> SEARCH_ABS
    SEARCH_ABS -->|"Primary"| CSE
    SEARCH_ABS -->|"Fallback"| SERP
    AKEL --> RETRIEVAL
    RETRIEVAL --> HTML
    RETRIEVAL --> PDF
    HEALTH -.->|"monitors"| LLM
    HEALTH -.->|"monitors"| SEARCH

    style FH fill:#e8f5e9,stroke:#2e7d32,color:#000
    style LLM fill:#e3f2fd,stroke:#1565c0,color:#000
    style SEARCH fill:#fff3e0,stroke:#e65100,color:#000
    style CONTENT fill:#f3e5f5,stroke:#6a1b9a,color:#000
{{/mermaid}}

//FactHarbor integrates with 4 LLM providers, 2 search providers, and web content sources. All external calls are routed through abstraction layers (model-tiering, web-search, retrieval) and monitored by the provider health system.//

== LLM Provider Integration ==

=== Model Tiering ===

The AKEL pipeline uses **per-task model tiering** to balance cost and quality. Lightweight tasks (claim extraction, evidence parsing) use cheaper budget models; critical tasks (verdict reasoning) use premium models.

{{mermaid}}
flowchart LR
    subgraph Tasks["Pipeline Tasks"]
        UNDERSTAND["Understand\n(budget)"]
        EXTRACT["Extract Evidence\n(budget)"]
        REFINE["Context Refinement\n(standard)"]
        VERDICT["Verdict Generation\n(premium)"]
    end

    subgraph Tiering["Model Tiering"]
        ROUTER["model-tiering.ts"]
    end

    subgraph SDK["Vercel AI SDK"]
        AISDK["generateText()\ngenerateObject()"]
    end

    subgraph Providers["Active Provider"]
        BUDGET["Budget Model"]
        STANDARD["Standard Model"]
        PREMIUM["Premium Model"]
    end

    UNDERSTAND --> ROUTER
    EXTRACT --> ROUTER
    REFINE --> ROUTER
    VERDICT --> ROUTER
    ROUTER --> AISDK
    AISDK --> BUDGET
    AISDK --> STANDARD
    AISDK --> PREMIUM

    style Tasks fill:#e8f5e9,stroke:#2e7d32,color:#000
    style Tiering fill:#e3f2fd,stroke:#1565c0,color:#000
    style Providers fill:#fff9c4,stroke:#f9a825,color:#000
{{/mermaid}}

//Pipeline tasks are routed to the appropriate model tier via model-tiering.ts. All LLM calls go through the Vercel AI SDK, which provides a unified interface across providers.//

=== Provider Model Mapping ===

|= Task |= Tier |= Anthropic |= OpenAI |= Google |= Mistral
| understand | Budget | Claude Haiku 4.5 | GPT-4.1-mini | Gemini 2.5-flash | (Anthropic fallback)
| extract_evidence | Budget | Claude Haiku 4.5 | GPT-4.1-mini | Gemini 2.5-flash | (Anthropic fallback)
| context_refinement | Standard | Claude Sonnet 4.5 | GPT-4.1 | Gemini 2.5-pro | (Anthropic fallback)
| verdict | Premium | Claude Opus 4.6 | GPT-4.1 | Gemini 2.5-pro | (Anthropic fallback)

=== LLM Configuration ===

* **Provider selection**: Single provider for all tasks, set via ##LLM_PROVIDER## environment variable (default: ##anthropic##)
* **Model overrides**: Individual models configurable via UCM (##modelUnderstand##, ##modelExtractEvidence##, ##modelVerdict##)
* **Deterministic mode**: ##FH_DETERMINISTIC=true## sets temperature to 0 for reproducible results
* **Structured output**: Uses Zod schemas with ##generateObject()## for type-safe LLM responses

== Search Providers ==

|= Provider |= Role |= Credentials Required
| **Google Custom Search Engine** | Primary search | ##GOOGLE_CSE_API_KEY##, ##GOOGLE_CSE_ID##
| **SerpAPI** | Fallback | ##SERPAPI_API_KEY##

**Auto mode** (default): Tries Google CSE first; falls back to SerpAPI on failure. Configurable via ##SearchConfig.provider## (##"auto"##, ##"google-cse"##, or ##"serpapi"##).

== Content Extraction ==

|= Source Type |= Library |= Capabilities
| HTML pages | cheerio | DOM parsing, text extraction, link extraction
| PDF documents | pdf2json | Text extraction from PDF files

Content is fetched with configurable timeouts and retry logic. No caching is currently implemented for fetched content (each analysis re-fetches all sources).

== Provider Health Monitoring ==

FactHarbor includes a **circuit breaker** per provider type (search, LLM) that detects outages and auto-pauses the system to prevent cascading failures.

{{mermaid}}
stateDiagram-v2
    [*] --> CLOSED: System starts healthy
    CLOSED --> OPEN: Consecutive failures >= threshold
    OPEN --> HALF_OPEN: Admin resumes system
    HALF_OPEN --> CLOSED: Next call succeeds
    HALF_OPEN --> OPEN: Next call fails

    note right of CLOSED
        Normal operation.
        Failures counted.
    end note

    note right of OPEN
        Circuit tripped.
        System auto-paused.
        New jobs stay QUEUED.
        Webhook fires to admin.
    end note

    note right of HALF_OPEN
        Admin resumed system.
        Testing if provider recovered.
    end note
{{/mermaid}}

//The circuit breaker starts CLOSED (healthy). After consecutive failures reach the threshold (default: 3), it opens â€” the system auto-pauses, queued jobs wait, and a webhook notifies the admin. When the admin resumes, the circuit enters HALF_OPEN to test recovery.//

=== Health Features ===

|= Feature |= Description
| **Circuit breaker** | Per-provider (search/LLM) with configurable threshold (##heuristicCircuitBreakerThreshold##, default: 3)
| **Auto-pause** | Runner queue halts when circuit trips; jobs stay QUEUED (not lost)
| **Webhook notifications** | Fire-and-forget POST to ##FH_WEBHOOK_URL## with optional HMAC signing (##FH_WEBHOOK_SECRET##)
| **Admin API** | ##GET /api/fh/system-health## (status), ##POST /api/fh/system-health## (resume/pause)
| **UI banner** | Amber warning banner when system is paused, with admin resume/pause controls
| **Error classification** | Categorises errors as provider_outage, rate_limit, timeout, or unknown

== Credential Requirements ==

|= Variable |= Required |= Purpose
| ##ANTHROPIC_API_KEY## | If using Anthropic | Claude API access
| ##OPENAI_API_KEY## | If using OpenAI | GPT API access
| ##GOOGLE_GENERATIVE_AI_API_KEY## | If using Google | Gemini API access
| ##MISTRAL_API_KEY## | If using Mistral | Mistral API access
| ##GOOGLE_CSE_API_KEY## | For search | Google Custom Search API
| ##GOOGLE_CSE_ID## | For search | Google Custom Search Engine ID
| ##SERPAPI_API_KEY## | For fallback search | SerpAPI access
| ##FH_WEBHOOK_URL## | Optional | Provider health webhook endpoint
| ##FH_WEBHOOK_SECRET## | Optional | HMAC-SHA256 webhook signature

----

**Navigation:** [[Architecture>>FactHarbor.Product Development.Specification.Architecture.WebHome]] | Prev: [[Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] | Next: [[Storage and Configuration>>FactHarbor.Product Development.Specification.Architecture.Storage and Configuration.WebHome]]
