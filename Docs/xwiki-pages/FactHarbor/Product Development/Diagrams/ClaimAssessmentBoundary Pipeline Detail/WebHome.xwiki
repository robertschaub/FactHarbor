= ClaimAssessmentBoundary Pipeline Detail =

{{info}}
**Current Architecture (v2.11.0+):** The ClaimAssessmentBoundary pipeline is the production default pipeline. Source: ##claimboundary-pipeline.ts## and ##verdict-stage.ts##. For architecture reference, see [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]].

Updated 2026-02-22 — Sequence diagram and LLM Call Budget enriched from architecture spec; Stage 4 verdict steps verified against ##verdict-stage.ts##.
{{/info}}

{{mermaid}}
flowchart TB
    subgraph Entry["Entry Point"]
        START["runClaimBoundaryAnalysis()"]
        INIT["Initialize State<br/>Budget tracker, timeline, config"]
    end

    subgraph Stage1["Stage 1: EXTRACT CLAIMS"]
        CLASSIFY["[LLM] Input Classification<br/>article vs claim, topic identification"]
        DECOMPOSE["[LLM] Claim Decomposition<br/>Extract atomic claims (maxAtomicClaims)"]
        PRELIM_SEARCH["[WEB] Preliminary Search<br/>Grounding pass (2 queries x 5 sources per claim)"]
        PRELIM_EXTRACT["[LLM] Preliminary Evidence<br/>Extract evidence from prelim sources"]
        GATE1["Gate 1: Claim Validation<br/>Filter opinion/prediction/vague<br/>Central claims auto-pass"]
        CLAIMS_OUT["AtomicClaim[] (5-15 claims)"]
    end

    subgraph Stage2["Stage 2: RESEARCH EVIDENCE"]
        RESEARCH_LOOP["Research Loop (maxTotalIterations)"]
        SUFFICIENCY["Claim Sufficiency Check<br/>claimSufficiencyThreshold (default: 3)"]
        CONTRADICTION["Contradiction Search<br/>Reserved iterations (default: 2)"]
        DECIDE_NEXT["decideNextResearch()<br/>Priority: unsufficient > central > high-harm"]
        SEARCH["[WEB] searchWebWithProvider()"]
        RELEVANCE["[LLM] classifyRelevance()<br/>Primary / secondary / unrelated"]
        FETCH["[WEB] fetchSourceContent()"]
        SR_PREFETCH["[EXT] prefetchSourceReliability()<br/>Batch lookup (async)"]
        EXTRACT["[LLM] extractEvidence()<br/>Per source > EvidenceItem[]"]
        EVIDENCE_OUT["EvidenceItem[] (boundary-unassigned)"]
    end

    subgraph Stage3["Stage 3: CLUSTER BOUNDARIES"]
        PROPOSE["[LLM] Propose ClaimAssessmentBoundaries<br/>Evidence-emergent clustering"]
        VALIDATE["Structural Validation<br/>Coverage, coherence, overlap checks"]
        MERGE{"Merge needed?<br/>(>maxClaimBoundaries)"}
        MERGE_BOUNDARIES["[LLM] Merge similar boundaries<br/>Reduce count, preserve coverage"]
        ASSIGN["Assign evidence to boundaries<br/>+ build coverage matrix"]
        BOUNDARIES_OUT["ClaimAssessmentBoundary[]<br/>CoverageMatrix"]
    end

    subgraph Stage4["Stage 4: VERDICT (verdict-stage.ts)"]
        ADVOCATE["[LLM] Step 1: Advocate Verdict (Sonnet)<br/>Initial verdicts for all claims<br/>with per-boundary signals"]
        SELF_CONSIST{"selfConsistencyMode?"}
        SC_CHECK["[LLM] Step 2: Self-Consistency (Sonnet x2)<br/>Re-run at temp=0.3, measure spread"]
        CHALLENGE["[LLM] Step 3: Adversarial Challenge (Sonnet)<br/>Argue against emerging verdicts"]
        RECONCILE["[LLM] Step 4: Reconciliation (Sonnet)<br/>Final verdicts incorporating challenges"]
        VERDICT_VAL["[LLM] Step 5: Verdict Validation (Haiku x2)<br/>Grounding check + direction check"]
        STRUCT_CHECK["Structural Consistency Check<br/>(deterministic)"]
        VERDICTS_OUT["CBClaimVerdict[]"]
    end

    subgraph Stage5["Stage 5: AGGREGATE (aggregateAssessment)"]
        TRIANG_STEP["computeTriangulationScore()<br/>Cross-boundary agreement per claim"]
        WEIGHTED_AGG["Weighted Aggregation<br/>Centrality x harm x confidence x<br/>triangulation x derivative weights"]
        NARRATIVE["[LLM] generateVerdictNarrative() (Sonnet)<br/>Headline, key finding, limitations"]
        GATE4["Gate 4: buildQualityGates()<br/>Classify confidence tiers"]
        ENVELOPE["Result Envelope<br/>schemaVersion: 3.0.0-cb"]
    end

    START --> INIT
    INIT --> CLASSIFY
    CLASSIFY --> DECOMPOSE
    DECOMPOSE --> PRELIM_SEARCH
    PRELIM_SEARCH --> PRELIM_EXTRACT
    PRELIM_EXTRACT --> GATE1
    GATE1 --> CLAIMS_OUT

    CLAIMS_OUT --> RESEARCH_LOOP
    RESEARCH_LOOP --> SUFFICIENCY
    SUFFICIENCY -->|"insufficient"| DECIDE_NEXT
    SUFFICIENCY -->|"sufficient"| CONTRADICTION
    CONTRADICTION --> DECIDE_NEXT
    DECIDE_NEXT --> SEARCH
    SEARCH --> RELEVANCE
    RELEVANCE --> FETCH
    FETCH --> SR_PREFETCH
    SR_PREFETCH --> EXTRACT
    EXTRACT --> EVIDENCE_OUT
    EVIDENCE_OUT -->|"loop continues"| RESEARCH_LOOP

    EVIDENCE_OUT --> PROPOSE
    PROPOSE --> VALIDATE
    VALIDATE --> MERGE
    MERGE -->|"yes"| MERGE_BOUNDARIES
    MERGE_BOUNDARIES --> ASSIGN
    MERGE -->|"no"| ASSIGN
    ASSIGN --> BOUNDARIES_OUT

    BOUNDARIES_OUT --> ADVOCATE
    ADVOCATE --> SELF_CONSIST
    SELF_CONSIST -->|"enabled"| SC_CHECK
    SELF_CONSIST -->|"disabled"| CHALLENGE
    SC_CHECK --> CHALLENGE
    CHALLENGE --> RECONCILE
    RECONCILE --> VERDICT_VAL
    VERDICT_VAL --> STRUCT_CHECK
    STRUCT_CHECK --> VERDICTS_OUT

    VERDICTS_OUT --> TRIANG_STEP
    TRIANG_STEP --> WEIGHTED_AGG
    WEIGHTED_AGG --> NARRATIVE
    NARRATIVE --> GATE4
    GATE4 --> ENVELOPE

    style Stage1 fill:#e8f5e9,stroke:#2e7d32,color:#000
    style Stage2 fill:#fff9c4,stroke:#f57f17,color:#000
    style Stage3 fill:#e3f2fd,stroke:#1565c0,color:#000
    style Stage4 fill:#f3e5f5,stroke:#7b1fa2,color:#000
    style Stage5 fill:#fce4ec,stroke:#c2185b,color:#000
{{/mermaid}}

== Key Features ==

* **Evidence-emergent boundaries:** ClaimAssessmentBoundaries emerge from evidence clustering (Stage 3), not pre-detected
* **5-step LLM debate:** Each verdict goes through structured debate (advocate > self-consistency > adversarial challenge > reconciliation > validation). Steps 2+3 run in parallel.
* **Self-consistency checking:** Optional re-run with temperature variance to detect unstable verdicts (stable/moderate/unstable spread thresholds)
* **Triangulation scoring:** Cross-boundary agreement assessment (strong/moderate/weak/conflicted levels)
* **Quality gates:** Gate 1 (claim validation) and Gate 4 (verdict confidence distribution) are mandatory checkpoints
* **UCM-configurable:** 24 pipeline parameters organized by stage, editable in Admin UI

== Comparison to Legacy Pipelines ==

| Aspect | Orchestrated (removed v2.11.0) | ClaimAssessmentBoundary (v2.11.0+) |
|--------|-------------------------------|-----------------------------------|
| **Context detection** | Pre-detected AnalysisContexts | Evidence-emergent ClaimAssessmentBoundaries |
| **Verdict generation** | Single LLM call per claim | 5-step debate pattern |
| **Cross-boundary analysis** | None | Triangulation scoring |
| **Self-consistency** | None | Optional spread checking |
| **Budget control** | Per-context iteration limits | Claim sufficiency + reserved contradiction iterations |
| **Schema version** | 2.x or 3.0.0 | 3.0.0-cb |

== Sequence Diagram ==

Component interaction during a full ClaimAssessmentBoundary pipeline analysis, including UCM configuration loading, contradiction search, and evidence quality filtering.

{{mermaid}}
sequenceDiagram
    participant UI as Client/UI
    participant API as API (ASP.NET)
    participant Runner as Runner (Next.js)
    participant Pipeline as ClaimAssessmentBoundary Pipeline
    participant LLM as LLM Service
    participant Search as Web Search
    participant SR as Source Reliability
    participant UCM as Config (UCM)

    UI->>API: POST /api/jobs (create job)
    API->>Runner: POST /api/internal/run-job
    Runner->>Pipeline: runClaimBoundaryAnalysis(jobId)

    Note over Pipeline: Stage 1: EXTRACT CLAIMS
    Pipeline->>UCM: getConfig("pipeline")
    Pipeline->>LLM: [LLM] Pass 1: extractAtomicClaims(inputText) — Haiku
    LLM-->>Pipeline: AtomicClaim[] (with centrality)
    Pipeline->>Search: [Search] preliminarySearch(thesis + top claims)
    Search-->>Pipeline: SearchResult[]
    Pipeline->>LLM: [LLM] Pass 1: extractPreliminaryEvidence(sources) — Haiku
    LLM-->>Pipeline: EvidenceItem[] + EvidenceScope[]
    Pipeline->>LLM: [LLM] Pass 2: groundedClaimExtraction(input + prelim evidence) — Sonnet
    LLM-->>Pipeline: AtomicClaim[] (refined, with groundingQuality)
    Pipeline->>Pipeline: filterCentralClaims()
    Pipeline->>LLM: [LLM] Gate 1: validateClaims(centralClaims) — Haiku
    LLM-->>Pipeline: Gate1Result

    Note over Pipeline: Stage 2: RESEARCH
    loop For each research iteration
        Pipeline->>LLM: [LLM] generateSearchQueries(claim) — Haiku
        LLM-->>Pipeline: SearchQuery[]
        Pipeline->>Search: [Search] execute(queries)
        Search-->>Pipeline: SearchResult[]
        Pipeline->>LLM: [LLM] classifyRelevance(results) — Haiku
        Pipeline->>Pipeline: [Ext] fetchSources(relevantUrls)
        Pipeline->>SR: [Ext] prefetchSourceReliability(urls)
        SR-->>Pipeline: reliabilityScores
        Pipeline->>LLM: [LLM] extractEvidence(sourceText) — Haiku
        LLM-->>Pipeline: EvidenceItem[] + EvidenceScope[] (incl. additionalDimensions)
        Pipeline->>LLM: [LLM] evidenceQualityFilter(items) — Haiku
        Pipeline->>API: [Ext] updateProgress(jobId, stage, %)
    end

    Note over Pipeline: Contradiction Search (reserved)
    Pipeline->>LLM: [LLM] generateContradictionQueries(claims, evidence) — Haiku
    Pipeline->>Search: [Search] execute(contraQueries)
    Pipeline->>LLM: [LLM] extractContraryEvidence(sources) — Haiku

    Note over Pipeline: Stage 3: CLUSTER BOUNDARIES
    Pipeline->>LLM: [LLM] clusterEvidenceScopes(allScopes, allEvidence) — Sonnet
    LLM-->>Pipeline: ClaimAssessmentBoundary[] + assignments
    Pipeline->>Pipeline: assignEvidenceToBoundaries()

    Note over Pipeline: Stage 4: VERDICT (LLM Debate — verdict-stage.ts)
    Pipeline->>LLM: [LLM] Step 1: advocateVerdict(claims, evidence, boundaries) — Sonnet
    LLM-->>Pipeline: ClaimVerdict[] (with per-boundary quantitative signals)

    par Self-Consistency + Adversarial Challenge (parallel)
        Pipeline->>LLM: [LLM] Step 2: selfConsistencyCheck(advocatePrompt x 2, temp=0.3) — Sonnet
        LLM-->>Pipeline: ConsistencyResult[] (spread per claim)
    and
        Pipeline->>LLM: [LLM] Step 3: adversarialChallenge(advocateVerdicts) — Sonnet
        LLM-->>Pipeline: ChallengeDocument (per-claim challenges)
    end

    Pipeline->>LLM: [LLM] Step 4: reconciliation(verdicts + challenges + consistency) — Sonnet
    LLM-->>Pipeline: Final ClaimVerdict[] (with challenge responses)
    Pipeline->>LLM: [LLM] Step 5a: validateVerdictGrounding(verdicts, evidence) — Haiku
    Pipeline->>LLM: [LLM] Step 5b: validateVerdictDirection(verdicts, evidence) — Haiku
    Pipeline->>Pipeline: structuralConsistencyCheck()
    Pipeline->>Pipeline: gate4ConfidenceCheck()

    Note over Pipeline: Stage 5: AGGREGATE
    Pipeline->>Pipeline: computeCoverageMatrix(claims, boundaries, evidence)
    Pipeline->>Pipeline: computeTriangulation(coverageMatrix, verdicts)
    Pipeline->>Pipeline: weightedAggregation(verdicts, triangulation, derivatives)
    Pipeline->>LLM: [LLM] generateVerdictNarrative(verdicts, boundaries, matrix) — Sonnet
    LLM-->>Pipeline: VerdictNarrative
    Pipeline->>Pipeline: assembleReport(claims, evidence, boundaries, verdicts, narrative)
    Pipeline->>API: [Ext] submitResults(jobId, report)
    API-->>UI: Job complete (poll/webhook)
{{/mermaid}}

== LLM Call Budget ==

|= Stage |= LLM Calls |= Model Tier |= Purpose
| Pass 1 Extraction | 1 | Haiku | Quick rough claim scan
| Pass 1 Evidence | 1-3 | Haiku | Extract evidence from preliminary sources
| Pass 2 Extraction | 1 | Sonnet | Evidence-grounded claim extraction (incl. groundingQuality)
| Gate 1 Validation | 1 | Haiku | Factual + specificity + grounding check
| Decomposition Retry | 0-1 | Haiku | Split vague claims (if needed)
| Gate 1 Retry | 0-4 | Haiku+Sonnet | If >50% fail, re-search + re-extract (max 1 retry)
| Research Queries | N (per claim per iteration) | Haiku | Search query generation
| Relevance Classification | N (per search result batch) | Haiku | Accept/reject search results
| Evidence Extraction | N (per relevant source) | Haiku | Extract evidence + mandatory EvidenceScope + isDerivative
| Scope Validation Retry | 0-N | Haiku | Re-extract items with missing scope (if needed)
| Evidence Quality | N (per evidence batch) | Haiku | Quality assessment
| Contradiction Queries | 1 | Haiku | Generate opposing search terms
| Boundary Clustering | 1 | Sonnet | Congruence-based scope clustering
| Step 1: Advocate | 1 | Sonnet | Advocate verdicts with per-boundary signals
| Step 2: Self-Consistency | 0 or 2 | Sonnet | Re-run advocate at elevated temp (parallel with Step 3)
| Step 3: Challenge | 1 | Sonnet | Devil's advocate challenges (parallel with Step 2)
| Step 4: Reconciliation | 1 | Sonnet | Final verdicts incorporating challenges + consistency
| Step 5: Validation | 2 (grounding + direction) | Haiku | Validate verdict quality
| Verdict Narrative | 1 | Sonnet | Generate structured VerdictNarrative for overall assessment

//Expected total: 18-37 LLM calls per analysis. The 5-7 added calls for the debate pattern + narrative (advocate + 0-2 consistency + challenger + reconciliation + narrative) are offset by eliminating context detection, canonicalization, refinement, and supplemental context calls.//

== Related Documentation ==

* [[AKEL Pipeline>>FactHarbor.Product Development.Specification.Architecture.AKEL Pipeline.WebHome]] — Architecture reference (pipeline variants, shared modules, budget controls)
* [[Pipeline Variant Dispatch>>FactHarbor.Product Development.Diagrams.Pipeline Variant Dispatch.WebHome]] — How pipelines are selected
* [[Quality Gates Flow>>FactHarbor.Product Development.Diagrams.Quality Gates Flow.WebHome]] — Gate 1 and Gate 4 details
* [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] — Entity relationships
* [[Entity Views>>FactHarbor.Product Development.Diagrams.Entity Views.WebHome]] — Multi-view entity reference
