{{info}}
**Current Implementation** - Uses Vercel AI SDK for multi-provider abstraction. Provider selected via ##LLM_PROVIDER## environment variable. Tiered model routing uses cheap models for extraction, premium for verdict reasoning.

Updated 2026-02-08 per documentation audit report.
{{/info}}

= LLM Abstraction Architecture =

{{mermaid}}

graph TB
    subgraph Pipelines[Twin-Path Pipelines]
        ORCH[Orchestrated orchestrated.ts]
        DYN[Monolithic Dynamic]
    end

    subgraph Tiering[Model Tiering]
        ROUTE[model-tiering.ts Task Router]
    end

    subgraph AISDK[Vercel AI SDK]
        SDK[AI SDK Core generateText generateObject]
        STREAM[Streaming Support streamText]
    end

    subgraph Providers[LLM Providers]
        ANT[Anthropic claude-haiku-4-5 / claude-sonnet-4-5]
        OAI[OpenAI gpt-4.1-mini / gpt-4.1]
        GOO[Google gemini-2.5-flash / gemini-2.5-pro]
        MIS[Mistral fallback to Anthropic models]
    end

    subgraph Config[Configuration]
        ENV[Environment Variables LLM_PROVIDER FH_DETERMINISTIC]
    end

    ORCH --> ROUTE
    DYN --> ROUTE
    ROUTE --> SDK
    SDK --> ANT
    SDK --> OAI
    SDK --> GOO
    SDK --> MIS
    ENV --> SDK
    ENV --> ROUTE

{{/mermaid}}

== Tiered Model Routing ==

Tasks are routed to appropriate model tiers for cost optimization:

|= Task Type |= Tier |= Purpose
| ##understand## | Budget | Claim extraction and classification
| ##extract_evidence## | Budget | Evidence extraction from sources
| ##context_refinement## | Standard | AnalysisContext refinement
| ##verdict## | Premium | Verdict reasoning (critical quality)
| ##supplemental## | Standard | Supplemental generation
| ##summary## | Standard | Summary generation

== Provider Model Mapping ==

|= Provider |= Budget (extract) |= Standard |= Premium (verdict)
| **Anthropic** | claude-haiku-4-5 | claude-haiku-4-5 | claude-sonnet-4-5
| **OpenAI** | gpt-4.1-mini | gpt-4.1 | gpt-4.1
| **Google** | gemini-2.5-flash | gemini-2.5-pro | gemini-2.5-pro
| **Mistral** | (falls back to Anthropic models) | |

== Environment Variables ==

|= Variable |= Default |= Options
| ##LLM_PROVIDER## | anthropic | anthropic, openai, google, mistral
| ##FH_DETERMINISTIC## | true | true = temperature 0, false = default

== Current Implementation ==

|= Feature |= Status |= Notes
| **Multi-provider support** | Implemented | Anthropic, OpenAI, Google, Mistral (Anthropic fallback)
| **Provider selection** | Implemented | Via ##LLM_PROVIDER## env var
| **Deterministic mode** | Implemented | ##FH_DETERMINISTIC=true## sets temperature 0
| **Tiered model routing** | Implemented | ##model-tiering.ts## routes tasks to budget/standard/premium models
| **Automatic failover** | Not implemented | Manual provider switch only
| **Per-stage provider** | Not implemented | Single provider for all stages (different models per tier)

== Future Enhancements ==

* **Automatic failover**: Chain providers for resilience
* **Per-stage optimization**: Different providers per pipeline stage
* **Local models**: Ollama/vLLM for on-premises deployment
