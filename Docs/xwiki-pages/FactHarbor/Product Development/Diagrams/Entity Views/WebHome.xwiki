{{info}}
**Multi-View Entity Reference (CB Pipeline v2.11.0+)** -- Five complementary ERD views of the FactHarbor entity landscape. Each view highlights a different aspect: overview, analysis result, target database, runtime processing, and UI visibility.

**Source of truth**: ##apps/web/src/lib/analyzer/types.ts##, ##apps/api/Data/Entities.cs##, ##apps/web/src/lib/config-storage.ts##

Updated 2026-02-22 per CB pipeline interfaces in ##types.ts##.
{{/info}}

= Entity Views =

Five views of the same entity landscape, each serving a different audience and purpose.

|= View |= Purpose |= Audience
| **[[Overview>>||anchor="HOverviewERD"]]** | Bird's-eye view of all entity groups | Everyone (entry point)
| **[[Analysis Result>>||anchor="HAnalysisResultEntities"]]** | Everything persisted in ##resultJson## | Developers, data architects
| **[[Target Database>>||anchor="HTargetDatabaseEntities"]]** | Future PostgreSQL table design | Database architects, backend developers
| **[[Runtime Process>>||anchor="HRuntimeProcessEntities"]]** | Transient entities during pipeline execution | Pipeline developers
| **[[UI Visible>>||anchor="HUI-VisibleEntities"]]** | What users see in the browser | Frontend developers, UX designers

=== Color Legend ===

All views use a consistent color scheme:

|= Color |= Meaning
| (% style="background-color:#c8e6c9; padding:4px;" %)Green | Core analysis (verdicts, claims)
| (% style="background-color:#e3f2fd; padding:4px;" %)Blue | Infrastructure (jobs, config, metrics)
| (% style="background-color:#fff3e0; padding:4px;" %)Orange | Evidence and sources
| (% style="background-color:#e1bee7; padding:4px;" %)Purple | Understanding and decomposition
| (% style="background-color:#fff9c4; padding:4px;" %)Yellow | Quality and validation
| (% style="background-color:#f5f5f5; padding:4px;" %)Grey | Runtime-only / transient
| (% style="background-color:#ffcdd2; padding:4px;" %)Red | Planned / not yet implemented

----

== Overview ERD ==

Bird's-eye view showing all major entity groups and their primary relationships. Minimal field detail -- use the other views for field-level information.

{{mermaid}}

erDiagram
    JOB ||--|| ANALYSIS_RESULT : produces
    JOB ||--o{ JOB_EVENT : logs
    JOB }o--|| CONFIG_SNAPSHOT : uses

    ANALYSIS_RESULT ||--|| CB_CLAIM_UNDERSTANDING : contains
    ANALYSIS_RESULT ||--o{ ATOMIC_CLAIM : contains
    ANALYSIS_RESULT ||--o{ EVIDENCE_ITEM : contains
    ANALYSIS_RESULT ||--o{ FETCHED_SOURCE : contains
    ANALYSIS_RESULT ||--o{ CLAIM_BOUNDARY : contains
    ANALYSIS_RESULT ||--o{ CB_CLAIM_VERDICT : contains
    ANALYSIS_RESULT ||--|| OVERALL_ASSESSMENT : aggregates
    ANALYSIS_RESULT ||--|| QUALITY_GATES : validated_by
    ANALYSIS_RESULT ||--|| TWO_PANEL_SUMMARY : summarized_as

    CB_CLAIM_UNDERSTANDING ||--o{ ATOMIC_CLAIM : extracts

    ATOMIC_CLAIM ||--o{ CB_CLAIM_VERDICT : receives
    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : cites
    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : contains
    BOUNDARY_FINDING }o--|| CLAIM_BOUNDARY : per_boundary
    EVIDENCE_ITEM }o--|| FETCHED_SOURCE : extracted_from
    EVIDENCE_ITEM }o--o| CLAIM_BOUNDARY : assigned_to

    OVERALL_ASSESSMENT ||--|| VERDICT_NARRATIVE : has
    OVERALL_ASSESSMENT ||--|| COVERAGE_MATRIX : has

    SOURCE_RELIABILITY ||--o{ FETCHED_SOURCE : scores

    JOB {
        string JobId_PK
        string Status
        string PipelineVariant
    }

    ANALYSIS_RESULT {
        string schemaVersion "3.0.0-cb"
        json ResultJson
    }

    CB_CLAIM_UNDERSTANDING {
        string detectedInputType
        string articleThesis
        string riskTier
    }

    ATOMIC_CLAIM {
        string id_PK
        string statement
        string centrality
        string harmPotential
    }

    CLAIM_BOUNDARY {
        string id_PK
        string name
        string shortName
    }

    CB_CLAIM_VERDICT {
        string id_PK
        string claimId_FK
        int truthPercentage
        string verdict
        int confidence
    }

    BOUNDARY_FINDING {
        string boundaryId_FK
        int truthPercentage
        string evidenceDirection
    }

    EVIDENCE_ITEM {
        string id_PK
        string statement
        string probativeValue
    }

    FETCHED_SOURCE {
        string id_PK
        string url
        float trackRecordScore
    }

    OVERALL_ASSESSMENT {
        int truthPercentage
        string verdict
        int confidence
    }

    VERDICT_NARRATIVE {
        string headline
        string keyFinding
    }

    COVERAGE_MATRIX {
        json claims_x_boundaries
    }

    QUALITY_GATES {
        boolean passed
        json gate1Stats
        json gate4Stats
    }

    TWO_PANEL_SUMMARY {
        json articleSummary
        json factharborAnalysis
    }

    CONFIG_SNAPSHOT {
        text hash_PK
        text config_type
    }

    JOB_EVENT {
        long Id_PK
        string JobId_FK
        string Level
        string Message
    }

    SOURCE_RELIABILITY {
        text domain_PK
        float score
        float confidence
    }

{{/mermaid}}

//Overview: JOB produces an ANALYSIS_RESULT containing decomposed claims (CB_CLAIM_UNDERSTANDING with ATOMIC_CLAIMs), per-claim verdicts (CB_CLAIM_VERDICT with per-boundary BOUNDARY_FINDINGs) supported by evidence (EVIDENCE_ITEM) from web sources (FETCHED_SOURCE), grouped into evidence-emergent CLAIM_BOUNDARYs, and aggregated into an OVERALL_ASSESSMENT with VERDICT_NARRATIVE and COVERAGE_MATRIX. Quality validation (QUALITY_GATES) and a user-facing summary (TWO_PANEL_SUMMARY) complete the result. SOURCE_RELIABILITY provides cached domain-level trust scores for FETCHED_SOURCEs.//

----

== Analysis Result Entities ==

Everything persisted in the ##resultJson## blob at the end of an analysis. This is the complete entity model inside the JSON. Grouped by pipeline stage.

=== Stage 1: Extract Claims ===

{{mermaid}}

erDiagram
    CB_CLAIM_UNDERSTANDING ||--o{ ATOMIC_CLAIM : extracts
    CB_CLAIM_UNDERSTANDING ||--|| GATE_1_STATS : produces

    CB_CLAIM_UNDERSTANDING {
        string detectedInputType "claim_or_article"
        string impliedClaim
        string articleThesis
        string backgroundDetails
        string riskTier "A_B_or_C"
        json distinctEvents
        json preliminaryEvidence
    }

    ATOMIC_CLAIM {
        string id_PK "AC_01_AC_02"
        string statement
        string category "factual_evaluative_procedural"
        string centrality "high_medium"
        string harmPotential "critical_high_medium_low"
        boolean isCentral "Always_true_(filtered)"
        string claimDirection "supports_thesis_contradicts_thesis_contextual"
        string verifiability "high_medium_low_none_(optional)"
        string checkWorthiness "high_medium"
        float specificityScore "0-1"
        string groundingQuality "strong_moderate_weak_none"
        json keyEntities
        json expectedEvidenceProfile
    }

    GATE_1_STATS {
        int totalClaims
        int passedOpinion
        int passedSpecificity
        int passedFidelity
        int filteredCount
        boolean overallPass
    }

{{/mermaid}}

//Stage 1 output: CB_CLAIM_UNDERSTANDING with ATOMIC_CLAIMs (the analytical units). Each claim has centrality, harmPotential (4-level: critical/high/medium/low), groundingQuality (strong/moderate/weak/none), specificityScore (0-1, Gate 1 minimum 0.6), and an expectedEvidenceProfile describing what evidence would verify or refute the claim. Gate 1 stats include passedFidelity (claim-to-input fidelity check). The optional verifiability field (B-6) independently assesses fact-checkability.//

=== Stage 2: Research ===

{{mermaid}}

erDiagram
    EVIDENCE_ITEM }o--|| FETCHED_SOURCE : extracted_from
    EVIDENCE_ITEM ||--o| EVIDENCE_SCOPE : has_scope

    SEARCH_QUERY ||--o{ FETCHED_SOURCE : found

    EVIDENCE_ITEM {
        string id_PK "EV_001_EV_002"
        string statement
        string category "statistic_expert_quote_event_legal_provision_etc"
        string sourceId_FK
        string sourceUrl
        string sourceTitle
        string sourceExcerpt
        string claimDirection "supports_contradicts_neutral"
        string sourceAuthority "primary_secondary_opinion"
        string evidenceBasis "scientific_documented_anecdotal_theoretical_pseudoscientific"
        string probativeValue "high_medium_low"
        float extractionConfidence "0-100"
        string sourceType "peer_reviewed_study_etc"
        json relevantClaimIds
        string claimBoundaryId_FK "Assigned_in_Stage_3"
        boolean isDerivative
        string derivedFromSourceUrl
        boolean derivativeClaimUnverified
        string scopeQuality "complete_partial_incomplete"
        boolean isContestedClaim
        boolean fromOppositeClaimSearch
    }

    EVIDENCE_SCOPE {
        string name
        string methodology
        string temporal
        string boundaries
        string geographic
        string sourceType
        map additionalDimensions
    }

    FETCHED_SOURCE {
        string id_PK
        string url
        string title
        string fullText
        float trackRecordScore "0.0-1.0"
        float trackRecordConfidence "0.0-1.0"
        boolean trackRecordConsensus
        string category
        boolean fetchSuccess
        string fetchedAt
        string searchQuery
    }

    SEARCH_QUERY {
        string query
        int iteration
        string focus
        int resultsCount
        string searchProvider
        string timestamp
        string error
    }

{{/mermaid}}

//Stage 2 output: EVIDENCE_ITEMs with EVIDENCE_SCOPEs from FETCHED_SOURCEs. CB pipeline additions: relevantClaimIds (which atomic claims evidence relates to), claimBoundaryId (assigned in Stage 3), isDerivative/derivedFromSourceUrl/derivativeClaimUnverified (derivative evidence tracking), scopeQuality assessment (complete/partial/incomplete), fromOppositeClaimSearch (contradiction research flag). Each SEARCH_QUERY records provider, results, and any error. FETCHED_SOURCE includes trackRecordScore, trackRecordConfidence, and trackRecordConsensus from Source Reliability evaluation.//

=== Stage 3: Cluster Boundaries ===

{{mermaid}}

erDiagram
    CLAIM_BOUNDARY ||--o{ EVIDENCE_SCOPE : "composed_of"
    EVIDENCE_ITEM }o--o| CLAIM_BOUNDARY : "assigned_to"

    CLAIM_BOUNDARY {
        string id_PK "CB_01_CB_02"
        string name "Human-readable_label"
        string shortName "Short_label_for_UI_tabs"
        string description
        string methodology "Dominant_methodology"
        string boundaries "Scope_boundaries"
        string geographic "Geographic_scope"
        string temporal "Temporal_scope"
        json constituentScopes "EvidenceScope[]"
        float internalCoherence "0-1"
        int evidenceCount
    }

    EVIDENCE_SCOPE {
        string name
        string methodology
        string temporal
    }

    EVIDENCE_ITEM {
        string claimBoundaryId_FK
    }

{{/mermaid}}

//Stage 3 output: CLAIM_BOUNDARYs (ClaimAssessmentBoundary) emerge from clustering compatible EVIDENCE_SCOPEs. Each boundary has a name, shortName (for UI tabs), derived methodology/boundaries/geographic/temporal from constituent scopes, internalCoherence (0-1), and evidenceCount. Evidence items receive their claimBoundaryId assignment in this stage.//

=== Stage 4: Verdict (LLM Debate) ===

{{mermaid}}

erDiagram
    CB_CLAIM_VERDICT ||--o{ BOUNDARY_FINDING : contains
    CB_CLAIM_VERDICT ||--|| CONSISTENCY_RESULT : has
    CB_CLAIM_VERDICT ||--o{ CHALLENGE_RESPONSE : has
    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : "cites_supporting"
    CB_CLAIM_VERDICT }o--o{ EVIDENCE_ITEM : "cites_contradicting"
    CB_CLAIM_VERDICT ||--|| TRIANGULATION_SCORE : has
    CB_CLAIM_VERDICT ||--o| TRUTH_PERCENTAGE_RANGE : has

    CB_CLAIM_VERDICT {
        string id_PK
        string claimId_FK
        float truthPercentage "0-100"
        string verdict "7-point_label"
        float confidence "0-100"
        string reasoning
        string harmPotential "critical_high_medium_low"
        boolean isContested
        json supportingEvidenceIds
        json contradictingEvidenceIds
        json boundaryFindings
        json consistencyResult
        json challengeResponses
        json triangulationScore
        json truthPercentageRange
        string misleadingness "not_potentially_highly_(optional)"
        string misleadingnessReason
    }

    BOUNDARY_FINDING {
        string boundaryId_FK
        string boundaryName
        float truthPercentage "Per-boundary_0-100"
        float confidence "Per-boundary_0-100"
        string evidenceDirection "supports_contradicts_mixed_neutral"
        int evidenceCount
    }

    CONSISTENCY_RESULT {
        string claimId_FK
        json percentages "Truth_%_from_each_run"
        float average
        float spread "max_minus_min"
        boolean stable "spread_within_threshold"
        boolean assessed "false_if_disabled"
    }

    CHALLENGE_RESPONSE {
        string challengeType "assumption_missing_evidence_methodology_weakness_independence_concern"
        string response
        boolean verdictAdjusted
        json adjustmentBasedOnChallengeIds
    }

    TRIANGULATION_SCORE {
        int boundaryCount
        int supporting
        int contradicting
        string level "strong_moderate_weak_conflicted"
        float factor "Weight_adjustment"
    }

    TRUTH_PERCENTAGE_RANGE {
        float min "0-100"
        float max "0-100"
    }

{{/mermaid}}

//Stage 4 output: CB_CLAIM_VERDICTs via 5-step LLM debate (Advocate -> Self-Consistency -> Adversarial Challenge -> Reconciliation -> Validation). Each verdict includes per-boundary BOUNDARY_FINDINGs, CONSISTENCY_RESULT (self-consistency spread across multiple LLM runs), CHALLENGE_RESPONSEs (how challenges were addressed, with adjustmentBasedOnChallengeIds for provenance), TRIANGULATION_SCORE (cross-boundary agreement), TRUTH_PERCENTAGE_RANGE (plausible range from consistency spread and boundary variance), and optional misleadingness assessment (B-7, output-only).//

=== Stage 5: Aggregate + Quality ===

{{mermaid}}

erDiagram
    OVERALL_ASSESSMENT ||--o{ CB_CLAIM_VERDICT : aggregates
    OVERALL_ASSESSMENT ||--o{ CLAIM_BOUNDARY : presents
    OVERALL_ASSESSMENT ||--|| VERDICT_NARRATIVE : has
    OVERALL_ASSESSMENT ||--|| COVERAGE_MATRIX : has
    OVERALL_ASSESSMENT ||--|| QUALITY_GATES : has
    OVERALL_ASSESSMENT ||--o| EXPLANATION_QUALITY_CHECK : has
    OVERALL_ASSESSMENT ||--o| TRUTH_PERCENTAGE_RANGE : has

    QUALITY_GATES ||--o| GATE1_STATS : claim_validation
    QUALITY_GATES ||--o| GATE4_STATS : verdict_confidence
    QUALITY_GATES ||--o| QUALITY_GATES_SUMMARY : high_level

    EXPLANATION_QUALITY_CHECK ||--|| EXPLANATION_STRUCTURAL_FINDINGS : has
    EXPLANATION_QUALITY_CHECK ||--o| EXPLANATION_RUBRIC_SCORES : has

    OVERALL_ASSESSMENT {
        float truthPercentage "0-100_weighted"
        string verdict "7-point_label"
        float confidence "0-100_weighted"
        boolean hasMultipleBoundaries
        json verdictNarrative
        json claimBoundaries
        json claimVerdicts
        json coverageMatrix
        json qualityGates
        json truthPercentageRange
        json explanationQualityCheck
    }

    VERDICT_NARRATIVE {
        string headline
        string evidenceBaseSummary
        string keyFinding
        json boundaryDisagreements
        string limitations
    }

    COVERAGE_MATRIX {
        json claims "Rows_(claim_IDs)"
        json boundaries "Columns_(boundary_IDs)"
        json counts "Evidence_per_cell"
    }

    QUALITY_GATES {
        boolean passed
    }

    QUALITY_GATES_SUMMARY {
        int totalEvidenceItems
        int totalSources
        int searchesPerformed
        boolean contradictionSearchPerformed
    }

    GATE1_STATS {
        int total
        int passed
        int filtered
        int centralKept
    }

    GATE4_STATS {
        int total
        int publishable
        int highConfidence
        int mediumConfidence
        int lowConfidence
        int insufficient
        int centralKept
    }

    TRUTH_PERCENTAGE_RANGE {
        float min "0-100"
        float max "0-100"
    }

    EXPLANATION_QUALITY_CHECK {
        string mode "structural_or_rubric"
    }

    EXPLANATION_STRUCTURAL_FINDINGS {
        boolean hasCitedEvidence
        boolean hasVerdictCategory
        boolean hasConfidenceStatement
        boolean hasLimitations
    }

    EXPLANATION_RUBRIC_SCORES {
        int clarity "1-5"
        int completeness "1-5"
        int neutrality "1-5"
        int evidenceSupport "1-5"
        int appropriateHedging "1-5"
        float overallScore "weighted_average"
        json flags
    }

    ANALYSIS_WARNING {
        string type
        string severity "error_warning_info"
        string message
        json details
    }

    TWO_PANEL_SUMMARY {
        json articleSummary
        json factharborAnalysis
    }

    PSEUDOSCIENCE_ANALYSIS {
        boolean isPseudoscience
        int confidence
        json categories
        json matchedPatterns
    }

{{/mermaid}}

//Stage 5 output: OVERALL_ASSESSMENT aggregates CB_CLAIM_VERDICTs with weighted averaging (centrality, harm, confidence, triangulation, derivative). VERDICT_NARRATIVE provides structured summary (headline, evidenceBaseSummary, keyFinding, boundaryDisagreements, limitations). COVERAGE_MATRIX maps claims to boundaries. QUALITY_GATES summarize Gate 1 (claim validation with total/passed/filtered/centralKept) and Gate 4 (verdict confidence with publishable/high/medium/low/insufficient/centralKept), plus QUALITY_GATES_SUMMARY (evidence and search counts). EXPLANATION_QUALITY_CHECK (B-8) provides Tier 1 structural findings and optional Tier 2 rubric scores. TRUTH_PERCENTAGE_RANGE gives the plausible overall range. TWO_PANEL_SUMMARY, PSEUDOSCIENCE_ANALYSIS, and ANALYSIS_WARNINGs complete the result.//

----

== Target Database Entities ==

Entities that should become PostgreSQL tables in the target architecture. Color indicates implementation status: green = exists as table, blue = target (currently in JSON blob), red = planned but not implemented.

{{mermaid}}

erDiagram
    JOBS ||--o{ JOB_EVENTS : logs
    JOBS ||--o| ANALYSIS_METRICS : tracked_by
    JOBS ||--o{ CONFIG_USAGE : snapshot
    CONFIG_USAGE }o--|| CONFIG_BLOBS : references
    CONFIG_ACTIVE }o--|| CONFIG_BLOBS : points_to

    JOBS ||--o{ ATOMIC_CLAIMS : produces
    JOBS ||--o{ CB_CLAIM_VERDICTS : produces
    JOBS ||--o{ EVIDENCE_ITEMS : produces
    JOBS ||--o{ FETCHED_SOURCES : produces
    JOBS ||--o{ CLAIM_BOUNDARIES : produces

    CB_CLAIM_VERDICTS }o--o{ EVIDENCE_ITEMS : supported_by
    CB_CLAIM_VERDICTS }o--|| ATOMIC_CLAIMS : for_claim
    EVIDENCE_ITEMS }o--|| FETCHED_SOURCES : from
    EVIDENCE_ITEMS }o--o| CLAIM_BOUNDARIES : assigned_to
    FETCHED_SOURCES }o--o| SOURCE_RELIABILITY : cached_score

    USERS ||--o{ FLAGS : reports
    FLAGS }o--|| JOBS : targets

    JOBS {
        string JobId_PK
        string Status
        int Progress
        string InputType
        string InputValue
        string InputPreview
        string PipelineVariant
        string ParentJobId_FK
        int RetryCount
        datetime RetriedFromUtc
        string RetryReason
        string PromptContentHash
        datetime PromptLoadedUtc
        datetime CreatedUtc
        datetime UpdatedUtc
        json ResultJson
        text ReportMarkdown
    }

    JOB_EVENTS {
        long Id_PK
        string JobId_FK
        datetime TsUtc
        string Level
        string Message
    }

    ANALYSIS_METRICS {
        guid Id_PK
        string JobId_FK
        json MetricsJson
        datetime CreatedUtc
    }

    CONFIG_BLOBS {
        text hash_PK
        text config_type
        json content
        text changed_by
        text change_reason
        datetime created_at
    }

    CONFIG_ACTIVE {
        text config_type_PK
        text blob_hash_FK
        datetime activated_at
    }

    CONFIG_USAGE {
        string job_id_FK
        text config_type
        text blob_hash_FK
        datetime snapshot_at
    }

    SOURCE_RELIABILITY {
        text domain_PK
        float score
        float confidence
        boolean consensus
        datetime evaluated_at
        int ttl_days
    }

    ATOMIC_CLAIMS {
        string id_PK "AC_01"
        string jobId_FK
        string statement
        string category
        string centrality
        string harmPotential "critical_high_medium_low"
        float specificityScore
        string groundingQuality
        string claimDirection
        json keyEntities
        json expectedEvidenceProfile
    }

    CB_CLAIM_VERDICTS {
        string id_PK
        string jobId_FK
        string claimId_FK
        int truthPercentage
        int confidence
        string verdict
        string reasoning
        string harmPotential
        boolean isContested
        json supportingEvidenceIds
        json contradictingEvidenceIds
        json boundaryFindings
        json consistencyResult
        json challengeResponses
        json triangulationScore
        json truthPercentageRange
        string misleadingness
    }

    EVIDENCE_ITEMS {
        string id_PK
        string jobId_FK
        string statement
        string category
        string sourceId_FK
        string probativeValue
        string claimDirection
        string claimBoundaryId_FK
        json relevantClaimIds
        boolean isDerivative
        string derivedFromSourceUrl
        boolean derivativeClaimUnverified
        string scopeQuality
        string sourceType
        float extractionConfidence
    }

    FETCHED_SOURCES {
        string id_PK
        string jobId_FK
        string url
        string domain
        string title
        float trackRecordScore
        float trackRecordConfidence
        boolean trackRecordConsensus
        boolean fetchSuccess
        datetime fetchedAt
    }

    CLAIM_BOUNDARIES {
        string id_PK "CB_01"
        string jobId_FK
        string name
        string shortName
        string description
        string methodology
        string geographic
        string temporal
        float internalCoherence
        int evidenceCount
    }

    USERS {
        uuid id_PK
        string username
        string email
        string role
        datetime created_at
    }

    FLAGS {
        uuid id_PK
        string entity_type
        string entity_id_FK
        uuid reported_by_FK
        string issue_type
        string status
    }

    QUALITY_METRICS {
        uuid id_PK
        string metric_type
        string category
        float value
        float target
        datetime timestamp
    }

{{/mermaid}}

=== Implementation Status ===

|= Table |= Status |= Technology |= Notes
| ##jobs## | Exists | .NET EF Core (SQLite) | Analysis results stored as JSON blob in ##ResultJson##. Includes retry tracking (ParentJobId, RetryCount) and prompt tracking (PromptContentHash).
| ##job_events## | Exists | .NET EF Core (SQLite) | Event log with SSE streaming
| ##analysis_metrics## | Exists | .NET EF Core (SQLite) | Metrics as JSON blob
| ##config_blobs## | Exists | Next.js better-sqlite3 | Immutable, content-addressed
| ##config_active## | Exists | Next.js better-sqlite3 | Activation pointers
| ##config_usage## | Exists | Next.js better-sqlite3 | Per-job config snapshots
| ##source_reliability## | Exists | Next.js better-sqlite3 | LLM-evaluated cache (90-day TTL)
| ##atomic_claims## | **Target** | PostgreSQL | Normalised from JSON blob (includes claimDirection, keyEntities, expectedEvidenceProfile)
| ##cb_claim_verdicts## | **Target** | PostgreSQL | Normalised from JSON blob (includes boundaryFindings, consistencyResult, challengeResponses, triangulationScore, truthPercentageRange, misleadingness)
| ##evidence_items## | **Target** | PostgreSQL | Normalised from JSON blob (includes claimBoundaryId, derivative flags, scopeQuality, sourceType)
| ##fetched_sources## | **Target** | PostgreSQL | Normalised from JSON blob (includes trackRecordConfidence, trackRecordConsensus)
| ##claim_boundaries## | **Target** | PostgreSQL | Normalised from JSON blob (includes geographic, temporal, internalCoherence)
| ##users## | **Planned** | PostgreSQL | Not yet implemented (Alpha phase)
| ##flags## | **Planned** | PostgreSQL | Not yet implemented (Alpha phase)
| ##quality_metrics## | **Planned** | PostgreSQL | Not yet implemented (time-series)

For detailed field descriptions, denormalisation strategy, and cost projections, see [[Target Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]].

----

== Runtime Process Entities ==

Entities that exist only during pipeline execution. These are transient -- they facilitate processing but are not directly stored in the result JSON. Some contribute data that flows into stored entities.

{{mermaid}}

flowchart TD
    subgraph Extract["EXTRACT CLAIMS (Stage 1)"]
        ICR["InputClassificationResult\nisComparative, isCompound\nclaimType, complexity"]
        G1M["Gate 1 Validation\nclaims filtered, fidelity check"]
    end

    subgraph Research["RESEARCH (Stage 2)"]
        RS["CBResearchState\n(main mutable container)\nunderstanding, evidenceItems\nsources, searchQueries, llmCalls\nmainIterationsUsed\ncontradictionIterations\nqueryBudgetUsageByClaim\nwarnings"]
        RD["ResearchDecision\ncomplete, focus, queries"]
        EQR["EvidenceQualityResult\nqualityAssessment, issues\nreasoning (per evidence)"]
        PV["ProvenanceValidation\nURL validation, excerpt check"]
        BT["BudgetTracker\ntokens, iterations\nllmCalls, budgetExceeded"]
    end

    subgraph Cluster["CLUSTER BOUNDARIES (Stage 3)"]
        SCOPE_CLUSTER["Scope Clustering\nEvidenceScope compatibility\nmerge vs separate decision"]
    end

    subgraph Verdict["VERDICT (Stage 4)"]
        ADV["Advocate Verdict\ninitial per-claim verdicts"]
        SC["Self-Consistency Check\ntemp=0.3, spread measurement"]
        CHAL["ChallengeDocument\nchallenge points per claim\nwith ChallengeValidation"]
        REC["Reconciliation\nfinal verdicts"]
        VV["Verdict Validation\ngrounding + direction checks"]
        G4M["Gate 4 Assessment\nconfidence distribution"]
    end

    subgraph Metrics["METRICS (Full Pipeline)"]
        MC["MetricsCollector\naccumulates all telemetry"]
        LCM["LLMCallMetric\ntokens, duration, success"]
        SQM["SearchQueryMetric\nquery, provider, duration"]
    end

    ICR -->|"data flows into"| CU["CBClaimUnderstanding\n(STORED in result)"]
    G1M -->|"summary"| QG["gate1Stats\n(STORED in result)"]
    EQR -->|"filters"| EI["EvidenceItems\n(STORED in result)"]
    PV -->|"filters"| EI
    SCOPE_CLUSTER -->|"creates"| CB["ClaimBoundaries\n(STORED in result)"]
    ADV -->|"initial"| VERD["CBClaimVerdicts\n(STORED in result)"]
    SC -->|"consistency"| VERD
    CHAL -->|"challenges"| VERD
    REC -->|"final"| VERD
    VV -->|"validates"| VERD
    G4M -->|"summary"| QG4["gate4Stats\n(STORED in result)"]
    BT -->|"summary"| BS["meta.budgetStats\n(STORED in result)"]
    RD -->|"discarded"| DISC["Discarded"]
    MC -->|"sent to API"| AM["AnalysisMetrics\n(STORED in separate DB)"]
    LCM --> MC
    SQM --> MC
    RS -->|"fields flow into"| RES["resultJson fields\n(STORED in result)"]

    style CU fill:#c8e6c9,stroke:#2e7d32,color:#000
    style QG fill:#fff9c4,stroke:#f9a825,color:#000
    style EI fill:#fff3e0,stroke:#e65100,color:#000
    style CB fill:#e1bee7,stroke:#7b1fa2,color:#000
    style VERD fill:#c8e6c9,stroke:#2e7d32,color:#000
    style QG4 fill:#fff9c4,stroke:#f9a825,color:#000
    style BS fill:#e3f2fd,stroke:#1565c0,color:#000
    style AM fill:#e3f2fd,stroke:#1565c0,color:#000
    style RES fill:#c8e6c9,stroke:#2e7d32,color:#000
    style DISC fill:#f5f5f5,stroke:#9e9e9e,color:#666

    style ICR fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style G1M fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style RS fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style RD fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style EQR fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style PV fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style BT fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style SCOPE_CLUSTER fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style ADV fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style SC fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style CHAL fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style REC fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style VV fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style G4M fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style MC fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style LCM fill:#f5f5f5,stroke:#9e9e9e,color:#000
    style SQM fill:#f5f5f5,stroke:#9e9e9e,color:#000

{{/mermaid}}

//Grey boxes = transient runtime entities. Colored boxes = stored destinations. Arrows show how runtime data flows into (or is discarded from) the final result. CBResearchState is the main mutable container through the pipeline, accumulating evidenceItems, sources, searchQueries, claimBoundaries, and warnings. ChallengeDocument contains ChallengePoints with ChallengeValidation (structural validation of evidence references). Source files: ##types.ts##, ##metrics.ts##, ##budgets.ts##, ##claimboundary-pipeline.ts##, ##verdict-stage.ts##.//

=== Runtime Entity Reference ===

|= Entity |= Source File |= Lifecycle |= Destination
| ##CBResearchState## | ##types.ts:971## | Full pipeline | Container -- fields distributed into resultJson. Includes queryBudgetUsageByClaim, iteration counters, and accumulated warnings.
| ##BudgetTracker## | ##budgets.ts## | Full pipeline | ##meta.budgetStats##
| ##ResearchDecision## | ##types.ts:599## | Per iteration | Discarded after use
| ##InputClassificationResult## | ##text-analysis-types.ts## | Extract phase | Data flows into ##CBClaimUnderstanding##
| ##EvidenceQualityResult## | ##text-analysis-types.ts## | Research phase | Filters ##EvidenceItems## (pass/fail)
| ##VerdictValidationResult## | ##types.ts:121## | Verdict phase | Advisory checks on ##CBClaimVerdicts##
| ##MetricsCollector## | ##metrics.ts## | Full pipeline | ##AnalysisMetrics## (separate DB)
| ##LLMCallMetric## | ##metrics.ts## | Per LLM call | Aggregated in ##MetricsCollector##
| ##SearchQueryMetric## | ##metrics.ts## | Per search call | Aggregated in ##MetricsCollector##
| ##ProvenanceValidation## | ##evidence-filter.ts## | Research phase | Filters evidence (pass/fail)
| ##ChallengeDocument## | ##types.ts:845## | Verdict Step 3 | Challenge points consumed by Reconciliation (Step 4)
| ##ChallengePoint## | ##types.ts:857## | Verdict Step 3 | Individual challenges with ChallengeValidation, consumed by Reconciliation
| ##ChallengeValidation## | ##types.ts:874## | Verdict Step 3 | Structural validation of evidence references (validIds/invalidIds)
| ##ConsistencyResult## | ##types.ts:820## | Verdict Step 2 | Stored in ##CBClaimVerdict.consistencyResult##

----

== UI-Visible Entities ==

Entities and fields that surface to users in the browser. Grouped by UI page.

{{mermaid}}

flowchart TD
    subgraph JobsList["/jobs - Jobs List Page"]
        JL_JOB["JOB\nstatus, progress\ninputPreview\npipelineVariant\ncreatedUtc"]
    end

    subgraph JobDetail["/jobs/id - Job Detail Page"]

        subgraph VerdictBanner["Verdict Banner"]
            JD_OA["OVERALL_ASSESSMENT\ntruthPercentage, verdict\nconfidence\ntruthPercentageRange"]
            JD_VN["VERDICT_NARRATIVE\nheadline, keyFinding\nlimitations"]
            JD_TPS["TWO_PANEL_SUMMARY\narticleSummary\nfactharborAnalysis"]
        end

        subgraph BoundarySection["ClaimAssessmentBoundary Tabs"]
            JD_CB["CLAIM_BOUNDARY\nname, shortName\nmethodology, temporal\ngeographic"]
            JD_BF["BOUNDARY_FINDING\ntruthPercentage, confidence\nevidenceDirection, evidenceCount"]
        end

        subgraph ClaimsSection["Claims Analyzed"]
            JD_AC["ATOMIC_CLAIM\nstatement, category\ncentrality, harmPotential"]
            JD_CV["CB_CLAIM_VERDICT\nverdict, truthPercentage\nconfidence, reasoning\nharmPotential, isContested\nmisleadingness"]
        end

        subgraph EvidenceSection["Evidence Panel"]
            JD_EI["EVIDENCE_ITEM\nstatement, category\nsourceTitle, claimDirection\nprobativeValue"]
            JD_ES["EVIDENCE_SCOPE\n(tooltip: methodology\nboundaries, geographic)"]
        end

        subgraph SourcesSection["Sources Tab"]
            JD_FS["FETCHED_SOURCE\nurl, title\ntrackRecordScore\nfetchSuccess"]
        end

        subgraph QualitySection["Quality Panel"]
            JD_QG["QUALITY_GATES\npassed, gate1Stats\ngate4Stats"]
            JD_AW["ANALYSIS_WARNING\ntype, severity\nmessage"]
            JD_EQC["EXPLANATION_QUALITY_CHECK\nmode, structuralFindings\nrubricScores (when rubric)"]
        end

        subgraph SearchSection["Search Queries"]
            JD_SQ["SEARCH_QUERY\nquery, resultsCount\nsearchProvider"]
        end
    end

    subgraph Admin["/admin - Admin Pages"]
        AD_HEALTH["Provider Health\nstate, consecutiveFailures"]
        AD_METRICS["ANALYSIS_METRICS\navgDuration, avgCost\ngate1PassRate\ngate4HighConfidenceRate"]
        AD_CONFIG["CONFIG_SNAPSHOT\npipelineConfig\nsearchConfig"]
    end

    JL_JOB -->|"click job"| JobDetail
    JD_AC -->|"has verdict"| JD_CV
    JD_CV -->|"references"| JD_EI
    JD_EI -->|"from"| JD_FS
    JD_CB -->|"findings"| JD_BF

    style JL_JOB fill:#e3f2fd,stroke:#1565c0,color:#000
    style JD_OA fill:#c8e6c9,stroke:#2e7d32,color:#000
    style JD_VN fill:#c8e6c9,stroke:#2e7d32,color:#000
    style JD_TPS fill:#c8e6c9,stroke:#2e7d32,color:#000
    style JD_CB fill:#e1bee7,stroke:#7b1fa2,color:#000
    style JD_BF fill:#e1bee7,stroke:#7b1fa2,color:#000
    style JD_AC fill:#e1bee7,stroke:#7b1fa2,color:#000
    style JD_CV fill:#c8e6c9,stroke:#2e7d32,color:#000
    style JD_EI fill:#fff3e0,stroke:#e65100,color:#000
    style JD_ES fill:#fff3e0,stroke:#e65100,color:#000
    style JD_FS fill:#fff3e0,stroke:#e65100,color:#000
    style JD_QG fill:#fff9c4,stroke:#f9a825,color:#000
    style JD_AW fill:#fff9c4,stroke:#f9a825,color:#000
    style JD_EQC fill:#fff9c4,stroke:#f9a825,color:#000
    style JD_SQ fill:#fff3e0,stroke:#e65100,color:#000
    style AD_HEALTH fill:#e3f2fd,stroke:#1565c0,color:#000
    style AD_METRICS fill:#e3f2fd,stroke:#1565c0,color:#000
    style AD_CONFIG fill:#e3f2fd,stroke:#1565c0,color:#000

{{/mermaid}}

//UI visibility: The Jobs list shows minimal JOB metadata. The Job detail page renders the full analysis result across multiple sections: verdict banner (OVERALL_ASSESSMENT with truthPercentageRange + VERDICT_NARRATIVE), ClaimAssessmentBoundary tabs with BOUNDARY_FINDINGs (including evidenceCount), claims section showing ATOMIC_CLAIMs with their CB_CLAIM_VERDICTs (including misleadingness when present), evidence panel, sources, quality gates (including EXPLANATION_QUALITY_CHECK from B-8), and search queries. Admin pages show operational data (provider health, aggregated metrics, config snapshots).//

=== What Users See vs. What's Internal ===

|= Entity |= User-Visible Fields |= Internal-Only Fields
| **AtomicClaim** | statement, category, centrality, harmPotential, claimDirection | specificityScore, groundingQuality, checkWorthiness, keyEntities, expectedEvidenceProfile, verifiability
| **CBClaimVerdict** | truthPercentage, verdict, confidence, reasoning, isContested, harmPotential, misleadingness | triangulationScore, consistencyResult details, challengeResponses, supportingEvidenceIds, contradictingEvidenceIds, truthPercentageRange details
| **EvidenceItem** | statement, category, sourceTitle, claimDirection, probativeValue | sourceId, extractionConfidence, scopeQuality, sourceAuthority, evidenceBasis, isDerivative, derivedFromSourceUrl, derivativeClaimUnverified, fromOppositeClaimSearch, isContestedClaim, relevantClaimIds
| **FetchedSource** | url, title, trackRecordScore, fetchSuccess | fullText, fetchedAt, searchQuery, trackRecordConfidence, trackRecordConsensus, category
| **ClaimBoundary** | name, shortName, description, methodology, geographic, temporal | internalCoherence, constituentScopes, evidenceCount
| **BoundaryFinding** | truthPercentage, confidence, evidenceDirection, evidenceCount | boundaryId (used for cross-referencing)
| **OverallAssessment** | truthPercentage, verdict, confidence, truthPercentageRange | hasMultipleBoundaries, coverageMatrix details
| **VerdictNarrative** | headline, keyFinding, limitations, boundaryDisagreements | evidenceBaseSummary
| **QualityGates** | passed, gate1Stats (counts), gate4Stats (counts) | Individual ClaimValidationResult, VerdictValidationResult per claim
| **ExplanationQualityCheck** | mode, structuralFindings, rubricScores (when rubric mode) | Internal scoring details

----

**Navigation:** [[Diagrams Index>>FactHarbor.Product Development.Diagrams.WebHome]] | [[Core Data Model ERD>>FactHarbor.Product Development.Diagrams.Core Data Model ERD.WebHome]] | [[Architecture Data Model>>FactHarbor.Product Development.Specification.Architecture.Data Model.WebHome]] | [[Target Data Model>>FactHarbor.Product Development.Specification.Data Model.WebHome]]
