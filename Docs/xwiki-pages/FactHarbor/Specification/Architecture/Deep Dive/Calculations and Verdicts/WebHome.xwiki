= Calculations and Verdicts =

{{info}}
**Developer Reference** — Verdict scale, counter-evidence handling, aggregation hierarchy, weighting formulas, and special-case guards that produce FactHarbor's final verdicts.

**Key Files**: ##apps/web/src/lib/analyzer/aggregation.ts##, ##apps/web/src/lib/analyzer/truth-scale.ts##, ##apps/web/src/lib/analyzer/orchestrated.ts##, ##apps/web/src/lib/analyzer/source-reliability.ts##
{{/info}}

**Version**: 2.6.35
**Status**: Operational

----

== 1. Verdict Scale (7-Point System) ==

FactHarbor uses a symmetric 7-point scale with truth percentages from 0-100%. The 43-57% range distinguishes between **MIXED** (high confidence, evidence on both sides) and **UNVERIFIED** (low confidence, insufficient evidence).

|= Verdict |= Range |= Confidence |= Description
| **TRUE** | 86-100% | - | Strong support, no credible counter-evidence
| **MOSTLY-TRUE** | 72-85% | - | Mostly supported, minor gaps
| **LEANING-TRUE** | 58-71% | - | Mixed evidence, leans positive
| **MIXED** | 43-57% | >= 60% | Evidence on both sides, roughly equal
| **UNVERIFIED** | 43-57% | < 60% | Insufficient evidence to judge
| **LEANING-FALSE** | 29-42% | - | More counter-evidence than support
| **MOSTLY-FALSE** | 15-28% | - | Strong counter-evidence
| **FALSE** | 0-14% | - | Direct contradiction

=== 1.1 MIXED vs UNVERIFIED ===

* **MIXED** (blue in UI): Substantial evidence exists, but is roughly equal on both sides. High confidence in the mixed state.
* **UNVERIFIED** (orange in UI): Not enough evidence to make any judgement. Low confidence due to insufficient information.

=== 1.2 percentageToClaimVerdict ===

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

{{code language="typescript"}}
const MIXED_CONFIDENCE_THRESHOLD = 60;

function percentageToClaimVerdict(truthPercentage: number, confidence?: number): ClaimVerdict7Point {
  if (truthPercentage >= 86) return "TRUE";
  if (truthPercentage >= 72) return "MOSTLY-TRUE";
  if (truthPercentage >= 58) return "LEANING-TRUE";
  if (truthPercentage >= 43) {
    const conf = confidence !== undefined ? normalizePercentage(confidence) : 0;
    return conf >= MIXED_CONFIDENCE_THRESHOLD ? "MIXED" : "UNVERIFIED";
  }
  if (truthPercentage >= 29) return "LEANING-FALSE";
  if (truthPercentage >= 15) return "MOSTLY-FALSE";
  return "FALSE";
}
{{/code}}

=== 1.3 truthFromBand ===

Converts confidence-adjusted bands to truth percentages:

{{code language="typescript"}}
function truthFromBand(band: "strong" | "partial" | "uncertain" | "refuted", confidence: number): number {
  const conf = normalizePercentage(confidence) / 100;
  switch (band) {
    case "strong":    return Math.round(72 + 28 * conf);  // 72-100%
    case "partial":   return Math.round(50 + 35 * conf);  // 50-85%
    case "uncertain": return Math.round(35 + 30 * conf);  // 35-65%
    case "refuted":   return Math.round(28 * (1 - conf)); // 0-28%
  }
}
{{/code}}

**Example ("strong" band with varying confidence)**:

|= Confidence |= Calculation |= Result |= Verdict
| High (90%) | 72 + 28 x 0.9 | 97% | **TRUE**
| Medium (60%) | 72 + 28 x 0.6 | 89% | **TRUE**
| Low (30%) | 72 + 28 x 0.3 | 80% | **MOSTLY-TRUE**

Same evidence band, but lower confidence pulls the verdict down within the band.

----

== 2. Counter-Evidence Handling ==

Counter-evidence is distinguished from mere contestation and influences verdict calculations. The v2.8 system separates **DOUBTED** (political criticism without documented evidence) from **CONTESTED** (actual documented counter-evidence).

=== 2.1 Doubted vs Contested ===

{{mermaid}}
flowchart TD
    subgraph Input["Opposition/Criticism"]
        OPP[Someone opposes or criticizes the claim]
    end

    OPP --> CHECK{Has documented<br/>counter-evidence?}

    CHECK -->|No evidence| DOUBTED["DOUBTED<br/>factualBasis: opinion/alleged"]
    CHECK -->|Some evidence| DISPUTED["CONTESTED<br/>factualBasis: disputed"]
    CHECK -->|Strong evidence| ESTABLISHED["CONTESTED<br/>factualBasis: established"]

    DOUBTED --> W1["Weight: 1.0x<br/>Full weight"]
    DISPUTED --> W2["Weight: 0.5x<br/>Reduced"]
    ESTABLISHED --> W3["Weight: 0.3x<br/>Heavily reduced"]

    style DOUBTED fill:#fff3e0,color:#000
    style DISPUTED fill:#ffecb3,color:#000
    style ESTABLISHED fill:#ffcdd2,color:#000
    style W1 fill:#c8e6c9,color:#000
    style W2 fill:#fff9c4,color:#000
    style W3 fill:#ffcdd2,color:#000
{{/mermaid}}

//Orange = doubted (opinion only). Yellow = disputed evidence. Red = established counter-evidence. Green = full weight preserved.//

**Key Distinction:**

* **DOUBTED** = Political criticism, rhetoric, accusations WITHOUT documented evidence -> Full weight (claim remains credible)
* **CONTESTED** = Has actual documented counter-evidence -> Reduced weight (genuine uncertainty)

**Implementation (v2.8):**

* ##validateContestation()## in ##aggregation.ts## -- KeyFactor-level validation (orchestrated pipeline)
* ##detectClaimContestation()## in ##aggregation.ts## -- Claim-level heuristic (shared)

=== 2.2 Evidence Item Categorization ===

**File**: ##apps/web/src/lib/analyzer/types.ts## (##EvidenceItem## interface)

Evidence items are categorized during extraction:

|= Category |= Description
| ##"evidence"## | Supporting evidence
| ##"criticism"## | Counter-evidence or opposing views
| ##"expert_quote"## | Expert testimony
| ##"statistic"## | Numerical data
| ##"legal_provision"## | Legal framework
| ##"event"## | Factual events

=== 2.3 Contestation Fields ===

{{code language="typescript"}}
interface EvidenceItem {
  category: "legal_provision" | "evidence" | "expert_quote" | "statistic" | "event" | "criticism";
  isContestedClaim?: boolean;  // True if this evidence item contests a claim
  claimSource?: string;         // Who makes the contested claim
}
{{/code}}

=== 2.4 Evidence-Based Contestation Penalties ===

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

Contestation with documented evidence reduces verdict scores:

{{code language="typescript"}}
const evidenceBasedContestation =
  cv.isContested &&
  (cv.factualBasis === "established" || cv.factualBasis === "disputed");

if (evidenceBasedContestation) {
  const penalty = cv.factualBasis === "established" ? 12 : 8;
  truthPct = Math.max(0, truthPct - penalty);
}
{{/code}}

|= factualBasis |= Penalty |= Rationale
| ##"established"## | -12 points | Strong documented counter-evidence
| ##"disputed"## | -8 points | Some documented counter-evidence
| ##"opinion"## | No penalty | Rhetoric only, no documented evidence

----

== 3. Aggregation Hierarchy ==

The aggregation system produces the final verdict by rolling up from individual evidence items through four levels to the overall answer.

{{mermaid}}
graph TD
    Evidence["Evidence Items<br/>(per source)"] --> ClaimVerdicts["Claim Verdicts<br/>(per claim)"]
    ClaimVerdicts --> WeightCalc["Weight Calculation<br/>━━━━━━━━━━━━━<br/>centrality: 2.0x<br/>harmPotential: 1.5x<br/>contested: 0.3-0.5x"]
    WeightCalc --> KeyFactors["Key Factor Verdicts"]
    KeyFactors --> ContextAnswers["Context Answers<br/>(per AnalysisContext)"]
    ContextAnswers --> OverallAnswer["Overall Answer"]

    ClaimVerdicts --> ArticleVerdict["Article Verdict"]

    style Evidence fill:#e8f5e9,color:#000
    style ClaimVerdicts fill:#c8e6c9,color:#000
    style WeightCalc fill:#e3f2fd,color:#000
    style KeyFactors fill:#bbdefb,color:#000
    style ContextAnswers fill:#fff9c4,color:#000
    style OverallAnswer fill:#fff3e0,color:#000
    style ArticleVerdict fill:#f3e5f5,color:#000
{{/mermaid}}

//Green = evidence and claims. Blue = weight calculation and key factors. Yellow = context answers. Orange = overall answer. Purple = article verdict (parallel output).//

=== 3.1 Weight Calculation ===

**Function**: ##getClaimWeight()## in ##aggregation.ts##

{{code language="typescript"}}
function getClaimWeight(claim: WeightedClaim): number {
  let weight = 1.0;

  // Centrality boost
  if (claim.centrality === "central") weight *= 2.0;

  // Harm potential boost
  if (claim.harmPotential === "high") weight *= 1.5;

  // Contestation reduction (only for documented counter-evidence)
  if (claim.isContested) {
    if (claim.factualBasis === "established") weight *= 0.3;
    else if (claim.factualBasis === "disputed") weight *= 0.5;
    // "opinion"/"alleged"/"unknown" = full weight (just doubted)
  }

  return weight;
}
{{/code}}

|= Factor |= Multiplier |= Condition
| Centrality | 2.0x | Claim is ##"central"## to the thesis
| Harm potential | 1.5x | Claim has ##"high"## harm potential
| Contested (established) | 0.3x | Counter-evidence is ##"established"##
| Contested (disputed) | 0.5x | Counter-evidence is ##"disputed"##
| Doubted (opinion) | 1.0x | No documented counter-evidence

=== 3.2 Level 1: Claim Verdicts ===

**Source**: LLM verdict generation + source reliability weighting (see [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]]).

Source reliability adjusts verdicts based on the credibility of evidence sources. See Section 7 below for the full formula.

=== 3.3 Level 2: Key Factor Verdicts ===

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

Key factors aggregate claims mapped to them:

{{code language="typescript"}}
const factorClaims = weightedClaimVerdicts.filter(v => v.keyFactorId === factor.id);
const factorAvgTruthPct = dedupeWeightedAverageTruth(factorClaims);

// Determine support based on average
if (factorAvgTruthPct >= 72) supports = "yes";
else if (factorAvgTruthPct < 43) supports = "no";
else supports = "neutral";
{{/code}}

=== 3.4 Level 3: Context Answers ===

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

AnalysisContexts aggregate key factors with contestation correction:

{{code language="typescript"}}
// Calculate effective negatives (contested negatives are down-weighted)
const effectiveNegatives = negativeFactors - (contestedNegatives * 0.7);

// If positive factors > effective negatives, boost verdict
if (answerTruthPct >= 72 && positiveFactors > effectiveNegatives) {
  // Already positive, no change needed
} else if (answerTruthPct < 72 && positiveFactors > effectiveNegatives) {
  correctedConfidence = Math.min(correctedConfidence, 78);
  answerTruthPct = truthFromBand("strong", correctedConfidence);
}
{{/code}}

=== 3.5 Level 4: Overall Answer ===

Overall answer averages AnalysisContext answers (de-duplicated):

{{code language="typescript"}}
const avgTruthPct = Math.round(
  correctedContextAnswers.reduce((sum, pa) => sum + pa.truthPercentage, 0) /
    correctedContextAnswers.length
);
{{/code}}

**Multi-context caveat**: When ##hasMultipleContexts## is true, the simple average may not be meaningful (e.g., "Legal fairness 85%" + "Scientific validity 30%" = 57.5%). The ##articleVerdictReliability## flag and ##articleVerdictReason## field signal this to the UI. See [[Context Detection>>FactHarbor.Specification.Architecture.Deep Dive.Context Detection.WebHome]] for details.

----

== 4. Near-Duplicate Claim Handling ==

=== 4.1 Problem ===

If the LLM generates multiple claims expressing the same idea, both would influence the overall verdict, effectively double-counting the same evidence.

=== 4.2 Solution: De-Duplication Weighting ===

**Function**: ##dedupeWeightedAverageTruth()## in ##aggregation.ts##

**Algorithm**:

1. **Tokenize** each claim text (lowercase, remove punctuation, filter short words)
1. **Calculate Jaccard similarity** between token sets
1. **Cluster** claims with similarity >= 0.6
1. **Weight** each cluster:
1*. Primary claim (highest truth%): weight = 1.0
1*. Duplicate claims: share weight = 0.5 / (n-1)
1. **Average** using cluster weights

=== 4.3 Example ===

{{code}}
Cluster 1: [Claim A (85%), Claim B (82%), Claim C (80%)]
  - Claim A: 85% x 1.0  = 85.0
  - Claim B: 82% x 0.25 = 20.5
  - Claim C: 80% x 0.25 = 20.0
  - Total weight: 1.5
  - Contribution: 125.5 / 1.5 = 83.7%

Cluster 2: [Claim D (90%)]
  - Claim D: 90% x 1.0 = 90.0
  - Total weight: 1.0
  - Contribution: 90.0 / 1.0 = 90.0%

Overall: (83.7 + 90.0) / 2 = 86.9%
{{/code}}

**UI impact**: All claims are still displayed. De-duplication only affects aggregation calculations, not visibility.

----

== 5. Dependency Handling ==

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

Claims can depend on other claims (e.g., "timing" depends on "attribution"). If a dependency is false, the dependent claim is excluded from aggregation to avoid double-counting the false prerequisite.

{{code language="typescript"}}
// Check if any dependency is false (truthPercentage < 43%)
const failedDeps = dependencies.filter((depId: string) => {
  const depVerdict = verdictMap.get(depId);
  return depVerdict && depVerdict.truthPercentage < 43;
});

if (failedDeps.length > 0) {
  verdict.dependencyFailed = true;
  verdict.failedDependencies = failedDeps;
}
{{/code}}

**Threshold**: ##truthPercentage < 43%## (below the LEANING-FALSE / UNVERIFIED boundary).

Claims with ##dependencyFailed = true## are excluded from aggregation (independent verdicts only).

----

== 6. Pseudoscience Escalation ==

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

Claims matching pseudoscience patterns (water memory, homeopathy, etc.) are automatically escalated to a capped verdict:

{{code language="typescript"}}
if (claimPseudo.isPseudoscience) {
  const escalation = escalatePseudoscienceVerdict(truthPct, finalConfidence, claimPseudo);
  truthPct = escalation.truthPercentage;  // Capped at 28% (FALSE)
  finalConfidence = escalation.confidence;
}
{{/code}}

**Effect**: Truth percentage auto-capped at **28%**, which maps to **MOSTLY-FALSE** or below. This prevents pseudoscientific claims from receiving favourable verdicts regardless of how persuasively they are presented.

----

== 7. Benchmark Guard (Proportionality Claims) ==

**File**: ##apps/web/src/lib/analyzer/orchestrated.ts##

Proportionality claims (e.g., "27-year sentence was proportionate") without comparative benchmark evidence are forced to uncertain:

{{code language="typescript"}}
const isEvaluativeOutcome = hasNumber && EVALUATIVE_OUTCOME_RE.test(claimText);
const hasBenchmarkEvidence = hasComparativeBenchmarkEvidenceFromFacts(factsById, cv.supportingEvidenceIds);

if (isEvaluativeOutcome && !hasBenchmarkEvidence) {
  truthPct = 50;  // Uncertain
  cv.confidence = Math.min(cv.confidence, 55);
  cv.reasoning += " (Insufficient comparative evidence to assess proportionality; treating as uncertain.)";
}
{{/code}}

**Effect**: Forces truth to 50% and caps confidence at 55%, preventing unsupported proportionality judgements.

----

== 8. Source Reliability Weighting ==

**Version**: v2.6.35+
**Files**: ##apps/web/src/lib/analyzer/source-reliability.ts##, ##apps/web/src/lib/analyzer/orchestrated.ts##

Source reliability scores influence verdict calculations by adjusting truth percentages based on the credibility of evidence sources. For full details on the reliability evaluation system, see [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]].

=== 8.1 Score = Weight ===

With the 7-band credibility scale, the LLM score directly represents reliability and is used as-is for verdict weighting:

{{code language="typescript"}}
function calculateEffectiveWeight(data: SourceReliabilityData): number {
  // Simple: score IS the weight
  // Confidence already filtered out low-quality evaluations (threshold gate)
  return data.score;
}
{{/code}}

|= Component |= Purpose
| **Score** | LLM-evaluated reliability (7-band scale, 0.0-1.0) -- used directly as weight
| **Confidence** | Quality gate (threshold: 65%) -- scores below threshold are rejected
| **Consensus** | Multi-model agreement (Claude + GPT-4 must agree within 15%)

=== 8.2 Verdict Adjustment Formula ===

{{code language="typescript"}}
// Average effective weight across all sources for a verdict
const avgWeight = sources.map(s => calculateEffectiveWeight(s))
  .reduce((a, b) => a + b) / sources.length;

// Pull verdict toward neutral (50) based on reliability
adjustedTruth = Math.round(50 + (originalTruth - 50) * avgWeight);

// Scale confidence by reliability
adjustedConfidence = Math.round(originalConfidence * (0.5 + avgWeight / 2));
{{/code}}

=== 8.3 Impact Examples ===

**High Reliability Source (Reuters, 95% score)**

{{code}}
Original verdict: 85% (MOSTLY-TRUE)
Adjusted: 50 + (85 - 50) x 0.95 = 83.3% -> 83% (MOSTLY-TRUE)
Impact: Verdict mostly preserved
{{/code}}

**Unknown Source (50% score -- neutral default)**

{{code}}
Original verdict: 85% (MOSTLY-TRUE)
Adjusted: 50 + (85 - 50) x 0.50 = 67.5% -> 68% (LEANING-TRUE)
Impact: Strong pull toward neutral (appropriate skepticism)
{{/code}}

**Low Reliability Source (27% score)**

{{code}}
Original verdict: 85% (MOSTLY-TRUE)
Adjusted: 50 + (85 - 50) x 0.27 = 59.5% -> 60% (LEANING-TRUE)
Impact: Strong pull toward neutral (unreliable source)
{{/code}}

**Multi-Source Averaging**

{{code}}
Verdict evidence from:
  - reuters.com:       95% score
  - bild.de:           44% score
  - unknown-blog.xyz:  50% score (default)

Average weight: (95 + 44 + 50) / 3 = 63%

Original: 85% -> Adjusted: 50 + (85 - 50) x 0.63 = 72% (LEANING-TRUE)
{{/code}}

=== 8.4 Unknown Source Handling ===

Sources not in the reliability cache are assigned ##defaultScore = 0.5## (symmetric scale centre), resulting in 50% weight (neutral). This applies appropriate skepticism without completely discounting evidence.

----

== 9. Related Documentation ==

* [[Quality Gates>>FactHarbor.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] -- Gate 1 (claim validation) and Gate 4 (confidence assessment)
* [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] -- Full source reliability evaluation system
* [[Context Detection>>FactHarbor.Specification.Architecture.Deep Dive.Context Detection.WebHome]] -- AnalysisContext detection, multi-context averaging reliability
* [[Confidence Calibration>>FactHarbor.Specification.Architecture.Deep Dive.Confidence Calibration.WebHome]] -- 4-layer calibration, recency penalty, low-source penalty
* [[Evidence Quality Filtering>>FactHarbor.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] -- 7-layer evidence filtering strategy
* [[Orchestrated Pipeline>>FactHarbor.Specification.Architecture.Deep Dive.Orchestrated Pipeline.WebHome]] -- Full 5-step pipeline flow
* [[Architecture>>FactHarbor.Specification.Architecture.WebHome]] -- Architecture overview

----

**Navigation:** [[Deep Dive Index>>FactHarbor.Specification.Architecture.Deep Dive.WebHome]] | Prev: [[Evidence Quality Filtering>>FactHarbor.Specification.Architecture.Deep Dive.Evidence Quality Filtering.WebHome]] | Next: [[Prompt Architecture>>FactHarbor.Specification.Architecture.Deep Dive.Prompt Architecture.WebHome]]
