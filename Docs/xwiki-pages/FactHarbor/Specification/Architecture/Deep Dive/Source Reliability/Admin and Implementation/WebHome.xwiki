= Source Reliability: Admin and Implementation =

== Admin Interface ==

Access the Source Reliability admin page at: ##/admin/source-reliability##

=== Features ===

* **Cache Statistics**: Total entries, average scores, expired count
* **Paginated Table**: View all cached scores with sorting
* **Cleanup**: Remove expired entries
* **Authentication**: Requires ##FH_ADMIN_KEY## in production

=== Admin Tasks ===

|= Task |= Estimated Time
| Check LLM cost dashboard | 5 min
| Spot-check 2-3 recent scores | 8 min
| Review any flagged issues | 2 min

----

== Design Principles ==

=== Evidence Over Authority ===

Source credibility is **supplementary**, not primary:

* Only evidence and counter-evidence matter — not who says it
* Authority does NOT automatically give weight
* A low-credibility source with documented evidence should be considered
* A high-credibility source making unsupported claims should be questioned

=== No Pre-seeded Data ===

All sources are evaluated identically by LLM:
* No hardcoded scores or external rating databases
* No manipulation concerns from third-party data
* Full transparency — every score comes from LLM evaluation

=== No Categorical Bias ===

* Domain type (.gov, .edu, .org) does NOT imply quality
* Scores derived from demonstrated track record, not institutional prestige
* Editorial independence matters — state control is a negative factor

=== Entity-Level Evaluation ===

When a domain is the primary digital outlet for a larger organization, the evaluation focuses on the reliability of the entire organization:
* **Legacy Media**: Public broadcasters and established legacy media evaluated based on institutional standards and editorial oversight
* **Consistency**: Prevents high-quality organizations from being underrated due to narrow domain-focused metrics

=== Dynamic Assessment ===

* Sources can gain or lose credibility over time
* Cache expires after 90 days (configurable)
* Re-evaluation happens automatically on cache miss

----

== Implementation Details ==

=== Key Files ===

|= File |= Purpose
| ##apps/web/src/lib/analyzer/source-reliability.ts## | Prefetch, sync lookup, evidence weighting
| ##apps/web/src/lib/source-reliability-cache.ts## | SQLite cache operations
| ##apps/web/src/app/api/internal/evaluate-source/route.ts## | LLM evaluation endpoint
| ##apps/web/src/app/admin/source-reliability/page.tsx## | Admin UI for cache management
| ##apps/web/src/app/api/admin/source-reliability/route.ts## | Admin API endpoint

=== Key Functions ===

{{code language="typescript"}}
// Phase 1: Call ONCE before analysis (async)
export async function prefetchSourceReliability(urls: string[]): Promise<PrefetchResult>;

// Phase 2: Call MANY times during analysis (sync, instant)
export function getTrackRecordScore(url: string): number | null;
export function getTrackRecordData(url: string): CachedReliabilityData | null;

// Phase 3: Apply to verdicts (sync)
export function applyEvidenceWeighting(
  claimVerdicts: ClaimVerdict[],
  facts: ExtractedFact[],
  sources: FetchedSource[]
): ClaimVerdict[];

// Effective weight calculation (used by monolithic pipelines)
export function calculateEffectiveWeight(data: SourceReliabilityData): number;

// Utilities
export function extractDomain(url: string): string | null;
export function isImportantSource(domain: string): boolean;
export function normalizeTrackRecordScore(score: number): number;
export function clampTruthPercentage(value: number): number;
export function clearPrefetchedScores(): void;
{{/code}}

=== Temporal Awareness (v2.6.35+) ===

The LLM evaluation prompt includes the current date and temporal guidance:
* **Government sources** vary by administration
* **Media outlets** can shift with ownership or editorial changes
* **Historical reputation** may not reflect current performance
* **Cache TTL** (90-day default) ensures scores are re-evaluated quarterly

=== Pipeline Integration ===

Source Reliability is integrated into all three pipelines:

|= Pipeline |= File |= Status |= Implementation
| **Orchestrated** | ##orchestrated.ts## | Full | Prefetch + lookup + evidence weighting
| **Monolithic Canonical** | ##monolithic-canonical.ts## | Full | Prefetch + lookup + verdict adjustment
| **Monolithic Dynamic** | ##monolithic-dynamic.ts## | Full | Prefetch + lookup + verdict adjustment

All pipelines follow the same pattern:
1. **Clear** prefetched scores at analysis start
1. **Prefetch** source reliability before fetching URLs
1. **Lookup** scores synchronously when creating sources
1. **Apply** weighting to verdicts

=== Score = Weight ===

With the 7-band scale, the LLM score directly represents reliability and is used as-is:

|= Component |= Purpose
| **Score** | LLM-evaluated reliability (7-band scale) — used directly as weight
| **Confidence** | Quality gate (threshold: 0.8) — scores below are rejected
| **Consensus** | Multi-model agreement (diff <= 0.20)

**Key Design Decisions**:
* **Score = Weight** — No transformation, what LLM says is what we use
* **Confidence is a gate, not a modifier** — If evaluation passes threshold, we trust it
* **Transparency** — A 70% score means 70% weight, no hidden calculations

=== Multi-Model Consensus ===

When ##sr.multiModel## is enabled (default):

1. Build an optional **evidence pack** (web search results) if ##sr.evalUseSearch=true##
1. Both Claude and secondary OpenAI model evaluate the source using the same evidence pack
1. Primary must return confidence >= threshold; secondary must return non-null score
1. Score difference must be <= consensus threshold (default 0.20)
1. Final score = **"better founded"** model output; tie-breaker = **lower score** (skeptical default)
1. If consensus fails -> return ##null## (unknown reliability)

----

== Cost and Performance ==

=== Cost Estimates ===

|= Mode |= Monthly Cost
| Multi-model (default) | $40-60
| Single-model | $20-30

The importance filter saves ~60% of LLM costs by skipping blog platforms and spam domains.

=== Success Metrics ===

|= Metric |= Target
| Cache hit rate (warm) | > 80%
| Blog skip rate | > 90%
| Confidence pass rate | > 85%
| Consensus rate | > 90%

=== Rollback Options ===

|= Issue |= Action
| LLM costs too high | Set ##sr.multiModel=false## in UCM
| Still too expensive | Set ##sr.enabled=false## in UCM
| Too many hallucinations | Raise ##sr.confidenceThreshold## to 0.9
| Low consensus rate | Lower ##sr.consensusThreshold##

----

== Troubleshooting ==

|= Issue |= Solution
| "Unauthorized" from evaluate endpoint | Set ##FH_INTERNAL_RUNNER_KEY## in ##.env.local##
| No scores appearing | Verify ##sr.enabled=true## in UCM
| Low-confidence evaluations | Ensure a search provider is configured for the evidence pack
| High LLM costs | Enable filter and use single model (##sr.multiModel=false##)
| Consensus failures | Lower ##sr.consensusThreshold##
| Score not affecting verdict | Check ##applyEvidenceWeighting## is called, verify ##trackRecordScore## on sources
| Admin page shows 401 | Enter admin key in the auth form, or set ##FH_ADMIN_KEY## in env

----

== Test Coverage ==

|= Test File |= Tests |= Coverage
| ##source-reliability.test.ts## | 42 | Domain extraction, importance filter, evidence weighting
| ##source-reliability-cache.test.ts## | 16 | SQLite operations, pagination, expiration
| ##source-reliability.integration.test.ts## | 13 | End-to-end pipeline flow
| ##evaluate-source.test.ts## | 19 | Rate limiting, consensus calculation
| **Total** | **90** |

Run tests:
{{code language="bash"}}
cd apps/web && npm test -- src/lib/analyzer/source-reliability.test.ts
cd apps/web && npm test -- src/lib/source-reliability-cache.test.ts
cd apps/web && npm test -- src/lib/analyzer/source-reliability.integration.test.ts
cd apps/web && npm test -- src/app/api/internal/evaluate-source/evaluate-source.test.ts
{{/code}}

----

**Navigation:** [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] | Prev: [[Refinement and Multi-Language>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.Refinement and Multi-Language.WebHome]]
