= Context and EvidenceScope Detection Guide =

**Version**: 1.0
**Status**: Consolidated Reference
**Date**: February 3, 2026

----

== 1. Overview and Purpose ==

This guide consolidates all information about FactHarbor's context detection system into a single reference. It explains:

* **What** contexts and evidence scopes are (definitions)
* **When** to use AnalysisContext vs EvidenceScope (decision tree)
* **How** contexts are detected across pipeline phases (flow)
* **Why** the system uses principle-based detection (approach)
* **Where** the implementation lives (code references)

**Target Audience**: Developers working on context detection, prompt engineers, and technical reviewers.

**Replaces scattered documentation in**:
* Context_Detection_via_EvidenceScope.md (principle-based approach, archived to Docs/ARCHIVE/ARCHITECTURE/)
* [[Architecture Overview — Architecture Summary>>FactHarbor.Specification.Implementation.Architecture Overview.Architecture Summary.WebHome]] (formerly Section 1.1)
* Calculations.md Section 2 (AnalysisContext definition, still in Docs/ARCHITECTURE/)
* [[TriplePath Architecture>>FactHarbor.Specification.Implementation.Pipeline Architecture.TriplePath Architecture.WebHome]] (context routing)

----

== 2. Terminology (CRITICAL) ==

=== 2.1 Core Definitions ===

**AnalysisContext** (Top-Level Bounded Analytical Frame):
* A distinct analytical frame requiring separate analysis
* **Scope**: Top-level question or thesis being analyzed
* **Example**: "Legal proceeding A fairness" vs "Legal proceeding B fairness" are distinct contexts
* **Storage**: ##article.analysisContexts## array, ##claim.contextId##, ##verdict.contextId##

**EvidenceScope** (Per-Evidence Source Methodology):
* Metadata about a single evidence item's boundaries and methodology
* **Scope**: Per-evidence source constraints (methodology, boundaries, temporal, geographic)
* **Example**: "Study used Framework A with full system boundary"
* **Storage**: ##evidenceItem.evidenceScope## object

**Key Distinction**: AnalysisContext = "What question am I answering?" vs EvidenceScope = "What boundaries did this evidence source use?"

=== 2.2 Terminology Usage Rules ===

**FactHarbor Entities** (use in prompts and code):
* **Evidence**: Information extracted from sources
* **Verdict**: Conclusions/assessments produced by analysis
* **AnalysisContext**: Top-level bounded analytical frame
* **EvidenceScope**: Per-Evidence source methodology metadata

**Avoid in prompts**:
* ~~fact/facts~~ -> Use "Evidence" / "Evidence items"
* ~~scope~~ (ambiguous) -> Use "AnalysisContext" or "EvidenceScope" explicitly

----

== 3. AnalysisContext vs EvidenceScope Decision Tree ==

{{mermaid}}
flowchart TD
    START{What am I describing?}

    START -->|Top-level question/thesis| Q1{Does it require<br/>separate analysis?}
    START -->|Evidence source metadata| SCOPE[Use EvidenceScope]

    Q1 -->|Yes - different questions| CTX[Use AnalysisContext]
    Q1 -->|No - same question,<br/>different perspectives| NOCTX[Single AnalysisContext]

    CTX --> EX1["Example: 'Proceeding A fairness'<br/>vs 'Proceeding B fairness'<br/>→ 2 AnalysisContexts"]

    NOCTX --> EX2["Example: 'Is claim X true?'<br/>with sources from different orgs<br/>→ 1 AnalysisContext"]

    SCOPE --> EX3["Example: Evidence item has<br/>evidenceScope: {<br/>  methodology: 'Framework A',<br/>  boundaries: 'Full system'<br/>}"]

    style CTX fill:#c8e6c9
    style SCOPE fill:#fff9c4
    style NOCTX fill:#e3f2fd
{{/mermaid}}

=== 3.1 When to Create Multiple AnalysisContexts ===

Create separate AnalysisContexts when:
1. **Different Questions**: Evidence answers fundamentally different questions
1*. "Was Proceeding A fair?" vs "Was Proceeding B fair?"
1*. "Is Method X effective?" vs "Is Method Y effective?"

1. **Incompatible Methodologies**: Combining conclusions would be misleading
1*. "Efficiency using Framework A boundary" vs "Efficiency using Framework B boundary"
1*. "Legal proceeding in Court A" vs "Legal proceeding in Court B"

1. **Distinct Temporal Subjects**: Time periods are the primary subject (not incidental)
1*. "Policy effectiveness in Period 1" vs "Policy effectiveness in Period 2"
1*. NOT: "Study conducted in 2023" (incidental date mention)

=== 3.2 When to Use EvidenceScope (Not AnalysisContext) ===

Use EvidenceScope metadata when:
1. **Source Methodology Markers**: Evidence states its analytical boundaries
1*. "This study uses ISO 14040 methodology"
1*. "Analysis limited to vehicle operation only"

1. **Provenance Tracking**: Distinguishing evidence source constraints
1*. Helps identify methodology mismatches
1*. Enables evidence quality assessment

1. **Verdict Enrichment**: Noting source boundaries without splitting contexts
1*. "Evidence from WTW study applied to TTW analysis" (mismatch flag)

----

== 4. Detection Pipeline Flow ==

=== 4.1 Multi-Phase Context Detection ===

{{mermaid}}
flowchart TB
    subgraph UNDERSTAND["Phase 1: UNDERSTAND"]
        Input[User Input] --> InitialDetection[Initial Context Detection<br/>━━━━━━━━━━━━━<br/>From input text alone<br/>No evidence yet]
        InitialDetection --> Claims[Claim Extraction<br/>━━━━━━━━━━━━━<br/>Claims tagged with contextId]
    end

    subgraph RESEARCH["Phase 2: RESEARCH"]
        Claims --> Search[Web Search]
        Search --> Sources[Source Documents]
    end

    subgraph EXTRACT["Phase 3: EXTRACT_EVIDENCE"]
        Sources --> EV[Evidence Extraction]
        EV --> ES[EvidenceScope Metadata<br/>━━━━━━━━━━━━━<br/>Per-evidence boundaries captured]
    end

    subgraph REFINE["Phase 4: CONTEXT_REFINEMENT"]
        InitialDetection --> Merge[Merge/Refine Contexts<br/>━━━━━━━━━━━━━<br/>Evidence-based discovery<br/>Overlap detection<br/>LLM-driven merge]
        EV --> Merge
        ES -.->|Informs context<br/>discovery| Merge
        Merge --> FinalContexts[Final Contexts<br/>━━━━━━━━━━━━━<br/>Max 5 contexts<br/>Warnings if 4-5]
    end

    style InitialDetection fill:#fff3e0
    style ES fill:#fff9c4
    style Merge fill:#c8e6c9
    style FinalContexts fill:#e3f2fd
{{/mermaid}}

=== 4.2 Context Detection Points ===

|= Phase |= Context Detection |= Uses Evidence? |= Source
| **UNDERSTAND** | Initial detection from input text | No | Input analysis
| **EXTRACT_EVIDENCE** | EvidenceScope metadata capture | Yes | Source documents
| **CONTEXT_REFINEMENT** | Evidence-based discovery & merge | Yes | EvidenceScope patterns

=== 4.3 Data Flow ===

**Phase 1: Initial Context Detection**
{{code language="typescript"}}
// Input: User query
// Output: Initial AnalysisContext candidates
interface AnalysisContext {
  id: string;              // e.g., "CTX_INST_A", "CTX_METHOD_X"
  name: string;            // Human-readable name
  shortName: string;       // Abbreviation
  institution?: string;    // Court, agency, organization
  methodology?: string;    // Standard/method used
  boundaries?: string;     // What's included/excluded
  temporal?: string;       // Time period
  subject: string;         // What's being analyzed
  status: "concluded" | "ongoing" | "pending" | "unknown";
}
{{/code}}

**Phase 3: EvidenceScope Capture**
{{code language="typescript"}}
// Input: Source document
// Output: Per-evidence EvidenceScope metadata
interface EvidenceScope {
  name: string;         // "Framework A", "Court B"
  methodology: string;  // "Standard X", "Full system boundary"
  boundaries: string;   // "Full system", "Subsystem only"
  geographic: string;   // "Region A"
  temporal: string;     // "Period 1"
}
{{/code}}

**Phase 4: Context Refinement**
{{code language="typescript"}}
// Input: Initial contexts + extracted evidence with EvidenceScope
// Process: Scan EvidenceScope patterns → validate → merge/create
// Output: Final AnalysisContext list (max 5)
{{/code}}

----

== 5. Principle-Based Detection Rules ==

=== 5.1 The Single Incompatibility Test ===

Instead of hardcoding specific terms or categories, FactHarbor uses **ONE universal test**:

> **"Would combining findings from this source with other sources be MISLEADING
> because they measure or analyze fundamentally different things?"**

* **YES** -> Extract EvidenceScope (document what makes it incompatible)
* **NO** -> Don't extract (boundaries are compatible enough)

=== 5.2 Key Principles ===

1. **Selective extraction**: Most analyses have 0-1 significant EvidenceScope patterns, max 2-3
1. **Explicit statements only**: Don't invent boundaries the source didn't state
1. **Incompatibility focus**: Only flag boundaries that would cause apples-to-oranges comparisons
1. **Synonym recognition**: Sources use various terms (scope, delimitations, limitations, inclusion criteria)

=== 5.3 EvidenceScope Detection Guidance (EXTRACT_EVIDENCE Phase) ===

**From**: ##apps/web/src/lib/analyzer/prompts/base/extract-evidence-base.ts##

{{code language="markdown"}}
## EVIDENCESCOPE: INCOMPATIBLE ANALYTICAL BOUNDARIES (SELECTIVE)

**THE SINGLE TEST**: Ask yourself:
"If I combined or averaged findings from this source with findings from other sources,
would the result be MISLEADING because they measure or analyze fundamentally different things?"

- YES → Extract EvidenceScope (document what makes it incompatible)
- NO → Don't extract (the boundaries are compatible enough)

**CONSTRAINTS**:
- Most analyses: 0-1 significant boundary patterns
- Complex comparisons: 2-3 patterns maximum
- If source doesn't explicitly state boundaries: Don't invent them
{{/code}}

=== 5.4 Context Discovery from EvidenceScope (CONTEXT_REFINEMENT Phase) ===

**From**: ##apps/web/src/lib/analyzer/prompts/base/context-refinement-base.ts##

{{code language="markdown"}}
## CONTEXT DISCOVERY FROM EVIDENCESCOPE PATTERNS

**The Core Question**: Do the incompatible boundaries found represent genuinely
different analytical frames that need separate verdicts?

**Create separate AnalysisContexts when**:
- EvidenceScope patterns show Evidence answering DIFFERENT QUESTIONS
- Combining conclusions from them would be MISLEADING
- They would require different evidence to evaluate
{{/code}}

=== 5.5 Pattern-Based Heuristics ===

**Comparative claims** (input asks "A vs B"):
* Evidence likely has data for A and data for B
* Check if A's scope differs from B's scope
* If yes, separate contexts

**Multi-process subjects** (input mentions institutions/proceedings):
* Evidence may reference multiple formal processes
* Different processes = different analytical frames
* Check if institutional references cluster

**Historical comparisons** (input asks about change over time):
* Evidence may cover different time periods
* Distinct events (not just dates) = distinct contexts
* Check if temporal references form distinct clusters

----

== 6. Overlap Detection and Merge Heuristics ==

=== 6.1 Context Overlap Problem ===

**Problem**: Over-splitting contexts leads to:
* High context counts (4-5+)
* Redundant analyses
* Confusing verdicts

**Solution**: LLM-driven merge heuristics with explicit guidance

=== 6.2 Merge Criteria ===

**Merge contexts when**:
1. **Subject Overlap**: Both contexts address the same core question
1*. "Institution A handling of Issue X" + "Institution A process for Issue X" -> Merge

1. **Temporal Continuity**: Time periods are parts of a continuous event
1*. "Policy Phase 1 (2020)" + "Policy Phase 2 (2021)" -> Merge if phases are related
1*. "2000s reform" + "1970s reform" -> Keep separate (distinct historical events)

1. **Methodological Compatibility**: Boundaries/standards are compatible
1*. "Study using Framework A, full boundary" + "Study using Framework A, subsystem" -> Keep separate
1*. "Study using Framework A" + "Study using Framework A variant" -> Merge if compatible

=== 6.3 Merge Validation ===

**From**: ##apps/web/src/lib/analyzer/prompts/base/context-refinement-base.ts##

For each potential merge:
* Does merging preserve analytical integrity?
* Would merged contexts answer a coherent question?
* Is evidence from both contexts compatible?

=== 6.4 Temporal Guidance ===

**Temporal markers create contexts when**:
* Time period is the PRIMARY SUBJECT (e.g., "Compare 2000s vs 1970s")
* NOT when it's incidental (e.g., "Study conducted in 2023")

**Example**:
* Do not split: "Decision made in 2020" + "Decision made in 2021" (incidental dates)
* Do split: "2020 reform analysis" + "2021 reform analysis" (distinct reform periods as subjects)

----

== 7. Context Count and User Guidance ==

=== 7.1 How Context Count Appears ===

**1. Progress Messages** (during analysis):
{{code}}
Detected: CLAIM with 5 claims | 3 CONTEXTS
{{/code}}

**2. Report Title** (final report):
{{code}}
Analyzed Content (3 contexts)
{{/code}}

**3. Reasoning Section** (verdict explanation):
{{code}}
Covers 3 contexts: INST_A, INST_B, SCOPE_GENERAL
{{/code}}

=== 7.2 Thresholds and Limits ===

|= Context Count |= Status |= Meaning
| **1-3** | Healthy | Typical for most analyses
| **4-5** | High | Complex analytical frame with multiple incompatible boundaries
| **5+** | Limit | System enforces max via ##contextDetectionMaxContexts## config

=== 7.3 User Guidance for High Context Counts ===

**When you see 4-5 contexts**:

1. **Review Context Names**
1*. Are these genuinely distinct analytical frames?
1*. Or did the system over-split based on minor differences?

1. **Examine Individual Context Verdicts**
1*. Each context gets separate analysis and verdict
1*. Look for meaningful differences in the evidence base per context

1. **Consider Refining Your Input**
1*. Provide more explicit guidance about the intended analytical frame
1*. Use temporal/methodological qualifiers to signal boundaries
1*. Example: Instead of "Is Policy X effective?", use "Is Policy X effective in achieving Goal Y using Metric Z?"

1. **Trust the Overall Verdict (with caveats)**
1*. Overall verdict = simple average of context verdicts
1*. Check ##articleVerdictReliability## field:
1**. ##"high"##: Contexts answer related questions, average is meaningful
1**. ##"low"##: Contexts answer different questions, focus on individual verdicts
1*. When reliability is low, de-emphasize overall average and highlight individual context verdicts

=== 7.4 Multi-Context Average Reliability ===

**Single Context (Most Common)**:
* Average = context verdict (identical)
* Overall verdict fully represents the analysis

**Multiple Contexts (Distinct Frames)**:
* Average may not be meaningful if contexts answer different questions
* Example: "Legal trial fairness (85%)" + "Scientific validity (30%)" = 57.5% average
* The 57.5% doesn't represent either question well

**Architecture Decision (v2.6.38)**:
* Simple averaging chosen over complex weighting schemes
* Transparency via ##articleVerdictReliability## flag and explicit messaging
* Individual context verdicts always preserved and displayed
* Future enhancement: Primary context detection

=== 7.5 Context Count Warnings ===

**Current Status**: Planned but not yet implemented

**Future Behavior**:
* Warnings when context count exceeds recommended thresholds
* Suggestions for input refinement or manual context review
* UI indicators for over-split contexts

----

== 8. Implementation Reference ==

=== 8.1 Core Files ===

**Context Detection Logic**:
* ##apps/web/src/lib/analyzer/analysis-contexts.ts## - Heuristic pre-detection, ##detectContexts()##
* ##apps/web/src/lib/analyzer/orchestrated.ts## - Orchestrated pipeline (primary)
* ##apps/web/src/lib/analyzer/monolithic-canonical.ts## - Canonical monolithic pipeline
* ##apps/web/src/lib/analyzer/monolithic-dynamic.ts## - Dynamic monolithic pipeline

**Prompt Templates**:
* ##apps/web/src/lib/analyzer/prompts/base/extract-evidence-base.ts## - EvidenceScope detection guidance
* ##apps/web/src/lib/analyzer/prompts/base/context-refinement-base.ts## - Context discovery and merge guidance

**Shared Modules**:
* ##apps/web/src/lib/analyzer/aggregation.ts## - Context verdict aggregation
* ##apps/web/src/lib/analyzer/claim-decomposition.ts## - Claim-to-context mapping

=== 8.2 Key Functions ===

**Context Detection**:
{{code language="typescript"}}
// Heuristic pre-detection (optional)
detectContexts(inputText: string): DetectedContext[]

// Format detected contexts for LLM hint
formatDetectedContextsHint(contexts: DetectedContext[]): string
{{/code}}

**Context Refinement**:
{{code language="typescript"}}
// LLM-driven context refinement with evidence
// Phase: CONTEXT_REFINEMENT
// Input: Initial contexts + extracted evidence with EvidenceScope
// Output: Final AnalysisContext list
{{/code}}

**Context Aggregation**:
{{code language="typescript"}}
// Calculate context answer from key factors
// File: aggregation.ts
calculateWeightedVerdictAverage(verdicts: ClaimVerdict[]): number
{{/code}}

=== 8.3 Data Structures ===

**AnalysisContext Interface**:
{{code language="typescript"}}
interface AnalysisContext {
  id: string;              // e.g., "CTX_TSE", "CTX_WTW"
  name: string;            // Human-readable name
  shortName: string;       // Abbreviation
  institution?: string;    // Court, agency, organization
  methodology?: string;    // Standard/method used
  boundaries?: string;     // What's included/excluded
  temporal?: string;       // Time period
  subject: string;         // What's being analyzed
  criteria?: string[];     // Evaluation criteria
  outcome?: string;        // Result if known
  status: "concluded" | "ongoing" | "pending" | "unknown";
  decisionMakers?: DecisionMaker[];
}
{{/code}}

**EvidenceScope Interface**:
{{code language="typescript"}}
interface EvidenceScope {
  name: string;         // "Framework A", "Framework B"
  methodology: string;  // "Standard X", "Standard Y"
  boundaries: string;   // "Full system boundary", "Subsystem boundary"
  geographic: string;   // "Region A"
  temporal: string;     // "Period 1"
}
{{/code}}

=== 8.4 Configuration ===

**Context Detection Settings** (UCM Pipeline Config):
{{code language="json"}}
{
  "contextDetectionMaxContexts": 5,      // Maximum contexts allowed
  "contextDetectionMergeThreshold": 0.7  // Similarity threshold for merging
}
{{/code}}

----

== 9. Examples ==

=== 9.1 Example 1: Institutional Boundary Indicates Distinct Contexts ===

**Input**: "Is Decision X justified?"

**Evidence**:
* Source A excerpt: "Institution A applies Standard A for Decision X..."
* Source B excerpt: "Institution B applies Standard B for Decision X..."

**EvidenceScope**:
* Source A: ##{ institution: "Institution A", methodology: "Standard A" }##
* Source B: ##{ institution: "Institution B", methodology: "Standard B" }##

**Result**: 2 AnalysisContexts
* CTX_INST_A: "Decision X justification (Institution A, Standard A)"
* CTX_INST_B: "Decision X justification (Institution B, Standard B)"

**Rationale**: Different formal bodies/standards -> likely distinct AnalysisContexts

----

=== 9.2 Example 2: Methodology Boundary Indicates Distinct Contexts ===

**Input**: "Is Method X more effective than Method Y?"

**Evidence**:
* Source A excerpt: "Using Framework A, we evaluate the full system boundary..."
* Source B excerpt: "Using Framework B, we evaluate only a subsystem boundary..."

**EvidenceScope**:
* Source A: ##{ methodology: "Framework A", boundaries: "Full system" }##
* Source B: ##{ methodology: "Framework B", boundaries: "Subsystem only" }##

**Result**: 2 AnalysisContexts
* CTX_FRAMEWORK_A: "Method effectiveness (Framework A, Full system)"
* CTX_FRAMEWORK_B: "Method effectiveness (Framework B, Subsystem)"

**Rationale**: Incompatible methodological boundaries -> distinct AnalysisContexts

----

=== 9.3 Example 3: Temporal Mention vs Temporal Subject ===

**Input**: "Did Policy Z have the intended outcome?"

**Evidence**:
* Source A excerpt: "We analyze outcomes during Period 1 (incidental date mention)."
* Source B excerpt: "This paper compares Period 1 vs Period 2 as the primary subject."

**EvidenceScope**:
* Source A: ##{ temporal: "Period 1" }## (incidental)
* Source B: ##{ temporal: "Period 1 vs Period 2" }## (primary subject)

**Result**: 2 AnalysisContexts only if temporal is primary subject
* If Source B makes time periods the primary subject: Split
* If Source A just mentions dates incidentally: Single context

**Rationale**: If time periods are the primary subject (not incidental), split contexts

----

=== 9.4 Example 4: Overlap Indicates Merge ===

**Input**: "Is Claim Q supported?"

**Evidence**:
* Source A and B both declare the same boundary and methodology in EvidenceScope
* Same standard, same population, same time window

**EvidenceScope**:
* Source A: ##{ methodology: "Standard X", boundaries: "Full system", temporal: "2020-2025" }##
* Source B: ##{ methodology: "Standard X", boundaries: "Full system", temporal: "2020-2025" }##

**Result**: 1 AnalysisContext
* CTX_GENERAL: "Claim Q support (Standard X, Full system, 2020-2025)"

**Rationale**: Strong overlap -> merge into a single AnalysisContext

----

== 10. Validation Examples (Generic-by-Design) ==

All examples use placeholders (no domain-specific entities).

=== 10.1 Legal Proceeding Fairness ===

**Input**: "Was Proceeding X conducted fairly?"

**Detected Contexts**:
* CTX_PROC_X: "Proceeding X fairness (Court A)"

**Evidence**:
* Fact 1: "Court A ruled in favor of Party A" (contextId: CTX_PROC_X)
* Fact 2: "Party B appealed to Court B" (creates new context)

**Refined Contexts**:
* CTX_PROC_X_COURT_A: "Proceeding X in Court A"
* CTX_PROC_X_COURT_B: "Proceeding X appeal in Court B"

**Verdict**: 2 separate verdicts (one per court)

----

=== 10.2 Efficiency Comparison with Methodology Mismatch ===

**Input**: "Is Technology X more efficient than Technology Y?"

**Detected Contexts**:
* CTX_EFFICIENCY: "Technology X vs Y efficiency"

**Evidence**:
* Fact 1: "Study A uses Framework A, full system boundary" (evidenceScope: Framework A)
* Fact 2: "Study B uses Framework B, subsystem only" (evidenceScope: Framework B)

**Refined Contexts**:
* CTX_FRAMEWORK_A: "Efficiency (Framework A, Full system)"
* CTX_FRAMEWORK_B: "Efficiency (Framework B, Subsystem)"

**Verdict**: 2 separate verdicts (one per methodology), with warning about methodology mismatch

----

=== 10.3 Historical Event with Temporal Continuity ===

**Input**: "Did Policy W achieve its goals?"

**Detected Contexts**:
* CTX_POLICY_W: "Policy W effectiveness"

**Evidence**:
* Fact 1: "Policy W Phase 1 (2020): Implemented action A" (temporal: 2020)
* Fact 2: "Policy W Phase 2 (2021): Expanded to action B" (temporal: 2021)

**Refined Contexts**:
* CTX_POLICY_W: "Policy W effectiveness (2020-2021)" (merged - continuous event)

**Verdict**: 1 verdict covering both phases (temporal continuity, not distinct events)

----

== 11. Related Documentation ==

* Calculations.md (see Calculations.md in local docs) - Verdict calculation methodology, AnalysisContext aggregation
* [[Quality Gates Reference>>FactHarbor.Specification.Implementation.Pipeline Architecture.Quality Gates Reference.WebHome]] - Quality gates reference (Gate 1, Gate 4)
* [[TriplePath Architecture>>FactHarbor.Specification.Implementation.Pipeline Architecture.TriplePath Architecture.WebHome]] - Pipeline architecture and context routing
* [[Terminology>>FactHarbor.Specification.Reference.Terminology.WebHome]] - AnalysisContext vs EvidenceScope definitions
* Evidence_Quality_Filtering.md (see Evidence_Quality_Filtering.md in local docs) - Evidence filtering and classification
* Changelog_v2.6.38_to_v2.6.40.md (archived) - Recent terminology fixes

----

== 12. Conclusion ==

FactHarbor's context detection system balances:
* **Precision**: Detecting genuinely distinct analytical frames
* **Generality**: No domain-specific hardcoding
* **Transparency**: Clear reasoning for splits and merges
* **User Control**: Guidance for refining inputs when needed

**Key Takeaways**:
1. Use **principle-based detection** (incompatibility test), not keyword matching
1. **Selective extraction** (0-3 EvidenceScope patterns max)
1. **Merge by default** (split only when necessary)
1. **Trust individual context verdicts** when overall average reliability is low

----

**Last Updated**: February 3, 2026
**Document Status**: Consolidated reference - combines 4 source documents