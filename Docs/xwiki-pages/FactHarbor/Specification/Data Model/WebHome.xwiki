= Data Model =

{{warning}}
**Implementation Status (v2.10.2)**

This specification describes the **target** normalized data model. Current implementation differs:

* **Storage**: JSON blobs in SQLite (not normalized PostgreSQL tables)
* **AnalysisContexts + KeyFactors**: Bounded analytical frames and decomposition questions embedded in result JSON (not separate stored entities)
* **Caching**: Redis cache not implemented; no claim caching yet
* **Source Scoring**: LLM + Cache architecture (batch prefetch → in-memory map → sync lookup). See `Docs/ARCHITECTURE/Source_Reliability.md`.
* **User System**: Not implemented (no authentication)
* **Evidence**: Uses `EvidenceItem` with `statement` field. Evidence IDs use E-prefix (E1, E2, E3...).

This specification remains valuable as the target architecture for future versions.
{{/warning}}

FactHarbor's data model is **simple, focused, designed for automated processing**.
== 1. Core Entities ==
=== 1.1 Claim ===
Fields: id, assertion, domain, **status** (Published/Hidden only), **confidence_score**, **risk_score**, completeness_score, version, views
==== Performance Optimization: Denormalized Fields ====
**Rationale**: Claims system is 95% reads, 5% writes. Denormalizing common data reduces joins and improves query performance by 70%.
**Additional cached fields in claims table**:
* **evidence_summary** (JSONB): Top 5 most relevant evidence snippets with scores
 * Avoids joining evidence table for listing/preview
 * Updated when evidence is added/removed
 * Format: `[{"text": "...", "source": "...", "relevance": 0.95}, ...]`
* **source_names** (TEXT[]): Array of source names for quick display
 * Avoids joining through evidence to sources
 * Updated when sources change
 * Format: `["New York Times", "Nature Journal", ...]`
* **scenario_count** (INTEGER): Number of scenarios for this claim
 * Quick metric without counting rows
 * Updated when scenarios added/removed
* **cache_updated_at** (TIMESTAMP): When denormalized data was last refreshed
 * Helps invalidate stale caches
 * Triggers background refresh if too old
**Update Strategy**:
* **Immediate**: Update on re-analysis (system-triggered)
* **Deferred**: Update via background job (non-critical)
* **Invalidation**: Clear cache when source data or UCM config changes significantly
**Trade-offs**:
* ✅ 70% fewer joins on common queries
* ✅ Much faster claim list/search pages
* ✅ Better user experience
* ⚠️ Small storage increase (~10%)
* ⚠️ Need to keep caches in sync
=== 1.2 Evidence ===
Fields: claim_id, source_id, excerpt, url, relevance_score, supports
=== 1.3 Source ===
**Purpose**: Track reliability of information sources over time
**Fields**:
* **id** (UUID): Unique identifier
* **name** (text): Source name (e.g., "New York Times", "Nature Journal")
* **domain** (text): Website domain (e.g., "nytimes.com")
* **type** (enum): NewsOutlet, AcademicJournal, GovernmentAgency, etc.
* **track_record_score** (0-100): Overall reliability score
* **accuracy_history** (JSON): Historical accuracy data
* **correction_frequency** (float): How often source publishes corrections
* **last_updated** (timestamp): When track record last recalculated
**How It Works**:
* Initial score based on source type (70 for academic journals, 30 for unknown)
* Updated by background scheduler
* Formula: accuracy_rate (50%) + correction_policy (20%) + editorial_standards (15%) + bias_transparency (10%) + longevity (5%)
* Track Record Check in AKEL pipeline: Adjusts evidence confidence based on source quality
* Quality thresholds: 90+=Exceptional, 70-89=Reliable, 50-69=Acceptable, 30-49=Questionable, <30=Unreliable

{{info}}
**Current Implementation (v2.6.40+):** Source reliability uses a **Pure LLM + Cache architecture** (v2.2). Sources are batch-evaluated via LLM calls, cached in-memory, and used synchronously during analysis. Multi-model consensus (Claude + GPT) is supported. There is no weekly background job — scores are computed on-demand and cached with configurable TTL. See `Docs/ARCHITECTURE/Source_Reliability.md`.
{{/info}}

**See**: SOURCE Track Record System documentation for complete details on calculation, updates, and usage
Fields: id, name, domain, **track_record_score**, **accuracy_history**, **correction_frequency**
**Key**: Automated source reliability tracking
==== Source Scoring Process (Separation of Concerns) ====
**Critical design principle**: Prevent circular dependencies between source scoring and claim analysis.
**The Problem**: 
* Source scores should influence claim verdicts
* Claim verdicts should update source scores
* But: Direct feedback creates circular dependency and potential feedback loops
**The Solution**: Temporal separation
==== Background Job (Source Scoring) ====
Runs independently of claim analysis:
{{code language="python"}}
def update_source_scores_batch():
 """
 Background job: Calculate source reliability
 Never triggered by individual claim analysis
 """
 # Analyze all claims from recent period
 claims = get_recent_claims()
 for source in get_all_sources():
 # Calculate accuracy metrics
 correct_verdicts = count_correct_verdicts_citing(source, claims)
 total_citations = count_total_citations(source, claims)
 accuracy = correct_verdicts / total_citations if total_citations > 0 else 0.5
 # Weight by claim importance
 weighted_score = calculate_weighted_score(source, claims)
 # Update source record
 source.track_record_score = weighted_score
 source.total_citations = total_citations
 source.last_updated = now()
 source.save()
 # Runs as scheduled batch job
 # Never during claim processing
{{/code}}
==== Real-Time Claim Analysis (AKEL) ====
Uses source scores but never updates them:
{{code language="python"}}
def analyze_claim(claim_text):
 """
 Real-time: Analyze claim using current source scores
 READ source scores, never UPDATE them
 """
 # Gather evidence
 evidence_list = gather_evidence(claim_text)
 for evidence in evidence_list:
 # READ source score (snapshot from last batch update)
 source = get_source(evidence.source_id)
 source_score = source.track_record_score
 # Use score to weight evidence
 evidence.weighted_relevance = evidence.relevance * source_score
 # Generate verdict using weighted evidence
 verdict = synthesize_verdict(evidence_list)
 # NEVER update source scores here
 # That happens in background batch job
 return verdict
{{/code}}
==== Periodic Audit (Quality Assurance) ====
Moderator review of flagged source scores:
* Verify scores make sense
* Detect gaming attempts
* Identify systematic biases
* Manual adjustments if needed
**Key Principles**:
✅ **Scoring and analysis are temporally separated**
* Source scoring: Scheduled batch job
* Claim analysis: Real-time processing
* Never update scores during analysis
✅ **One-way data flow during processing**
* Claims READ source scores
* Claims NEVER WRITE source scores
* Updates happen in background only
✅ **Predictable update cycle**
* Sources update every Sunday 2 AM
* Claims always use last week's scores
* No mid-week score changes
✅ **Audit trail**
* Log all score changes
* Track score history
* Explainable calculations
**Benefits**:
* No circular dependencies
* Predictable behavior
* Easier to reason about
* Simpler testing
* Clear audit trail
**Example Timeline**:
{{{
Sunday 2 AM: Calculate source scores for past week
 → NYT score: 0.87 (up from 0.85)
 → Blog X score: 0.52 (down from 0.61)
Monday-Saturday: Claims processed using these scores
 → All claims this week use NYT=0.87
 → All claims this week use Blog X=0.52
Next Sunday 2 AM: Recalculate scores including this week's claims
 → NYT score: 0.89 (trending up)
 → Blog X score: 0.48 (trending down)
}}}
=== 1.4 Scenario ===

**Purpose**: Different interpretations or contexts for evaluating claims
**Key Concept**: Scenarios are extracted from evidence, not generated arbitrarily. Each scenario represents a specific context, assumption set, or condition under which a claim should be evaluated.
**Relationship**: One-to-many with Claims (**simplified for V1.0**: scenario belongs to single claim)
**Fields**:
* **id** (UUID): Unique identifier
* **claim_id** (UUID): Foreign key to claim (one-to-many)
* **description** (text): Human-readable description of the scenario
* **assumptions** (JSONB): Key assumptions that define this scenario context
* **extracted_from** (UUID): Reference to evidence that this scenario was extracted from
* **created_at** (timestamp): When scenario was created
* **updated_at** (timestamp): Last modification
**How Found**: Evidence search → Extract context → Create scenario → Link to claim
**Example**: 
For claim "Vaccines reduce hospitalization":
* Scenario 1: "Clinical trials (healthy adults 18-65, original strain)" from trial paper
* Scenario 2: "Real-world data (diverse population, Omicron variant)" from hospital data
* Scenario 3: "Immunocompromised patients" from specialist study
**V2.0 Evolution**: Many-to-many relationship can be added if users request cross-claim scenario sharing. For V1.0, keeping scenarios tied to single claims simplifies queries and reduces complexity without limiting functionality.

=== 1.5 Verdict ===

**Purpose**: Assessment of a claim within a specific scenario context. Each verdict provides a conclusion about whether the claim is supported, refuted, or uncertain given the scenario's assumptions and available evidence.

**Core Fields**:
* **id** (UUID): Primary key
* **scenario_id** (UUID FK): The scenario being assessed
* **likelihood_range** (text): Probabilistic assessment (e.g., "0.40-0.65 (uncertain)", "0.75-0.85 (likely true)")
* **confidence** (decimal 0-1): How confident we are in this assessment
* **explanation_summary** (text): Human-readable reasoning explaining the verdict
* **uncertainty_factors** (text array): Specific factors limiting confidence (e.g., "Small sample sizes", "Lifestyle confounds", "Long-term effects unknown")
* **created_at** (timestamp): When verdict was created
* **updated_at** (timestamp): Last modification

**Immutability**: Analysis outputs (including verdicts) are immutable. To improve verdict quality, update UCM configuration and trigger re-analysis. Each analysis job records the UCM config snapshot used.

**Relationship**: Each Scenario has one Verdict. When a new analysis is triggered with updated UCM config, a new job produces new results.

**Example**:
For claim "Exercise improves mental health" in scenario "Clinical trials (healthy adults, structured programs)":
* Initial analysis: likelihood_range="0.40-0.65 (uncertain)", uncertainty_factors=["Small sample sizes", "Short-term studies only"]
* Re-analysis with improved UCM config: likelihood_range="0.70-0.85 (likely true)", uncertainty_factors=["Lifestyle confounds remain"]
* New analysis job references the updated UCM config snapshot

**Key Design**: Analysis outputs are immutable. Quality improvements flow through UCM configuration changes, not data edits.

=== 1.6 User ===
Fields: username, email, **role** (Reader/User/UCM Administrator/Moderator)

=== Roles ===
**reader** (guest, anonymous):
* Browse and search published analyses
* View analysis results and evidence

**user** (registered, logged in):
* Everything Reader can do
* Submit URLs/text for analysis (rate-limited)
* Flag quality issues
* View own submission history

**ucm_administrator** (appointed by Governing Team):
* Manage UCM configuration (prompt templates, quality thresholds, model selection)
* View config change history and audit trail
* Activate/deactivate config versions
* Trigger re-analysis with updated config
* View system metrics

**moderator** (appointed):
* Handle abuse reports
* Hide harmful content
* Ban users for policy violations
* Does NOT manage content quality (that is automated)
=== 1.7 UCM Config ===

**Tables**:
* **config_blobs**: Immutable, content-addressed config versions (hash-based deduplication)
* **config_active**: Points to currently active config blob per config type
* **config_usage**: Links each analysis job to the exact config snapshot used

**Config Types Managed**:
* Prompt templates (claim extraction, evidence gathering, verdict generation)
* Quality gate thresholds (confidence minimums, source count requirements)
* Model selection (which LLM for which pipeline phase)
* Source reliability rules (domain mappings, scoring parameters)
* Evidence quality filtering parameters

**Audit Trail**:
* Every config change creates a new immutable blob
* Config blobs are never deleted — complete history preserved
* Each analysis job records which config snapshot was active at execution time
* Reports can be reproduced by re-running with the same config snapshot
=== 1.8 Flag ===
Fields: entity_id, reported_by, issue_type, status, resolution_note
=== 1.9 QualityMetric ===
**Fields**: metric_type, category, value, target, timestamp
**Purpose**: Time-series quality tracking
**Usage**:
* **Continuous monitoring**: Hourly calculation of error rates, confidence scores, processing times
* **Quality dashboard**: Real-time display with trend charts
* **Alerting**: Automatic alerts when metrics exceed thresholds
* **A/B testing**: Compare control vs treatment metrics
* **Improvement validation**: Measure before/after changes
**Example**: `{type: "ErrorRate", category: "Politics", value: 0.12, target: 0.10, timestamp: "2025-12-17"}`
=== 1.10 ErrorPattern ===
**Fields**: error_category, claim_id, description, root_cause, frequency, status
**Purpose**: Capture errors to trigger system improvements
**Usage**:
* **Error capture**: When users flag issues or system detects problems
* **Pattern analysis**: Periodic grouping by category and frequency
* **Improvement workflow**: Analyze → Fix → Test → Deploy → Re-process → Monitor
* **Metrics**: Track error rate reduction over time
**Example**: `{category: "WrongSource", description: "Unreliable tabloid cited", root_cause: "No quality check", frequency: 23, status: "Fixed"}`

== 1.4 Core Data Model ERD ==

{{include reference="FactHarbor.Specification.Diagrams.Core Data Model ERD.WebHome"/}}

== 1.5 User Class Diagram ==
{{include reference="FactHarbor.Specification.Diagrams.User Class Diagram.WebHome"/}}
== 2. Versioning Strategy ==

**Analysis Data Is Immutable**:
* **Claims, Evidence, Verdicts**: Created once per analysis job, never modified
* **Sources**: Track record continuously updated (scoring data, not editorial content)
* **QualityMetric**: Time-series data (each record is a point in time)
* **ErrorPattern**: System improvement queue (status tracked)

**UCM Configuration Is Versioned**:
* Every config change creates a new immutable blob in ##config_blobs##
* ##config_active## tracks which config version is currently in use
* ##config_usage## links each analysis job to the config snapshot used
* Complete config history preserved — blobs never deleted

**Example**:
{{{
UCM Config V1: prompt_template = "Analyze the following claim..."
 → UCM Administrator updates config →
UCM Config V2: prompt_template = "Analyze the following claim with focus on source quality..."
 → config_blobs stores both versions (content-addressed by hash)
 → Reports reference which config version was used for their analysis
}}}
== 2.5. Storage vs Computation Strategy ==
**Critical architectural decision**: What to persist in databases vs compute dynamically?
**Trade-off**:
* **Store more**: Better reproducibility, faster, lower LLM costs | Higher storage/maintenance costs
* **Compute more**: Lower storage/maintenance costs | Slower, higher LLM costs, less reproducible
=== Recommendation: Hybrid Approach ===
**STORE (in PostgreSQL):**
==== Claims (Current State + History) ====
* **What**: assertion, domain, status, created_at, updated_at, version
* **Why**: Core entity, must be persistent
* **Also store**: confidence_score (computed once, then cached)
* **Size**: ~1 KB per claim
* **Growth**: Linear with claims
* **Decision**: ✅ STORE - Essential
==== Evidence (All Records) ====
* **What**: claim_id, source_id, excerpt, url, relevance_score, supports, extracted_at
* **Why**: Hard to re-gather, reproducibility
* **Size**: ~2 KB per evidence (with excerpt)
* **Growth**: 3-10 evidence per claim
* **Decision**: ✅ STORE - Essential for reproducibility
==== Sources (Track Records) ====
* **What**: name, domain, track_record_score, accuracy_history, correction_frequency
* **Why**: Continuously updated, expensive to recompute
* **Size**: ~500 bytes per source
* **Growth**: Slow (limited number of sources)
* **Decision**: ✅ STORE - Essential for quality
==== UCM Config History ====
* **What**: config_blobs (immutable), config_active (activation pointers), config_usage (per-job snapshots)
* **Why**: Audit trail, reproducibility, config rollback
* **Size**: ~5 KB per config version
* **Growth**: Linear with config changes (infrequent — admin-driven)
* **Retention**: Config blobs never deleted (complete history)
* **Decision**: ✅ STORE - Essential for reproducibility and audit
==== Flags (User Reports) ====
* **What**: entity_id, reported_by, issue_type, description, status
* **Why**: Error detection, system improvement triggers
* **Size**: ~500 bytes per flag
* **Growth**: 5-high percentage of claims get flagged
* **Decision**: ✅ STORE - Essential for improvement
==== ErrorPatterns (System Improvement) ====
* **What**: error_category, claim_id, description, root_cause, frequency, status
* **Why**: Learning loop, prevent recurring errors
* **Size**: ~1 KB per pattern
* **Growth**: Slow (limited patterns, many fixed)
* **Decision**: ✅ STORE - Essential for learning
==== QualityMetrics (Time Series) ====
* **What**: metric_type, category, value, target, timestamp
* **Why**: Trend analysis, cannot recreate historical metrics
* **Size**: ~200 bytes per metric
* **Growth**: Growth depends on collection frequency and metric type count
* **Retention**: 2 years hot, then aggregate and archive
* **Decision**: ✅ STORE - Essential for monitoring
**STORE (Computed Once, Then Cached):**
==== Analysis Summary ====
* **What**: Neutral text summary of claim analysis (200-500 words)
* **Computed**: Once by AKEL when claim first analyzed
* **Stored in**: Claim table (text field)
* **Recomputed**: Only when system significantly improves OR claim edited
* **Why store**: Expensive to regenerate ($0.01-0.05 per analysis), doesn't change often
* **Size**: ~2 KB per claim
* **Decision**: ✅ STORE (cached) - Cost-effective
==== Confidence Score ====
* **What**: 0-100 score of analysis confidence
* **Computed**: Once by AKEL
* **Stored in**: Claim table (integer field)
* **Recomputed**: When evidence added, source track record changes significantly, or system improves
* **Why store**: Cheap to store, expensive to compute, users need it fast
* **Size**: 4 bytes per claim
* **Decision**: ✅ STORE (cached) - Performance critical
==== Risk Score ====
* **What**: 0-100 score of claim risk level
* **Computed**: Once by AKEL
* **Stored in**: Claim table (integer field)
* **Recomputed**: When domain changes, evidence changes, or controversy detected
* **Why store**: Same as confidence score
* **Size**: 4 bytes per claim
* **Decision**: ✅ STORE (cached) - Performance critical
**COMPUTE DYNAMICALLY (Never Store):**
==== Scenarios ==== ⚠️ CRITICAL DECISION
* **What**: 2-5 possible interpretations of claim with assumptions
* **Current design**: Stored in Scenario table
* **Alternative**: Compute on-demand when user views claim details
* **Storage cost**: ~1 KB per scenario × 3 scenarios average = ~3 KB per claim
* **Compute cost**: $0.005-0.01 per request (LLM API call)
* **Frequency**: Viewed in detail by ~20% of users
* **Trade-off analysis**:
 - IF STORED: 1M claims × 3 KB = 3 GB storage, $0.05/month, fast access
 - IF COMPUTED: 1M claims × 20% views × $0.01 = $2,000/month in LLM costs
* **Reproducibility**: Scenarios may improve as AI improves (good to recompute)
* **Speed**: Computed = 5-8 seconds delay, Stored = instant
* **Decision**: ✅ STORE (hybrid approach below)
**Scenario Strategy** (APPROVED):
1. **Store scenarios** initially when claim analyzed
2. **Mark as stale** when system improves significantly
3. **Recompute on next view** if marked stale
4. **Cache for 30 days** if frequently accessed
5. **Result**: Best of both worlds - speed + freshness
==== Verdict Synthesis ==== 
* **What**: Final conclusion text synthesizing all scenarios
* **Compute cost**: $0.002-0.005 per request
* **Frequency**: Every time claim viewed
* **Why not store**: Changes as evidence/scenarios change, users want fresh analysis
* **Speed**: 2-3 seconds (acceptable)
**Alternative**: Store "last verdict" as cached field, recompute only if marked stale by UCM config change
* **Recommendation**: ✅ STORE cached version, mark stale when changes occur
==== Search Results ====
* **What**: Lists of claims matching search query
* **Compute from**: Elasticsearch index
* **Cache**: 15 minutes in Redis for popular queries
* **Why not store permanently**: Constantly changing, infinite possible queries
==== Aggregated Statistics ====
* **What**: "Total claims: 1,234,567", "Average confidence: 78%", etc.
* **Compute from**: Database queries
* **Cache**: 1 hour in Redis
* **Why not store**: Can be derived, relatively cheap to compute
==== User Role ====
* **What**: Role assignment (Reader / UCM Administrator / Moderator)
* **Current design**: Stored in User table
* **Notes**: Manual assignment by Governing Team. No reputation system — roles are appointed, not earned.
* **Decision**: ✅ STORE - Simple role field
=== Summary Table ===
| Data Type | Storage | Compute | Size per Claim | Decision | Rationale |
|-----------|---------|---------|----------------|----------|-----------|
| Claim core | ✅ | - | 1 KB | STORE | Essential |
| Evidence | ✅ | - | 2 KB × 5 = 10 KB | STORE | Reproducibility |
| Sources | ✅ | - | 500 B (shared) | STORE | Track record |
| UCM config history | ✅ | - | 5 KB per version (infrequent) | STORE | Reproducibility |
| Analysis summary | ✅ | Once | 2 KB | STORE (cached) | Cost-effective |
| Confidence score | ✅ | Once | 4 B | STORE (cached) | Fast access |
| Risk score | ✅ | Once | 4 B | STORE (cached) | Fast access |
| Scenarios | ✅ | When stale | 3 KB | STORE (hybrid) | Balance cost/speed |
| Verdict | ✅ | When stale | 1 KB | STORE (cached) | Fast access |
| Flags | ✅ | - | 500 B × 10% = 50 B avg | STORE | Improvement |
| ErrorPatterns | ✅ | - | 1 KB (global) | STORE | Learning |
| QualityMetrics | ✅ | - | 200 B (time series) | STORE | Trending |
| Search results | - | ✅ | - | COMPUTE + 15min cache | Dynamic |
| Aggregations | - | ✅ | - | COMPUTE + 1hr cache | Derivable |
**Total storage per claim**: ~18 KB (without edits and flags)
**For 1 million claims**:
* **Storage**: ~18 GB (manageable)
* **PostgreSQL**: ~$50/month (standard instance)
* **Redis cache**: ~$20/month (1 GB instance)
* **S3 archives**: ~$5/month (old edits)
* **Total**: ~$75/month infrastructure
**LLM cost savings by caching**:
* Analysis summary stored: Save $0.03 per claim = $30K per 1M claims
* Scenarios stored: Save $0.01 per claim × 20% views = $2K per 1M claims 
* Verdict stored: Save $0.003 per claim = $3K per 1M claims
* **Total savings**: ~$35K per 1M claims vs recomputing every time
=== Recomputation Triggers ===
**When to mark cached data as stale and recompute:**
1. **UCM config updated** → Mark affected analyses stale, trigger re-analysis
2. **Source track record changes >10 points** → Recompute: confidence score, verdict
3. **New evidence sources available** → Trigger re-analysis for affected claims
4. **Quality gate thresholds changed** → Re-evaluate existing verdicts against new thresholds
**Recomputation strategy**:
* **Eager**: Immediately re-analyse (for targeted UCM config changes)
* **Lazy**: Re-analyse on next view (for broad system improvements)
* **Batch**: Periodic re-evaluation of stale analyses
=== Database Size Projection ===
**Year 1**: 10K claims
* Storage: 180 MB
* Cost: $10/month
**Year 3**: 100K claims 
* Storage: 1.8 GB
* Cost: $30/month
**Year 5**: 1M claims
* Storage: 18 GB 
* Cost: $75/month
**Year 10**: 10M claims
* Storage: 180 GB
* Cost: $300/month
* Optimization: Archive old claims to S3 ($5/TB/month)
**Conclusion**: Storage costs are manageable, LLM cost savings are substantial.
== 3. Key Simplifications ==
* **Two content states only**: Published, Hidden
* **Three user roles only**: Reader, UCM Administrator, Moderator
* **Immutable analysis data**: No data editing or versioning — improve via UCM config
* **UCM config versioning**: Content-addressed immutable blobs with per-job snapshots
* **Source track records**: Continuous evaluation
== 3. What Gets Stored in the Database ==
=== 3.1 Primary Storage (PostgreSQL) ===
**Claims Table**:
* Current state only (latest version)
* Fields: id, assertion, domain, status, confidence_score, risk_score, completeness_score, version, created_at, updated_at
**Evidence Table**:
* All evidence records
* Fields: id, claim_id, source_id, excerpt, url, relevance_score, supports, extracted_at, archived
**Scenario Table**:
* All scenarios for each claim
* Fields: id, claim_id, description, assumptions (text array), confidence, created_by, created_at
**Source Table**:
* Track record database (continuously updated)
* Fields: id, name, domain, type, track_record_score, accuracy_history (JSON), correction_frequency, last_updated, claim_count, corrections_count
**User Table**:
* All user accounts
* Fields: id, username, email, role (Reader/UCM Administrator/Moderator), created_at, last_active
**UCM Config Tables**:
* config_blobs: Immutable config versions (hash, config_type, content, changed_by, change_reason, created_at)
* config_active: Currently active config per type (config_type, blob_hash, activated_at, activated_by)
* config_usage: Per-job config snapshots (job_id, config_type, blob_hash, snapshot_at)
**Flag Table**:
* User-reported issues
* Fields: id, entity_type, entity_id, reported_by, issue_type, description, status, resolved_by, resolution_note, created_at, resolved_at
**ErrorPattern Table**:
* System improvement queue
* Fields: id, error_category, claim_id, description, root_cause, frequency, status, created_at, fixed_at
**QualityMetric Table**:
* Time-series quality data
* Fields: id, metric_type, metric_category, value, target, timestamp
=== 3.2 What's NOT Stored (Computed on-the-fly) ===
* **Verdicts**: Synthesized from evidence + scenarios when requested
* **Risk scores**: Recalculated based on current factors
* **Aggregated statistics**: Computed from base data
* **Search results**: Generated from Elasticsearch index
=== 3.3 Cache Layer (Redis) ===

{{warning}}
**Implementation Status:** Redis caching is **NOT YET IMPLEMENTED**. Current implementation has no caching layer.
{{/warning}}

**Cached for performance (Planned)**:
* Frequently accessed claims (TTL: 1 hour)
* Search results (TTL: 15 minutes)
* User sessions (TTL: 24 hours)
* Source track records (TTL: 1 hour)
=== 3.4 File Storage (S3) ===
**Archived content**:
* Old processing logs (>3 months)
* Evidence documents (archived copies)
* Database backups
* Export files
=== 3.5 Search Index (Elasticsearch) ===
**Indexed for search**:
* Claim assertions (full-text)
* Evidence excerpts (full-text)
* Scenario descriptions (full-text)
* Source names (autocomplete)
Synchronized from PostgreSQL via change data capture or periodic sync.
== 4. Related Pages ==
* [[Architecture>>FactHarbor.Specification.Architecture.WebHome]]
* [[Requirements>>FactHarbor.Specification.Requirements.WebHome]]
* [[Workflows>>FactHarbor.Specification.Workflows.WebHome]]