= Scope Definition Guidelines: EvidenceScope vs AnalysisContext =

**Version**: 2.6.41
**Date**: 2026-01-29
**Audience**: Developers, Prompt Engineers
**Related**: [[Terminology>>FactHarbor.Specification.Reference.Terminology.WebHome]], AGENTS.md

----

== Table of Contents ==

1. [[Quick Reference>>||anchor="H1.QuickReference"]]
1. [[EvidenceScope>>||anchor="H2.EvidenceScope"]]
1. [[AnalysisContext>>||anchor="H3.AnalysisContext"]]
1. [[Decision Tree>>||anchor="H4.DecisionTree"]]
1. [[Common Mistakes>>||anchor="H5.CommonMistakes"]]
1. [[Code Examples>>||anchor="H6.CodeExamples"]]

----

== 1. Quick Reference ==

=== TL;DR ===

|= Concept |= Purpose |= Scope |= Cardinality
| **EvidenceScope** | Source methodology metadata | Per-evidence-item | Optional (0-1 per item)
| **AnalysisContext** | Bounded analytical frame | Per-analysis | Required (1+ per analysis)

**Rule of Thumb**:
* **EvidenceScope** = "What methodology/boundaries does THIS source use?"
* **AnalysisContext** = "What DISTINCT FRAMES should we analyze SEPARATELY?"

=== When in Doubt ===

**Use AnalysisContext** if:
* Different analytical frames that need separate verdicts
* Comparing across institutional/regulatory/methodological boundaries
* Input explicitly requests comparison ("X vs Y")

**Use EvidenceScope** if:
* Metadata about a source's methodology
* Information to help calibrate source reliability
* Details that DON'T require splitting the analysis

----

== 2. EvidenceScope ==

=== 2.1 Definition ===

**EvidenceScope** = Per-evidence-item metadata describing the source's **methodology, boundaries, geography, or temporal frame**.

**Location**: Attached to individual ##EvidenceItem## objects

{{code language="typescript"}}
interface EvidenceItem {
  id: string;
  statement: string;  // The evidence statement
  category: string;
  sourceId: string;
  sourceUrl: string;
  sourceExcerpt: string;

  // Optional: Source methodology metadata
  evidenceScope?: {
    name: string;           // E.g., "Well-to-Wheel Analysis"
    boundaries?: string;    // E.g., "Cradle-to-grave"
    geography?: string;     // E.g., "EU regulations"
    temporal?: string;      // E.g., "2020-2023 data"
    sourceType?: SourceType; // E.g., "peer_reviewed_study"
  };
}
{{/code}}

=== 2.2 Purpose ===

**Primary Use**: Source reliability calibration

When a source uses a specific methodology or measurement boundary, EvidenceScope captures that metadata so the system can:
1. **Calibrate reliability** based on methodology rigor
1. **Explain verdicts** by showing which methodologies contributed
1. **Detect conflicting methodologies** (same claim, different measurement boundaries)

**Secondary Use**: Context for verdict explanation

EvidenceScope helps users understand WHY a particular piece of evidence was weighted a certain way.

=== 2.3 When to Use EvidenceScope ===

**DO use EvidenceScope when**:

|= Scenario |= Example
| **Source uses specific methodology** | "Well-to-Wheel analysis" (vs Tank-to-Wheel)
| **Source has geographic boundaries** | "EU regulations" (vs US standards)
| **Source has temporal boundaries** | "2020-2023 data" (vs historical averages)
| **Source type affects reliability** | Peer-reviewed study vs blog post
| **Methodology impacts interpretation** | Life-cycle assessment boundaries matter

**DON'T use EvidenceScope when**:

|= Scenario |= What to Use Instead
| **Different claims need separate verdicts** | Use AnalysisContext
| **Comparing institutional perspectives** | Use AnalysisContext
| **Multiple distinct questions in input** | Use AnalysisContext
| **Source is just general information** | No scope needed

=== 2.4 EvidenceScope Examples ===

==== Example 1: Energy Efficiency Methodologies ====

**Input**: "Hydrogen cars are more energy-efficient than electric cars"

**Evidence with EvidenceScope**:
{{code language="typescript"}}
{
  id: "S1-E1",
  statement: "Well-to-Wheel analysis shows hydrogen vehicles require 2.5x more energy than EVs",
  category: "statistic",
  sourceUrl: "https://nature.com/articles/12345",
  sourceExcerpt: "Using Well-to-Wheel methodology including production, transport, and usage...",
  evidenceScope: {
    name: "Well-to-Wheel Energy Analysis",
    boundaries: "Production + Transport + Usage phases",
    sourceType: "peer_reviewed_study"
  }
}
{{/code}}

**Why EvidenceScope?**:
* Methodology (Well-to-Wheel) is metadata about the source
* Doesn't require splitting into separate AnalysisContexts
* Helps calibrate reliability (peer-reviewed, comprehensive methodology)

==== Example 2: Geographic Regulatory Boundaries ====

**Input**: "Company X violated environmental regulations"

**Evidence with EvidenceScope**:
{{code language="typescript"}}
{
  id: "S2-E1",
  statement: "Company X's emissions exceeded the permitted limit by 15%",
  category: "statistic",
  sourceUrl: "https://epa.gov/reports/2023",
  sourceExcerpt: "EPA audit found Company X's facility emitted 15% above the US federal limit...",
  evidenceScope: {
    name: "US EPA Regulations",
    geography: "United States",
    temporal: "2023 audit"
  }
}
{{/code}}

**Why EvidenceScope?**:
* Geography (US EPA) is context for interpreting the violation
* Not comparing across jurisdictions (just stating a violation occurred)
* Helps explain which regulatory framework applies

==== Example 3: Temporal Data Boundaries ====

**Input**: "Climate change is accelerating"

**Evidence with EvidenceScope**:
{{code language="typescript"}}
{
  id: "S3-E1",
  statement: "Global temperature anomaly increased by 0.8°C from 2000 to 2020",
  category: "statistic",
  sourceUrl: "https://nasa.gov/giss/data",
  sourceExcerpt: "NASA GISS temperature records show a 0.8°C increase over the 2000-2020 period...",
  evidenceScope: {
    name: "NASA GISS Temperature Records",
    temporal: "2000-2020 timeframe",
    boundaries: "Global mean surface temperature",
    sourceType: "government_report"
  }
}
{{/code}}

**Why EvidenceScope?**:
* Temporal boundary (2000-2020) is metadata about the measurement
* Not comparing different time periods (just documenting one period)
* Helps calibrate reliability (government report, recent data)

----

== 3. AnalysisContext ==

=== 3.1 Definition ===

**AnalysisContext** = A **bounded analytical frame** that requires SEPARATE analysis and its own verdict.

**Location**: Top-level in analysis result, contains multiple evidence items

{{code language="typescript"}}
interface AnalysisContext {
  id: string;                    // E.g., "CTX_WTW", "CTX_TTW"
  name: string;                  // E.g., "Well-to-Wheel Analysis"
  type: "methodological" | "geographical" | "temporal" | "institutional" | "outcome" | "legal";
  description: string;           // Why this is a distinct frame
  relevance: "high" | "medium" | "low";
  metadata?: {
    phase?: string;              // E.g., "production", "usage"
    boundaries?: string;         // E.g., "cradle-to-grave"
    geography?: string;          // E.g., "EU regulations"
  };
}
{{/code}}

=== 3.2 Purpose ===

**Primary Use**: Separate verdict generation

When an input involves MULTIPLE DISTINCT QUESTIONS that need separate answers, create AnalysisContexts for each question.

**Example**: "Hydrogen cars are more energy-efficient than electric cars"
* Context 1: Production phase efficiency (upstream)
* Context 2: Usage phase efficiency (downstream)
* → Need TWO verdicts (may differ: production=worse, usage=better)

=== 3.3 When to Use AnalysisContext ===

**DO use AnalysisContext when**:

|= Scenario |= Example
| **Explicit comparison** | "X vs Y" → Context for X, Context for Y
| **Multiple distinct questions** | "Is X safe AND effective?" → Safety context, Efficacy context
| **Institutional perspective matters** | "EU says X, US says Y" → EU context, US context
| **Methodology changes answer** | "WTW shows X, TTW shows Y" → WTW context, TTW context
| **Different legal jurisdictions** | "Legal in CA, illegal in TX" → CA context, TX context
| **Temporal comparison** | "Was true in 2020, false in 2023" → 2020 context, 2023 context

**DON'T use AnalysisContext when**:

|= Scenario |= What to Use Instead
| **Source just has methodology metadata** | Use EvidenceScope
| **Single question, multiple sources** | Single AnalysisContext + multiple EvidenceScope
| **Perspectives on SAME claim** | Single AnalysisContext (perspectives = evidence)
| **General research (no distinct frames)** | Single default AnalysisContext

=== 3.4 AnalysisContext Examples ===

==== Example 1: Methodological Comparison ====

**Input**: "Hydrogen cars are more energy-efficient than electric cars"

**AnalysisContexts** (2 required):
{{code language="typescript"}}
[
  {
    id: "CTX_PRODUCTION",
    name: "Production Phase Efficiency",
    type: "methodological",
    description: "Energy efficiency during vehicle and fuel production (upstream)",
    metadata: {
      phase: "production",
      boundaries: "upstream (cradle-to-gate)"
    }
  },
  {
    id: "CTX_USAGE",
    name: "Usage Phase Efficiency",
    type: "methodological",
    description: "Energy efficiency during actual vehicle operation (downstream)",
    metadata: {
      phase: "usage",
      boundaries: "downstream (tank-to-wheel)"
    }
  }
]
{{/code}}

**Why AnalysisContext?**:
* **Different questions**: "Is production efficient?" vs "Is usage efficient?"
* **May have different answers**: Production (hydrogen worse), Usage (hydrogen better)
* **Need separate verdicts**: Can't combine into single verdict without losing information

==== Example 2: Institutional Perspectives (Safety Claim) ====

**Input**: "Drug X is safe for treating condition Y"

**AnalysisContexts** (2+ depending on evidence):
{{code language="typescript"}}
[
  {
    id: "CTX_FDA",
    name: "FDA Safety Assessment",
    type: "institutional",
    description: "US FDA evaluation and approval status",
    metadata: {
      institution: "US FDA",
      geography: "United States"
    }
  },
  {
    id: "CTX_EMA",
    name: "EMA Safety Assessment",
    type: "institutional",
    description: "European Medicines Agency evaluation",
    metadata: {
      institution: "EMA",
      geography: "European Union"
    }
  }
]
{{/code}}

**Why AnalysisContext?**:
* **Different regulatory bodies** may reach different conclusions
* **Need separate verdicts** for each jurisdiction
* **User cares about the split**: "Is it safe in the US?" vs "Is it safe in EU?"

==== Example 3: Temporal Comparison ====

**Input**: "Company X's stock is a good investment"

**AnalysisContexts** (2 if temporal split evident):
{{code language="typescript"}}
[
  {
    id: "CTX_HISTORICAL",
    name: "Historical Performance",
    type: "temporal",
    description: "Stock performance 2020-2023 (past data)",
    metadata: {
      temporal: "2020-2023",
      boundaries: "historical data"
    }
  },
  {
    id: "CTX_FUTURE",
    name: "Future Outlook",
    type: "temporal",
    description: "Projected performance 2024-2026 (forecasts)",
    metadata: {
      temporal: "2024-2026",
      boundaries: "analyst forecasts"
    }
  }
]
{{/code}}

**Why AnalysisContext?**:
* **Different timeframes** = different questions
* **Past vs Future** may have opposite verdicts (was good, won't be good)
* **Investment decisions require both perspectives**

----

== 4. Decision Tree ==

{{code}}
Input Claim
│
├─ Does the input involve MULTIPLE DISTINCT QUESTIONS?
│  ├─ YES → Use AnalysisContext (one per question)
│  │   Examples:
│  │   - "X vs Y" (comparison)
│  │   - "Is X safe AND effective?" (multiple questions)
│  │   - "EU says X, US says Y" (institutional perspectives)
│  │
│  └─ NO → Single AnalysisContext (default)
│      │
│      └─ Do sources use SPECIFIC METHODOLOGIES/BOUNDARIES?
│         ├─ YES → Use EvidenceScope (per-evidence-item)
│         │   Examples:
│         │   - Well-to-Wheel methodology (metadata)
│         │   - EU regulations (geographic context)
│         │   - 2020-2023 data (temporal boundary)
│         │
│         └─ NO → No scope needed (plain evidence)
{{/code}}

=== Decision Checklist ===

Before creating AnalysisContext:

* Does this represent a DISTINCT QUESTION that needs its own verdict?
* Would the answer be different across these frames?
* Would combining them into one verdict LOSE important information?
* Is the user asking for a comparison or multi-part question?

If all YES → Create AnalysisContext

If any NO → Consider EvidenceScope or no scope

----

== 5. Common Mistakes ==

=== Mistake 1: Using AnalysisContext for Source Metadata ===

**Wrong**:
{{code language="typescript"}}
// Input: "Hydrogen efficiency is better than electricity"
// Creating AnalysisContext for EVERY source's methodology
analysisContexts: [
  { name: "Nature Study (Well-to-Wheel)" },
  { name: "MIT Report (Tank-to-Wheel)" },
  { name: "EPA Analysis (Life-Cycle)" },
]
{{/code}}

**Problem**: These are NOT distinct questions, they're just different sources with different methodologies.

**Correct**:
{{code language="typescript"}}
// Single AnalysisContext for the question
analysisContexts: [
  { name: "Energy Efficiency Comparison" }
]

// Evidence items with EvidenceScope for methodology
evidence: [
  {
    statement: "Nature study found X using WTW methodology",
    evidenceScope: { name: "Well-to-Wheel Analysis", sourceType: "peer_reviewed_study" }
  },
  {
    statement: "MIT report found Y using TTW methodology",
    evidenceScope: { name: "Tank-to-Wheel Analysis", sourceType: "organization_report" }
  },
]
{{/code}}

=== Mistake 2: Creating AnalysisContext for Every Perspective ===

**Wrong**:
{{code language="typescript"}}
// Input: "The trial was fair"
// Creating AnalysisContext for every critic/supporter
analysisContexts: [
  { name: "Defense Attorney Perspective" },
  { name: "Prosecution Perspective" },
  { name: "Human Rights Groups Perspective" },
  { name: "Government Spokesperson Perspective" },
]
{{/code}}

**Problem**: These are perspectives on the SAME question ("Was the trial fair?"), not distinct questions.

**Correct**:
{{code language="typescript"}}
// Single AnalysisContext for the question
analysisContexts: [
  { name: "Trial Fairness Assessment" }
]

// Evidence from different perspectives (just evidence, not contexts)
evidence: [
  { statement: "Defense attorney claimed procedural violations", claimDirection: "contradicts" },
  { statement: "Court records show procedures followed Article 47", claimDirection: "supports" },
  { statement: "Human Rights Watch documented irregularities", claimDirection: "contradicts" },
]
{{/code}}

=== Mistake 3: Confusing EvidenceScope with Source Attribution ===

**Wrong**:
{{code language="typescript"}}
// Putting source name in EvidenceScope
evidenceScope: {
  name: "According to Nature Journal"  // This is just attribution, not scope
}
{{/code}}

**Correct**:
{{code language="typescript"}}
// EvidenceScope is for METHODOLOGY, not source name
evidenceScope: {
  name: "Peer-Reviewed Life-Cycle Assessment",  // The methodology used
  boundaries: "Cradle-to-grave",
  sourceType: "peer_reviewed_study"
}

// Source name goes in sourceTitle
sourceTitle: "Nature Journal Article"
{{/code}}

=== Mistake 4: Over-Splitting AnalysisContexts ===

**Wrong**:
{{code language="typescript"}}
// Input: "Climate change is accelerating"
// Creating context for every data source
analysisContexts: [
  { name: "NASA GISS Data" },
  { name: "NOAA Records" },
  { name: "IPCC Reports" },
]
{{/code}}

**Problem**: All sources answer the SAME question ("Is climate change accelerating?"), just from different data sources.

**Correct**:
{{code language="typescript"}}
// Single AnalysisContext for the question
analysisContexts: [
  { name: "Climate Change Acceleration Assessment" }
]

// Evidence from multiple sources (with EvidenceScope for data boundaries)
evidence: [
  {
    statement: "NASA data shows 0.8°C increase 2000-2020",
    evidenceScope: { name: "NASA GISS Temperature Records", temporal: "2000-2020" }
  },
  {
    statement: "NOAA records confirm accelerating trend",
    evidenceScope: { name: "NOAA Climate Data", temporal: "1980-2023" }
  },
]
{{/code}}

----

== 6. Code Examples ==

=== 6.1 Creating EvidenceScope ===

{{code language="typescript"}}
import type { EvidenceItem } from "@/lib/analyzer/types";

function extractEvidenceWithScope(source: Source): EvidenceItem[] {
  const items: EvidenceItem[] = [];

  // Extract evidence statement
  const statement = extractStatementFromSource(source);

  // Detect if source uses specific methodology
  const hasMethodology = detectMethodology(source);

  if (hasMethodology) {
    // Add EvidenceScope for methodology metadata
    items.push({
      id: generateId(source.id),
      statement: statement,
      category: categorizeEvidence(statement),
      sourceId: source.id,
      sourceUrl: source.url,
      sourceExcerpt: source.excerpt,
      evidenceScope: {
        name: hasMethodology.name,           // E.g., "Well-to-Wheel Analysis"
        boundaries: hasMethodology.boundaries, // E.g., "Cradle-to-grave"
        sourceType: classifySourceType(source), // E.g., "peer_reviewed_study"
      },
    });
  } else {
    // No scope needed (plain evidence)
    items.push({
      id: generateId(source.id),
      statement: statement,
      category: categorizeEvidence(statement),
      sourceId: source.id,
      sourceUrl: source.url,
      sourceExcerpt: source.excerpt,
      // evidenceScope: undefined (omit if not needed)
    });
  }

  return items;
}
{{/code}}

=== 6.2 Creating AnalysisContext ===

{{code language="typescript"}}
import type { AnalysisContext } from "@/lib/analyzer/types";

function detectAnalysisContexts(input: string): AnalysisContext[] {
  const contexts: AnalysisContext[] = [];

  // Check for explicit comparison ("X vs Y")
  if (isComparisonInput(input)) {
    // Extract terms being compared
    const { termA, termB } = extractComparisonTerms(input);

    contexts.push({
      id: "CTX_A",
      name: `${termA} Analysis`,
      type: "methodological",
      description: `Assessment of ${termA}`,
      relevance: "high",
    });

    contexts.push({
      id: "CTX_B",
      name: `${termB} Analysis`,
      type: "methodological",
      description: `Assessment of ${termB}`,
      relevance: "high",
    });

    return contexts;
  }

  // Check for multi-part question ("Is X safe AND effective?")
  if (isMultiPartQuestion(input)) {
    const parts = extractQuestionParts(input);

    for (const part of parts) {
      contexts.push({
        id: generateContextId(part),
        name: `${part.aspect} Assessment`,
        type: "outcome",
        description: `Evaluation of ${part.aspect}`,
        relevance: part.importance,
      });
    }

    return contexts;
  }

  // Default: Single context
  contexts.push({
    id: "CTX_DEFAULT",
    name: "Primary Analysis",
    type: "methodological",
    description: "Main analytical frame for this input",
    relevance: "high",
  });

  return contexts;
}
{{/code}}

=== 6.3 Deciding Between EvidenceScope and AnalysisContext ===

{{code language="typescript"}}
function shouldUseAnalysisContext(input: string, sources: Source[]): boolean {
  // Rule 1: Explicit comparison → AnalysisContext
  if (input.match(/\b(vs|versus|compared to|versus)\b/i)) {
    return true;
  }

  // Rule 2: Multiple distinct questions → AnalysisContext
  if (input.match(/\b(and|also|moreover)\b/i) && hasMultipleQuestions(input)) {
    return true;
  }

  // Rule 3: Institutional split evident → AnalysisContext
  const institutions = extractInstitutions(sources);
  if (institutions.length > 1 && institutionsDisagree(sources)) {
    return true;
  }

  // Rule 4: Temporal split evident → AnalysisContext
  const timeframes = extractTimeframes(sources);
  if (timeframes.length > 1 && timeframesRelevant(input)) {
    return true;
  }

  // Default: Single context (use EvidenceScope for methodology)
  return false;
}

function shouldUseEvidenceScope(source: Source): boolean {
  // Check for methodology indicators
  if (source.text.match(/\b(methodology|method|approach|framework)\b/i)) {
    return true;
  }

  // Check for boundary keywords
  if (source.text.match(/\b(well-to-wheel|tank-to-wheel|life-cycle|cradle-to-grave)\b/i)) {
    return true;
  }

  // Check for geographic boundaries
  if (source.text.match(/\b(EU regulations|US standards|California law)\b/i)) {
    return true;
  }

  // Check for temporal boundaries
  if (source.text.match(/\b(\d{4}-\d{4} data|historical|recent|current)\b/i)) {
    return true;
  }

  return false;
}
{{/code}}

----

== Appendix: Real-World Examples ==

=== Example A: Hydrogen vs Electric Cars (Correct Split) ===

**Input**: "Hydrogen cars are more energy-efficient than electric cars"

**Correct Approach**:
{{code language="typescript"}}
// TWO AnalysisContexts (different questions)
analysisContexts: [
  { id: "CTX_PRODUCTION", name: "Production Phase Efficiency" },
  { id: "CTX_USAGE", name: "Usage Phase Efficiency" },
]

// Evidence with EvidenceScope (methodology metadata)
evidence: [
  {
    id: "S1-E1",
    statement: "Hydrogen production requires 2.5x more energy than EV battery production",
    contextId: "CTX_PRODUCTION",
    evidenceScope: { name: "Well-to-Wheel Production Analysis", boundaries: "Upstream" }
  },
  {
    id: "S2-E1",
    statement: "Hydrogen fuel cells are 60% efficient vs 90% for EV motors",
    contextId: "CTX_USAGE",
    evidenceScope: { name: "Tank-to-Wheel Usage Analysis", boundaries: "Downstream" }
  },
]
{{/code}}

=== Example B: Trial Fairness (No Split Needed) ===

**Input**: "The Bolsonaro trial was fair"

**Correct Approach**:
{{code language="typescript"}}
// ONE AnalysisContext (single question)
analysisContexts: [
  { id: "CTX_FAIRNESS", name: "Trial Fairness Assessment" },
]

// Evidence from multiple perspectives (just evidence, NO separate contexts)
evidence: [
  {
    id: "S1-E1",
    statement: "Defense claimed procedural violations",
    claimDirection: "contradicts",
    // NO evidenceScope (just a perspective, not methodology)
  },
  {
    id: "S2-E1",
    statement: "Court records show Article 47 procedures followed",
    claimDirection: "supports",
    // NO evidenceScope (plain evidence)
  },
]
{{/code}}

----

**Document Version**: 1.0
**Last Updated**: 2026-01-29
**Next Review**: When scope detection logic changes
**Maintained by**: Plan Coordinator