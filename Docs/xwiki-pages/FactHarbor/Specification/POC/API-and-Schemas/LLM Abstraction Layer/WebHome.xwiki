= LLM Abstraction Layer =

== 6.1 Design Principle ==

**FactHarbor uses provider-agnostic LLM abstraction** to avoid vendor lock-in and enable:

* **Provider switching:** Change LLM providers without code changes
* **Cost optimization:** Use different providers for different stages
* **Resilience:** Automatic fallback if primary provider fails
* **Cross-checking:** Compare outputs from multiple providers
* **A/B testing:** Test new models without deployment changes

**Implementation:** All LLM calls go through an abstraction layer that routes to configured providers.

----

== 6.2 LLM Provider Interface ==

**Abstract Interface:**

{{{
interface LLMProvider {
  // Core methods
  complete(prompt: string, options: CompletionOptions): Promise<CompletionResponse>
  stream(prompt: string, options: CompletionOptions): AsyncIterator<StreamChunk>

  // Provider metadata
  getName(): string
  getMaxTokens(): number
  getCostPer1kTokens(): { input: number, output: number }

  // Health check
  isAvailable(): Promise<boolean>
}

interface CompletionOptions {
  model?: string
  maxTokens?: number
  temperature?: number
  stopSequences?: string[]
  systemPrompt?: string
}
}}}

----

== 6.3 Supported Providers (POC1) ==

**Primary Provider (Default):**

* **Anthropic Claude API**
  * Models (examples; not normative): Provider-default FAST model, Provider-default REASONING model, Provider-default HEAVY model (optional)
  * Used by default in POC1
  * Best quality for holistic analysis

**Secondary Providers (Future):**

* **OpenAI API**
  * Models: GPT-4o, GPT-4o-mini
  * For cost comparison

* **Google Vertex AI**
  * Models: Gemini 1.5 Pro, Gemini 1.5 Flash
  * For diversity in evidence gathering

* **Local Models** (Post-POC)
  * Models: Llama 3.1, Mistral
  * For privacy-sensitive deployments

----

== 6.4 Provider Configuration ==

**Environment Variables:**

{{{
# Primary provider
LLM_PRIMARY_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Fallback provider
LLM_FALLBACK_PROVIDER=openai
OPENAI_API_KEY=sk-...

# Provider selection per stage
LLM_STAGE1_PROVIDER=anthropic
LLM_STAGE1_MODEL=claude-haiku-4
LLM_STAGE2_PROVIDER=anthropic
LLM_STAGE2_MODEL=claude-sonnet-4-5-20250929
LLM_STAGE3_PROVIDER=anthropic
LLM_STAGE3_MODEL=claude-sonnet-4-5-20250929

# Cost limits
LLM_MAX_COST_PER_REQUEST=1.00
}}}

**Database Configuration (Alternative):**

{{{{
{
  "providers": [
    {
      "name": "anthropic",
      "api_key_ref": "vault://anthropic-api-key",
      "enabled": true,
      "priority": 1
    },
    {
      "name": "openai",
      "api_key_ref": "vault://openai-api-key",
      "enabled": true,
      "priority": 2
    }
  ],
  "stage_config": {
    "stage1": {
      "provider": "anthropic",
      "model": "claude-haiku-4-5-20251001",
      "max_tokens": 4096,
      "temperature": 0.0
    },
    "stage2": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929",
      "max_tokens": 16384,
      "temperature": 0.3
    },
    "stage3": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929",
      "max_tokens": 8192,
      "temperature": 0.2
    }
  }
}
}}}

----

== 6.5 Stage-Specific Models (POC1 Defaults) ==

**Stage 1: Claim Extraction**

* **Default:** Anthropic Provider-default FAST model
* **Alternative:** OpenAI GPT-4o-mini, Google Gemini 1.5 Flash
* **Rationale:** Fast, cheap, simple task
* **Cost:** ~$0.003 per article

**Stage 2: Claim Analysis** (CACHEABLE)

* **Default:** Anthropic Provider-default REASONING model
* **Alternative:** OpenAI GPT-4o, Google Gemini 1.5 Pro
* **Rationale:** High-quality analysis, cached 90 days
* **Cost:** ~$0.081 per NEW claim

**Stage 3: Holistic Assessment**

* **Default:** Anthropic Provider-default REASONING model
* **Alternative:** OpenAI GPT-4o, Provider-default HEAVY model (optional) (for high-stakes)
* **Rationale:** Complex reasoning, logical fallacy detection
* **Cost:** ~$0.030 per article

**Cost Comparison (Example):**

|=Stage|=Anthropic (Default)|=OpenAI Alternative|=Google Alternative
|Stage 1|Provider-default FAST model ($0.003)|GPT-4o-mini ($0.002)|Gemini Flash ($0.002)
|Stage 2|Provider-default REASONING model ($0.081)|GPT-4o ($0.045)|Gemini Pro ($0.050)
|Stage 3|Provider-default REASONING model ($0.030)|GPT-4o ($0.018)|Gemini Pro ($0.020)
|**Total (0% cache)**|**$0.114**|**$0.065**|**$0.072**

**Note:** POC1 uses Anthropic exclusively for consistency. Multi-provider support planned for POC2.

----

== 6.6 Failover Strategy ==

**Automatic Failover:**

{{{
async function completeLLM(stage: string, prompt: string): Promise<string> {
  const primaryProvider = getProviderForStage(stage)
  const fallbackProvider = getFallbackProvider()

  try {
    return await primaryProvider.complete(prompt)
  } catch (error) {
    if (error.type === 'rate_limit' || error.type === 'service_unavailable') {
      logger.warn(`Primary provider failed, using fallback`)
      return await fallbackProvider.complete(prompt)
    }
    throw error
  }
}
}}}

**Fallback Priority:**

1. **Primary:** Configured provider for stage
2. **Secondary:** Fallback provider (if configured)
3. **Cache:** Return cached result (if available for Stage 2)
4. **Error:** Return 503 Service Unavailable

----

== 6.7 Provider Selection API ==

**Admin Endpoint:** POST /admin/v1/llm/configure

**Update provider for specific stage:**

{{{{
{
  "stage": "stage2",
  "provider": "openai",
  "model": "gpt-4o",
  "max_tokens": 16384,
  "temperature": 0.3
}
}}}

**Response:** 200 OK

{{{{
{
  "message": "LLM configuration updated",
  "stage": "stage2",
  "previous": {
    "provider": "anthropic",
    "model": "claude-sonnet-4-5-20250929"
  },
  "current": {
    "provider": "openai",
    "model": "gpt-4o"
  },
  "cost_impact": {
    "previous_cost_per_claim": 0.081,
    "new_cost_per_claim": 0.045,
    "savings_percent": 44
  }
}
}}}

**Get current configuration:**

GET /admin/v1/llm/config

{{{{
{
  "providers": ["anthropic", "openai"],
  "primary": "anthropic",
  "fallback": "openai",
  "stages": {
    "stage1": {
      "provider": "anthropic",
      "model": "claude-haiku-4-5-20251001",
      "cost_per_request": 0.003
    },
    "stage2": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929",
      "cost_per_new_claim": 0.081
    },
    "stage3": {
      "provider": "anthropic",
      "model": "claude-sonnet-4-5-20250929",
      "cost_per_request": 0.030
    }
  }
}
}}}

----

== 6.8 Implementation Notes ==

**Provider Adapter Pattern:**

{{{
class AnthropicProvider implements LLMProvider {
  async complete(prompt: string, options: CompletionOptions) {
    const response = await anthropic.messages.create({
      model: options.model || 'claude-sonnet-4-5-20250929',
      max_tokens: options.maxTokens || 4096,
      messages: [{ role: 'user', content: prompt }],
      system: options.systemPrompt
    })
    return response.content[0].text
  }
}

class OpenAIProvider implements LLMProvider {
  async complete(prompt: string, options: CompletionOptions) {
    const response = await openai.chat.completions.create({
      model: options.model || 'gpt-4o',
      max_tokens: options.maxTokens || 4096,
      messages: [
        { role: 'system', content: options.systemPrompt },
        { role: 'user', content: prompt }
      ]
    })
    return response.choices[0].message.content
  }
}
}}}

**Provider Registry:**

{{{
const providers = new Map<string, LLMProvider>()
providers.set('anthropic', new AnthropicProvider())
providers.set('openai', new OpenAIProvider())
providers.set('google', new GoogleProvider())

function getProvider(name: string): LLMProvider {
  return providers.get(name) || providers.get(config.primaryProvider)
}
}}}

----

**Navigation:** [[API & Schemas>>FactHarbor.Specification.POC.API-and-Schemas.WebHome]] | Prev: [[Pipeline Architecture>>FactHarbor.Specification.POC.API-and-Schemas.Pipeline Architecture.WebHome]] | Next: [[REST API Contract>>FactHarbor.Specification.POC.API-and-Schemas.REST API Contract.WebHome]]
