= Media Source Database =

**Internal name**: SourceReliability Service
**Version**: 1.4
**Status**: Operational

----

== What It Is ==

The **Media Source Database** is FactHarbor's system for evaluating and tracking the reliability of media sources. Every time FactHarbor encounters a source during analysis, it assesses that source's credibility and factors the result into the verdict.

This is **not** a static list. Sources are evaluated on-demand by AI, cross-checked by a second model, and cached for 90 days. Any source on the internet can be evaluated — there is no pre-seeded database to go stale.

{{info}}
**Implementation Details:** See [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] for the technical implementation: LLM-powered scoring, batch prefetch architecture, cache strategy, and dual-model cross-checking.
{{/info}}

== Why It Exists ==

Evidence is only as strong as its source. A claim backed by Reuters and peer-reviewed research is stronger than one backed by a known disinformation site. The Media Source Database makes this transparent:

* **Users see** which sources were used and how reliable each one is
* **Verdicts reflect** source quality — unreliable sources pull verdicts toward neutral
* **No hidden assumptions** — every score is traceable to an LLM evaluation with cited evidence

== How It Works ==

=== 1. Evaluation ===

When a source is first encountered:

1. The system searches for independent assessments (fact-checker ratings, journalism reviews, track record evidence)
1. **Claude** evaluates the source using this evidence pack
1. **A second model** (OpenAI) cross-checks and refines the result
1. If both models agree (score difference ≤ 0.20) and confidence is high enough (≥ 0.80), the score is accepted
1. If models disagree or confidence is low, the source is marked as "unknown" (neutral)

=== 2. Caching ===

Accepted scores are cached for **90 days**. On cache expiry, the source is re-evaluated — capturing changes in source reliability over time (ownership changes, editorial shifts, corrections record).

=== 3. Verdict Impact ===

Source reliability scores act as **weights on evidence**. A verdict backed by highly reliable sources keeps its strength. A verdict backed by unreliable sources is pulled toward neutral (50%).

{{code}}
Example:
  Original verdict: 80% (Strong True)
  Average source score: 0.50 (Mixed reliability)
  Adjusted verdict: 65% (Leaning True)
{{/code}}

----

== The 7-Band Credibility Scale ==

|= Score |= Rating |= Meaning |= Verdict Effect
| 0.86 – 1.00 | **Highly Reliable** | Verified accuracy, rigorous corrections (e.g., wire services, standards bodies) | Verdict fully preserved
| 0.72 – 0.85 | **Reliable** | Consistent accuracy, professional editorial standards | Verdict mostly preserved
| 0.58 – 0.71 | **Leaning Reliable** | Often accurate, occasional errors, corrects when notified | Moderate preservation
| 0.43 – 0.57 | **Mixed** | Variable accuracy or inconsistent quality by topic/author | Neutral zone
| 0.29 – 0.42 | **Leaning Unreliable** | Often inaccurate or bias significantly affects reporting | Pulls toward neutral
| 0.15 – 0.28 | **Unreliable** | Pattern of false claims or ignores corrections | Strong pull toward neutral
| 0.00 – 0.14 | **Highly Unreliable** | Fabricates content or documented disinformation source | Maximum skepticism

**Unknown sources** (not yet evaluated or evaluation inconclusive) receive a default score of **0.50** (neutral center).

----

== What Users See ==

For every analysis, users can see:

* **Per-source credibility badge** — color-coded rating (Highly Reliable → Highly Unreliable)
* **Score and confidence** — the numeric reliability score and how confident the evaluation is
* **Bias indicator** — political spectrum or other bias classification (noted, not penalized unless it affects accuracy)
* **Impact on verdict** — how source reliability weighted the final verdict

----

== Design Principles ==

=== Evidence Over Authority ===

Source credibility is **supplementary**, not primary. A low-credibility source with documented evidence should still be considered. A high-credibility source making unsupported claims should be questioned. The evidence itself always matters more than who says it.

=== No Pre-seeded Data ===

All sources are evaluated identically by AI — no hardcoded scores, no external rating databases baked in. Every score is traceable to an LLM evaluation with cited evidence. This ensures full transparency and eliminates hidden bias.

=== Track Record Over Prestige ===

Domain type (.gov, .edu, .org) does **not** imply quality. Scores are derived from demonstrated track record: accuracy history, correction practices, editorial independence, and fact-checker assessments. Brand recognition alone does not inflate scores.

=== Recency Matters ===

The last 24 months matter most. Historical reputation does not excuse recent failures. Source reliability can change over time due to ownership changes, editorial shifts, or political transitions.

=== Skeptical Default ===

High reliability scores require stronger evidence than low ones. Absence of negative evidence does not equal reliability — unknown sources get neutral (0.50), not high scores.

----

== Source Type Score Caps (UCM Configurable) ==

Certain source types have **default score caps** defined in the SR prompt template. These are UCM configurable — a UCM Administrator can adjust the caps by editing the Source Reliability prompt template.

**Default caps:**

|= Source Type |= Default Cap |= Default Rating
| Propaganda outlet | 0.14 | Highly Unreliable
| Known disinformation | 0.14 | Highly Unreliable
| State-controlled media | 0.42 | Leaning Unreliable
| User-generated content platform | 0.42 | Leaning Unreliable

The caps are **prompt-driven** — the LLM is instructed to respect them during evaluation. The code validates compliance and adds a caveat if the LLM exceeds a cap (e.g., if it found mitigating evidence), but does not override the LLM output.

If evidence suggests a source has reformed, the correct action is to **reclassify the source type**, not exceed the cap.

----

== Multi-Language Support ==

The system detects the **publication language** of each source and performs dual-language searches:

* **English queries** — for international fact-checker coverage
* **Translated queries** — for regional fact-checkers in the source's language

This ensures that, for example, German-language sources are evaluated against assessments from CORRECTIV, Mimikama, and dpa-Faktencheck — not just English-language fact-checkers.

Supported: German, French, Spanish, Portuguese, Italian, Dutch, Polish, Russian, Swedish, Norwegian, Danish, Finnish, Czech, Hungarian, Turkish, Japanese, Chinese, Korean, Arabic.

----

== Configuration ==

The Media Source Database is configured via **UCM** (Admin → Config → Source Reliability). Key settings:

|= Setting |= Default |= Description
| Enabled | Yes | Enable/disable source evaluation
| Multi-model | Yes | Use two models for cross-checking
| Cache TTL | 90 days | How long to cache evaluations
| Confidence threshold | 0.80 | Minimum LLM confidence to accept a score
| Consensus threshold | 0.20 | Maximum score difference between models
| Default score | 0.50 | Score for unknown sources (neutral center)

See [[Source Reliability>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] for full configuration reference and implementation details.

----

== Future: Standalone Application ==

The Media Source Database is planned to become a **separate application and web service**, independent of the FactHarbor analysis pipeline.

=== Rationale ===

Source reliability evaluation is a general-purpose capability with value beyond FactHarbor's claim analysis:

* **Journalists** need quick source credibility checks without running a full claim analysis
* **Researchers** want to query the database directly via API
* **Other fact-checking tools** could integrate source reliability as a service
* **Decoupling** allows independent scaling, deployment, and release cycles

=== Architecture ===

{{code}}
Current (embedded):
  FactHarbor Analysis Pipeline → [SR Service (internal)]

Future (standalone):
  FactHarbor Analysis Pipeline ──→ Media Source Database API
  External consumers ─────────────→ Media Source Database API
  Browser / direct users ─────────→ Media Source Database Web UI
{{/code}}

=== Shared User Account System ===

The Media Source Database will share FactHarbor's user account system. Users log in once and have access to both FactHarbor and the Media Source Database.

**Roles carry over:**

|= Role |= FactHarbor |= Media Source Database
| **Reader** (Guest) | Browse and view analyses | Browse and search source evaluations
| **User** (Registered) | Submit URLs/text for analysis (rate-limited) | Look up source credibility (rate-limited), flag incorrect evaluations
| **UCM Administrator** | Manage pipeline configuration | Manage SR configuration (evaluation parameters, cache, scoring rules)
| **Moderator** | Handle abuse and community health | Handle abuse and community health

=== API (Future) ===

The standalone service will expose a public API:

|= Endpoint |= Method |= Description |= Auth
| ##/api/v1/sources/{domain}## | GET | Look up source credibility | Reader (guest)
| ##/api/v1/sources/{domain}/evaluate## | POST | Request fresh evaluation | User (rate-limited)
| ##/api/v1/sources## | GET | Search/list evaluated sources | Reader (guest)
| ##/api/v1/sources/{domain}/flag## | POST | Flag incorrect evaluation | User (authenticated)

=== Migration Path ===

1. **Current**: SR Service runs as embedded module within FactHarbor (SQLite cache, internal API)
1. **Next**: Extract SR Service into separate deployable package (shared database, separate process)
1. **Future**: Standalone web application with its own UI, public API, and shared authentication via FactHarbor's user account system

----

== Related Pages ==

* [[Source Reliability (Deep Dive)>>FactHarbor.Specification.Architecture.Deep Dive.Source Reliability.WebHome]] — Technical architecture, code integration, and API details
* [[Source Reliability Export Guide>>FactHarbor.User Guides.Source Reliability Export.WebHome]] — How to export evaluation data
* [[Quality Gates>>FactHarbor.Specification.Architecture.Deep Dive.Quality Gates.WebHome]] — How quality gates interact with source reliability
* [[Evidence and Verdict Workflow>>FactHarbor.Specification.Diagrams.Evidence and Verdict Workflow.WebHome]] — How evidence flows through the system
