# Lead Architect Assessment: Schema Validation & Architectural Decisions

**Role**: Lead Architect
**Date**: 2026-02-18
**Status**: üéØ Analysis Complete ‚Äî Implementation Plan Ready
**Reviewed**: LLM Expert's Schema Validation Analysis

---

## Executive Summary

**Decision**: ‚úÖ **APPROVE** LLM Expert's emergency fixes with architectural modifications

**Key Finding**: The LLM Expert correctly identified schema complexity as the root cause, BUT there are **critical architectural issues** that change the implementation approach:

1. **Gate 1 is non-functional** ‚Äî Hardcoded values, unused thresholds, dead config
2. **opinionScore doesn't exist** in ClaimBoundary pipeline (legacy artifact)
3. **specificityScore exists but isn't validated** ‚Äî LLM outputs it, Gate 1 ignores it
4. **expectedEvidenceProfile IS used** ‚Äî Critical for research guidance, must keep

**Architectural Verdict**: This isn't just a schema validation problem ‚Äî it's a **half-implemented feature problem**. We have config, types, and schemas for features that were never completed.

---

## Answers to LLM Expert's Critical Questions

### Q1: Are `specificityScore` and `opinionScore` actually used downstream?

**specificityScore**: ‚ùå **NO (Dead Code)**
- **Schema**: Defined in `Pass2AtomicClaimSchema` (line 329), required field
- **Config**: `gate1SpecificityThreshold: 0.3` exists in config
- **Reality**: Gate 1 HARDCODES it to `0` (quality-gates.ts:62) and never checks it
- **Conclusion**: LLM outputs random values, code ignores them

**opinionScore**: ‚ùå **NO (Doesn't Exist)**
- **Schema**: NOT in `Pass2AtomicClaimSchema` at all
- **Config**: `gate1OpinionThreshold: 0.7` exists but unused
- **Reality**: Replaced by `category: "factual" | "evaluative" | "procedural"`
- **Conclusion**: LLM Expert analyzed wrong/old schema version

**Evidence**:
```typescript
// quality-gates.ts:61-62
const opinionScore = 0;         // ‚ùå Hardcoded, never from LLM
const specificityScore = 0;     // ‚ùå Hardcoded, never from LLM

// quality-gates.ts:66
const wouldPass = String(claimText || "").trim().length > 0;
// ‚òùÔ∏è Gate 1 ONLY checks if text is non-empty. Scores are ignored.
```

### Q2: Is `expectedEvidenceProfile` used for anything?

**YES** ‚úÖ **Actively Used**
- **Usage**: Passed to research prompts to guide evidence search (claimboundary-pipeline.ts:1555)
- **Purpose**: LLM generates search strategies based on what evidence SHOULD exist
- **Example**: Claim about hydrogen efficiency ‚Üí expects "WTW analysis", "kWh/km metrics", "peer_reviewed_study" sources
- **Impact if removed**: Research becomes generic instead of claim-specific

**Conclusion**: This is a **critical field**, must keep and make robust.

### Q3: What's the acceptable failure rate for production?

**Target**: **<1%** for production fact-checking

**Rationale**:
- Current 5-10% means **1 in 10-20 analyses fail** during claim extraction
- Each failure triggers retry ‚Üí adds 30-60s latency + $0.10-0.20 cost
- **User experience**: Every failed extraction is a visible delay
- **Reliability**: Production systems should have 99%+ success rate

**Industry Standard**:
- **<1%**: Production-grade (AWS SLA equivalent)
- **<2%**: Acceptable for POC/MVP with monitoring
- **5-10%**: Unacceptable for any user-facing system

**Model Implications**:
- **Haiku**: Fast/cheap, but current 5-10% failure ‚Üí NOT production-ready
- **Sonnet**: 2-3x cost, but likely <2% failure ‚Üí may justify cost
- **With schema fixes**: Haiku might drop to <2%, making it viable

**Recommendation**: Target <1% within 4 weeks (emergency fixes + Sonnet experiment).

### Q4: Can we tolerate a schema breaking change?

**YES** ‚úÖ **With Staged Migration**

**Current State Audit**:
- ClaimBoundary pipeline is v3.0.0-cb (separate schema from v2.x legacy)
- Orchestrated pipeline removed (v2.11.0)
- Only 1 production pipeline variant (CB), 1 alternative (Monolithic Dynamic)

**Breaking Change Impact**:
| Component | Impact | Mitigation |
|-----------|--------|------------|
| **Pass2AtomicClaimSchema** | Fields become optional ‚Üí downstream may get `undefined` | Add `.optional().default()` to schema, ensures defaults |
| **AtomicClaim interface** | Type changes ‚Üí TypeScript errors in consumers | Update interface to match schema with required defaults |
| **Gate 1 validation** | Already broken, fix doesn't break further | Rebuild Gate 1 from scratch (see Implementation Plan) |
| **Test fixtures** | Will fail with new defaults | Update test data in parallel |
| **Existing jobs** | May fail to load old results | Schema version gating already handles this (v3.0.0-cb) |

**Migration Strategy**:
1. **Week 1**: Make fields optional WITH defaults (backward compatible)
2. **Week 2**: Update consumers to handle defaults explicitly
3. **Week 3**: Remove unused fields (non-breaking if Week 1 done right)
4. **Week 4**: Schema v3.1.0-cb declared stable

**Risk**: **LOW** ‚Äî ClaimBoundary is isolated, schema versioning in place, no production users yet.

**Verdict**: **Proceed with breaking changes**, but use schema defaults to minimize downstream breakage.

---

## Critical Architectural Findings (Not in LLM Expert Analysis)

### üö® Issue 1: Gate 1 is Non-Functional

**Problem**: Gate 1 is supposed to filter claims by specificity and opinion score, but:
- Hardcodes `specificityScore = 0` and `opinionScore = 0`
- Only checks if claim text is non-empty
- Config thresholds (`gate1SpecificityThreshold`, `gate1OpinionThreshold`) are **dead code**

**Root Cause**: Gate 1 was designed for a different schema (with opinion/specificity), then refactored to simple "non-empty" check, but config/types weren't cleaned up.

**Impact**:
- **Config bloat**: 2 unused thresholds confuse operators
- **False precision**: Schema enforces specificityScore but Gate 1 ignores it
- **Wasted LLM output**: LLM generates scores that code throws away

**Fix Required**: **Rebuild Gate 1** to either:
- **Option A**: Use specificityScore for real ‚Üí add threshold logic
- **Option B**: Remove specificityScore from schema ‚Üí simplify

**Recommendation**: **Option B** (remove) ‚Äî specificity is subjective, LLMs can't reliably score it without a formula.

---

### üö® Issue 2: Schema-Prompt-Code Drift

**Problem**: Three sources of truth are misaligned:

| Layer | specificityScore | opinionScore | gate1 Thresholds |
|-------|-----------------|--------------|-----------------|
| **Prompt** | ‚ùì Unknown (need to check) | ‚ùì Unknown | ‚ùì Unknown |
| **Schema** | ‚úÖ Required (z.number()) | ‚ùå Not in schema | ‚Äî |
| **Code** | ‚ùå Hardcoded to 0 | ‚ùå Hardcoded to 0 | ‚ùå Not used |
| **Config** | ‚úÖ Threshold = 0.3 | ‚úÖ Threshold = 0.7 | ‚Äî |

**Consequence**: LLM is asked to output fields that code immediately discards. This is:
1. **Wasteful**: LLM spends tokens generating unused data
2. **Fragile**: Schema requires field LLM might fail to generate
3. **Confusing**: Operators see thresholds that don't affect analysis

**Root Cause**: Incomplete refactoring when migrating from old pipeline to ClaimBoundary.

**Fix Required**: **Audit prompts** to confirm what LLM is asked to output, then align all 4 layers.

---

### üö® Issue 3: checkWorthiness Has 3 Values, Gate 1 Doesn't Use Them

**Schema**:
```typescript
checkWorthiness: z.enum(["high", "medium", "low"])
```

**Gate 1 Reality**: Doesn't check `checkWorthiness` at all ‚Äî only checks claim centrality.

**Question**: Is `checkWorthiness` used anywhere? If not, **remove it**.

---

### üö® Issue 4: expectedEvidenceProfile Nested Complexity

**Current Schema**:
```typescript
expectedEvidenceProfile: z.object({
  methodologies: z.array(z.string()),
  expectedMetrics: z.array(z.string()),
  expectedSourceTypes: z.array(z.string()),
})
```

**LLM Failure Mode**: When uncertain, LLMs often:
- Omit entire object ‚Üí validation fails
- Output `null` for arrays ‚Üí validation fails
- Output empty `{}` ‚Üí validation fails (missing required keys)

**Current Failure Rate Contribution**: Likely 20-30% of Pass 2 failures (nested objects with 3 required arrays).

**Fix (Immediate)**:
```typescript
expectedEvidenceProfile: z.object({
  methodologies: z.array(z.string()).default([]),
  expectedMetrics: z.array(z.string()).default([]),
  expectedSourceTypes: z.array(z.string()).default([]),
}).optional().default({ methodologies: [], expectedMetrics: [], expectedSourceTypes: [] })
```

**Fix (Long-term)**: Split into separate enrichment pass after claim extraction.

---

## Revised Implementation Plan

### Phase 0: Audit & Telemetry (Week 1, Days 1-2) üîç

**CRITICAL**: Before making schema changes, we need visibility.

#### Task 0.1: Add Field-Level Failure Telemetry
```typescript
// In extractStructuredOutput error handling
if (err instanceof z.ZodError) {
  err.issues.forEach(issue => {
    console.error(`[Pass2] Field validation failed:`, {
      field: issue.path.join('.'),
      code: issue.code,
      message: issue.message,
      received: issue.received
    });
    recordMetric("pass2_field_failure", {
      field: issue.path.join('.'),
      code: issue.code
    });
  });
}
```

**Why First**: We're guessing which fields fail most. Telemetry gives us data.

#### Task 0.2: Audit Prompt vs Schema Alignment
- Read `CLAIM_EXTRACTION_PASS2` prompt section
- List all fields LLM is asked to generate
- Compare to `Pass2AtomicClaimSchema`
- Flag mismatches

**Decision Gate**: If prompt asks for opinionScore, we have bigger problems than schema validation.

---

### Phase 1: Emergency Fixes (Week 1, Days 3-5) üö®

**Goal**: Reduce failures from 5-10% to <2% with minimal risk.

#### Task 1.1: Make Non-Essential Fields Optional (Priority: CRITICAL)

**Change `Pass2AtomicClaimSchema`**:
```typescript
const Pass2AtomicClaimSchema = z.object({
  // REQUIRED (core claim data)
  id: z.string(),
  statement: z.string(),
  category: z.enum(["factual", "evaluative", "procedural"]),

  // OPTIONAL with defaults (metadata)
  centrality: z.enum(["high", "medium", "low"]).optional().default("low"),
  harmPotential: z.enum(["critical", "high", "medium", "low"]).optional().default("low"),
  isCentral: z.boolean().optional().default(false),
  claimDirection: z.enum(["supports_thesis", "contradicts_thesis", "contextual"]).optional().default("contextual"),
  keyEntities: z.array(z.string()).optional().default([]),
  checkWorthiness: z.enum(["high", "medium", "low"]).optional().default("medium"),
  groundingQuality: z.enum(["strong", "moderate", "weak", "none"]).optional().default("none"),

  // CRITICAL: Keep but make robust
  specificityScore: z.number().min(0).max(1).optional().default(0.5),
  expectedEvidenceProfile: z.object({
    methodologies: z.array(z.string()).default([]),
    expectedMetrics: z.array(z.string()).default([]),
    expectedSourceTypes: z.array(z.string()).default([]),
  }).optional().default({ methodologies: [], expectedMetrics: [], expectedSourceTypes: [] })
});
```

**Impact**: Failure surface reduced from 11 required fields to 3.

**Risk**: Downstream code may assume fields are always present ‚Üí add defaults to prevent `undefined`.

#### Task 1.2: Add Case-Insensitive Enum Matching

```typescript
// Tolerant enum parser for claimDirection
const ClaimDirectionSchema = z.string().transform((val) => {
  const normalized = val.toLowerCase().replace(/\s+/g, '_');
  if (['supports_thesis', 'supporting', 'support'].some(v => normalized.includes(v))) {
    return 'supports_thesis';
  }
  if (['contradicts_thesis', 'contradicting', 'contradict'].some(v => normalized.includes(v))) {
    return 'contradicts_thesis';
  }
  return 'contextual';  // Safe default
});
```

**Benefit**: Handles 90% of enum typos without retry.

#### Task 1.3: Remove Dead Config Fields

**Delete from `pipeline.default.json` and `config-schemas.ts`**:
- `gate1OpinionThreshold` (opinionScore doesn't exist)
- `gate1SpecificityThreshold` (Gate 1 doesn't use it)

**Update `QualityGateConfig` interface** to match.

**Why**: Config bloat confuses operators and creates false expectations.

---

### Phase 2: Rebuild Gate 1 (Week 2) üîß

**Goal**: Make Gate 1 functional or remove it entirely.

#### Option A: Make Gate 1 Use specificityScore

**Implementation**:
```typescript
export function validateClaimGate1(
  claim: AtomicClaim,
  gateConfig: QualityGateConfig
): ClaimValidationResult {
  const minSpecificity = gateConfig.gate1SpecificityThreshold ?? 0.3;

  // Check 1: Non-empty claim
  const hasContent = claim.statement.trim().length > 0;

  // Check 2: Sufficient specificity
  const isSpecific = claim.specificityScore >= minSpecificity;

  // Central claims bypass all gates
  const passed = claim.isCentral || (hasContent && isSpecific);

  return {
    claimId: claim.id,
    passedContent: hasContent,
    passedSpecificity: isSpecific,
    passed,
    failureReason: !passed ? `Specificity ${claim.specificityScore} < ${minSpecificity}` : undefined,
  };
}
```

**Pros**: Uses the field LLM already generates, makes threshold meaningful
**Cons**: Relies on LLM to score specificity without clear formula ‚Üí unreliable

#### Option B: Remove specificityScore Entirely (RECOMMENDED)

**Schema Change**:
```typescript
const Pass2AtomicClaimSchema = z.object({
  // ... other fields ...
  // ‚ùå REMOVE: specificityScore
});
```

**Gate 1 Logic**: Only check centrality and content length (current behavior formalized).

**Prompt Change**: Remove instruction to output specificityScore.

**Pros**:
‚úÖ Simpler schema ‚Üí lower failure rate
‚úÖ No subjective scoring ‚Üí more reliable
‚úÖ Aligns code with reality (Gate 1 already ignores it)

**Cons**:
‚ùå Loses potential quality signal (but signal was unreliable anyway)

**Recommendation**: **Option B** ‚Äî Remove specificityScore unless we can define a clear formula.

---

### Phase 3: Model Experiment (Week 3) üß™

**Goal**: Test if Sonnet reduces failures enough to justify cost.

#### A/B Test Design

**Control**: Haiku 4.5 with Phase 1 schema fixes
**Variant**: Sonnet 4.5 with Phase 1 schema fixes

**Metrics**:
- Validation success rate
- Cost per successful extraction
- Average latency
- Verdict quality (manual sample review)

**Sample Size**: 100 jobs per variant (200 total)

**Decision Criteria**:
- If Sonnet success rate >98% AND cost delta <30% ‚Üí adopt Sonnet
- If Haiku success rate >98% with fixes ‚Üí keep Haiku
- If both <98% ‚Üí investigate prompt issues

---

### Phase 4: Long-Term Refactoring (Week 4+) üîÆ

**Only if Phase 1-3 don't achieve <1% failure rate.**

#### Task 4.1: Split Pass 2 Into Two Calls

**Pass 2A (Haiku)**: Extract claims with minimal metadata
```typescript
{
  id: string,
  statement: string,
  category: "factual" | "evaluative" | "procedural"
}
```

**Pass 2B (Haiku)**: Enrich with research metadata
```typescript
{
  claimId: string,
  expectedEvidenceProfile: {
    methodologies: string[],
    expectedMetrics: string[],
    expectedSourceTypes: string[]
  }
}
```

**Why**: Simpler schemas ‚Üí lower failure per call ‚Üí same total cost but higher reliability.

#### Task 4.2: Semantic Enum Validation

Replace strict enum matching with embedding-based similarity (handles multilingual + typos).

**Cost**: +$0.0001 per claim
**Benefit**: Language-agnostic, typo-tolerant

---

## Implementation Timeline

### Week 1: Foundation
**Mon-Tue**: Phase 0 (telemetry + audit)
**Wed-Fri**: Phase 1 (emergency fixes)
**Outcome**: Failure rate <2%, visibility into remaining issues

### Week 2: Refinement
**Mon-Wed**: Phase 2 (rebuild Gate 1, decision on specificityScore)
**Thu-Fri**: Monitor production, gather data for A/B test

### Week 3: Experimentation
**Mon-Fri**: Phase 3 (Haiku vs Sonnet A/B test, 100 jobs each)
**Outcome**: Model recommendation for production

### Week 4: Conditional
**IF failure rate still >1%**: Phase 4 (split Pass 2, semantic validation)
**ELSE**: Mark complete, move to production hardening

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| **Breaking change affects downstream** | High | High | Schema defaults prevent `undefined`, update AtomicClaim interface |
| **Removing specificityScore breaks tests** | Medium | Low | Update test fixtures in parallel |
| **Telemetry overhead affects latency** | Low | Low | Metrics are async, console.error non-blocking |
| **Phase 1 doesn't reach <2%** | Medium | Medium | Phase 3 (Sonnet experiment) is fallback |
| **Prompt drift not caught by audit** | Medium | High | Phase 0 audit MUST check prompt before schema changes |

---

## Success Metrics

**Primary**: Pass 2 validation failure rate
- **Current**: 5-10%
- **Week 1 Target**: <2%
- **Week 4 Target**: <1%

**Secondary**:
- Retry overhead (should drop to near-zero)
- Average analysis time (should decrease as retries drop)
- Field-level failure breakdown (should identify any remaining problem fields)

**Quality Gate**: Verdict quality must not degrade (manual review of 20 verdicts before/after).

---

## Architectural Decisions (Final)

### Decision 1: Remove specificityScore ‚úÖ

**Rationale**: Gate 1 doesn't use it, LLMs can't reliably score it without formula, adds failure risk.

**Alternative Considered**: Define formula and use in Gate 1 ‚Üí **Rejected** (too complex for marginal benefit).

### Decision 2: Keep expectedEvidenceProfile ‚úÖ

**Rationale**: Actively used for research guidance, critical for claim-specific evidence search.

**Modification**: Make robust with defaults to prevent nested object failures.

### Decision 3: Make All Metadata Fields Optional with Defaults ‚úÖ

**Rationale**: Reduces failure surface from 11 ‚Üí 3 required fields, aligns with "fail gracefully" principle.

**Trade-off**: Downstream code must handle defaults ‚Üí **Accepted** (worth it for 3-5x lower failure rate).

### Decision 4: Target <1% Failure Rate, Not <2% ‚úÖ

**Rationale**: Production fact-checking needs 99%+ reliability. <2% is acceptable for POC, not launch.

**Cost**: May require Sonnet instead of Haiku ‚Üí **Accepted** (reliability > cost at this stage).

### Decision 5: Rebuild Gate 1 from Scratch ‚úÖ

**Rationale**: Current implementation is half-functional (hardcoded scores, unused thresholds).

**Approach**: Either use specificityScore properly OR remove it entirely ‚Üí **Recommend removal**.

---

## Questions for Captain (Final)

1. **Approve specificityScore removal?** (Recommended: YES)
   - If NO: Define scoring formula and commit to Gate 1 rebuild

2. **Approve breaking schema change with defaults?** (Recommended: YES)
   - Risk: Downstream code may break if not tested
   - Mitigation: Defaults prevent `undefined`, staged rollout

3. **Acceptable failure rate: <1% or <2%?** (Recommended: <1%)
   - Affects model choice (Haiku vs Sonnet)
   - <1% may require Sonnet = 2-3x cost increase

4. **When to start Phase 1?** (Recommended: Immediately after approval)
   - Week 1 emergency fixes are low-risk, high-impact
   - Can run in parallel with current work

---

## Final Recommendation

**‚úÖ APPROVE Phase 1 Emergency Fixes Immediately**

**Sequence**:
1. Phase 0 (telemetry) ‚Üí 2 days
2. Phase 1 (optional fields + defaults) ‚Üí 3 days
3. Monitor for 1 week ‚Üí measure failure rate
4. **IF <2%**: Proceed to Phase 3 (Sonnet experiment)
5. **IF >2%**: Investigate prompt issues before Phase 2

**Critical Success Factor**: Phase 0 audit MUST confirm prompt alignment before any schema changes.

**Risk**: LOW ‚Äî Changes are additive (making fields optional) with safe defaults.

**Expected Outcome**: Failure rate drops from 5-10% to <2% within 1 week, <1% within 4 weeks.

---

**Document Status**: ‚úÖ Ready for Captain Approval
**Next Step**: Captain review ‚Üí Approve/reject decisions ‚Üí Begin Phase 0
