<<<<<<< HEAD
# Meeting Prep: Elliott Ash — LLM Political Bias & FactHarbor

**For:** Meeting with Elliott Ash (ETH Zurich)
**Date:** 2026-02-19
**Starting point:** [Aligning LLMs with Diverse Political Viewpoints](https://aclanthology.org/2024.emnlp-main.412/) (Stammbach, Widmer, Cho, Gulcehre, Ash — EMNLP 2024)
**Reviewed by:** Claude Opus 4.6, Claude Sonnet 4.6, GPT-5.3 Codex (all merged below)

---

## 1. The Paper in Brief

LLMs have measurable political bias. ChatGPT aligns with the Swiss Green Liberal Party at 58% overlap. When asked to represent diverse viewpoints, it produces near-identical responses (Jaccard similarity 0.48).

**Solution:** Fine-tune Llama 3 8B on 100K real comments from Swiss parliamentary candidates ([smartvote.ch](https://smartvote.ch)) using ORPO (Odds Ratio Preference Optimization) — a monolithic preference method that pushes apart responses with different political metadata in a single training pass.

**Results:**

| Metric | ChatGPT zero-shot | ChatGPT few-shot | **Llama 3 ORPO** |
|--------|-------------------|-----------------|------------------|
| Diversity (Jaccard, lower=better) | 0.48 | 0.34 | **0.24** |
| Accuracy (MAUVE, higher=better) | 0.24 | 0.49 | **0.64** |
| Human preference | baseline | — | **~60% preferred** |

**Key insight:** ORPO pushes apart responses that use similar language but express different political stances. The "balanced overviews" pattern (generate per-perspective stances → synthesize) eliminates false consensus.

### Applicability caveat

The paper measures **stance generation** quality, not **claim verification** accuracy. Evidence grounding fundamentally changes the bias dynamics. The findings apply to FactHarbor primarily for evaluatively ambiguous claims where evidence is contested — not for factually clear claims. *(All three reviewers agree)*

### Paper limitations (methodology critiques, NOT FactHarbor weaknesses)

1. Only Llama 3 8B tested — larger models may not need alignment
2. Swiss-specific — unusually favorable context (proportional representation, 85% candidate participation)
3. Weak human eval — 2 annotators, Cohen's kappa 0.55 (moderate), 0.84 excludes ties
4. Jaccard is a poor diversity metric — word overlap, not semantic similarity. MAUVE results are more robust
5. Circular training/evaluation — ORPO trained on same comments used as MAUVE reference
6. No hallucination analysis, no temporal-drift control, no adversarial robustness testing

---

## 2. Ash's Research Portfolio — What to Know

### Tier 1: Directly relevant to FactHarbor

**Climinator** (npj Climate Action 2025) — [Link](https://www.nature.com/articles/s44168-025-00215-8)
Automated climate claim fact-checking. **Mediator-Advocate framework** with structurally independent advocates (RAG on IPCC corpus vs. general GPT-4o). >96% accuracy. Adding adversarial NIPCC advocate increased debate rounds from 1 to 18 on contested claims — correctly detecting genuine controversy.
*Critical comparison:* Climinator uses different models with different corpora. FactHarbor uses the same Sonnet model for all debate roles.

**AFaCTA** (ACL 2024) — [Link](https://aclanthology.org/2024.acl-long.104/)
LLM-assisted factual claim detection with PoliClaim dataset. Uses **3 predefined reasoning paths** for consistency calibration — structurally different from FactHarbor's temperature-based self-consistency. Path-based consistency could detect bias that temperature variation cannot.

**Knowledge Base Choice** (JDIQ 2023) — [Link](https://dl.acm.org/doi/10.1145/3561389)
No universally best knowledge base. Domain overlap drives accuracy. Combining multiple KBs offers minimal benefit over the single best match. Pipeline confidence predicts KB effectiveness without ground-truth labels. Advocates "data-centric claim checking."
*Most actionable for FactHarbor:* Web search = choosing a KB at runtime. Search strategy should be claim-domain-aware.

**BallotBot** (Working paper 2025) — [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5168217)
Randomized experiment (California 2024 election). AI chatbot with official voter guide info. Key findings: improved in-depth answers, **reduced overconfidence**, lowered info costs for less-informed voters, **no effect on voting direction**. Validates that balanced AI information informs without steering.

### Tier 2: Relevant to specific concerns

| Paper | Venue | Key finding for FactHarbor |
|-------|-------|---------------------------|
| **Media Slant is Contagious** | Economic Journal (cond. accepted) | Media slant propagates through **framing, not topic selection**. 24M articles show "diverse" sources may share inherited bias. Validates C13. |
| **In-Group Bias in Indian Judiciary** | REStat 2025 | ML bias detection on 5M cases. Found tight zero bias. Methodology (quasi-random assignment → measure directional skew) relevant for C10 calibration design. |
| **Conservative News Media & Criminal Justice** | EJ 2024 | Fox News exposure measurably increases incarceration. Media bias has real-world consequences. |

### Tier 3: Background

**Variational Best-of-N** (ICLR 2025) — alignment inference optimization. **Emotion and Reason in Political Language** (EJ 2022) — text-based emotion-rationality scale on 6M speeches. **Apertus** (2025) — open multilingual LLM. **LePaRD** (ACL 2024) — judicial citation dataset.

### Dominik Stammbach (lead author)

Postdoc at Princeton CITP. PhD from ETH (Spring 2024) on data-centric fact-checking. Current focus: legal NLP, misinformation detection, AI for access to justice.

---

## 3. FactHarbor Position: Strengths, Weaknesses, Opportunities

### Architectural Strengths (genuine differentiators)

1. **Evidence-first pipeline.** Verdicts must cite fetched evidence with structural ID validation. Eliminates the paper's core finding (zero-shot fabrication).
2. **Multi-perspective verdicts.** `BoundaryFindings[]` and `TriangulationScore` surface disagreement structurally, not as post-hoc synthesis. *(Codex caveat: this is methodological plurality, not necessarily ideological plurality.)*
3. **Mandatory contradiction search.** Pipeline explicitly searches for opposing evidence — architecturally enforced.
4. **Rich metadata conditioning.** EvidenceScope, SourceType, claimDirection, sourceAuthority, evidenceBasis — richer than the paper's party + language + issue template. *(Codex caveat: metadata exists but isn't guaranteed to be decisive in adjudication.)*
5. **Input neutrality.** "Was X fair?" = "X was fair" (≤4% tolerance), with test suite.

**Honest assessment:** These are real strengths versus zero-shot LLMs. But "good process architecture" is not the same as "demonstrated bias mitigation outcomes" *(Codex's core critique)*. Without outcome-level measurement, claiming "mitigated" is premature.

### Five highest-priority gaps (from 19 concerns assessed by three reviewers)

1. **C10: No empirical bias measurement** — Critical. Highest priority, directly actionable.
2. **C9: Self-consistency rewards stable bias** — High. Illusory control providing false assurance.
3. **C8: Advisory-only validation** — High. Detection without correction on high-harm claims.
4. **C13: Evidence pool bias** — High. Bias injection before any LLM reasoning.
5. **C17/C18: Prompt injection + refusal asymmetry** — High. Novel attack vectors that amplify political bias.

---

## 4. Meeting Questions (prioritized)

### Must-ask (pick 3-4 for a single meeting)

1. **Residual bias after architectural mitigation.** "Given our evidence-first architecture with contradiction search and debate, how much residual political bias do you estimate remains? Is architecture sufficient, or is model-level intervention unavoidable?"

2. **Single-model vs. multi-model debate.** "Climinator uses structurally different advocates. We use the same Sonnet for all roles. Did you see qualitatively different outcomes with structural independence? Is 'performative adversarialism' a real concern?"

3. **Minimum viable bias measurement.** "What's the smallest benchmark design that distinguishes model prior bias from evidence-pool bias? What does a good political-skew calibration harness look like?"

4. **Evidence pool bias diagnostics.** "Your KB Choice paper shows domain overlap drives accuracy. Our web search chooses a KB at runtime. How should we detect when the evidence pool is poorly matched or politically skewed?"

### If time allows

5. **AFaCTA path-consistency vs. temperature-consistency.** Which produces more stable calibration on contested claims?
6. **Search bias compounding.** Does search engine ranking bias compound with or cancel LLM reasoning bias?
7. **NIPCC stress test analog.** Could we use a deliberately skeptical advocate to test whether our debate genuinely surfaces controversy?
8. **Refusal asymmetry.** How should a fact-checking system handle topic-dependent model refusals without introducing directional bias?

---

## 5. Actionable Recommendations (priority order)

**Principle: Measure before redesign.** *(Codex's key strategic insight — build baseline metrics first, then tie every architectural change to measured improvement.)*

| Priority | Action | Effort | Impact |
|----------|--------|--------|--------|
| **1** | **Political bias calibration harness** — 20-30 balanced claim pairs (mirrored framings, multilingual variants), measure verdict skew direction/magnitude | Low (~1 day, ~$5-10) | **Critical** — foundational |
| **2** | **Instrument failure modes** — track refusal/degradation rates by topic, provider, stage. Detect C18 (refusal asymmetry) | Low | High — reveals invisible bias |
| **3** | **Make validation blocking for high-harm claims** — `validateVerdicts()` returns verdicts unchanged; for `harmPotential >= "high"`, clamp confidence or force UNVERIFIED | Medium | High — closes C8 |
| **4** | **Separate the challenger model** — different provider for VERDICT_CHALLENGER (e.g., GPT-4o if advocate is Sonnet) | Medium | High — closes C1/C16 |
| **5** | **Evidence pool balance diagnostics** — detect and report when evidence pool is politically one-sided | Medium | Medium-High — closes C13 |
| **6** | **Add "politically contested" warning + range reporting** — show plausible verdict range, not just point estimate, on contested claims | High (long-term) | High — epistemic honesty |

---

## 6. References

### The Paper
- Stammbach et al. (2024). Aligning LLMs with Diverse Political Viewpoints. EMNLP 2024. [ACL Anthology](https://aclanthology.org/2024.emnlp-main.412/) | [arXiv](https://arxiv.org/abs/2406.14155) | [Code](https://github.com/dominiksinsaarland/swiss_alignment)

### Ash Group — Fact-Checking & Claims
- Climinator — [Link](https://www.nature.com/articles/s44168-025-00215-8) | AFaCTA — [Link](https://aclanthology.org/2024.acl-long.104/) | KB Choice — [ACM](https://dl.acm.org/doi/10.1145/3561389)

### Ash Group — Political Bias & Media
- BallotBot — [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5168217) | Media Slant — [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3712218) | Emotion & Reason — [Link](https://academic.oup.com/ej/article/132/643/1037/6490125)

### Ash Group — Bias Detection & Alignment
- Indian Judiciary Bias — [Link](https://direct.mit.edu/rest/article-abstract/doi/10.1162/rest_a_01569/128265) | vBoN — [arXiv](https://arxiv.org/abs/2407.06057) | Apertus (2025) | LePaRD (ACL 2024)

### People
- Elliott Ash: [elliottash.com](https://elliottash.com/) | [ETH Zurich](https://lawecon.ethz.ch/group/professors/ash.html)
- Dominik Stammbach: [Princeton CITP](https://citp.princeton.edu/people/dominik-stammbach/) | [Personal site](https://dominik-stammbach.github.io/)

=======
# Paper Review: Aligning Large Language Models with Diverse Political Viewpoints

**Status:** Reference document for meeting with Elliott Ash
**Date:** 2026-02-19
**Prepared by:** LLM Expert (Claude Code, Opus 4.6)
**Source:** [ACL Anthology](https://aclanthology.org/2024.emnlp-main.412/) | [arXiv](https://arxiv.org/abs/2406.14155)

---

## Paper Identity

| Field | Value |
|-------|-------|
| **Title** | Aligning Large Language Models with Diverse Political Viewpoints |
| **Authors** | Dominik Stammbach, Philine Widmer, Eunjung Cho, Caglar Gulcehre, Elliott Ash |
| **Venue** | EMNLP 2024 (Main, short paper), Miami, Florida |
| **Pages** | 7257-7267 |
| **Code/Data** | [github.com/dominiksinsaarland/swiss_alignment](https://github.com/dominiksinsaarland/swiss_alignment) |

---

## 1. Executive Summary (What You Need to Know)

LLMs like ChatGPT exhibit measurable political bias (confirmed as progressive/liberal/pro-environment, aligning with Swiss Green Liberal Party). This paper fine-tunes Llama 3 8B on 100,000 real comments from Swiss parliamentary candidates using **conditional generation + ORPO** (Odds Ratio Preference Optimization). The resulting model generates party-specific political stances that are **more diverse, more accurate, and preferred by human evaluators** compared to ChatGPT. They also propose a simple pipeline to generate **balanced multi-perspective overviews** from the aligned model.

### Why This Matters for FactHarbor

This paper is directly relevant to FactHarbor's core challenges:

1. **LLM bias is real and measurable.** ChatGPT's political alignment was empirically verified (58% overlap with Swiss Green Liberal Party). Any fact-checking system using LLMs inherits this bias unless actively mitigated.
2. **Preference optimization (ORPO/DPO) can separate subtle viewpoints.** The technique pushes apart responses that differ only in nuance — exactly what FactHarbor needs for distinguishing claim directions and evidence interpretations.
3. **"Balanced overviews" as a design pattern.** Their two-step approach (generate per-perspective stances, then synthesize) parallels FactHarbor's multi-evidence verdict aggregation.
4. **Conditional generation on metadata.** Their prompt template conditions on party + language + issue — analogous to conditioning on EvidenceScope + SourceType + claim direction.
5. **Evidence that zero-shot LLMs produce false consensus.** ChatGPT generates near-identical responses for all parties, creating an illusion of agreement. This directly threatens fact-checking systems that rely on zero-shot LLM reasoning about contested topics.

---

## 2. Problem Statement

LLMs have become ubiquitous decision aids, and research confirms that chatbot interactions can shift user views and behaviors. In the political domain this could influence elections — "one of the most important decision-making processes in democracies." Yet:

- All first-generation LLMs exhibit identifiable political biases (Rozado 2023; Hartmann et al. 2023; Motoki et al. 2024; Rutinowski et al. 2024)
- ChatGPT specifically shows progressive, liberal, and pro-environmental biases
- When asked to represent diverse viewpoints, ChatGPT produces near-identical responses across parties (Jaccard similarity ~0.48)

---

## 3. Dataset: Smartvote

| Property | Value |
|----------|-------|
| **Source** | [smartvote.ch](https://smartvote.ch) — Swiss voting advice application |
| **Size** | ~100,000 candidate comments |
| **Questions** | ~200 distinct political issues |
| **Election cycles** | 2015, 2019, 2023 |
| **Languages** | German (75.5%), French (22.2%), Italian (2.2%) |
| **Adoption** | 85% of Swiss candidates have a profile; 1 in 5 voters consult it |
| **Metadata** | 33 attributes: party, language, canton, age, profession, stance |
| **Splits** | Train: 92,986 / Dev: 4,262 (7 unseen 2023 issues) / Test: 5,488 (7 unseen 2023 issues) |

Key property: Candidates write free-text **comments explaining their positions**, not just yes/no answers. This gives genuine human-authored political reasoning tied to party identity.

---

## 4. Methodology

### 4.1 Conditional Generation

The prompt template conditions generation on structured metadata:

> "You are a helpful Swiss policy advisor. You are in political party **P**, and you reply in **L**" — responding to question **Q**.

This is conceptually simple but powerful: the model learns that the same question Q should produce different responses depending on party P.

### 4.2 ORPO (Odds Ratio Preference Optimization)

The core alignment technique. ORPO is a **monolithic** preference optimization method that combines SFT and preference learning in a single loss:

```
L_ORPO = E[(x, y_w, y_l)] [L_SFT + lambda * L_OR]
```

Where:
- `y_w` = preferred response (comment from party P on issue Q in language L)
- `y_l` = rejected response (comment on **same** issue in **same** language but from a **different** party)
- The odds ratio loss increases preferred choice likelihood while decreasing rejected choice likelihood

**Key insight:** ORPO "pushes apart comments with different metadata, although they might only differ in subtle nuances." This is the critical capability — distinguishing positions that use similar language but express different political stances.

### 4.3 Implementation

| Detail | Value |
|--------|-------|
| Base model | Llama 3 8B (4-bit quantized, unsloth) |
| Fine-tuning | LoRA (Low-Rank Adaptation) |
| Framework | Hugging Face TRL library |
| Alternatives tried | DPO and RLHF — "unsatisfactory outputs" without hyperparameter tuning |

### 4.4 Balanced Overviews Procedure

A simple two-step pipeline:

1. **Generate:** For a given policy question, generate position statements for each political party using the aligned model
2. **Synthesize:** Feed all party positions to GPT-4o to create a balanced summary

**Result comparison (question: "Should the state promote equal educational opportunities?"):**
- **ChatGPT zero-shot:** False consensus — suggests all parties support tuition vouchers for low-income families
- **ORPO-aligned model:** Accurately captures real disagreements — SP emphasizes equality, Die Mitte inclusive funding, FDP cantonal responsibility, SVP rejects focus on low-achievers

---

## 5. Results

### 5.1 Diversity (Jaccard Similarity — lower = more diverse)

| Model | Avg Similarity |
|-------|---------------|
| ChatGPT zero-shot | 0.48 |
| ChatGPT few-shot | 0.34 |
| Llama 3 SFT | 0.33 |
| **Llama 3 ORPO** | **0.24** |

ORPO achieves **50% reduction** in cross-party similarity vs. ChatGPT zero-shot.

### 5.2 Accuracy (MAUVE Score — higher = closer to human references)

| Model | Dev | Test | Combined |
|-------|-----|------|----------|
| ChatGPT zero-shot | 0.36 | 0.25 | 0.24 |
| Llama 3 zero-shot | 0.27 | 0.03 | 0.08 |
| GPT-4o zero-shot | 0.22 | 0.25 | 0.16 |
| ChatGPT few-shot | 0.49 | 0.59 | 0.49 |
| Llama 3 SFT | 0.48 | 0.48 | 0.38 |
| **Llama 3 ORPO** | **0.63** | **0.71** | **0.64** |

ORPO-aligned model achieves **highest scores across all splits**. Even few-shot ChatGPT (0.49) falls far behind.

### 5.3 Human Evaluation

- Cohen's kappa: 0.55 overall, **0.84 excluding ties** (near-perfect agreement)
- ORPO preferred in **~60% of comparisons**
- When ORPO loses, it's primarily on nuance (44% win rate), not accuracy (60% win rate)
- Evaluators: Swiss political science graduate + Swiss city mayor for gold standard deliberation

---

## 6. Related Work & Elliott Ash's Research Context

### Papers Cited in This Work
- **Rozado 2023; Hartmann et al. 2023; Motoki et al. 2024; Rutinowski et al. 2024** — Political bias documentation in ChatGPT
- **Jiang et al. 2022 (CommunityLM)** — Aligned models with specific political leanings using dedicated corpora
- **Feng et al. 2024** — Concurrent work: fine-tunes ensembles of community-aligned models for balanced overview generation
- **Bakker et al. 2022** — Consensus statements generation
- **Zellers et al. 2019; Zhou et al. 2023** — Conditional text generation with metadata

### Elliott Ash's Related Work (for meeting context)

Elliott Ash is Associate Professor of Law, Economics, and Data Science at ETH Zurich. His research combines applied microeconometrics with NLP/ML/AI to understand law and politics.

| Paper | Venue | Relevance |
|-------|-------|-----------|
| **AFaCTA** (Ni, Shi, Stammbach, Sachan, Ash, Leippold) | ACL 2024 | LLM-assisted annotation for factual claim detection; created PoliClaim dataset. Uses calibrated confidence along 3 predefined reasoning paths. Directly relevant to claim extraction and factual verification. |
| **LePaRD** (Mahari, Stammbach, Ash, Pentland) | ACL 2024 | Large-scale judicial citation dataset. Shows Ash's interest in structured evidence retrieval from authoritative sources. |
| **Climinator** (Stammbach, Leippold, Ash et al.) | npj Climate Action 2025 | Automated fact-checking of climate claims using a **Mediator-Advocate framework** — multiple LLMs debate against authoritative sources (IPCC reports, peer-reviewed literature). Achieved >96% binary classification accuracy. Architecture is a multi-LLM debate pattern grounded in deliberation literature. |
| **Apertus** (Ash, Ni, Hoyle et al.) | 2025 | First large-scale open multilingual language model. Demonstrates investment in multilingual LLM capabilities. |

### Dominik Stammbach (lead author)

Now a postdoctoral researcher at Princeton CITP/Polaris Lab. PhD from ETH Zurich (Spring 2024) on **data-centric automated fact-checking** — investigating how to extract relevant evidence from long documents and exploring the roles of different knowledge bases in automated fact checking. His current focus: legal NLP, misinformation detection (climate change, greenwashing), and AI for access to justice.

---

## 7. Key Takeaways & Discussion Points

### What's Strong
1. **Clean experimental design.** Real political data from a well-adopted platform, not synthetic scenarios.
2. **ORPO over DPO/RLHF.** Monolithic loss is simpler and worked better out-of-the-box. One training pass instead of two.
3. **Balanced overviews are practical.** The generate-per-perspective-then-synthesize pattern is immediately applicable.
4. **Multilingual by nature.** Swiss data spans German/French/Italian — alignment works across languages.
5. **Replicable.** Code and data are public.

### Limitations & Open Questions
1. **Only Llama 3 8B tested.** No 70B, no Mistral/Mixtral results. Larger models might not need as much alignment.
2. **Swiss-specific.** Transferability to other countries' political landscapes is claimed but not demonstrated.
3. **Small human evaluation.** Primarily one annotator + deliberation with a mayor. Not a large-scale evaluation.
4. **DPO/RLHF dismissed quickly.** Authors didn't tune hyperparameters — these might work with more effort.
5. **Metadata alignment beyond party failed.** Attempts to condition on canton, age, gender (with Mistral + DPO) failed. Llama 3 + ORPO might succeed but untested.
6. **No analysis of hallucination rates** in aligned vs. unaligned models.
7. **Temporal decay.** Data spans 2015-2023; party positions evolve. No evaluation of temporal robustness.

### Questions to Explore with Elliott Ash

1. **AFaCTA's 3-reasoning-path calibration** for factual claim detection — how well does this transfer beyond political speech? Has it been tested on mixed-domain claims?
2. **Climinator's Mediator-Advocate framework** — what did they learn about debate convergence? How many rounds? When does it fail? How does the mediator decide?
3. **ORPO for evidence interpretation?** Could ORPO-style preference optimization help align models to distinguish between "evidence supports claim" vs. "evidence opposes claim" when the textual difference is subtle?
4. **Multilingual alignment:** Apertus and this paper both handle multilingual content. What are the practical gotchas for analysis across languages?
5. **Balanced overviews at scale:** Their generate-then-synthesize approach works for ~10 parties. How would it scale to dozens of evidence sources with varying reliability?
6. **Political bias in fact-checking:** If LLMs have measurable political bias, what safeguards do they recommend for fact-checking systems that must remain neutral?
7. **Data collection for alignment:** Smartvote gave them 100K labeled examples. For fact-checking, where would equivalent training data come from?

---

## 8. Three Strategic Positions on LLMs and Political Content

The paper proposes three approaches (worth discussing):

1. **Refuse entirely.** LLMs should decline to generate political opinions, prioritizing impartiality.
2. **Always produce balanced overviews.** Every political query gets a multi-perspective summary.
3. **Transparent alignment.** LLMs explicitly declare their political alignment and give users control to switch perspectives.

---

## 9. Technical Details for Reference

### ORPO Loss Function
```
L_ORPO = E[(x, y_w, y_l)] [L_SFT + lambda * L_OR]

Where:
- L_SFT = standard supervised fine-tuning loss on preferred responses
- L_OR = odds ratio loss that increases P(y_w) while decreasing P(y_l)
- lambda = weighting hyperparameter
```

### Prompt Template
```
System: You are a helpful Swiss policy advisor. You are in political party {P},
        and you reply in {L}.
User: {Q}
```

### Evaluation Metrics
- **Jaccard similarity:** Word overlap between party-specific generations (diversity measure)
- **MAUVE score:** Distributional gap between generated and human-reference text (quality measure, uses multilingual RoBERTa embeddings)
- **Human evaluation:** Pairwise preference with Cohen's kappa agreement

### ChatGPT Bias Verification Method
They completed the 2023 Swiss smartvote survey with ChatGPT (temperature=0) using a forced-choice system prompt. Result: 58% of highest-overlap candidates were from the Green Liberal Party (GLP/JGLP), confirming progressive bias in the Swiss context.

---

## 10. Key References

- Stammbach et al. (2024). Aligning Large Language Models with Diverse Political Viewpoints. EMNLP 2024. [Link](https://aclanthology.org/2024.emnlp-main.412/)
- Ni et al. (2024). AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators. ACL 2024. [Link](https://aclanthology.org/2024.acl-long.104/)
- Stammbach, Leippold et al. (2025). Automated fact-checking of climate claims with large language models. npj Climate Action. [Link](https://www.nature.com/articles/s44168-025-00215-8)
- Mahari, Stammbach, Ash, Pentland (2024). LePaRD: A Large-Scale Dataset of Judicial Citations to Precedent. ACL 2024.
- Elliott Ash's research profile: [elliottash.com](https://elliottash.com/) | [ETH Zurich](https://lawecon.ethz.ch/group/professors/ash.html)
- Dominik Stammbach: [Princeton CITP](https://citp.princeton.edu/people/dominik-stammbach/) | [Personal site](https://dominik-stammbach.github.io/)
- Code/Data: [github.com/dominiksinsaarland/swiss_alignment](https://github.com/dominiksinsaarland/swiss_alignment)
>>>>>>> 0d97c065bc41de618899fd72c4df0dadd4dcd8d8
