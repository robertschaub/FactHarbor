# Political Bias Mitigation â€” Stammbach/Ash Low-Hanging Fruits

**Date:** 2026-02-19
**Status:** ðŸ”§ Implementation Complete (4/5 code actions) â€” Pending Review
**Author:** Lead Architect (Claude Opus 4.6)
**Origin:** `Docs/Knowledge/Stammbach_Ash_LLM_Political_Alignment_EMNLP2024.md` â€” meeting prep for Elliott Ash (ETH Zurich)
**Guiding Principle:** "Measure before redesign" (Codex's strategic insight from the 3-model paper review)

---

## Summary

The Stammbach/Ash EMNLP 2024 paper analysis (3-model review: Opus + Sonnet + GPT-5.3 Codex) identified 19 concerns (C1â€“C19) and 6 prioritized recommendations. This document reports the implementation of the 4 lowest-effort code actions. One measurement action (calibration harness) remains.

**Test results:** 877 tests passing (was 853), build green. All changes are additive and non-breaking.

**Code changes:** 6 files, +616 lines (production + tests).

---

## Actions Implemented

### Action 1: Metrics Infrastructure Assessment

**Status:** Already wired â€” no code changes needed
**Addresses:** C17/C18 (refusal asymmetry detection), prerequisite for bias measurement

**Finding:** Current_Status.md listed this as "CRITICAL: Metrics Infrastructure Not Integrated" â€” but investigation revealed the CB pipeline already has phase-level metrics wired in:
- `initializeMetrics()`, `startPhase()`/`endPhase()`, `recordGate1Stats()`, `recordGate4Stats()`, `recordOutputQuality()`, `finalizeMetrics()` â€” all already called in `claimboundary-pipeline.ts`
- API endpoint (`MetricsController.cs`), DB entity (`AnalysisMetrics`), admin dashboard (`/admin/metrics`) â€” all exist

**Remaining gap:** Per-LLM-call recording. `callLLMWithMetrics()` wrapper is defined (line 98 of claimboundary-pipeline.ts) but not used by any of the 14 `generateText` call sites. This is low priority â€” phase-level timing is sufficient for bias detection; per-call metrics are a cost-optimization concern.

**Decision:** Status doc is stale. Phase-level metrics are functional. No code change needed.

---

### Action 3: High-Harm Confidence Floor (C8)

**Status:** Implemented + tested
**Addresses:** C8 (advisory-only validation) â€” high-harm claims with low confidence should not get definitive verdicts
**Tests added:** 9

#### Problem

`validateVerdicts()` in verdict-stage.ts is advisory-only â€” it logs warnings but returns verdicts unchanged. A high-harm claim (e.g., medical, legal, electoral) with low confidence (e.g., 35%) would still get a definitive verdict like "MOSTLY-TRUE" instead of "UNVERIFIED".

#### Solution

New function `enforceHarmConfidenceFloor()` inserted between validation (Step 5) and confidence classification (Gate 4). For claims with `harmPotential` of "critical" or "high", if `confidence < highHarmMinConfidence`, the verdict is downgraded to UNVERIFIED.

#### Design Decisions

1. **Standalone function, not modification to `validateVerdicts()`** â€” keeps validation advisory (per existing design) and adds enforcement as a separate, testable step
2. **Threshold in CalcConfig (UCM)** â€” tunable at runtime without redeployment (default: 50%)
3. **Any truth% affected** â€” unlike the truth-scale UNVERIFIED band (43â€“57% truth), this applies at any truth% when harm is high and confidence is low. A "MOSTLY-TRUE at 30% confidence on a high-harm claim" becomes UNVERIFIED.
4. **Threshold=0 disables** â€” escape hatch for testing or specific use cases

#### Files Changed

| File | Change |
|------|--------|
| `apps/web/src/lib/analyzer/verdict-stage.ts` | Added `enforceHarmConfidenceFloor()`, wired as Step 5b in `runVerdictStage()`, added `highHarmMinConfidence` to `VerdictStageConfig` |
| `apps/web/src/lib/config-schemas.ts` | Added `highHarmMinConfidence` to `CalcConfigSchema` (int, 0â€“100, optional, default 50) |
| `apps/web/src/lib/analyzer/claimboundary-pipeline.ts` | Wired `highHarmMinConfidence` through `buildVerdictStageConfig()` |
| `apps/web/test/unit/lib/analyzer/verdict-stage.test.ts` | 9 tests: high/critical harm + low confidence â†’ UNVERIFIED; medium/low harm unaffected; already-UNVERIFIED unchanged; threshold=0 disables; exact boundary; mixed verdicts |

---

### Action 4: Configurable Debate Model Tiers (C1/C16)

**Status:** Implemented + tested
**Addresses:** C1 (single-model debate), C16 (performative adversarialism) â€” Climinator paper shows structurally different advocates surface genuine controversy
**Tests added:** 5

#### Problem

`verdict-stage.ts` hardcoded `{ tier: "sonnet" }` for all 4 debate role LLM calls (advocate, self-consistency, challenger, reconciler). This means the same model argues for and against verdicts â€” potentially producing "performative adversarialism" where the challenger doesn't genuinely challenge.

#### Solution

New `debateModelTiers` config object with per-role tier selection. All 4 debate calls now use config-driven tiers. Validation (Step 5) intentionally remains `"haiku"` â€” it's a structural check, not a debate role.

#### Design Decisions

1. **Per-role, not per-step** â€” self-consistency uses a separate tier from advocate, even though it runs the same prompt, because the purpose differs
2. **Tier values limited to `"haiku" | "sonnet"`** â€” matches the existing `LLMCallFn` abstraction. The underlying `createProductionLLMCall` maps "sonnet" â†’ premium model (verdict task), "haiku" â†’ budget model (understand task). Full provider separation (e.g., GPT-4o as challenger vs. Sonnet advocate) requires extending `LLMCallFn` to support provider selection â€” flagged as **follow-up**.
3. **UCM-configurable via PipelineConfig** â€” `debateModelTiers` added to `PipelineConfigSchema` (all fields optional, defaults to sonnet)
4. **Default = no behavior change** â€” all roles default to "sonnet", preserving current behavior unless explicitly overridden
5. **Added config parameter to `adversarialChallenge()`** â€” previously didn't accept config; now takes `VerdictStageConfig` for tier access

#### Files Changed

| File | Change |
|------|--------|
| `apps/web/src/lib/analyzer/verdict-stage.ts` | `debateModelTiers` in `VerdictStageConfig` interface + defaults; 4 LLM calls use config-driven tiers; `adversarialChallenge()` now accepts config |
| `apps/web/src/lib/config-schemas.ts` | `debateModelTiers` in `PipelineConfigSchema` (optional object with 4 optional tier fields) |
| `apps/web/src/lib/analyzer/claimboundary-pipeline.ts` | `buildVerdictStageConfig()` wires `debateModelTiers` from UCM with sonnet defaults |
| `apps/web/test/unit/lib/analyzer/verdict-stage.test.ts` | 5 tests: each role passes correct tier to LLM call; default config assertion |

#### Follow-up Needed

- **Full provider separation:** To run GPT-4o as challenger against Sonnet advocate (the Climinator pattern), `LLMCallFn` needs a provider parameter, not just tier. This is a larger refactor â€” the current change is the necessary prerequisite (config infrastructure + call-site separation).

---

### Action 5: Evidence Pool Balance Metric (C13)

**Status:** Implemented + tested
**Addresses:** C13 (evidence pool bias) â€” bias injected at evidence gathering, before any LLM reasoning
**Tests added:** 10

#### Problem

If web search returns politically one-sided evidence (all supporting or all contradicting), every downstream verdict inherits that skew. The KB Choice paper (Ash, JDIQ 2023) shows domain overlap drives accuracy â€” a poorly matched evidence pool produces biased results regardless of pipeline quality. No detection existed.

#### Solution

New `assessEvidenceBalance()` function computes directional balance of the evidence pool using existing `claimDirection` metadata. Runs between Stage 2 (research) and Stage 3 (clustering). When evidence is heavily skewed, an `evidence_pool_imbalance` warning is emitted and balance metrics are included in the result JSON.

#### Design Decisions

1. **Computed from existing `claimDirection` field** â€” no new LLM calls needed. Every evidence item already has `"supports" | "contradicts" | "neutral"` from extraction.
2. **Balance ratio = supporting / (supporting + contradicting)** â€” 0.5 = perfectly balanced. Neutral items excluded from ratio (they don't indicate directional bias).
3. **Minimum 3 directional items required** â€” with <3 directional items, skew detection is meaningless (e.g., 1 supporting + 0 contradicting = 100% skew but statistically insignificant).
4. **Majority-ratio comparison** â€” uses `max(ratio, 1-ratio) > threshold` (strict) instead of `ratio >= threshold || ratio <= 1-threshold` to avoid JavaScript floating-point issues (1â€“0.8 â‰  0.2).
5. **Threshold in CalcConfig (UCM)** â€” `evidenceBalanceSkewThreshold` (0.5â€“1.0, default 0.8). Set to 1.0 to disable (strict `>` means `1.0 > 1.0` is false). Warning message reports majority-side percentage (e.g., "contradicting (80%)" not "contradicting (20%)").
6. **Warning, not blocking** â€” detection first, automatic rebalancing is a follow-up. Warning surfaces in `analysisWarnings` (visible in UI via FallbackReport).
7. **Metrics in result JSON** â€” `meta.evidenceBalance` includes supporting/contradicting/neutral counts, total, balanceRatio, and isSkewed flag for analysis/debugging.

#### Files Changed

| File | Change |
|------|--------|
| `apps/web/src/lib/analyzer/claimboundary-pipeline.ts` | Added `EvidenceBalanceMetrics` interface + `assessEvidenceBalance()` function; wired between Stage 2 and Stage 3; balance metrics in `resultJson.meta.evidenceBalance`; CalcConfig loaded at pipeline start |
| `apps/web/src/lib/analyzer/types.ts` | Added `"evidence_pool_imbalance"` to `AnalysisWarningType` union |
| `apps/web/src/lib/config-schemas.ts` | Added `evidenceBalanceSkewThreshold` to `CalcConfigSchema` (number, 0.5â€“1.0, optional, default 0.8) |
| `apps/web/test/unit/lib/analyzer/claimboundary-pipeline.test.ts` | 10 tests: balanced pool, skewed supporting, skewed contradicting, below threshold, neutral/undefined handling, all-neutral (NaN ratio), insufficient directional items, empty pool, custom threshold, threshold=1.0 |

---

## Action NOT Implemented (Deferred)

### Action 2: Political Bias Calibration Harness

**Status:** Pending â€” separate session recommended
**Addresses:** C10 (no empirical bias measurement) â€” the #1 recommendation, "foundational"
**Estimated effort:** ~1 day + $5â€“10 LLM API cost

This is the measurement counterpart to the code changes above. Without it, we can claim "good process architecture" but not "demonstrated bias mitigation outcomes."

**What it requires:**
1. 20â€“30 balanced claim pairs as test fixture (mirrored political framings, multilingual EN/DE/FR variants)
2. Test script that runs both framings through the CB pipeline
3. Measurement: verdict direction skew, confidence asymmetry, evidence pool balance per direction
4. Output: CSV/JSON for analysis

**Why deferred:** Fundamentally different from the code changes â€” it's a measurement tool requiring real LLM calls ($5â€“10), careful fixture design, and human review of results. Best tackled as a dedicated session with budget approval.

**Prerequisite:** All 4 implemented actions (especially Action 5 â€” evidence balance) feed into this harness. The harness will measure whether the architectural mitigations actually reduce bias.

---

## Action NOT In Scope

### Action 6: "Politically Contested" Warning + Range Reporting

**Status:** Not low-hanging â€” deferred to backlog
**Effort:** 8+ hours (new verdict output fields, UI changes, epistemic framing redesign)

---

## UCM Configuration Summary

New UCM-configurable parameters added:

| Parameter | Schema | Default | Location |
|-----------|--------|---------|----------|
| `highHarmMinConfidence` | CalcConfig | 50 | `CalcConfigSchema` |
| `evidenceBalanceSkewThreshold` | CalcConfig | 0.8 | `CalcConfigSchema` |
| `debateModelTiers.advocate` | PipelineConfig | "sonnet" | `PipelineConfigSchema` |
| `debateModelTiers.selfConsistency` | PipelineConfig | "sonnet" | `PipelineConfigSchema` |
| `debateModelTiers.challenger` | PipelineConfig | "sonnet" | `PipelineConfigSchema` |
| `debateModelTiers.reconciler` | PipelineConfig | "sonnet" | `PipelineConfigSchema` |

All defaults preserve existing behavior â€” no behavior change without explicit UCM configuration.

---

## Test Coverage

| Action | New Tests | Coverage |
|--------|-----------|----------|
| 3 â€” Harm confidence floor | 9 | All harm levels, threshold boundary, disable path, mixed verdicts |
| 4 â€” Debate model tiers | 5 | Each role's tier passthrough, default config |
| 5 â€” Evidence balance | 10 | Balanced/skewed/neutral/empty pools, custom thresholds, floating-point edge case |
| **Total** | **24** | **877 tests passing (was 853)** |

---

## Relationship to Existing Backlog

- **Action 1** addresses Known Issue #2 (metrics not integrated) â€” finding: stale status doc, phase-level metrics already wired
- **Action 2** addresses Backlog "Phase 5h: Test coverage" (neutrality/adversarial tests) â€” deferred
- **Actions 3â€“5** are new items from the Stammbach/Ash paper analysis, not previously in backlog

---

## Review Checklist

- [x] **Action 3 (harm floor):** `highHarmMinConfidence=50` â€” reasonable default; keep until calibration data exists.
- [x] **Action 4 (model tiers):** `"haiku" | "sonnet"` sufficient for now; provider-level separation is a separate `LLMCallFn` refactor.
- [x] **Action 5 (evidence balance):** 80% threshold good; keep warning-only (don't block high-harm yet).
- [x] **Action 5 (evidence balance):** Config-consistency risk â€” see Known Issue below.
- [x] **Action 2 (calibration harness):** Budget approved (~$5â€“10) with explicit cap and fixed fixture spec first.
- [x] **General:** Defaults sensible. Threshold=1.0 disable semantics fixed (see Post-Review Fixes).

---

## Post-Review Fixes (2026-02-19)

Two medium findings fixed after code review:

### Fix 1: Contradicting-side percentage display (Medium)

**Problem:** When evidence pool was skewed toward contradicting (balanceRatio=0.2), the warning reported "contradicting (20%)" â€” but 20% is the *supporting* ratio, not the contradicting majority.

**Fix:** Changed `pct` to `majorityPct = Math.round(Math.max(ratio, 1-ratio) * 100)`. Now correctly reports "contradicting (80%)".

### Fix 2: Threshold=1.0 disable semantics (Medium)

**Problem:** Doc said "Set to 1.0 to disable" but code used `>=` comparison, so `1.0 >= 1.0` was true â€” 100% one-sided pools were still flagged. Code, docs, and test were inconsistent.

**Fix:** Changed comparison from `majorityRatio >= skewThreshold` to `majorityRatio > skewThreshold` (strict). Now `1.0 > 1.0` is false â€” threshold=1.0 genuinely disables detection. Updated test to expect `isSkewed=false` at threshold=1.0.

**Consequence for default threshold:** With strict `>`, the default threshold 0.8 means pools with *exactly* 80% majority are NOT flagged; only pools with *more than* 80% are flagged. This is the correct semantic: 80% is the boundary, not the trigger.

---

## Known Issues

### Config Consistency Risk (Low â€” flagged for future fix)

CalcConfig is now loaded at pipeline start (line 190) for evidence balance, but is loaded again independently in Stage 4 (`generateVerdicts`, line 2888) and Stage 5 (`aggregateAssessment`, line 3077). If UCM config is changed mid-job, different stages may use different config snapshots.

**Impact:** Low in practice â€” jobs take seconds to minutes, and config changes are infrequent. But for deterministic behavior, all stages should use the same config snapshot.

**Recommended fix:** Thread a single per-job config snapshot through all stages. This is a broader pipeline refactor (affects all config loading, not just this change) â€” add to backlog as "single per-job config snapshot."

### Stale Status Documentation

`Current_Status.md` Known Issue #2 still says "Metrics Infrastructure Not Integrated." Investigation in Action 1 found this is stale â€” phase-level metrics are already wired. Should be updated to reflect actual state (gap is per-LLM-call recording only).

---

## Files Changed (Code Only)

```
apps/web/src/lib/analyzer/claimboundary-pipeline.ts  (+101)  â€” balance function, pipeline wiring, CalcConfig loading
apps/web/src/lib/analyzer/verdict-stage.ts           (+100)  â€” harm floor, debate tiers, config interface
apps/web/src/lib/analyzer/types.ts                   (+3)    â€” evidence_pool_imbalance warning type
apps/web/src/lib/config-schemas.ts                   (+29)   â€” 6 new UCM parameters
apps/web/test/unit/lib/analyzer/claimboundary-pipeline.test.ts  (+109)  â€” 10 balance tests
apps/web/test/unit/lib/analyzer/verdict-stage.test.ts           (+283)  â€” 14 harm floor + tier tests
```
